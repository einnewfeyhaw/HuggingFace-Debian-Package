Description: Fixing Ruff and quality
 TODO: Put a short summary on the line above and replace this paragraph
 with a longer explanation of this change. Complete the meta-information
 with other relevant fields (see below for details). To make it easier, the
 information below has been extracted from the changelog. Adjust it or drop
 it.
 .
 python3-huggingface-hub (0.29.3-1) UNRELEASED; urgency=medium
 .
   * Initial release. (Closes: #nnnn)  <nnnn is the bug number of your ITP>
Author: anudeep <anudeep@unknown>

---
The information above should follow the Patch Tagging Guidelines, please
checkout https://dep.debian.net/deps/dep3/ to learn about the format. Here
are templates for supplementary fields that you might want to add:

Origin: (upstream|backport|vendor|other), (<patch-url>|commit:<commit-id>)
Bug: <upstream-bugtracker-url>
Bug-Debian: https://bugs.debian.org/<bugnumber>
Bug-Ubuntu: https://launchpad.net/bugs/<bugnumber>
Forwarded: (no|not-needed|<patch-forwarded-url>)
Applied-Upstream: <version>, (<commit-url>|commit:<commid-id>)
Reviewed-By: <name and email of someone who approved/reviewed the patch>
Last-Update: 2025-03-24

--- python3-huggingface-hub-0.29.3.orig/Makefile
+++ python3-huggingface-hub-0.29.3/Makefile
@@ -7,7 +7,7 @@ check_dirs := contrib src tests utils se
 quality:
 	ruff check $(check_dirs)  # linter
 	ruff format --check $(check_dirs) # formatter
-	python utils/check_inference_input_params.py
+	PYTHONPATH=$(CURDIR)/src python utils/check_inference_input_params.py
 	python utils/check_contrib_list.py
 	python utils/check_static_imports.py
 	python utils/check_all_variable.py
--- python3-huggingface-hub-0.29.3.orig/contrib/sentence_transformers/test_sentence_transformers.py
+++ python3-huggingface-hub-0.29.3/contrib/sentence_transformers/test_sentence_transformers.py
@@ -26,9 +26,7 @@ def test_from_pretrained(multi_qa_model:
     print("Similarity:", util.dot_score(query_embedding, passage_embedding))
 
 
-def test_push_to_hub(
-    multi_qa_model: SentenceTransformer, repo_name: str, user: str, cleanup_repo: None
-) -> None:
+def test_push_to_hub(multi_qa_model: SentenceTransformer, repo_name: str, user: str, cleanup_repo: None) -> None:
     multi_qa_model.save_to_hub(repo_name, organization=user)
 
     # Sleep to ensure that model_info isn't called too soon
--- python3-huggingface-hub-0.29.3.orig/contrib/utils.py
+++ python3-huggingface-hub-0.29.3/contrib/utils.py
@@ -41,10 +41,7 @@ def production_endpoint() -> Generator:
 
     patchers = (
         [patch(target + ".ENDPOINT", PROD_ENDPOINT) for target in ENDPOINT_TARGETS]
-        + [
-            patch(target + ".HUGGINGFACE_CO_URL_TEMPLATE", PROD_URL_TEMPLATE)
-            for target in URL_TEMPLATE_TARGETS
-        ]
+        + [patch(target + ".HUGGINGFACE_CO_URL_TEMPLATE", PROD_URL_TEMPLATE) for target in URL_TEMPLATE_TARGETS]
         + [patch.object(api, "endpoint", PROD_URL_TEMPLATE)]
     )
 
--- python3-huggingface-hub-0.29.3.orig/setup.py
+++ python3-huggingface-hub-0.29.3/setup.py
@@ -116,9 +116,7 @@ setup(
     packages=find_packages("src"),
     extras_require=extras,
     entry_points={
-        "console_scripts": [
-            "huggingface-cli=huggingface_hub.commands.huggingface_cli:main"
-        ],
+        "console_scripts": ["huggingface-cli=huggingface_hub.commands.huggingface_cli:main"],
         "fsspec.specs": "hf=huggingface_hub.HfFileSystem",
     },
     python_requires=">=3.8.0",
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/__init__.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/__init__.py
@@ -947,9 +947,7 @@ def _attach(package_name, submodules=Non
     else:
         submodules = set(submodules)
 
-    attr_to_modules = {
-        attr: mod for mod, attrs in submod_attrs.items() for attr in attrs
-    }
+    attr_to_modules = {attr: mod for mod, attrs in submod_attrs.items() for attr in attrs}
 
     def __getattr__(name):
         if name in submodules:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_commit_api.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_commit_api.py
@@ -185,9 +185,7 @@ class CommitOperationAdd:
         if isinstance(self.path_or_fileobj, str):
             path_or_fileobj = os.path.normpath(os.path.expanduser(self.path_or_fileobj))
             if not os.path.isfile(path_or_fileobj):
-                raise ValueError(
-                    f"Provided path: '{path_or_fileobj}' is not a file on the local file system"
-                )
+                raise ValueError(f"Provided path: '{path_or_fileobj}' is not a file on the local file system")
         elif not isinstance(self.path_or_fileobj, (io.BufferedIOBase, bytes)):
             # ^^ Inspired from: https://stackoverflow.com/questions/44584829/how-to-determine-if-file-is-opened-in-binary-or-text-mode
             raise ValueError(
@@ -248,9 +246,7 @@ class CommitOperationAdd:
         config.json: 100%|█████████████████████████| 8.19k/8.19k [00:02<00:00, 3.72kB/s]
         ```
         """
-        if isinstance(self.path_or_fileobj, str) or isinstance(
-            self.path_or_fileobj, Path
-        ):
+        if isinstance(self.path_or_fileobj, str) or isinstance(self.path_or_fileobj, Path):
             if with_tqdm:
                 with tqdm_stream_file(self.path_or_fileobj) as file:
                     yield file
@@ -457,14 +453,10 @@ def _upload_lfs_files(
                 endpoint=endpoint,
             )
         except Exception as exc:
-            raise RuntimeError(
-                f"Error while uploading '{operation.path_in_repo}' to the Hub."
-            ) from exc
+            raise RuntimeError(f"Error while uploading '{operation.path_in_repo}' to the Hub.") from exc
 
     if constants.HF_HUB_ENABLE_HF_TRANSFER:
-        logger.debug(
-            f"Uploading {len(filtered_actions)} LFS files to the Hub using `hf_transfer`."
-        )
+        logger.debug(f"Uploading {len(filtered_actions)} LFS files to the Hub using `hf_transfer`.")
         for action in hf_tqdm(filtered_actions, name="huggingface_hub.lfs_upload"):
             _wrapped_lfs_upload(action)
     elif len(filtered_actions) == 1:
@@ -566,15 +558,9 @@ def _fetch_upload_modes(
         )
         hf_raise_for_status(resp)
         preupload_info = _validate_preupload_info(resp.json())
-        upload_modes.update(
-            **{file["path"]: file["uploadMode"] for file in preupload_info["files"]}
-        )
-        should_ignore_info.update(
-            **{file["path"]: file["shouldIgnore"] for file in preupload_info["files"]}
-        )
-        oid_info.update(
-            **{file["path"]: file.get("oid") for file in preupload_info["files"]}
-        )
+        upload_modes.update(**{file["path"]: file["uploadMode"] for file in preupload_info["files"]})
+        should_ignore_info.update(**{file["path"]: file["shouldIgnore"] for file in preupload_info["files"]})
+        oid_info.update(**{file["path"]: file.get("oid") for file in preupload_info["files"]})
 
     # Set upload mode for each addition operation
     for addition in additions:
@@ -686,9 +672,7 @@ def _fetch_files_to_copy(
                     f"Cannot copy {operation.src_path_in_repo} at revision "
                     f"{src_revision or revision}: file is missing on repo."
                 )
-            operation._src_oid = oid_info.get(
-                (operation.src_path_in_repo, operation.src_revision)
-            )
+            operation._src_oid = oid_info.get((operation.src_path_in_repo, operation.src_revision))
             operation._dest_oid = oid_info.get((operation.path_in_repo, revision))
     return files_to_copy
 
@@ -724,17 +708,12 @@ def _prepare_commit_payload(
     for operation in operations:
         # Skip ignored files
         if isinstance(operation, CommitOperationAdd) and operation._should_ignore:
-            logger.debug(
-                f"Skipping file '{operation.path_in_repo}' in commit (ignored by gitignore file)."
-            )
+            logger.debug(f"Skipping file '{operation.path_in_repo}' in commit (ignored by gitignore file).")
             nb_ignored_files += 1
             continue
 
         # 2.a. Case adding a regular file
-        if (
-            isinstance(operation, CommitOperationAdd)
-            and operation._upload_mode == "regular"
-        ):
+        if isinstance(operation, CommitOperationAdd) and operation._upload_mode == "regular":
             yield {
                 "key": "file",
                 "value": {
@@ -744,10 +723,7 @@ def _prepare_commit_payload(
                 },
             }
         # 2.b. Case adding an LFS file
-        elif (
-            isinstance(operation, CommitOperationAdd)
-            and operation._upload_mode == "lfs"
-        ):
+        elif isinstance(operation, CommitOperationAdd) and operation._upload_mode == "lfs":
             yield {
                 "key": "lfsFile",
                 "value": {
@@ -765,9 +741,7 @@ def _prepare_commit_payload(
             }
         # 2.d. Case copying a file or folder
         elif isinstance(operation, CommitOperationCopy):
-            file_to_copy = files_to_copy[
-                (operation.src_path_in_repo, operation.src_revision)
-            ]
+            file_to_copy = files_to_copy[(operation.src_path_in_repo, operation.src_revision)]
             if isinstance(file_to_copy, bytes):
                 yield {
                     "key": "file",
@@ -798,6 +772,4 @@ def _prepare_commit_payload(
             )
 
     if nb_ignored_files > 0:
-        logger.info(
-            f"Skipped {nb_ignored_files} file(s) in commit (ignored by gitignore file)."
-        )
+        logger.info(f"Skipped {nb_ignored_files} file(s) in commit (ignored by gitignore file).")
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_commit_scheduler.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_commit_scheduler.py
@@ -127,24 +127,18 @@ class CommitScheduler:
         self.ignore_patterns = ignore_patterns + DEFAULT_IGNORE_PATTERNS
 
         if self.folder_path.is_file():
-            raise ValueError(
-                f"'folder_path' must be a directory, not a file: '{self.folder_path}'."
-            )
+            raise ValueError(f"'folder_path' must be a directory, not a file: '{self.folder_path}'.")
         self.folder_path.mkdir(parents=True, exist_ok=True)
 
         # Repository
-        repo_url = self.api.create_repo(
-            repo_id=repo_id, private=private, repo_type=repo_type, exist_ok=True
-        )
+        repo_url = self.api.create_repo(repo_id=repo_id, private=private, repo_type=repo_type, exist_ok=True)
         self.repo_id = repo_url.repo_id
         self.repo_type = repo_type
         self.revision = revision
         self.token = token
 
         # Keep track of already uploaded files
-        self.last_uploaded: Dict[Path, float] = (
-            {}
-        )  # key is local path, value is timestamp
+        self.last_uploaded: Dict[Path, float] = {}  # key is local path, value is timestamp
 
         # Scheduler
         if not every > 0:
@@ -153,9 +147,7 @@ class CommitScheduler:
         self.every = every
         self.squash_history = squash_history
 
-        logger.info(
-            f"Scheduled job to push '{self.folder_path}' to '{self.repo_id}' every {self.every} minutes."
-        )
+        logger.info(f"Scheduled job to push '{self.folder_path}' to '{self.repo_id}' every {self.every} minutes.")
         self._scheduler_thread = Thread(target=self._run_scheduler, daemon=True)
         self._scheduler_thread.start()
         atexit.register(self._push_to_hub)
@@ -203,14 +195,10 @@ class CommitScheduler:
             value = self.push_to_hub()
             if self.squash_history:
                 logger.info("(Background) squashing repo history.")
-                self.api.super_squash_history(
-                    repo_id=self.repo_id, repo_type=self.repo_type, branch=self.revision
-                )
+                self.api.super_squash_history(repo_id=self.repo_id, repo_type=self.repo_type, branch=self.revision)
             return value
         except Exception as e:
-            logger.error(
-                f"Error while pushing to Hub: {e}"
-            )  # Depending on the setup, error might be silenced
+            logger.error(f"Error while pushing to Hub: {e}")  # Depending on the setup, error might be silenced
             raise
 
     def push_to_hub(self) -> Optional[CommitInfo]:
@@ -238,9 +226,7 @@ class CommitScheduler:
             # List files from folder (taken from `_prepare_upload_folder_additions`)
             relpath_to_abspath = {
                 path.relative_to(self.folder_path).as_posix(): path
-                for path in sorted(
-                    self.folder_path.glob("**/*")
-                )  # sorted to be deterministic
+                for path in sorted(self.folder_path.glob("**/*"))  # sorted to be deterministic
                 if path.is_file()
             }
             prefix = f"{self.path_in_repo.strip('/')}/" if self.path_in_repo else ""
@@ -254,10 +240,7 @@ class CommitScheduler:
             ):
                 local_path = relpath_to_abspath[relpath]
                 stat = local_path.stat()
-                if (
-                    self.last_uploaded.get(local_path) is None
-                    or self.last_uploaded[local_path] != stat.st_mtime
-                ):
+                if self.last_uploaded.get(local_path) is None or self.last_uploaded[local_path] != stat.st_mtime:
                     files_to_upload.append(
                         _FileToUpload(
                             local_path=local_path,
@@ -277,9 +260,7 @@ class CommitScheduler:
         add_operations = [
             CommitOperationAdd(
                 # Cap the file to its current size, even if the user append data to it while a scheduled commit is happening
-                path_or_fileobj=PartialFileIO(
-                    file_to_upload.local_path, size_limit=file_to_upload.size_limit
-                ),
+                path_or_fileobj=PartialFileIO(file_to_upload.local_path, size_limit=file_to_upload.size_limit),
                 path_in_repo=file_to_upload.path_in_repo,
             )
             for file_to_upload in files_to_upload
@@ -330,9 +311,7 @@ class PartialFileIO(BytesIO):
         return super().__del__()
 
     def __repr__(self) -> str:
-        return (
-            f"<PartialFileIO file_path={self._file_path} size_limit={self._size_limit}>"
-        )
+        return f"<PartialFileIO file_path={self._file_path} size_limit={self._size_limit}>"
 
     def __len__(self) -> int:
         return self._size_limit
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_inference_endpoints.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_inference_endpoints.py
@@ -181,9 +181,7 @@ class InferenceEndpoint:
             token=self._token,  # type: ignore[arg-type] # boolean token shouldn't be possible. In practice it's ok.
         )
 
-    def wait(
-        self, timeout: Optional[int] = None, refresh_every: int = 5
-    ) -> "InferenceEndpoint":
+    def wait(self, timeout: Optional[int] = None, refresh_every: int = 5) -> "InferenceEndpoint":
         """Wait for the Inference Endpoint to be deployed.
 
         Information from the server will be fetched every 1s. If the Inference Endpoint is not deployed after `timeout`
@@ -223,21 +221,15 @@ class InferenceEndpoint:
                 )
             if self.status == InferenceEndpointStatus.RUNNING and self.url is not None:
                 # Verify the endpoint is actually reachable
-                response = get_session().get(
-                    self.url, headers=self._api._build_hf_headers(token=self._token)
-                )
+                response = get_session().get(self.url, headers=self._api._build_hf_headers(token=self._token))
                 if response.status_code == 200:
                     logger.info("Inference Endpoint is ready to be used.")
                     return self
 
             if timeout is not None:
                 if time.time() - start > timeout:
-                    raise InferenceEndpointTimeoutError(
-                        "Timeout while waiting for Inference Endpoint to be deployed."
-                    )
-            logger.info(
-                f"Inference Endpoint is not deployed yet ({self.status}). Waiting {refresh_every}s..."
-            )
+                    raise InferenceEndpointTimeoutError("Timeout while waiting for Inference Endpoint to be deployed.")
+            logger.info(f"Inference Endpoint is not deployed yet ({self.status}). Waiting {refresh_every}s...")
             time.sleep(refresh_every)
             self.fetch()
 
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_local_folder.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_local_folder.py
@@ -86,10 +86,7 @@ class LocalDownloadFilePaths:
 
     def incomplete_path(self, etag: str) -> Path:
         """Return the path where a file will be temporarily downloaded before being moved to `file_path`."""
-        return (
-            self.metadata_path.parent
-            / f"{_short_hash(self.metadata_path.name)}.{etag}.incomplete"
-        )
+        return self.metadata_path.parent / f"{_short_hash(self.metadata_path.name)}.{etag}.incomplete"
 
 
 @dataclass(frozen=True)
@@ -207,27 +204,20 @@ def get_local_download_paths(local_dir:
                 " owner to rename this file."
             )
     file_path = local_dir / sanitized_filename
-    metadata_path = (
-        _huggingface_dir(local_dir) / "download" / f"{sanitized_filename}.metadata"
-    )
+    metadata_path = _huggingface_dir(local_dir) / "download" / f"{sanitized_filename}.metadata"
     lock_path = metadata_path.with_suffix(".lock")
 
     # Some Windows versions do not allow for paths longer than 255 characters.
     # In this case, we must specify it as an extended path by using the "\\?\" prefix
     if os.name == "nt":
-        if (
-            not str(local_dir).startswith("\\\\?\\")
-            and len(os.path.abspath(lock_path)) > 255
-        ):
+        if not str(local_dir).startswith("\\\\?\\") and len(os.path.abspath(lock_path)) > 255:
             file_path = Path("\\\\?\\" + os.path.abspath(file_path))
             lock_path = Path("\\\\?\\" + os.path.abspath(lock_path))
             metadata_path = Path("\\\\?\\" + os.path.abspath(metadata_path))
 
     file_path.parent.mkdir(parents=True, exist_ok=True)
     metadata_path.parent.mkdir(parents=True, exist_ok=True)
-    return LocalDownloadFilePaths(
-        file_path=file_path, lock_path=lock_path, metadata_path=metadata_path
-    )
+    return LocalDownloadFilePaths(file_path=file_path, lock_path=lock_path, metadata_path=metadata_path)
 
 
 def get_local_upload_paths(local_dir: Path, filename: str) -> LocalUploadFilePaths:
@@ -254,18 +244,13 @@ def get_local_upload_paths(local_dir: Pa
                 " owner to rename this file."
             )
     file_path = local_dir / sanitized_filename
-    metadata_path = (
-        _huggingface_dir(local_dir) / "upload" / f"{sanitized_filename}.metadata"
-    )
+    metadata_path = _huggingface_dir(local_dir) / "upload" / f"{sanitized_filename}.metadata"
     lock_path = metadata_path.with_suffix(".lock")
 
     # Some Windows versions do not allow for paths longer than 255 characters.
     # In this case, we must specify it as an extended path by using the "\\?\" prefix
     if os.name == "nt":
-        if (
-            not str(local_dir).startswith("\\\\?\\")
-            and len(os.path.abspath(lock_path)) > 255
-        ):
+        if not str(local_dir).startswith("\\\\?\\") and len(os.path.abspath(lock_path)) > 255:
             file_path = Path("\\\\?\\" + os.path.abspath(file_path))
             lock_path = Path("\\\\?\\" + os.path.abspath(lock_path))
             metadata_path = Path("\\\\?\\" + os.path.abspath(metadata_path))
@@ -280,9 +265,7 @@ def get_local_upload_paths(local_dir: Pa
     )
 
 
-def read_download_metadata(
-    local_dir: Path, filename: str
-) -> Optional[LocalDownloadFileMetadata]:
+def read_download_metadata(local_dir: Path, filename: str) -> Optional[LocalDownloadFileMetadata]:
     """Read metadata about a file in the local directory related to a download process.
 
     Args:
@@ -316,9 +299,7 @@ def read_download_metadata(
                 try:
                     paths.metadata_path.unlink()
                 except Exception as e:
-                    logger.warning(
-                        f"Could not remove corrupted metadata file {paths.metadata_path}: {e}"
-                    )
+                    logger.warning(f"Could not remove corrupted metadata file {paths.metadata_path}: {e}")
 
             try:
                 # check if the file exists and hasn't been modified since the metadata was saved
@@ -327,9 +308,7 @@ def read_download_metadata(
                     stat.st_mtime - 1 <= metadata.timestamp
                 ):  # allow 1s difference as stat.st_mtime might not be precise
                     return metadata
-                logger.info(
-                    f"Ignored metadata for '{filename}' (outdated). Will re-compute hash."
-                )
+                logger.info(f"Ignored metadata for '{filename}' (outdated). Will re-compute hash.")
             except FileNotFoundError:
                 # file does not exist => metadata is outdated
                 return None
@@ -360,9 +339,7 @@ def read_upload_metadata(local_dir: Path
                     size = int(f.readline().strip())  # never None
 
                     _should_ignore = f.readline().strip()
-                    should_ignore = (
-                        None if _should_ignore == "" else bool(int(_should_ignore))
-                    )
+                    should_ignore = None if _should_ignore == "" else bool(int(_should_ignore))
 
                     _sha256 = f.readline().strip()
                     sha256 = None if _sha256 == "" else _sha256
@@ -370,9 +347,7 @@ def read_upload_metadata(local_dir: Path
                     _upload_mode = f.readline().strip()
                     upload_mode = None if _upload_mode == "" else _upload_mode
                     if upload_mode not in (None, "regular", "lfs"):
-                        raise ValueError(
-                            f"Invalid upload mode in metadata {paths.path_in_repo}: {upload_mode}"
-                        )
+                        raise ValueError(f"Invalid upload mode in metadata {paths.path_in_repo}: {upload_mode}")
 
                     is_uploaded = bool(int(f.readline().strip()))
                     is_committed = bool(int(f.readline().strip()))
@@ -394,30 +369,22 @@ def read_upload_metadata(local_dir: Path
                 try:
                     paths.metadata_path.unlink()
                 except Exception as e:
-                    logger.warning(
-                        f"Could not remove corrupted metadata file {paths.metadata_path}: {e}"
-                    )
+                    logger.warning(f"Could not remove corrupted metadata file {paths.metadata_path}: {e}")
 
             # TODO: can we do better?
             if (
                 metadata.timestamp is not None
                 and metadata.is_uploaded  # file was uploaded
                 and not metadata.is_committed  # but not committed
-                and time.time() - metadata.timestamp
-                > 20 * 3600  # and it's been more than 20 hours
+                and time.time() - metadata.timestamp > 20 * 3600  # and it's been more than 20 hours
             ):  # => we consider it as garbage-collected by S3
                 metadata.is_uploaded = False
 
             # check if the file exists and hasn't been modified since the metadata was saved
             try:
-                if (
-                    metadata.timestamp is not None
-                    and paths.file_path.stat().st_mtime <= metadata.timestamp
-                ):
+                if metadata.timestamp is not None and paths.file_path.stat().st_mtime <= metadata.timestamp:
                     return metadata
-                logger.info(
-                    f"Ignored metadata for '{filename}' (outdated). Will re-compute hash."
-                )
+                logger.info(f"Ignored metadata for '{filename}' (outdated). Will re-compute hash.")
             except FileNotFoundError:
                 # file does not exist => metadata is outdated
                 pass
@@ -426,9 +393,7 @@ def read_upload_metadata(local_dir: Path
     return LocalUploadFileMetadata(size=paths.file_path.stat().st_size)
 
 
-def write_download_metadata(
-    local_dir: Path, filename: str, commit_hash: str, etag: str
-) -> None:
+def write_download_metadata(local_dir: Path, filename: str, commit_hash: str, etag: str) -> None:
     """Write metadata about a file in the local directory related to a download process.
 
     Args:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_login.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_login.py
@@ -142,9 +142,7 @@ def logout(token_name: Optional[str] = N
         [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError):
             If the access token name is not found.
     """
-    if (
-        get_token() is None and not get_stored_tokens()
-    ):  # No active token and no saved access tokens
+    if get_token() is None and not get_stored_tokens():  # No active token and no saved access tokens
         logger.warning("Not logged in!")
         return
     if not token_name:
@@ -192,9 +190,7 @@ def auth_switch(token_name: str, add_to_
     """
     token = _get_token_by_name(token_name)
     if not token:
-        raise ValueError(
-            f"Access token {token_name} not found in {constants.HF_STORED_TOKENS_PATH}"
-        )
+        raise ValueError(f"Access token {token_name} not found in {constants.HF_STORED_TOKENS_PATH}")
     # Write token to HF_TOKEN_PATH
     _set_active_token(token_name, add_to_git_credential)
     logger.info(f"The current active token is: {token_name}")
@@ -229,11 +225,7 @@ def auth_list() -> None:
         masked_token = f"{token[:3]}****{token[-4:]}" if token != "<not set>" else token
         is_current = "*" if token == current_token else " "
 
-        print(
-            f"{is_current} {{:<{max_offset}}}| {{:<15}}".format(
-                token_name, masked_token
-            )
-        )
+        print(f"{is_current} {{:<{max_offset}}}| {{:<15}}".format(token_name, masked_token))
 
     if _get_token_from_environment():
         logger.warning(
@@ -256,9 +248,7 @@ def auth_list() -> None:
     custom_message="Fine-grained tokens added complexity to the permissions, making it irrelevant to check if a token has 'write' access.",
 )
 @_deprecate_positional_args(version="1.0")
-def interpreter_login(
-    *, new_session: bool = True, write_permission: bool = False
-) -> None:
+def interpreter_login(*, new_session: bool = True, write_permission: bool = False) -> None:
     """
     Displays a prompt to log in to the HF website and store the token.
 
@@ -358,14 +348,10 @@ def notebook_login(*, new_session: bool
         logger.info("User is already logged in.")
         return
 
-    box_layout = widgets.Layout(
-        display="flex", flex_flow="column", align_items="center", width="50%"
-    )
+    box_layout = widgets.Layout(display="flex", flex_flow="column", align_items="center", width="50%")
 
     token_widget = widgets.Password(description="Token:")
-    git_checkbox_widget = widgets.Checkbox(
-        value=True, description="Add token as git credential?"
-    )
+    git_checkbox_widget = widgets.Checkbox(value=True, description="Add token as git credential?")
     token_finish_button = widgets.Button(description="Login")
 
     login_token_widget = widgets.VBox(
@@ -396,9 +382,7 @@ def notebook_login(*, new_session: bool
         except Exception as error:
             message = str(error)
         # Print result (success message or error)
-        login_token_widget.children = [
-            widgets.Label(line) for line in message.split("\n") if line.strip()
-        ]
+        login_token_widget.children = [widgets.Label(line) for line in message.split("\n") if line.strip()]
 
     token_finish_button.on_click(login_token_event)
 
@@ -415,9 +399,7 @@ def _login(
     from .hf_api import whoami  # avoid circular import
 
     if token.startswith("api_org"):
-        raise ValueError(
-            "You must use your personal account token, not an organization token."
-        )
+        raise ValueError("You must use your personal account token, not an organization token.")
 
     token_info = whoami(token)
     permission = token_info["auth"]["accessToken"]["role"]
@@ -427,9 +409,7 @@ def _login(
     # Store token locally
     _save_token(token=token, token_name=token_name)
     # Set active token
-    _set_active_token(
-        token_name=token_name, add_to_git_credential=add_to_git_credential
-    )
+    _set_active_token(token_name=token_name, add_to_git_credential=add_to_git_credential)
     logger.info("Login successful.")
     if _get_token_from_environment():
         logger.warning(
@@ -474,9 +454,7 @@ def _set_active_token(
     """
     token = _get_token_by_name(token_name)
     if not token:
-        raise ValueError(
-            f"Token {token_name} not found in {constants.HF_STORED_TOKENS_PATH}"
-        )
+        raise ValueError(f"Token {token_name} not found in {constants.HF_STORED_TOKENS_PATH}")
     if add_to_git_credential:
         if _is_git_credential_helper_configured():
             set_git_credential(token)
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_snapshot_download.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_snapshot_download.py
@@ -145,13 +145,9 @@ def snapshot_download(
     if repo_type is None:
         repo_type = "model"
     if repo_type not in constants.REPO_TYPES:
-        raise ValueError(
-            f"Invalid repo type: {repo_type}. Accepted repo types are: {str(constants.REPO_TYPES)}"
-        )
+        raise ValueError(f"Invalid repo type: {repo_type}. Accepted repo types are: {str(constants.REPO_TYPES)}")
 
-    storage_folder = os.path.join(
-        cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type)
-    )
+    storage_folder = os.path.join(cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type))
 
     repo_info: Union[ModelInfo, DatasetInfo, SpaceInfo, None] = None
     api_call_error: Optional[Exception] = None
@@ -166,9 +162,7 @@ def snapshot_download(
                 endpoint=endpoint,
                 headers=headers,
             )
-            repo_info = api.repo_info(
-                repo_id=repo_id, repo_type=repo_type, revision=revision, token=token
-            )
+            repo_info = api.repo_info(repo_id=repo_id, repo_type=repo_type, revision=revision, token=token)
         except (requests.exceptions.SSLError, requests.exceptions.ProxyError):
             # Actually raise for those subclasses of ConnectionError
             raise
@@ -243,9 +237,7 @@ def snapshot_download(
                 "outgoing traffic has been disabled. To enable repo look-ups and downloads online, set "
                 "'HF_HUB_OFFLINE=0' as environment variable."
             ) from api_call_error
-        elif isinstance(api_call_error, RepositoryNotFoundError) or isinstance(
-            api_call_error, GatedRepoError
-        ):
+        elif isinstance(api_call_error, RepositoryNotFoundError) or isinstance(api_call_error, GatedRepoError):
             # Repo not found => let's raise the actual error
             raise api_call_error
         else:
@@ -258,12 +250,8 @@ def snapshot_download(
 
     # At this stage, internet connection is up and running
     # => let's download the files!
-    assert (
-        repo_info.sha is not None
-    ), "Repo info returned from server must have a revision sha."
-    assert (
-        repo_info.siblings is not None
-    ), "Repo info returned from server must have a siblings list."
+    assert repo_info.sha is not None, "Repo info returned from server must have a revision sha."
+    assert repo_info.siblings is not None, "Repo info returned from server must have a siblings list."
     filtered_repo_files = list(
         filter_repo_objects(
             items=[f.rfilename for f in repo_info.siblings],
@@ -283,9 +271,7 @@ def snapshot_download(
             with open(ref_path, "w") as f:
                 f.write(commit_hash)
         except OSError as e:
-            logger.warning(
-                f"Ignored error while writing commit hash to {ref_path}: {e}."
-            )
+            logger.warning(f"Ignored error while writing commit hash to {ref_path}: {e}.")
 
     # we pass the commit_hash to hf_hub_download
     # so no network call happens if we already
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_tensorboard_logger.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_tensorboard_logger.py
@@ -148,9 +148,7 @@ class HFSummaryWriter(SummaryWriter):
 
         # Check logdir has been correctly initialized and fail early otherwise. In practice, SummaryWriter takes care of it.
         if not isinstance(self.logdir, str):
-            raise ValueError(
-                f"`self.logdir` must be a string. Got '{self.logdir}' of type {type(self.logdir)}."
-            )
+            raise ValueError(f"`self.logdir` must be a string. Got '{self.logdir}' of type {type(self.logdir)}.")
 
         # Append logdir name to `path_in_repo`
         if path_in_repo is None or path_in_repo == "":
@@ -180,9 +178,7 @@ class HFSummaryWriter(SummaryWriter):
 
         # Add `hf-summary-writer` tag to the model card metadata
         try:
-            card = ModelCard.load(
-                repo_id_or_path=self.repo_id, repo_type=self.repo_type
-            )
+            card = ModelCard.load(repo_id_or_path=self.repo_id, repo_type=self.repo_type)
         except EntryNotFoundError:
             card = ModelCard("")
         tags = card.data.get("tags", [])
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_upload_large_folder.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_upload_large_folder.py
@@ -95,25 +95,17 @@ def upload_large_folder_internal(
         num_workers = max(nb_cores - 2, 2)  # Use all but 2 cores, or at least 2 cores
 
     # 2. Create repo if missing
-    repo_url = api.create_repo(
-        repo_id=repo_id, repo_type=repo_type, private=private, exist_ok=True
-    )
+    repo_url = api.create_repo(repo_id=repo_id, repo_type=repo_type, private=private, exist_ok=True)
     logger.info(f"Repo created: {repo_url}")
     repo_id = repo_url.repo_id
 
     # 3. List files to upload
     filtered_paths_list = filter_repo_objects(
-        (
-            path.relative_to(folder_path).as_posix()
-            for path in folder_path.glob("**/*")
-            if path.is_file()
-        ),
+        (path.relative_to(folder_path).as_posix() for path in folder_path.glob("**/*") if path.is_file()),
         allow_patterns=allow_patterns,
         ignore_patterns=ignore_patterns,
     )
-    paths_list = [
-        get_local_upload_paths(folder_path, relpath) for relpath in filtered_paths_list
-    ]
+    paths_list = [get_local_upload_paths(folder_path, relpath) for relpath in filtered_paths_list]
     logger.info(f"Found {len(paths_list)} candidate files to upload")
 
     # Read metadata for each file
@@ -210,9 +202,7 @@ class LargeUploadStatus:
             elif not metadata.is_committed:
                 self.queue_commit.put(item)
             else:
-                logger.debug(
-                    f"Skipping file {paths.path_in_repo} (already uploaded and committed)"
-                )
+                logger.debug(f"Skipping file {paths.path_in_repo} (already uploaded and committed)")
 
     def current_report(self) -> str:
         """Generate a report of the current status of the large upload."""
@@ -279,10 +269,7 @@ class LargeUploadStatus:
 
     def is_done(self) -> bool:
         with self.lock:
-            return all(
-                metadata.is_committed or metadata.should_ignore
-                for _, metadata in self.items
-            )
+            return all(metadata.is_committed or metadata.should_ignore for _, metadata in self.items)
 
 
 def _worker_job(
@@ -434,10 +421,7 @@ def _determine_next_job(
             return (WorkerJob.GET_UPLOAD_MODE, _get_n(status.queue_get_upload_mode, 50))
 
         # 4. Preupload LFS file if at least 1 file and no worker is preuploading LFS
-        elif (
-            status.queue_preupload_lfs.qsize() > 0
-            and status.nb_workers_preupload_lfs == 0
-        ):
+        elif status.queue_preupload_lfs.qsize() > 0 and status.nb_workers_preupload_lfs == 0:
             status.nb_workers_preupload_lfs += 1
             logger.debug("Job: preupload LFS (no other worker preuploading LFS)")
             return (WorkerJob.PREUPLOAD_LFS, _get_one(status.queue_preupload_lfs))
@@ -449,10 +433,7 @@ def _determine_next_job(
             return (WorkerJob.SHA256, _get_one(status.queue_sha256))
 
         # 6. Get upload mode if at least 1 file and no worker is getting upload mode
-        elif (
-            status.queue_get_upload_mode.qsize() > 0
-            and status.nb_workers_get_upload_mode == 0
-        ):
+        elif status.queue_get_upload_mode.qsize() > 0 and status.nb_workers_get_upload_mode == 0:
             status.nb_workers_get_upload_mode += 1
             logger.debug("Job: get upload mode (no other worker getting upload mode)")
             return (WorkerJob.GET_UPLOAD_MODE, _get_n(status.queue_get_upload_mode, 50))
@@ -460,8 +441,7 @@ def _determine_next_job(
         # 7. Preupload LFS file if at least 1 file
         #    Skip if hf_transfer is enabled and there is already a worker preuploading LFS
         elif status.queue_preupload_lfs.qsize() > 0 and (
-            status.nb_workers_preupload_lfs == 0
-            or not constants.HF_HUB_ENABLE_HF_TRANSFER
+            status.nb_workers_preupload_lfs == 0 or not constants.HF_HUB_ENABLE_HF_TRANSFER
         ):
             status.nb_workers_preupload_lfs += 1
             logger.debug("Job: preupload LFS")
@@ -507,10 +487,7 @@ def _determine_next_job(
             return (WorkerJob.COMMIT, _get_items_to_commit(status.queue_commit))
 
         # 12. If all queues are empty, exit
-        elif all(
-            metadata.is_committed or metadata.should_ignore
-            for _, metadata in status.items
-        ):
+        elif all(metadata.is_committed or metadata.should_ignore for _, metadata in status.items):
             logger.info("All files have been processed! Exiting worker.")
             return None
 
@@ -535,9 +512,7 @@ def _compute_sha256(item: JOB_ITEM_T) ->
     metadata.save(paths)
 
 
-def _get_upload_mode(
-    items: List[JOB_ITEM_T], api: "HfApi", repo_id: str, repo_type: str, revision: str
-) -> None:
+def _get_upload_mode(items: List[JOB_ITEM_T], api: "HfApi", repo_id: str, repo_type: str, revision: str) -> None:
     """Get upload mode for each file and update metadata.
 
     Also receive info if the file should be ignored.
@@ -557,9 +532,7 @@ def _get_upload_mode(
         metadata.save(paths)
 
 
-def _preupload_lfs(
-    item: JOB_ITEM_T, api: "HfApi", repo_id: str, repo_type: str, revision: str
-) -> None:
+def _preupload_lfs(item: JOB_ITEM_T, api: "HfApi", repo_id: str, repo_type: str, revision: str) -> None:
     """Preupload LFS file and update metadata."""
     paths, metadata = item
     addition = _build_hacky_operation(item)
@@ -574,9 +547,7 @@ def _preupload_lfs(
     metadata.save(paths)
 
 
-def _commit(
-    items: List[JOB_ITEM_T], api: "HfApi", repo_id: str, repo_type: str, revision: str
-) -> None:
+def _commit(items: List[JOB_ITEM_T], api: "HfApi", repo_id: str, repo_type: str, revision: str) -> None:
     """Commit files to the repo."""
     additions = [_build_hacky_operation(item) for item in items]
     api.create_commit(
@@ -604,16 +575,12 @@ class HackyCommitOperationAdd(CommitOper
 
 def _build_hacky_operation(item: JOB_ITEM_T) -> HackyCommitOperationAdd:
     paths, metadata = item
-    operation = HackyCommitOperationAdd(
-        path_in_repo=paths.path_in_repo, path_or_fileobj=paths.file_path
-    )
+    operation = HackyCommitOperationAdd(path_in_repo=paths.path_in_repo, path_or_fileobj=paths.file_path)
     with paths.file_path.open("rb") as file:
         sample = file.peek(512)[:512]
     if metadata.sha256 is None:
         raise ValueError("sha256 must have been computed by now!")
-    operation.upload_info = UploadInfo(
-        sha256=bytes.fromhex(metadata.sha256), size=metadata.size, sample=sample
-    )
+    operation.upload_info = UploadInfo(sha256=bytes.fromhex(metadata.sha256), size=metadata.size, sample=sample)
     return operation
 
 
@@ -641,10 +608,7 @@ def _get_items_to_commit(queue: "queue.Q
             return items
 
         # If we have enough items => commit them
-        if (
-            nb_lfs >= MAX_NB_LFS_FILES_PER_COMMIT
-            or nb_regular >= MAX_NB_REGULAR_FILES_PER_COMMIT
-        ):
+        if nb_lfs >= MAX_NB_LFS_FILES_PER_COMMIT or nb_regular >= MAX_NB_REGULAR_FILES_PER_COMMIT:
             return items
 
         # Else, get a new item and increase counter
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_webhooks_server.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_webhooks_server.py
@@ -178,28 +178,18 @@ class WebhooksServer:
         for path, func in self.registered_webhooks.items():
             # Add secret check if required
             if self.webhook_secret is not None:
-                func = _wrap_webhook_to_check_secret(
-                    func, webhook_secret=self.webhook_secret
-                )
+                func = _wrap_webhook_to_check_secret(func, webhook_secret=self.webhook_secret)
 
             # Add route to FastAPI app
             self.fastapi_app.post(path)(func)
 
         # Print instructions and block main thread
         space_host = os.environ.get("SPACE_HOST")
-        url = (
-            "https://" + space_host
-            if space_host is not None
-            else (ui.share_url or ui.local_url)
-        )
+        url = "https://" + space_host if space_host is not None else (ui.share_url or ui.local_url)
         url = url.strip("/")
         message = "\nWebhooks are correctly setup and ready to use:"
-        message += "\n" + "\n".join(
-            f"  - POST {url}{webhook}" for webhook in self.registered_webhooks
-        )
-        message += (
-            "\nGo to https://huggingface.co/settings/webhooks to setup your webhooks."
-        )
+        message += "\n" + "\n".join(f"  - POST {url}{webhook}" for webhook in self.registered_webhooks)
+        message += "\nGo to https://huggingface.co/settings/webhooks to setup your webhooks."
         print(message)
 
         if not prevent_thread_lock:
@@ -330,9 +320,7 @@ def _get_global_app() -> WebhooksServer:
 
 def _warn_on_empty_secret(webhook_secret: Optional[str]) -> None:
     if webhook_secret is None:
-        print(
-            "Webhook secret is not defined. This means your webhook endpoints will be open to everyone."
-        )
+        print("Webhook secret is not defined. This means your webhook endpoints will be open to everyone.")
         print(
             "To add a secret, set `WEBHOOK_SECRET` as environment variable or pass it at initialization: "
             "\n\t`app = WebhooksServer(webhook_secret='my_secret', ...)`"
@@ -371,9 +359,7 @@ def _wrap_webhook_to_check_secret(func:
     async def _protected_func(request: Request, **kwargs):
         request_secret = request.headers.get("x-webhook-secret")
         if request_secret is None:
-            return JSONResponse(
-                {"error": "x-webhook-secret header not set."}, status_code=401
-            )
+            return JSONResponse({"error": "x-webhook-secret header not set."}, status_code=401)
         if request_secret != webhook_secret:
             return JSONResponse({"error": "Invalid webhook secret."}, status_code=403)
 
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/delete_cache.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/delete_cache.py
@@ -102,9 +102,7 @@ _CANCEL_DELETION_STR = "CANCEL_DELETION"
 class DeleteCacheCommand(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        delete_cache_parser = parser.add_parser(
-            "delete-cache", help="Delete revisions from the cache directory."
-        )
+        delete_cache_parser = parser.add_parser("delete-cache", help="Delete revisions from the cache directory.")
 
         delete_cache_parser.add_argument(
             "--dir",
@@ -141,10 +139,7 @@ class DeleteCacheCommand(BaseHuggingface
 
         # If deletion is not cancelled
         if len(selected_hashes) > 0 and _CANCEL_DELETION_STR not in selected_hashes:
-            confirm_message = (
-                _get_expectations_str(hf_cache_info, selected_hashes)
-                + " Confirm deletion ?"
-            )
+            confirm_message = _get_expectations_str(hf_cache_info, selected_hashes) + " Confirm deletion ?"
 
             # Confirm deletion
             if self.disable_tui:
@@ -175,9 +170,7 @@ def _manual_review_tui(hf_cache_info: HF
     Displays a multi-select menu in the terminal (TUI).
     """
     # Define multiselect list
-    choices = _get_tui_choices_from_scan(
-        repos=hf_cache_info.repos, preselected=preselected
-    )
+    choices = _get_tui_choices_from_scan(repos=hf_cache_info.repos, preselected=preselected)
     checkbox = inquirer.checkbox(
         message="Select revisions to delete:",
         choices=choices,  # List of revisions with some pre-selection
@@ -187,9 +180,7 @@ def _manual_review_tui(hf_cache_info: HF
         # deletion.
         instruction=_get_expectations_str(
             hf_cache_info,
-            selected_hashes=[
-                c.value for c in choices if isinstance(c, Choice) and c.enabled
-            ],
+            selected_hashes=[c.value for c in choices if isinstance(c, Choice) and c.enabled],
         ),
         # We use the long instruction to should keybindings instructions to the user
         long_instruction="Press <space> to select, <enter> to validate and <ctrl+c> to quit without modification.",
@@ -204,11 +195,7 @@ def _manual_review_tui(hf_cache_info: HF
         # a revision hash is selected/unselected.
         checkbox._instruction = _get_expectations_str(
             hf_cache_info,
-            selected_hashes=[
-                choice["value"]
-                for choice in checkbox.content_control.choices
-                if choice["enabled"]
-            ],
+            selected_hashes=[choice["value"] for choice in checkbox.content_control.choices if choice["enabled"]],
         )
 
     checkbox.kb_func_lookup["toggle"].append({"func": _update_expectations})
@@ -226,9 +213,7 @@ def _ask_for_confirmation_tui(message: s
     return inquirer.confirm(message, default=default).execute()
 
 
-def _get_tui_choices_from_scan(
-    repos: Iterable[CachedRepoInfo], preselected: List[str]
-) -> List:
+def _get_tui_choices_from_scan(repos: Iterable[CachedRepoInfo], preselected: List[str]) -> List:
     """Build a list of choices from the scanned repos.
 
     Args:
@@ -279,9 +264,7 @@ def _get_tui_choices_from_scan(
     return choices
 
 
-def _manual_review_no_tui(
-    hf_cache_info: HFCacheInfo, preselected: List[str]
-) -> List[str]:
+def _manual_review_no_tui(hf_cache_info: HFCacheInfo, preselected: List[str]) -> List[str]:
     """Ask the user for a manual review of the revisions to delete.
 
     Used when TUI is disabled. Manual review happens in a separate tmp file that the
@@ -354,9 +337,7 @@ def _ask_for_confirmation_no_tui(message
         print(f"Invalid input. Must be one of {ALL}")
 
 
-def _get_expectations_str(
-    hf_cache_info: HFCacheInfo, selected_hashes: List[str]
-) -> str:
+def _get_expectations_str(hf_cache_info: HFCacheInfo, selected_hashes: List[str]) -> str:
     """Format a string to display to the user how much space would be saved.
 
     Example:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/download.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/download.py
@@ -53,9 +53,7 @@ logger = logging.get_logger(__name__)
 class DownloadCommand(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        download_parser = parser.add_parser(
-            "download", help="Download files from the Hub"
-        )
+        download_parser = parser.add_parser("download", help="Download files from the Hub")
         download_parser.add_argument(
             "repo_id",
             type=str,
@@ -107,9 +105,7 @@ class DownloadCommand(BaseHuggingfaceCLI
         download_parser.add_argument(
             "--local-dir-use-symlinks",
             choices=["auto", "True", "False"],
-            help=(
-                "Deprecated and ignored. Downloading to a local directory does not use symlinks anymore."
-            ),
+            help=("Deprecated and ignored. Downloading to a local directory does not use symlinks anymore."),
         )
         download_parser.add_argument(
             "--force-download",
@@ -176,13 +172,9 @@ class DownloadCommand(BaseHuggingfaceCLI
         # Warn user if patterns are ignored
         if len(self.filenames) > 0:
             if self.include is not None and len(self.include) > 0:
-                warnings.warn(
-                    "Ignoring `--include` since filenames have being explicitly set."
-                )
+                warnings.warn("Ignoring `--include` since filenames have being explicitly set.")
             if self.exclude is not None and len(self.exclude) > 0:
-                warnings.warn(
-                    "Ignoring `--exclude` since filenames have being explicitly set."
-                )
+                warnings.warn("Ignoring `--exclude` since filenames have being explicitly set.")
 
         # Single file to download: use `hf_hub_download`
         if len(self.filenames) == 1:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/env.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/env.py
@@ -29,9 +29,7 @@ class EnvironmentCommand(BaseHuggingface
 
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        env_parser = parser.add_parser(
-            "env", help="Print information about the environment."
-        )
+        env_parser = parser.add_parser("env", help="Print information about the environment.")
         env_parser.set_defaults(func=EnvironmentCommand)
 
     def run(self) -> None:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/huggingface_cli.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/huggingface_cli.py
@@ -28,9 +28,7 @@ from huggingface_hub.commands.version im
 
 
 def main():
-    parser = ArgumentParser(
-        "huggingface-cli", usage="huggingface-cli <command> [<args>]"
-    )
+    parser = ArgumentParser("huggingface-cli", usage="huggingface-cli <command> [<args>]")
     commands_parser = parser.add_subparsers(help="huggingface-cli command helpers")
 
     # Register commands
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/lfs.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/lfs.py
@@ -60,9 +60,7 @@ class LfsCommands(BaseHuggingfaceCLIComm
             "lfs-enable-largefiles",
             help="Configure your repository to enable upload of files > 5GB.",
         )
-        enable_parser.add_argument(
-            "path", type=str, help="Local path to repository you want to configure."
-        )
+        enable_parser.add_argument("path", type=str, help="Local path to repository you want to configure.")
         enable_parser.set_defaults(func=lambda args: LfsEnableCommand(args))
 
         # Command will get called by git-lfs, do not call it directly.
@@ -123,9 +121,7 @@ class LfsUploadCommand:
         # sends initiation data to the process over stdin.
         # This tells the process useful information about the configuration.
         init_msg = json.loads(sys.stdin.readline().strip())
-        if not (
-            init_msg.get("event") == "init" and init_msg.get("operation") == "upload"
-        ):
+        if not (init_msg.get("event") == "init" and init_msg.get("operation") == "upload"):
             write_msg({"error": {"code": 32, "message": "Wrong lfs init operation"}})
             sys.exit(1)
 
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/repo_files.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/repo_files.py
@@ -76,9 +76,7 @@ class DeleteFilesSubCommand:
 class RepoFilesCommand(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        repo_files_parser = parser.add_parser(
-            "repo-files", help="Manage files in a repo on the Hub"
-        )
+        repo_files_parser = parser.add_parser("repo-files", help="Manage files in a repo on the Hub")
         repo_files_parser.add_argument(
             "repo_id",
             type=str,
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/scan_cache.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/scan_cache.py
@@ -33,9 +33,7 @@ from ._cli_utils import ANSI, tabulate
 class ScanCacheCommand(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        scan_cache_parser = parser.add_parser(
-            "scan-cache", help="Scan cache directory."
-        )
+        scan_cache_parser = parser.add_parser("scan-cache", help="Scan cache directory.")
 
         scan_cache_parser.add_argument(
             "--dir",
@@ -168,9 +166,7 @@ def get_table(hf_cache_info: HFCacheInfo
                     str(revision.snapshot_path),
                 ]
                 for repo in sorted(hf_cache_info.repos, key=lambda repo: repo.repo_path)
-                for revision in sorted(
-                    repo.revisions, key=lambda revision: revision.commit_hash
-                )
+                for revision in sorted(repo.revisions, key=lambda revision: revision.commit_hash)
             ],
             headers=[
                 "REPO ID",
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/tag.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/tag.py
@@ -47,9 +47,7 @@ from ._cli_utils import ANSI
 class TagCommands(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        tag_parser = parser.add_parser(
-            "tag", help="(create, list, delete) tags for a repo in the hub"
-        )
+        tag_parser = parser.add_parser("tag", help="(create, list, delete) tags for a repo in the hub")
 
         tag_parser.add_argument(
             "repo_id",
@@ -62,9 +60,7 @@ class TagCommands(BaseHuggingfaceCLIComm
             type=str,
             help="The name of the tag for creation or deletion.",
         )
-        tag_parser.add_argument(
-            "-m", "--message", type=str, help="The description of the tag to create."
-        )
+        tag_parser.add_argument("-m", "--message", type=str, help="The description of the tag to create.")
         tag_parser.add_argument("--revision", type=str, help="The git revision to tag.")
         tag_parser.add_argument(
             "--token",
@@ -84,12 +80,8 @@ class TagCommands(BaseHuggingfaceCLIComm
             help="Answer Yes to prompts automatically.",
         )
 
-        tag_parser.add_argument(
-            "-l", "--list", action="store_true", help="List tags for a repository."
-        )
-        tag_parser.add_argument(
-            "-d", "--delete", action="store_true", help="Delete a tag for a repository."
-        )
+        tag_parser.add_argument("-l", "--list", action="store_true", help="List tags for a repository.")
+        tag_parser.add_argument("-d", "--delete", action="store_true", help="Delete a tag for a repository.")
 
         tag_parser.set_defaults(func=lambda args: handle_commands(args))
 
@@ -116,9 +108,7 @@ class TagCommand:
 
 class TagCreateCommand(TagCommand):
     def run(self):
-        print(
-            f"You are about to create tag {ANSI.bold(self.args.tag)} on {self.repo_type} {ANSI.bold(self.repo_id)}"
-        )
+        print(f"You are about to create tag {ANSI.bold(self.args.tag)} on {self.repo_type} {ANSI.bold(self.repo_id)}")
 
         try:
             self.api.create_tag(
@@ -136,9 +126,7 @@ class TagCreateCommand(TagCommand):
             exit(1)
         except HfHubHTTPError as e:
             if e.response.status_code == 409:
-                print(
-                    f"Tag {ANSI.bold(self.args.tag)} already exists on {ANSI.bold(self.repo_id)}"
-                )
+                print(f"Tag {ANSI.bold(self.args.tag)} already exists on {ANSI.bold(self.repo_id)}")
                 exit(1)
             raise e
 
@@ -169,9 +157,7 @@ class TagListCommand(TagCommand):
 
 class TagDeleteCommand(TagCommand):
     def run(self):
-        print(
-            f"You are about to delete tag {ANSI.bold(self.args.tag)} on {self.repo_type} {ANSI.bold(self.repo_id)}"
-        )
+        print(f"You are about to delete tag {ANSI.bold(self.args.tag)} on {self.repo_type} {ANSI.bold(self.repo_id)}")
 
         if not self.args.yes:
             choice = input("Proceed? [Y/n] ").lower()
@@ -179,15 +165,11 @@ class TagDeleteCommand(TagCommand):
                 print("Abort")
                 exit()
         try:
-            self.api.delete_tag(
-                repo_id=self.repo_id, tag=self.args.tag, repo_type=self.repo_type
-            )
+            self.api.delete_tag(repo_id=self.repo_id, tag=self.args.tag, repo_type=self.repo_type)
         except RepositoryNotFoundError:
             print(f"{self.repo_type.capitalize()} {ANSI.bold(self.repo_id)} not found.")
             exit(1)
         except RevisionNotFoundError:
-            print(
-                f"Tag {ANSI.bold(self.args.tag)} not found on {ANSI.bold(self.repo_id)}"
-            )
+            print(f"Tag {ANSI.bold(self.args.tag)} not found on {ANSI.bold(self.repo_id)}")
             exit(1)
         print(f"Tag {ANSI.bold(self.args.tag)} deleted on {ANSI.bold(self.repo_id)}")
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/upload.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/upload.py
@@ -64,9 +64,7 @@ logger = logging.get_logger(__name__)
 class UploadCommand(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        upload_parser = parser.add_parser(
-            "upload", help="Upload a file or a folder to a repo on the Hub"
-        )
+        upload_parser = parser.add_parser("upload", help="Upload a file or a folder to a repo on the Hub")
         upload_parser.add_argument(
             "repo_id",
             type=str,
@@ -176,9 +174,7 @@ class UploadCommand(BaseHuggingfaceCLICo
         self.every: Optional[float] = args.every
 
         # Resolve `local_path` and `path_in_repo`
-        repo_name: str = args.repo_id.split("/")[
-            -1
-        ]  # e.g. "Wauplin/my-cool-model" => "my-cool-model"
+        repo_name: str = args.repo_id.split("/")[-1]  # e.g. "Wauplin/my-cool-model" => "my-cool-model"
         self.local_path: str
         self.path_in_repo: str
         if args.local_path is None and os.path.isfile(repo_name):
@@ -192,9 +188,7 @@ class UploadCommand(BaseHuggingfaceCLICo
         elif args.local_path is None:
             # Implicit case 3: user provided only a repo_id that does not match a local file or folder
             # => the user must explicitly provide a local_path => raise exception
-            raise ValueError(
-                f"'{repo_name}' is not a local file or folder. Please set `local_path` explicitly."
-            )
+            raise ValueError(f"'{repo_name}' is not a local file or folder. Please set `local_path` explicitly.")
         elif args.path_in_repo is None and os.path.isfile(args.local_path):
             # Explicit local path to file, no path in repo => upload it at root with same name
             self.local_path = args.local_path
@@ -241,9 +235,7 @@ class UploadCommand(BaseHuggingfaceCLICo
                 # If file => watch entire folder + use allow_patterns
                 folder_path = os.path.dirname(self.local_path)
                 path_in_repo = (
-                    self.path_in_repo[
-                        : -len(self.local_path)
-                    ]  # remove filename from path_in_repo
+                    self.path_in_repo[: -len(self.local_path)]  # remove filename from path_in_repo
                     if self.path_in_repo.endswith(self.local_path)
                     else self.path_in_repo
                 )
@@ -255,9 +247,7 @@ class UploadCommand(BaseHuggingfaceCLICo
                 allow_patterns = self.include or []
                 ignore_patterns = self.exclude or []
                 if self.delete is not None and len(self.delete) > 0:
-                    warnings.warn(
-                        "Ignoring `--delete` when uploading with scheduled commits."
-                    )
+                    warnings.warn("Ignoring `--delete` when uploading with scheduled commits.")
 
             scheduler = CommitScheduler(
                 folder_path=folder_path,
@@ -271,9 +261,7 @@ class UploadCommand(BaseHuggingfaceCLICo
                 every=self.every,
                 hf_api=self.api,
             )
-            print(
-                f"Scheduling commits every {self.every} minutes to {scheduler.repo_id}."
-            )
+            print(f"Scheduling commits every {self.every} minutes to {scheduler.repo_id}.")
             try:  # Block main thread until KeyboardInterrupt
                 while True:
                     time.sleep(100)
@@ -297,9 +285,7 @@ class UploadCommand(BaseHuggingfaceCLICo
         # Check if branch already exists and if not, create it
         if self.revision is not None and not self.create_pr:
             try:
-                self.api.repo_info(
-                    repo_id=repo_id, repo_type=self.repo_type, revision=self.revision
-                )
+                self.api.repo_info(repo_id=repo_id, repo_type=self.repo_type, revision=self.revision)
             except RevisionNotFoundError:
                 logger.info(f"Branch '{self.revision}' not found. Creating it...")
                 self.api.create_branch(
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/upload_large_folder.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/upload_large_folder.py
@@ -32,17 +32,13 @@ logger = logging.get_logger(__name__)
 class UploadLargeFolderCommand(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        subparser = parser.add_parser(
-            "upload-large-folder", help="Upload a large folder to a repo on the Hub"
-        )
+        subparser = parser.add_parser("upload-large-folder", help="Upload a large folder to a repo on the Hub")
         subparser.add_argument(
             "repo_id",
             type=str,
             help="The ID of the repo to upload to (e.g. `username/repo-name`).",
         )
-        subparser.add_argument(
-            "local_path", type=str, help="Local path to the file or folder to upload."
-        )
+        subparser.add_argument("local_path", type=str, help="Local path to the file or folder to upload.")
         subparser.add_argument(
             "--repo-type",
             choices=["model", "dataset", "space"],
@@ -51,9 +47,7 @@ class UploadLargeFolderCommand(BaseHuggi
         subparser.add_argument(
             "--revision",
             type=str,
-            help=(
-                "An optional Git revision to push to. It can be a branch name or a PR reference."
-            ),
+            help=("An optional Git revision to push to. It can be a branch name or a PR reference."),
         )
         subparser.add_argument(
             "--private",
@@ -89,9 +83,7 @@ class UploadLargeFolderCommand(BaseHuggi
             action="store_true",
             help="Whether to disable regular status report.",
         )
-        subparser.add_argument(
-            "--no-bars", action="store_true", help="Whether to disable progress bars."
-        )
+        subparser.add_argument("--no-bars", action="store_true", help="Whether to disable progress bars.")
         subparser.set_defaults(func=UploadLargeFolderCommand)
 
     def __init__(self, args: Namespace) -> None:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/user.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/user.py
@@ -77,9 +77,7 @@ except ImportError:
 class UserCommands(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        login_parser = parser.add_parser(
-            "login", help="Log in using a token from huggingface.co/settings/tokens"
-        )
+        login_parser = parser.add_parser("login", help="Log in using a token from huggingface.co/settings/tokens")
         login_parser.add_argument(
             "--token",
             type=str,
@@ -91,9 +89,7 @@ class UserCommands(BaseHuggingfaceCLICom
             help="Optional: Save token to git credential helper.",
         )
         login_parser.set_defaults(func=lambda args: LoginCommand(args))
-        whoami_parser = parser.add_parser(
-            "whoami", help="Find out which huggingface.co account you are logged in as."
-        )
+        whoami_parser = parser.add_parser("whoami", help="Find out which huggingface.co account you are logged in as.")
         whoami_parser.set_defaults(func=lambda args: WhoamiCommand(args))
 
         logout_parser = parser.add_parser("logout", help="Log out")
@@ -104,13 +100,9 @@ class UserCommands(BaseHuggingfaceCLICom
         )
         logout_parser.set_defaults(func=lambda args: LogoutCommand(args))
 
-        auth_parser = parser.add_parser(
-            "auth", help="Other authentication related commands"
-        )
+        auth_parser = parser.add_parser("auth", help="Other authentication related commands")
         auth_subparsers = auth_parser.add_subparsers(help="Authentication subcommands")
-        auth_switch_parser = auth_subparsers.add_parser(
-            "switch", help="Switch between access tokens"
-        )
+        auth_switch_parser = auth_subparsers.add_parser("switch", help="Switch between access tokens")
         auth_switch_parser.add_argument(
             "--token-name",
             type=str,
@@ -122,20 +114,12 @@ class UserCommands(BaseHuggingfaceCLICom
             help="Optional: Save token to git credential helper.",
         )
         auth_switch_parser.set_defaults(func=lambda args: AuthSwitchCommand(args))
-        auth_list_parser = auth_subparsers.add_parser(
-            "list", help="List all stored access tokens"
-        )
+        auth_list_parser = auth_subparsers.add_parser("list", help="List all stored access tokens")
         auth_list_parser.set_defaults(func=lambda args: AuthListCommand(args))
         # new system: git-based repo system
-        repo_parser = parser.add_parser(
-            "repo", help="{create} Commands to interact with your huggingface.co repos."
-        )
-        repo_subparsers = repo_parser.add_subparsers(
-            help="huggingface.co repos related commands"
-        )
-        repo_create_parser = repo_subparsers.add_parser(
-            "create", help="Create a new repo on huggingface.co"
-        )
+        repo_parser = parser.add_parser("repo", help="{create} Commands to interact with your huggingface.co repos.")
+        repo_subparsers = repo_parser.add_subparsers(help="huggingface.co repos related commands")
+        repo_create_parser = repo_subparsers.add_parser("create", help="Create a new repo on huggingface.co")
         repo_create_parser.add_argument(
             "name",
             type=str,
@@ -146,9 +130,7 @@ class UserCommands(BaseHuggingfaceCLICom
             type=str,
             help='Optional: repo_type: set to "dataset" or "space" if creating a dataset or space, default is model.',
         )
-        repo_create_parser.add_argument(
-            "--organization", type=str, help="Optional: organization namespace."
-        )
+        repo_create_parser.add_argument("--organization", type=str, help="Optional: organization namespace.")
         repo_create_parser.add_argument(
             "--space_sdk",
             type=str,
@@ -212,9 +194,7 @@ class AuthSwitchCommand(BaseUserCommand)
             print(f"{i}. {token_name}")
         while True:
             try:
-                choice = input(
-                    "Enter the number of the token to switch to (or 'q' to quit): "
-                )
+                choice = input("Enter the number of the token to switch to (or 'q' to quit): ")
                 if choice.lower() == "q":
                     return None
                 index = int(choice) - 1
@@ -291,9 +271,7 @@ class RepoCreateCommand(BaseUserCommand)
         print("")
 
         user = self._api.whoami(token)["name"]
-        namespace = (
-            self.args.organization if self.args.organization is not None else user
-        )
+        namespace = self.args.organization if self.args.organization is not None else user
 
         repo_id = f"{namespace}/{self.args.name}"
 
@@ -326,8 +304,6 @@ class RepoCreateCommand(BaseUserCommand)
             exit(1)
         print("\nYour repo now lives at:")
         print(f"  {ANSI.bold(url)}")
-        print(
-            "\nYou can clone it locally with the command below, and commit/push as usual."
-        )
+        print("\nYou can clone it locally with the command below, and commit/push as usual.")
         print(f"\n  git clone {url}")
         print("")
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/version.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/version.py
@@ -30,9 +30,7 @@ class VersionCommand(BaseHuggingfaceCLIC
 
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        version_parser = parser.add_parser(
-            "version", help="Print information about the huggingface-cli version."
-        )
+        version_parser = parser.add_parser("version", help="Print information about the huggingface-cli version.")
         version_parser.set_defaults(func=VersionCommand)
 
     def run(self) -> None:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/community.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/community.py
@@ -81,9 +81,7 @@ class Discussion:
         """Returns the URL of the discussion on the Hub."""
         if self.repo_type is None or self.repo_type == constants.REPO_TYPE_MODEL:
             return f"{self.endpoint}/{self.repo_id}/discussions/{self.num}"
-        return (
-            f"{self.endpoint}/{self.repo_type}s/{self.repo_id}/discussions/{self.num}"
-        )
+        return f"{self.endpoint}/{self.repo_type}s/{self.repo_id}/discussions/{self.num}"
 
 
 @dataclass
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/constants.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/constants.py
@@ -70,17 +70,13 @@ HUGGINGFACE_CO_URL_TEMPLATE = ENDPOINT +
 
 if _staging_mode:
     ENDPOINT = _HF_DEFAULT_STAGING_ENDPOINT
-    HUGGINGFACE_CO_URL_TEMPLATE = (
-        _HF_DEFAULT_STAGING_ENDPOINT + "/{repo_id}/resolve/{revision}/{filename}"
-    )
+    HUGGINGFACE_CO_URL_TEMPLATE = _HF_DEFAULT_STAGING_ENDPOINT + "/{repo_id}/resolve/{revision}/{filename}"
 
 HUGGINGFACE_HEADER_X_REPO_COMMIT = "X-Repo-Commit"
 HUGGINGFACE_HEADER_X_LINKED_ETAG = "X-Linked-Etag"
 HUGGINGFACE_HEADER_X_LINKED_SIZE = "X-Linked-Size"
 
-INFERENCE_ENDPOINT = os.environ.get(
-    "HF_INFERENCE_ENDPOINT", "https://api-inference.huggingface.co"
-)
+INFERENCE_ENDPOINT = os.environ.get("HF_INFERENCE_ENDPOINT", "https://api-inference.huggingface.co")
 
 # See https://huggingface.co/docs/inference-endpoints/index
 INFERENCE_ENDPOINTS_ENDPOINT = "https://api.endpoints.huggingface.cloud/v2"
@@ -110,13 +106,9 @@ REPO_TYPES_MAPPING = {
 }
 
 DiscussionTypeFilter = Literal["all", "discussion", "pull_request"]
-DISCUSSION_TYPES: Tuple[DiscussionTypeFilter, ...] = typing.get_args(
-    DiscussionTypeFilter
-)
+DISCUSSION_TYPES: Tuple[DiscussionTypeFilter, ...] = typing.get_args(DiscussionTypeFilter)
 DiscussionStatusFilter = Literal["all", "open", "closed"]
-DISCUSSION_STATUS: Tuple[DiscussionTypeFilter, ...] = typing.get_args(
-    DiscussionStatusFilter
-)
+DISCUSSION_STATUS: Tuple[DiscussionTypeFilter, ...] = typing.get_args(DiscussionStatusFilter)
 
 # Webhook subscription types
 WEBHOOK_DOMAIN_T = Literal["repo", "discussions"]
@@ -136,17 +128,13 @@ default_assets_cache_path = os.path.join
 
 # Legacy env variables
 HUGGINGFACE_HUB_CACHE = os.getenv("HUGGINGFACE_HUB_CACHE", default_cache_path)
-HUGGINGFACE_ASSETS_CACHE = os.getenv(
-    "HUGGINGFACE_ASSETS_CACHE", default_assets_cache_path
-)
+HUGGINGFACE_ASSETS_CACHE = os.getenv("HUGGINGFACE_ASSETS_CACHE", default_assets_cache_path)
 
 # New env variables
 HF_HUB_CACHE = os.getenv("HF_HUB_CACHE", HUGGINGFACE_HUB_CACHE)
 HF_ASSETS_CACHE = os.getenv("HF_ASSETS_CACHE", HUGGINGFACE_ASSETS_CACHE)
 
-HF_HUB_OFFLINE = _is_true(
-    os.environ.get("HF_HUB_OFFLINE") or os.environ.get("TRANSFORMERS_OFFLINE")
-)
+HF_HUB_OFFLINE = _is_true(os.environ.get("HF_HUB_OFFLINE") or os.environ.get("TRANSFORMERS_OFFLINE"))
 
 # If set, log level will be set to DEBUG and all requests made to the Hub will be logged
 # as curl commands for reproducibility.
@@ -166,9 +154,7 @@ if _staging_mode:
     # In staging mode, we use a different cache to ensure we don't mix up production and staging data or tokens
     # In practice in `huggingface_hub` tests, we monkeypatch these values with temporary directories. The following
     # lines are only used in third-party libraries tests (e.g. `transformers`, `diffusers`, etc.).
-    _staging_home = os.path.join(
-        os.path.expanduser("~"), ".cache", "huggingface_staging"
-    )
+    _staging_home = os.path.join(os.path.expanduser("~"), ".cache", "huggingface_staging")
     HUGGINGFACE_HUB_CACHE = os.path.join(_staging_home, "hub")
     HF_TOKEN_PATH = os.path.join(_staging_home, "token")
 
@@ -179,25 +165,17 @@ if _staging_mode:
 # TL;DR: env variable has priority over code
 __HF_HUB_DISABLE_PROGRESS_BARS = os.environ.get("HF_HUB_DISABLE_PROGRESS_BARS")
 HF_HUB_DISABLE_PROGRESS_BARS: Optional[bool] = (
-    _is_true(__HF_HUB_DISABLE_PROGRESS_BARS)
-    if __HF_HUB_DISABLE_PROGRESS_BARS is not None
-    else None
+    _is_true(__HF_HUB_DISABLE_PROGRESS_BARS) if __HF_HUB_DISABLE_PROGRESS_BARS is not None else None
 )
 
 # Disable warning on machines that do not support symlinks (e.g. Windows non-developer)
-HF_HUB_DISABLE_SYMLINKS_WARNING: bool = _is_true(
-    os.environ.get("HF_HUB_DISABLE_SYMLINKS_WARNING")
-)
+HF_HUB_DISABLE_SYMLINKS_WARNING: bool = _is_true(os.environ.get("HF_HUB_DISABLE_SYMLINKS_WARNING"))
 
 # Disable warning when using experimental features
-HF_HUB_DISABLE_EXPERIMENTAL_WARNING: bool = _is_true(
-    os.environ.get("HF_HUB_DISABLE_EXPERIMENTAL_WARNING")
-)
+HF_HUB_DISABLE_EXPERIMENTAL_WARNING: bool = _is_true(os.environ.get("HF_HUB_DISABLE_EXPERIMENTAL_WARNING"))
 
 # Disable sending the cached token by default is all HTTP requests to the Hub
-HF_HUB_DISABLE_IMPLICIT_TOKEN: bool = _is_true(
-    os.environ.get("HF_HUB_DISABLE_IMPLICIT_TOKEN")
-)
+HF_HUB_DISABLE_IMPLICIT_TOKEN: bool = _is_true(os.environ.get("HF_HUB_DISABLE_IMPLICIT_TOKEN"))
 
 # Enable fast-download using external dependency "hf_transfer"
 # See:
@@ -209,19 +187,14 @@ HF_HUB_ENABLE_HF_TRANSFER: bool = _is_tr
 # UNUSED
 # We don't use symlinks in local dir anymore.
 HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD: int = (
-    _as_int(os.environ.get("HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD"))
-    or 5 * 1024 * 1024
+    _as_int(os.environ.get("HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD")) or 5 * 1024 * 1024
 )
 
 # Used to override the etag timeout on a system level
-HF_HUB_ETAG_TIMEOUT: int = (
-    _as_int(os.environ.get("HF_HUB_ETAG_TIMEOUT")) or DEFAULT_ETAG_TIMEOUT
-)
+HF_HUB_ETAG_TIMEOUT: int = _as_int(os.environ.get("HF_HUB_ETAG_TIMEOUT")) or DEFAULT_ETAG_TIMEOUT
 
 # Used to override the get request timeout on a system level
-HF_HUB_DOWNLOAD_TIMEOUT: int = (
-    _as_int(os.environ.get("HF_HUB_DOWNLOAD_TIMEOUT")) or DEFAULT_DOWNLOAD_TIMEOUT
-)
+HF_HUB_DOWNLOAD_TIMEOUT: int = _as_int(os.environ.get("HF_HUB_DOWNLOAD_TIMEOUT")) or DEFAULT_DOWNLOAD_TIMEOUT
 
 # Allows to add information about the requester in the user-agent (eg. partner name)
 HF_HUB_USER_AGENT_ORIGIN: Optional[str] = os.environ.get("HF_HUB_USER_AGENT_ORIGIN")
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/errors.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/errors.py
@@ -67,16 +67,9 @@ class HfHubHTTPError(HTTPError):
     ```
     """
 
-    def __init__(
-        self,
-        message: str,
-        response: Optional[Response] = None,
-        *,
-        server_message: Optional[str] = None
-    ):
+    def __init__(self, message: str, response: Optional[Response] = None, *, server_message: Optional[str] = None):
         self.request_id = (
-            response.headers.get("x-request-id")
-            or response.headers.get("X-Amzn-Trace-Id")
+            response.headers.get("x-request-id") or response.headers.get("X-Amzn-Trace-Id")
             if response is not None
             else None
         )
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/fastai_utils.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/fastai_utils.py
@@ -143,15 +143,11 @@ def _check_fastai_fastcore_pyproject_ver
     # If the package is specified but not the version (e.g. "fastai" instead of "fastai=2.4"), the default versions are the highest.
     fastai_packages = [pck for pck in package_versions if pck.startswith("fastai")]
     if len(fastai_packages) == 0:
-        logger.warning(
-            "The repository does not have a fastai version specified in the `pyproject.toml`."
-        )
+        logger.warning("The repository does not have a fastai version specified in the `pyproject.toml`.")
     # fastai_version is an empty string if not specified
     else:
         fastai_version = str(fastai_packages[0]).partition("=")[2]
-        if fastai_version != "" and version.Version(fastai_version) < version.Version(
-            fastai_min_version
-        ):
+        if fastai_version != "" and version.Version(fastai_version) < version.Version(fastai_min_version):
             raise ImportError(
                 "`from_pretrained_fastai` requires"
                 f" fastai>={fastai_min_version} version but the model to load uses"
@@ -160,15 +156,11 @@ def _check_fastai_fastcore_pyproject_ver
 
     fastcore_packages = [pck for pck in package_versions if pck.startswith("fastcore")]
     if len(fastcore_packages) == 0:
-        logger.warning(
-            "The repository does not have a fastcore version specified in the `pyproject.toml`."
-        )
+        logger.warning("The repository does not have a fastcore version specified in the `pyproject.toml`.")
     # fastcore_version is an empty string if not specified
     else:
         fastcore_version = str(fastcore_packages[0]).partition("=")[2]
-        if fastcore_version != "" and version.Version(
-            fastcore_version
-        ) < version.Version(fastcore_min_version):
+        if fastcore_version != "" and version.Version(fastcore_version) < version.Version(fastcore_min_version):
             raise ImportError(
                 "`from_pretrained_fastai` requires"
                 f" fastcore>={fastcore_min_version} version, but you are using fastcore"
@@ -278,9 +270,7 @@ def _save_pretrained_fastai(
     # if the user provides config then we update it with the fastai and fastcore versions in CONFIG_TEMPLATE.
     if config is not None:
         if not isinstance(config, dict):
-            raise RuntimeError(
-                f"Provided config should be a dict. Got: '{type(config)}'"
-            )
+            raise RuntimeError(f"Provided config should be a dict. Got: '{type(config)}'")
         path = os.path.join(save_directory, constants.CONFIG_NAME)
         with open(path, "w") as f:
             json.dump(config, f)
@@ -416,9 +406,7 @@ def push_to_hub_fastai(
     """
     _check_fastai_fastcore_versions()
     api = HfApi(endpoint=api_endpoint)
-    repo_id = api.create_repo(
-        repo_id=repo_id, token=token, private=private, exist_ok=True
-    ).repo_id
+    repo_id = api.create_repo(repo_id=repo_id, token=token, private=private, exist_ok=True).repo_id
 
     # Push the files to the repo in a single commit
     with SoftTemporaryDirectory() as tmp:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/file_download.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/file_download.py
@@ -363,13 +363,9 @@ def http_get(
     hf_transfer = None
     if constants.HF_HUB_ENABLE_HF_TRANSFER:
         if resume_size != 0:
-            warnings.warn(
-                "'hf_transfer' does not support `resume_size`: falling back to regular download method"
-            )
+            warnings.warn("'hf_transfer' does not support `resume_size`: falling back to regular download method")
         elif proxies is not None:
-            warnings.warn(
-                "'hf_transfer' does not support `proxies`: falling back to regular download method"
-            )
+            warnings.warn("'hf_transfer' does not support `proxies`: falling back to regular download method")
         else:
             try:
                 import hf_transfer  # type: ignore[no-redef]
@@ -438,14 +434,8 @@ def http_get(
     )
 
     with progress_cm as progress:
-        if (
-            hf_transfer
-            and total is not None
-            and total > 5 * constants.DOWNLOAD_CHUNK_SIZE
-        ):
-            supports_callback = (
-                "callback" in inspect.signature(hf_transfer.download).parameters
-            )
+        if hf_transfer and total is not None and total > 5 * constants.DOWNLOAD_CHUNK_SIZE:
+            supports_callback = "callback" in inspect.signature(hf_transfer.download).parameters
             if not supports_callback:
                 warnings.warn(
                     "You are using an outdated version of `hf_transfer`. "
@@ -470,9 +460,7 @@ def http_get(
                 ) from e
             if not supports_callback:
                 progress.update(total)
-            if expected_size is not None and expected_size != os.path.getsize(
-                temp_file.name
-            ):
+            if expected_size is not None and expected_size != os.path.getsize(temp_file.name):
                 raise EnvironmentError(
                     consistency_error_message.format(
                         actual_size=os.path.getsize(temp_file.name),
@@ -624,9 +612,7 @@ def _create_symlink(src: str, dst: str,
             os.symlink(src_rel_or_abs, abs_dst)
             return
         except FileExistsError:
-            if os.path.islink(abs_dst) and os.path.realpath(
-                abs_dst
-            ) == os.path.realpath(abs_src):
+            if os.path.islink(abs_dst) and os.path.realpath(abs_dst) == os.path.realpath(abs_src):
                 # `abs_dst` already exists and is a symlink to the `abs_src` blob. It is most likely that the file has
                 # been cached twice concurrently (exactly between `os.remove` and `os.symlink`). Do nothing.
                 return
@@ -648,9 +634,7 @@ def _create_symlink(src: str, dst: str,
         shutil.copyfile(abs_src, abs_dst)
 
 
-def _cache_commit_hash_for_specific_revision(
-    storage_folder: str, revision: str, commit_hash: str
-) -> None:
+def _cache_commit_hash_for_specific_revision(storage_folder: str, revision: str, commit_hash: str) -> None:
     """Cache reference between a revision (tag, branch or truncated commit hash) and the corresponding commit hash.
 
     Does nothing if `revision` is already a proper `commit_hash` or reference is already cached.
@@ -688,9 +672,7 @@ def _check_disk_space(expected_size: int
     """
 
     target_dir = Path(target_dir)  # format as `Path`
-    for path in [target_dir] + list(
-        target_dir.parents
-    ):  # first check target_dir, then each parents one by one
+    for path in [target_dir] + list(target_dir.parents):  # first check target_dir, then each parents one by one
         try:
             target_dir_free = shutil.disk_usage(path).free
             if target_dir_free < expected_size:
@@ -700,9 +682,7 @@ def _check_disk_space(expected_size: int
                     f"The target location {target_dir} only has {target_dir_free / 1e6:.2f} MB free disk space."
                 )
             return
-        except (
-            OSError
-        ):  # raise on anything: file does not exist or space disk cannot be checked
+        except OSError:  # raise on anything: file does not exist or space disk cannot be checked
             pass
 
 
@@ -865,9 +845,7 @@ def hf_hub_download(
     if repo_type is None:
         repo_type = "model"
     if repo_type not in constants.REPO_TYPES:
-        raise ValueError(
-            f"Invalid repo type: {repo_type}. Accepted repo types are: {str(constants.REPO_TYPES)}"
-        )
+        raise ValueError(f"Invalid repo type: {repo_type}. Accepted repo types are: {str(constants.REPO_TYPES)}")
 
     hf_headers = build_hf_headers(
         token=token,
@@ -951,9 +929,7 @@ def _hf_hub_download_to_cache_dir(
     Method should not be called directly. Please use `hf_hub_download` instead.
     """
     locks_dir = os.path.join(cache_dir, ".locks")
-    storage_folder = os.path.join(
-        cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type)
-    )
+    storage_folder = os.path.join(cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type))
 
     # cross platform transcription of filename, to be used as a local file path.
     relative_filename = os.path.join(*filename.split("/"))
@@ -972,21 +948,19 @@ def _hf_hub_download_to_cache_dir(
 
     # Try to get metadata (etag, commit_hash, url, size) from the server.
     # If we can't, a HEAD request error is returned.
-    (url_to_download, etag, commit_hash, expected_size, head_call_error) = (
-        _get_metadata_or_catch_error(
-            repo_id=repo_id,
-            filename=filename,
-            repo_type=repo_type,
-            revision=revision,
-            endpoint=endpoint,
-            proxies=proxies,
-            etag_timeout=etag_timeout,
-            headers=headers,
-            token=token,
-            local_files_only=local_files_only,
-            storage_folder=storage_folder,
-            relative_filename=relative_filename,
-        )
+    (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(
+        repo_id=repo_id,
+        filename=filename,
+        repo_type=repo_type,
+        revision=revision,
+        endpoint=endpoint,
+        proxies=proxies,
+        etag_timeout=etag_timeout,
+        headers=headers,
+        token=token,
+        local_files_only=local_files_only,
+        storage_folder=storage_folder,
+        relative_filename=relative_filename,
     )
 
     # etag can be None for several reasons:
@@ -1013,9 +987,7 @@ def _hf_hub_download_to_cache_dir(
 
             # Return pointer file if exists
             if commit_hash is not None:
-                pointer_path = _get_pointer_path(
-                    storage_folder, commit_hash, relative_filename
-                )
+                pointer_path = _get_pointer_path(storage_folder, commit_hash, relative_filename)
                 if os.path.exists(pointer_path) and not force_download:
                     return pointer_path
 
@@ -1025,12 +997,8 @@ def _hf_hub_download_to_cache_dir(
     # From now on, etag, commit_hash, url and size are not None.
     assert etag is not None, "etag must have been retrieved from server"
     assert commit_hash is not None, "commit_hash must have been retrieved from server"
-    assert (
-        url_to_download is not None
-    ), "file location must have been retrieved from server"
-    assert (
-        expected_size is not None
-    ), "expected_size must have been retrieved from server"
+    assert url_to_download is not None, "file location must have been retrieved from server"
+    assert expected_size is not None, "expected_size must have been retrieved from server"
     blob_path = os.path.join(storage_folder, "blobs", etag)
     pointer_path = _get_pointer_path(storage_folder, commit_hash, relative_filename)
 
@@ -1129,19 +1097,17 @@ def _hf_hub_download_to_local_dir(
         return str(paths.file_path)
 
     # Local file doesn't exist or commit_hash doesn't match => we need the etag
-    (url_to_download, etag, commit_hash, expected_size, head_call_error) = (
-        _get_metadata_or_catch_error(
-            repo_id=repo_id,
-            filename=filename,
-            repo_type=repo_type,
-            revision=revision,
-            endpoint=endpoint,
-            proxies=proxies,
-            etag_timeout=etag_timeout,
-            headers=headers,
-            token=token,
-            local_files_only=local_files_only,
-        )
+    (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(
+        repo_id=repo_id,
+        filename=filename,
+        repo_type=repo_type,
+        revision=revision,
+        endpoint=endpoint,
+        proxies=proxies,
+        etag_timeout=etag_timeout,
+        headers=headers,
+        token=token,
+        local_files_only=local_files_only,
     )
 
     if head_call_error is not None:
@@ -1157,12 +1123,8 @@ def _hf_hub_download_to_local_dir(
     # From now on, etag, commit_hash, url and size are not None.
     assert etag is not None, "etag must have been retrieved from server"
     assert commit_hash is not None, "commit_hash must have been retrieved from server"
-    assert (
-        url_to_download is not None
-    ), "file location must have been retrieved from server"
-    assert (
-        expected_size is not None
-    ), "expected_size must have been retrieved from server"
+    assert url_to_download is not None, "file location must have been retrieved from server"
+    assert expected_size is not None, "expected_size must have been retrieved from server"
 
     # Local file exists => check if it's up-to-date
     if not force_download and paths.file_path.is_file():
@@ -1229,9 +1191,7 @@ def _hf_hub_download_to_local_dir(
             force_download=force_download,
         )
 
-    write_download_metadata(
-        local_dir=local_dir, filename=filename, commit_hash=commit_hash, etag=etag
-    )
+    write_download_metadata(local_dir=local_dir, filename=filename, commit_hash=commit_hash, etag=etag)
     return str(paths.file_path)
 
 
@@ -1290,9 +1250,7 @@ def try_to_load_from_cache(
     if repo_type is None:
         repo_type = "model"
     if repo_type not in constants.REPO_TYPES:
-        raise ValueError(
-            f"Invalid repo type: {repo_type}. Accepted repo types are: {str(constants.REPO_TYPES)}"
-        )
+        raise ValueError(f"Invalid repo type: {repo_type}. Accepted repo types are: {str(constants.REPO_TYPES)}")
     if cache_dir is None:
         cache_dir = constants.HF_HUB_CACHE
 
@@ -1398,17 +1356,13 @@ def get_hf_file_metadata(
         commit_hash=r.headers.get(constants.HUGGINGFACE_HEADER_X_REPO_COMMIT),
         # We favor a custom header indicating the etag of the linked resource, and
         # we fallback to the regular etag header.
-        etag=_normalize_etag(
-            r.headers.get(constants.HUGGINGFACE_HEADER_X_LINKED_ETAG)
-            or r.headers.get("ETag")
-        ),
+        etag=_normalize_etag(r.headers.get(constants.HUGGINGFACE_HEADER_X_LINKED_ETAG) or r.headers.get("ETag")),
         # Either from response headers (if redirected) or defaults to request url
         # Do not use directly `url`, as `_request_wrapper` might have followed relative
         # redirects.
         location=r.headers.get("Location") or r.request.url,  # type: ignore
         size=_int_or_none(
-            r.headers.get(constants.HUGGINGFACE_HEADER_X_LINKED_SIZE)
-            or r.headers.get("Content-Length")
+            r.headers.get(constants.HUGGINGFACE_HEADER_X_LINKED_SIZE) or r.headers.get("Content-Length")
         ),
     )
 
@@ -1454,9 +1408,7 @@ def _get_metadata_or_catch_error(
             ),
         )
 
-    url = hf_hub_url(
-        repo_id, filename, repo_type=repo_type, revision=revision, endpoint=endpoint
-    )
+    url = hf_hub_url(repo_id, filename, repo_type=repo_type, revision=revision, endpoint=endpoint)
     url_to_download: str = url
     etag: Optional[str] = None
     commit_hash: Optional[str] = None
@@ -1478,16 +1430,9 @@ def _get_metadata_or_catch_error(
             except EntryNotFoundError as http_error:
                 if storage_folder is not None and relative_filename is not None:
                     # Cache the non-existence of the file
-                    commit_hash = http_error.response.headers.get(
-                        constants.HUGGINGFACE_HEADER_X_REPO_COMMIT
-                    )
+                    commit_hash = http_error.response.headers.get(constants.HUGGINGFACE_HEADER_X_REPO_COMMIT)
                     if commit_hash is not None:
-                        no_exist_file_path = (
-                            Path(storage_folder)
-                            / ".no_exist"
-                            / commit_hash
-                            / relative_filename
-                        )
+                        no_exist_file_path = Path(storage_folder) / ".no_exist" / commit_hash / relative_filename
                         try:
                             no_exist_file_path.parent.mkdir(parents=True, exist_ok=True)
                             no_exist_file_path.touch()
@@ -1495,9 +1440,7 @@ def _get_metadata_or_catch_error(
                             logger.error(
                                 f"Could not cache non-existence of file. Will ignore error and continue. Error: {e}"
                             )
-                        _cache_commit_hash_for_specific_revision(
-                            storage_folder, revision, commit_hash
-                        )
+                        _cache_commit_hash_for_specific_revision(storage_folder, revision, commit_hash)
                 raise
 
             # Commit hash must exist
@@ -1520,9 +1463,7 @@ def _get_metadata_or_catch_error(
             # Size must exist
             expected_size = metadata.size
             if expected_size is None:
-                raise FileMetadataError(
-                    "Distant resource does not have a Content-Length."
-                )
+                raise FileMetadataError("Distant resource does not have a Content-Length.")
 
             # In case of a redirect, save an extra redirect on the request.get call,
             # and ensure we download the exact atomic version even if it changed
@@ -1571,24 +1512,16 @@ def _get_metadata_or_catch_error(
     return (url_to_download, etag, commit_hash, expected_size, head_error_call)  # type: ignore [return-value]
 
 
-def _raise_on_head_call_error(
-    head_call_error: Exception, force_download: bool, local_files_only: bool
-) -> NoReturn:
+def _raise_on_head_call_error(head_call_error: Exception, force_download: bool, local_files_only: bool) -> NoReturn:
     """Raise an appropriate error when the HEAD call failed and we cannot locate a local file."""
     # No head call => we cannot force download.
     if force_download:
         if local_files_only:
-            raise ValueError(
-                "Cannot pass 'force_download=True' and 'local_files_only=True' at the same time."
-            )
+            raise ValueError("Cannot pass 'force_download=True' and 'local_files_only=True' at the same time.")
         elif isinstance(head_call_error, OfflineModeIsEnabled):
-            raise ValueError(
-                "Cannot pass 'force_download=True' when offline mode is enabled."
-            ) from head_call_error
+            raise ValueError("Cannot pass 'force_download=True' when offline mode is enabled.") from head_call_error
         else:
-            raise ValueError(
-                "Force download failed due to the above error."
-            ) from head_call_error
+            raise ValueError("Force download failed due to the above error.") from head_call_error
 
     # No head call + couldn't find an appropriate file on disk => raise an error.
     if local_files_only:
@@ -1597,8 +1530,7 @@ def _raise_on_head_call_error(
             " hf.co look-ups and downloads online, set 'local_files_only' to False."
         )
     elif isinstance(head_call_error, (RepositoryNotFoundError, GatedRepoError)) or (
-        isinstance(head_call_error, HfHubHTTPError)
-        and head_call_error.response.status_code == 401
+        isinstance(head_call_error, HfHubHTTPError) and head_call_error.response.status_code == 401
     ):
         # Repo not found or gated => let's raise the actual error
         # Unauthorized => likely a token issue => let's raise the actual error
@@ -1639,9 +1571,7 @@ def _download_to_tmp_and_move(
         # Do nothing if already exists (except if force_download=True)
         return
 
-    if incomplete_path.exists() and (
-        force_download or (constants.HF_HUB_ENABLE_HF_TRANSFER and not proxies)
-    ):
+    if incomplete_path.exists() and (force_download or (constants.HF_HUB_ENABLE_HF_TRANSFER and not proxies)):
         # By default, we will try to resume the download if possible.
         # However, if the user has set `force_download=True` or if `hf_transfer` is enabled, then we should
         # not resume the download => delete the incomplete file.
@@ -1733,16 +1663,11 @@ def _copy_no_matter_what(src: str, dst:
         shutil.copyfile(src, dst)
 
 
-def _get_pointer_path(
-    storage_folder: str, revision: str, relative_filename: str
-) -> str:
+def _get_pointer_path(storage_folder: str, revision: str, relative_filename: str) -> str:
     # Using `os.path.abspath` instead of `Path.resolve()` to avoid resolving symlinks
     snapshot_path = os.path.join(storage_folder, "snapshots")
     pointer_path = os.path.join(snapshot_path, revision, relative_filename)
-    if (
-        Path(os.path.abspath(snapshot_path))
-        not in Path(os.path.abspath(pointer_path)).parents
-    ):
+    if Path(os.path.abspath(snapshot_path)) not in Path(os.path.abspath(pointer_path)).parents:
         raise ValueError(
             "Invalid pointer path: cannot create pointer path in snapshot folder if"
             f" `storage_folder='{storage_folder}'`, `revision='{revision}'` and"
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/hf_api.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/hf_api.py
@@ -227,9 +227,7 @@ _AUTH_CHECK_NO_REPO_ERROR_MESSAGE = (
 logger = logging.get_logger(__name__)
 
 
-def repo_type_and_id_from_hf_id(
-    hf_id: str, hub_url: Optional[str] = None
-) -> Tuple[Optional[str], Optional[str], str]:
+def repo_type_and_id_from_hf_id(hf_id: str, hub_url: Optional[str] = None) -> Tuple[Optional[str], Optional[str], str]:
     """
     Returns the repo type and ID from a huggingface.co URL linking to a
     repository
@@ -260,9 +258,7 @@ def repo_type_and_id_from_hf_id(
     """
     input_hf_id = hf_id
 
-    hub_url = re.sub(
-        r"https?://", "", hub_url if hub_url is not None else constants.ENDPOINT
-    )
+    hub_url = re.sub(r"https?://", "", hub_url if hub_url is not None else constants.ENDPOINT)
     is_hf_url = hub_url in hf_id and "@" not in hf_id
 
     HFFS_PREFIX = "hf://"
@@ -304,9 +300,7 @@ def repo_type_and_id_from_hf_id(
             repo_id = url_segments[0]
             namespace, repo_type = None, None
     else:
-        raise ValueError(
-            f"Unable to retrieve user and repo ID from the passed HF ID: {hf_id}"
-        )
+        raise ValueError(f"Unable to retrieve user and repo ID from the passed HF ID: {hf_id}")
 
     # Check if repo type is known (mapping "spaces" => "space" + empty value => `None`)
     if repo_type in constants.REPO_TYPES_MAPPING:
@@ -571,9 +565,7 @@ class RepoUrl(str):
         super().__init__()
         # Parse URL
         self.endpoint = endpoint or constants.ENDPOINT
-        repo_type, namespace, repo_name = repo_type_and_id_from_hf_id(
-            self, hub_url=self.endpoint
-        )
+        repo_type, namespace, repo_name = repo_type_and_id_from_hf_id(self, hub_url=self.endpoint)
 
         # Populate fields
         self.namespace = namespace
@@ -656,9 +648,7 @@ class RepoFile:
         self.blob_id = kwargs.pop("oid")
         lfs = kwargs.pop("lfs", None)
         if lfs is not None:
-            lfs = BlobLfsInfo(
-                size=lfs["size"], sha256=lfs["oid"], pointer_size=lfs["pointerSize"]
-            )
+            lfs = BlobLfsInfo(size=lfs["size"], sha256=lfs["oid"], pointer_size=lfs["pointerSize"])
         self.lfs = lfs
         last_commit = kwargs.pop("lastCommit", None) or kwargs.pop("last_commit", None)
         if last_commit is not None:
@@ -839,9 +829,7 @@ class ModelInfo:
         self.id = kwargs.pop("id")
         self.author = kwargs.pop("author", None)
         self.sha = kwargs.pop("sha", None)
-        last_modified = kwargs.pop("lastModified", None) or kwargs.pop(
-            "last_modified", None
-        )
+        last_modified = kwargs.pop("lastModified", None) or kwargs.pop("last_modified", None)
         self.last_modified = parse_datetime(last_modified) if last_modified else None
         created_at = kwargs.pop("createdAt", None) or kwargs.pop("created_at", None)
         self.created_at = parse_datetime(created_at) if created_at else None
@@ -869,22 +857,14 @@ class ModelInfo:
 
         card_data = kwargs.pop("cardData", None) or kwargs.pop("card_data", None)
         self.card_data = (
-            ModelCardData(**card_data, ignore_metadata_errors=True)
-            if isinstance(card_data, dict)
-            else card_data
+            ModelCardData(**card_data, ignore_metadata_errors=True) if isinstance(card_data, dict) else card_data
         )
 
         self.widget_data = kwargs.pop("widgetData", None)
-        self.model_index = kwargs.pop("model-index", None) or kwargs.pop(
-            "model_index", None
-        )
+        self.model_index = kwargs.pop("model-index", None) or kwargs.pop("model_index", None)
         self.config = kwargs.pop("config", None)
-        transformers_info = kwargs.pop("transformersInfo", None) or kwargs.pop(
-            "transformers_info", None
-        )
-        self.transformers_info = (
-            TransformersInfo(**transformers_info) if transformers_info else None
-        )
+        transformers_info = kwargs.pop("transformersInfo", None) or kwargs.pop("transformers_info", None)
+        self.transformers_info = TransformersInfo(**transformers_info) if transformers_info else None
         siblings = kwargs.pop("siblings", None)
         self.siblings = (
             [
@@ -998,9 +978,7 @@ class DatasetInfo:
         self.sha = kwargs.pop("sha", None)
         created_at = kwargs.pop("createdAt", None) or kwargs.pop("created_at", None)
         self.created_at = parse_datetime(created_at) if created_at else None
-        last_modified = kwargs.pop("lastModified", None) or kwargs.pop(
-            "last_modified", None
-        )
+        last_modified = kwargs.pop("lastModified", None) or kwargs.pop("last_modified", None)
         self.last_modified = parse_datetime(last_modified) if last_modified else None
         self.private = kwargs.pop("private", None)
         self.gated = kwargs.pop("gated", None)
@@ -1014,9 +992,7 @@ class DatasetInfo:
 
         card_data = kwargs.pop("cardData", None) or kwargs.pop("card_data", None)
         self.card_data = (
-            DatasetCardData(**card_data, ignore_metadata_errors=True)
-            if isinstance(card_data, dict)
-            else card_data
+            DatasetCardData(**card_data, ignore_metadata_errors=True) if isinstance(card_data, dict) else card_data
         )
         siblings = kwargs.pop("siblings", None)
         self.siblings = (
@@ -1129,9 +1105,7 @@ class SpaceInfo:
         self.sha = kwargs.pop("sha", None)
         created_at = kwargs.pop("createdAt", None) or kwargs.pop("created_at", None)
         self.created_at = parse_datetime(created_at) if created_at else None
-        last_modified = kwargs.pop("lastModified", None) or kwargs.pop(
-            "last_modified", None
-        )
+        last_modified = kwargs.pop("lastModified", None) or kwargs.pop("last_modified", None)
         self.last_modified = parse_datetime(last_modified) if last_modified else None
         self.private = kwargs.pop("private", None)
         self.gated = kwargs.pop("gated", None)
@@ -1144,9 +1118,7 @@ class SpaceInfo:
         self.trending_score = kwargs.pop("trendingScore", None)
         card_data = kwargs.pop("cardData", None) or kwargs.pop("card_data", None)
         self.card_data = (
-            SpaceCardData(**card_data, ignore_metadata_errors=True)
-            if isinstance(card_data, dict)
-            else card_data
+            SpaceCardData(**card_data, ignore_metadata_errors=True) if isinstance(card_data, dict) else card_data
         )
         siblings = kwargs.pop("siblings", None)
         self.siblings = (
@@ -1545,26 +1517,18 @@ class PaperInfo:
         paper = kwargs.pop("paper", {})
         self.id = kwargs.pop("id", None) or paper.pop("id", None)
         authors = paper.pop("authors", None) or kwargs.pop("authors", None)
-        self.authors = (
-            [author.pop("name", None) for author in authors] if authors else None
-        )
+        self.authors = [author.pop("name", None) for author in authors] if authors else None
         published_at = paper.pop("publishedAt", None) or kwargs.pop("publishedAt", None)
         self.published_at = parse_datetime(published_at) if published_at else None
         self.title = kwargs.pop("title", None)
         self.source = kwargs.pop("source", None)
         self.summary = paper.pop("summary", None) or kwargs.pop("summary", None)
         self.upvotes = paper.pop("upvotes", None) or kwargs.pop("upvotes", None)
-        self.discussion_id = paper.pop("discussionId", None) or kwargs.pop(
-            "discussionId", None
-        )
+        self.discussion_id = paper.pop("discussionId", None) or kwargs.pop("discussionId", None)
         self.comments = kwargs.pop("numComments", 0)
-        submitted_at = kwargs.pop("publishedAt", None) or kwargs.pop(
-            "submittedOnDailyAt", None
-        )
+        submitted_at = kwargs.pop("publishedAt", None) or kwargs.pop("submittedOnDailyAt", None)
         self.submitted_at = parse_datetime(submitted_at) if submitted_at else None
-        submitted_by = kwargs.pop("submittedBy", None) or kwargs.pop(
-            "submittedOnDailyBy", None
-        )
+        submitted_by = kwargs.pop("submittedBy", None) or kwargs.pop("submittedOnDailyBy", None)
         self.submitted_by = User(**submitted_by) if submitted_by else None
 
         # forward compatibility
@@ -1724,9 +1688,7 @@ class HfApi:
                 )
             elif effective_token == _get_token_from_file():
                 error_message += " The token stored is invalid. Please run `huggingface-cli login` to update it."
-            raise HTTPError(
-                error_message, request=e.request, response=e.response
-            ) from e
+            raise HTTPError(error_message, request=e.request, response=e.response) from e
         return r.json()
 
     @_deprecate_method(
@@ -1924,14 +1886,10 @@ class HfApi:
         ```
         """
         if expand and (full or cardData or fetch_config):
-            raise ValueError(
-                "`expand` cannot be used if `full`, `cardData` or `fetch_config` are passed."
-            )
+            raise ValueError("`expand` cannot be used if `full`, `cardData` or `fetch_config` are passed.")
 
         if emissions_thresholds is not None and cardData is None:
-            raise ValueError(
-                "`emissions_thresholds` were passed without setting `cardData=True`."
-            )
+            raise ValueError("`emissions_thresholds` were passed without setting `cardData=True`.")
 
         path = f"{self.endpoint}/api/models"
         headers = self._build_hf_headers(token=token)
@@ -1979,11 +1937,7 @@ class HfApi:
             params["sort"] = (
                 "lastModified"
                 if sort == "last_modified"
-                else (
-                    "trendingScore"
-                    if sort == "trending_score"
-                    else "createdAt" if sort == "created_at" else sort
-                )
+                else ("trendingScore" if sort == "trending_score" else "createdAt" if sort == "created_at" else sort)
             )
         if direction is not None:
             params["direction"] = direction
@@ -2008,9 +1962,7 @@ class HfApi:
             if "siblings" not in item:
                 item["siblings"] = None
             model_info = ModelInfo(**item)
-            if emissions_thresholds is None or _is_emission_within_threshold(
-                model_info, *emissions_thresholds
-            ):
+            if emissions_thresholds is None or _is_emission_within_threshold(model_info, *emissions_thresholds):
                 yield model_info
 
     @validate_hf_hub_args
@@ -2199,11 +2151,7 @@ class HfApi:
             params["sort"] = (
                 "lastModified"
                 if sort == "last_modified"
-                else (
-                    "trendingScore"
-                    if sort == "trending_score"
-                    else "createdAt" if sort == "created_at" else sort
-                )
+                else ("trendingScore" if sort == "trending_score" else "createdAt" if sort == "created_at" else sort)
             )
         if direction is not None:
             params["direction"] = direction
@@ -2303,11 +2251,7 @@ class HfApi:
             params["sort"] = (
                 "lastModified"
                 if sort == "last_modified"
-                else (
-                    "trendingScore"
-                    if sort == "trending_score"
-                    else "createdAt" if sort == "created_at" else sort
-                )
+                else ("trendingScore" if sort == "trending_score" else "createdAt" if sort == "created_at" else sort)
             )
         if direction is not None:
             params["direction"] = direction
@@ -2457,21 +2401,9 @@ class HfApi:
         return UserLikes(
             user=user,
             total=len(likes),
-            models=[
-                like["repo"]["name"]
-                for like in likes
-                if like["repo"]["type"] == "model"
-            ],
-            datasets=[
-                like["repo"]["name"]
-                for like in likes
-                if like["repo"]["type"] == "dataset"
-            ],
-            spaces=[
-                like["repo"]["name"]
-                for like in likes
-                if like["repo"]["type"] == "space"
-            ],
+            models=[like["repo"]["name"] for like in likes if like["repo"]["type"] == "model"],
+            datasets=[like["repo"]["name"] for like in likes if like["repo"]["type"] == "dataset"],
+            spaces=[like["repo"]["name"] for like in likes if like["repo"]["type"] == "space"],
         )
 
     @validate_hf_hub_args
@@ -2510,9 +2442,7 @@ class HfApi:
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
         path = f"{self.endpoint}/api/{repo_type}s/{repo_id}/likers"
-        for liker in paginate(
-            path, params={}, headers=self._build_hf_headers(token=token)
-        ):
+        for liker in paginate(path, params={}, headers=self._build_hf_headers(token=token)):
             yield User(
                 username=liker["user"],
                 fullname=liker["fullname"],
@@ -2577,17 +2507,13 @@ class HfApi:
         </Tip>
         """
         if expand and (securityStatus or files_metadata):
-            raise ValueError(
-                "`expand` cannot be used if `securityStatus` or `files_metadata` are set."
-            )
+            raise ValueError("`expand` cannot be used if `securityStatus` or `files_metadata` are set.")
 
         headers = self._build_hf_headers(token=token)
         path = (
             f"{self.endpoint}/api/models/{repo_id}"
             if revision is None
-            else (
-                f"{self.endpoint}/api/models/{repo_id}/revision/{quote(revision, safe='')}"
-            )
+            else (f"{self.endpoint}/api/models/{repo_id}/revision/{quote(revision, safe='')}")
         )
         params: Dict = {}
         if securityStatus:
@@ -2661,9 +2587,7 @@ class HfApi:
         path = (
             f"{self.endpoint}/api/datasets/{repo_id}"
             if revision is None
-            else (
-                f"{self.endpoint}/api/datasets/{repo_id}/revision/{quote(revision, safe='')}"
-            )
+            else (f"{self.endpoint}/api/datasets/{repo_id}/revision/{quote(revision, safe='')}")
         )
         params: Dict = {}
         if files_metadata:
@@ -2736,9 +2660,7 @@ class HfApi:
         path = (
             f"{self.endpoint}/api/spaces/{repo_id}"
             if revision is None
-            else (
-                f"{self.endpoint}/api/spaces/{repo_id}/revision/{quote(revision, safe='')}"
-            )
+            else (f"{self.endpoint}/api/spaces/{repo_id}/revision/{quote(revision, safe='')}")
         )
         params: Dict = {}
         if files_metadata:
@@ -2760,9 +2682,7 @@ class HfApi:
         repo_type: Optional[str] = None,
         timeout: Optional[float] = None,
         files_metadata: bool = False,
-        expand: Optional[
-            Union[ExpandModelProperty_T, ExpandDatasetProperty_T, ExpandSpaceProperty_T]
-        ] = None,
+        expand: Optional[Union[ExpandModelProperty_T, ExpandDatasetProperty_T, ExpandSpaceProperty_T]] = None,
         token: Union[bool, str, None] = None,
     ) -> Union[ModelInfo, DatasetInfo, SpaceInfo]:
         """
@@ -2911,9 +2831,7 @@ class HfApi:
             ```
         """
         try:
-            self.repo_info(
-                repo_id=repo_id, revision=revision, repo_type=repo_type, token=token
-            )
+            self.repo_info(repo_id=repo_id, revision=revision, repo_type=repo_type, token=token)
             return True
         except RevisionNotFoundError:
             return False
@@ -3147,27 +3065,17 @@ class HfApi:
             ```
         """
         repo_type = repo_type or constants.REPO_TYPE_MODEL
-        revision = (
-            quote(revision, safe="")
-            if revision is not None
-            else constants.DEFAULT_REVISION
-        )
+        revision = quote(revision, safe="") if revision is not None else constants.DEFAULT_REVISION
         headers = self._build_hf_headers(token=token)
 
-        encoded_path_in_repo = (
-            "/" + quote(path_in_repo, safe="") if path_in_repo else ""
-        )
+        encoded_path_in_repo = "/" + quote(path_in_repo, safe="") if path_in_repo else ""
         tree_url = f"{self.endpoint}/api/{repo_type}s/{repo_id}/tree/{revision}{encoded_path_in_repo}"
         for path_info in paginate(
             path=tree_url,
             headers=headers,
             params={"recursive": recursive, "expand": expand},
         ):
-            yield (
-                RepoFile(**path_info)
-                if path_info["type"] == "file"
-                else RepoFolder(**path_info)
-            )
+            yield (RepoFile(**path_info) if path_info["type"] == "file" else RepoFolder(**path_info))
 
     @validate_hf_hub_args
     def list_repo_refs(
@@ -3230,18 +3138,14 @@ class HfApi:
         data = response.json()
 
         def _format_as_git_ref_info(item: Dict) -> GitRefInfo:
-            return GitRefInfo(
-                name=item["name"], ref=item["ref"], target_commit=item["targetCommit"]
-            )
+            return GitRefInfo(name=item["name"], ref=item["ref"], target_commit=item["targetCommit"])
 
         return GitRefs(
             branches=[_format_as_git_ref_info(item) for item in data["branches"]],
             converts=[_format_as_git_ref_info(item) for item in data["converts"]],
             tags=[_format_as_git_ref_info(item) for item in data["tags"]],
             pull_requests=(
-                [_format_as_git_ref_info(item) for item in data["pullRequests"]]
-                if include_pull_requests
-                else None
+                [_format_as_git_ref_info(item) for item in data["pullRequests"]] if include_pull_requests else None
             ),
         )
 
@@ -3311,11 +3215,7 @@ class HfApi:
                 If revision is not found (error 404) on the repo.
         """
         repo_type = repo_type or constants.REPO_TYPE_MODEL
-        revision = (
-            quote(revision, safe="")
-            if revision is not None
-            else constants.DEFAULT_REVISION
-        )
+        revision = quote(revision, safe="") if revision is not None else constants.DEFAULT_REVISION
 
         # Paginate over results and return the list of commits.
         return [
@@ -3394,11 +3294,7 @@ class HfApi:
         ```
         """
         repo_type = repo_type or constants.REPO_TYPE_MODEL
-        revision = (
-            quote(revision, safe="")
-            if revision is not None
-            else constants.DEFAULT_REVISION
-        )
+        revision = quote(revision, safe="") if revision is not None else constants.DEFAULT_REVISION
         headers = self._build_hf_headers(token=token)
 
         response = get_session().post(
@@ -3412,11 +3308,7 @@ class HfApi:
         hf_raise_for_status(response)
         paths_info = response.json()
         return [
-            (
-                RepoFile(**path_info)
-                if path_info["type"] == "file"
-                else RepoFolder(**path_info)
-            )
+            (RepoFile(**path_info) if path_info["type"] == "file" else RepoFolder(**path_info))
             for path_info in paths_info
         ]
 
@@ -3500,14 +3392,10 @@ class HfApi:
         # Prepare request
         url = f"{self.endpoint}/api/{repo_type}s/{repo_id}/super-squash/{quote(branch, safe='')}"
         headers = self._build_hf_headers(token=token)
-        commit_message = (
-            commit_message or f"Super-squash branch '{branch}' using huggingface_hub"
-        )
+        commit_message = commit_message or f"Super-squash branch '{branch}' using huggingface_hub"
 
         # Super-squash
-        response = get_session().post(
-            url=url, headers=headers, json={"message": commit_message}
-        )
+        response = get_session().post(url=url, headers=headers, json={"message": commit_message})
         hf_raise_for_status(response)
 
     @validate_hf_hub_args
@@ -3592,15 +3480,11 @@ class HfApi:
                     f" of {constants.SPACES_SDK_TYPES} when repo_type is 'space'`"
                 )
             if space_sdk not in constants.SPACES_SDK_TYPES:
-                raise ValueError(
-                    f"Invalid space_sdk. Please choose one of {constants.SPACES_SDK_TYPES}."
-                )
+                raise ValueError(f"Invalid space_sdk. Please choose one of {constants.SPACES_SDK_TYPES}.")
             json["sdk"] = space_sdk
 
         if space_sdk is not None and repo_type != "space":
-            warnings.warn(
-                "Ignoring provided space_sdk because repo_type is not 'space'."
-            )
+            warnings.warn("Ignoring provided space_sdk because repo_type is not 'space'.")
 
         function_args = [
             "space_hardware",
@@ -3627,14 +3511,10 @@ class HfApi:
         if repo_type == "space":
             json.update({k: v for k, v in zip(json_keys, values) if v is not None})
         else:
-            provided_space_args = [
-                key for key, value in zip(function_args, values) if value is not None
-            ]
+            provided_space_args = [key for key, value in zip(function_args, values) if value is not None]
 
             if provided_space_args:
-                warnings.warn(
-                    f"Ignoring provided {', '.join(provided_space_args)} because repo_type is not 'space'."
-                )
+                warnings.warn(f"Ignoring provided {', '.join(provided_space_args)} because repo_type is not 'space'.")
 
         if getattr(self, "_lfsmultipartthresh", None):
             # Testing purposes only.
@@ -3647,20 +3527,14 @@ class HfApi:
         headers = self._build_hf_headers(token=token)
         while True:
             r = get_session().post(path, headers=headers, json=json)
-            if (
-                r.status_code == 409
-                and "Cannot create repo: another conflicting operation is in progress"
-                in r.text
-            ):
+            if r.status_code == 409 and "Cannot create repo: another conflicting operation is in progress" in r.text:
                 # Since https://github.com/huggingface/moon-landing/pull/7272 (private repo), it is not possible to
                 # concurrently create repos on the Hub for a same user. This is rarely an issue, except when running
                 # tests. To avoid any inconvenience, we retry to create the repo for this specific error.
                 # NOTE: This could have being fixed directly in the tests but adding it here should fixed CIs for all
                 # dependent libraries.
                 # NOTE: If a fix is implemented server-side, we should be able to remove this retry mechanism.
-                logger.debug(
-                    "Create repo failed due to a concurrency issue. Retrying..."
-                )
+                logger.debug("Create repo failed due to a concurrency issue. Retrying...")
                 continue
             break
 
@@ -3735,9 +3609,7 @@ class HfApi:
             if not missing_ok:
                 raise
 
-    @_deprecate_method(
-        version="0.32", message="Please use `update_repo_settings` instead."
-    )
+    @_deprecate_method(version="0.32", message="Please use `update_repo_settings` instead.")
     @validate_hf_hub_args
     def update_repo_visibility(
         self,
@@ -3780,9 +3652,7 @@ class HfApi:
         </Tip>
         """
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(
-                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
-            )
+            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL  # default repo type
 
@@ -3842,9 +3712,7 @@ class HfApi:
         """
 
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(
-                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
-            )
+            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL  # default repo type
 
@@ -3860,9 +3728,7 @@ class HfApi:
 
         if gated is not None:
             if gated not in ["auto", "manual", False]:
-                raise ValueError(
-                    f"Invalid gated status, must be one of 'auto', 'manual', or False. Got '{gated}'."
-                )
+                raise ValueError(f"Invalid gated status, must be one of 'auto', 'manual', or False. Got '{gated}'.")
             payload["gated"] = gated
 
         if private is not None:
@@ -3918,14 +3784,10 @@ class HfApi:
         </Tip>
         """
         if len(from_id.split("/")) != 2:
-            raise ValueError(
-                f"Invalid repo_id: {from_id}. It should have a namespace (:namespace:/:repo_name:)"
-            )
+            raise ValueError(f"Invalid repo_id: {from_id}. It should have a namespace (:namespace:/:repo_name:)")
 
         if len(to_id.split("/")) != 2:
-            raise ValueError(
-                f"Invalid repo_id: {to_id}. It should have a namespace (:namespace:/:repo_name:)"
-            )
+            raise ValueError(f"Invalid repo_id: {to_id}. It should have a namespace (:namespace:/:repo_name:)")
 
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL  # Hub won't accept `None`.
@@ -3959,7 +3821,8 @@ class HfApi:
         num_threads: int = 5,
         parent_commit: Optional[str] = None,
         run_as_future: Literal[False] = ...,
-    ) -> CommitInfo: ...
+    ) -> CommitInfo:
+        ...
 
     @overload
     def create_commit(
@@ -3976,7 +3839,8 @@ class HfApi:
         num_threads: int = 5,
         parent_commit: Optional[str] = None,
         run_as_future: Literal[True] = ...,
-    ) -> Future[CommitInfo]: ...
+    ) -> Future[CommitInfo]:
+        ...
 
     @validate_hf_hub_args
     @future_compatible
@@ -4098,9 +3962,7 @@ class HfApi:
                 If repository is not found (error 404): wrong repo_id/repo_type, private
                 but not authenticated or repo does not exist.
         """
-        if parent_commit is not None and not constants.REGEX_COMMIT_OID.fullmatch(
-            parent_commit
-        ):
+        if parent_commit is not None and not constants.REGEX_COMMIT_OID.fullmatch(parent_commit):
             raise ValueError(
                 f"`parent_commit` is not a valid commit OID. It must match the following regex: {constants.REGEX_COMMIT_OID}"
             )
@@ -4108,14 +3970,10 @@ class HfApi:
         if commit_message is None or len(commit_message) == 0:
             raise ValueError("`commit_message` can't be empty, please pass a value.")
 
-        commit_description = (
-            commit_description if commit_description is not None else ""
-        )
+        commit_description = commit_description if commit_description is not None else ""
         repo_type = repo_type if repo_type is not None else constants.REPO_TYPE_MODEL
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(
-                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
-            )
+            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
         unquoted_revision = revision or constants.DEFAULT_REVISION
         revision = quote(unquoted_revision, safe="")
         create_pr = create_pr if create_pr is not None else False
@@ -4191,9 +4049,7 @@ class HfApi:
                 and operation._remote_oid == operation._local_oid
             ):
                 # File already exists on the Hub and has not changed: we can skip it.
-                logger.debug(
-                    f"Skipping upload for '{operation.path_in_repo}' as the file has not changed."
-                )
+                logger.debug(f"Skipping upload for '{operation.path_in_repo}' as the file has not changed.")
                 continue
             if (
                 isinstance(operation, CommitOperationCopy)
@@ -4213,9 +4069,7 @@ class HfApi:
 
         # Return early if empty commit
         if len(operations_without_no_op) == 0:
-            logger.warning(
-                "No files have been modified since last commit. Skipping to prevent empty commit."
-            )
+            logger.warning("No files have been modified since last commit. Skipping to prevent empty commit.")
 
             # Get latest commit info
             try:
@@ -4263,9 +4117,7 @@ class HfApi:
         params = {"create_pr": "1"} if create_pr else None
 
         try:
-            commit_resp = get_session().post(
-                url=commit_url, headers=headers, data=data, params=params
-            )
+            commit_resp = get_session().post(url=commit_url, headers=headers, data=data, params=params)
             hf_raise_for_status(commit_resp, endpoint_name="commit")
         except RepositoryNotFoundError as e:
             e.append_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)
@@ -4378,14 +4230,8 @@ class HfApi:
         """
         repo_type = repo_type if repo_type is not None else constants.REPO_TYPE_MODEL
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(
-                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
-            )
-        revision = (
-            quote(revision, safe="")
-            if revision is not None
-            else constants.DEFAULT_REVISION
-        )
+            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
+        revision = quote(revision, safe="") if revision is not None else constants.DEFAULT_REVISION
         create_pr = create_pr if create_pr is not None else False
         headers = self._build_hf_headers(token=token)
 
@@ -4399,9 +4245,7 @@ class HfApi:
                         break
 
         # Filter out already uploaded files
-        new_additions = [
-            addition for addition in additions if not addition._is_uploaded
-        ]
+        new_additions = [addition for addition in additions if not addition._is_uploaded]
 
         # Check which new files are LFS
         try:
@@ -4420,17 +4264,13 @@ class HfApi:
             raise
 
         # Filter out regular files
-        new_lfs_additions = [
-            addition for addition in new_additions if addition._upload_mode == "lfs"
-        ]
+        new_lfs_additions = [addition for addition in new_additions if addition._upload_mode == "lfs"]
 
         # Filter out files listed in .gitignore
         new_lfs_additions_to_upload = []
         for addition in new_lfs_additions:
             if addition._should_ignore:
-                logger.debug(
-                    f"Skipping upload for LFS file '{addition.path_in_repo}' (ignored by gitignore file)."
-                )
+                logger.debug(f"Skipping upload for LFS file '{addition.path_in_repo}' (ignored by gitignore file).")
             else:
                 new_lfs_additions_to_upload.append(addition)
         if len(new_lfs_additions) != len(new_lfs_additions_to_upload):
@@ -4472,7 +4312,8 @@ class HfApi:
         create_pr: Optional[bool] = None,
         parent_commit: Optional[str] = None,
         run_as_future: Literal[False] = ...,
-    ) -> CommitInfo: ...
+    ) -> CommitInfo:
+        ...
 
     @overload
     def upload_file(
@@ -4489,7 +4330,8 @@ class HfApi:
         create_pr: Optional[bool] = None,
         parent_commit: Optional[str] = None,
         run_as_future: Literal[True] = ...,
-    ) -> Future[CommitInfo]: ...
+    ) -> Future[CommitInfo]:
+        ...
 
     @validate_hf_hub_args
     @future_compatible
@@ -4620,14 +4462,10 @@ class HfApi:
         ```
         """
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(
-                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
-            )
+            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
 
         commit_message = (
-            commit_message
-            if commit_message is not None
-            else f"Upload {path_in_repo} with huggingface_hub"
+            commit_message if commit_message is not None else f"Upload {path_in_repo} with huggingface_hub"
         )
         operation = CommitOperationAdd(
             path_or_fileobj=path_or_fileobj,
@@ -4681,7 +4519,8 @@ class HfApi:
         ignore_patterns: Optional[Union[List[str], str]] = None,
         delete_patterns: Optional[Union[List[str], str]] = None,
         run_as_future: Literal[False] = ...,
-    ) -> CommitInfo: ...
+    ) -> CommitInfo:
+        ...
 
     @overload
     def upload_folder(  # type: ignore
@@ -4701,7 +4540,8 @@ class HfApi:
         ignore_patterns: Optional[Union[List[str], str]] = None,
         delete_patterns: Optional[Union[List[str], str]] = None,
         run_as_future: Literal[True] = ...,
-    ) -> Future[CommitInfo]: ...
+    ) -> Future[CommitInfo]:
+        ...
 
     @validate_hf_hub_args
     @future_compatible
@@ -4866,9 +4706,7 @@ class HfApi:
         ```
         """
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(
-                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
-            )
+            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
 
         # By default, upload folder to the root directory in repo.
         if path_in_repo is None:
@@ -4902,9 +4740,7 @@ class HfApi:
         if len(add_operations) > 0:
             added_paths = set(op.path_in_repo for op in add_operations)
             delete_operations = [
-                delete_op
-                for delete_op in delete_operations
-                if delete_op.path_in_repo not in added_paths
+                delete_op for delete_op in delete_operations if delete_op.path_in_repo not in added_paths
             ]
         commit_operations = delete_operations + add_operations
 
@@ -5013,9 +4849,7 @@ class HfApi:
 
         """
         commit_message = (
-            commit_message
-            if commit_message is not None
-            else f"Delete {path_in_repo} with huggingface_hub"
+            commit_message if commit_message is not None else f"Delete {path_in_repo} with huggingface_hub"
         )
 
         operations = [CommitOperationDelete(path_in_repo=path_in_repo)]
@@ -5098,9 +4932,7 @@ class HfApi:
         )
 
         if commit_message is None:
-            commit_message = (
-                f"Delete files {' '.join(delete_patterns)} with huggingface_hub"
-            )
+            commit_message = f"Delete files {' '.join(delete_patterns)} with huggingface_hub"
 
         return self.create_commit(
             repo_id=repo_id,
@@ -5172,14 +5004,10 @@ class HfApi:
             repo_id=repo_id,
             repo_type=repo_type,
             token=token,
-            operations=[
-                CommitOperationDelete(path_in_repo=path_in_repo, is_folder=True)
-            ],
+            operations=[CommitOperationDelete(path_in_repo=path_in_repo, is_folder=True)],
             revision=revision,
             commit_message=(
-                commit_message
-                if commit_message is not None
-                else f"Delete folder {path_in_repo} with huggingface_hub"
+                commit_message if commit_message is not None else f"Delete folder {path_in_repo} with huggingface_hub"
             ),
             commit_description=commit_description,
             create_pr=create_pr,
@@ -5700,8 +5528,7 @@ class HfApi:
                 metadata=None,
                 sharded=False,
                 weight_map={
-                    tensor_name: constants.SAFETENSORS_SINGLE_FILE
-                    for tensor_name in file_metadata.tensors.keys()
+                    tensor_name: constants.SAFETENSORS_SINGLE_FILE for tensor_name in file_metadata.tensors.keys()
                 },
                 files_metadata={constants.SAFETENSORS_SINGLE_FILE: file_metadata},
             )
@@ -5813,9 +5640,7 @@ class HfApi:
         # We assume fetching 100kb is faster than making 2 GET requests. Therefore we always fetch the first 100kb to
         # avoid the 2nd GET in most cases.
         # See https://github.com/huggingface/huggingface_hub/pull/1855#discussion_r1404286419.
-        response = get_session().get(
-            url, headers={**_headers, "range": "bytes=0-100000"}
-        )
+        response = get_session().get(url, headers={**_headers, "range": "bytes=0-100000"})
         hf_raise_for_status(response)
 
         # 2. Parse metadata size
@@ -5831,9 +5656,7 @@ class HfApi:
         if metadata_size <= 100000:
             metadata_as_bytes = response.content[8 : 8 + metadata_size]
         else:  # 3.b. Request full metadata
-            response = get_session().get(
-                url, headers={**_headers, "range": f"bytes=8-{metadata_size + 7}"}
-            )
+            response = get_session().get(url, headers={**_headers, "range": f"bytes=8-{metadata_size + 7}"})
             hf_raise_for_status(response)
             metadata_as_bytes = response.content
 
@@ -5939,9 +5762,7 @@ class HfApi:
             elif exist_ok and e.response.status_code == 403:
                 # No write permission on the namespace but branch might already exist
                 try:
-                    refs = self.list_repo_refs(
-                        repo_id=repo_id, repo_type=repo_type, token=token
-                    )
+                    refs = self.list_repo_refs(repo_id=repo_id, repo_type=repo_type, token=token)
                     for branch_ref in refs.branches:
                         if branch_ref.name == branch:
                             return  # Branch already exists => do not raise
@@ -6058,11 +5879,7 @@ class HfApi:
         """
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
-        revision = (
-            quote(revision, safe="")
-            if revision is not None
-            else constants.DEFAULT_REVISION
-        )
+        revision = quote(revision, safe="") if revision is not None else constants.DEFAULT_REVISION
 
         # Prepare request
         tag_url = f"{self.endpoint}/api/{repo_type}s/{repo_id}/tag/{revision}"
@@ -6226,25 +6043,13 @@ class HfApi:
             ```
         """
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(
-                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
-            )
+            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
-        if (
-            discussion_type is not None
-            and discussion_type not in constants.DISCUSSION_TYPES
-        ):
-            raise ValueError(
-                f"Invalid discussion_type, must be one of {constants.DISCUSSION_TYPES}"
-            )
-        if (
-            discussion_status is not None
-            and discussion_status not in constants.DISCUSSION_STATUS
-        ):
-            raise ValueError(
-                f"Invalid discussion_status, must be one of {constants.DISCUSSION_STATUS}"
-            )
+        if discussion_type is not None and discussion_type not in constants.DISCUSSION_TYPES:
+            raise ValueError(f"Invalid discussion_type, must be one of {constants.DISCUSSION_TYPES}")
+        if discussion_status is not None and discussion_status not in constants.DISCUSSION_STATUS:
+            raise ValueError(f"Invalid discussion_status, must be one of {constants.DISCUSSION_STATUS}")
 
         headers = self._build_hf_headers(token=token)
         path = f"{self.endpoint}/api/{repo_type}s/{repo_id}/discussions"
@@ -6332,15 +6137,11 @@ class HfApi:
         if not isinstance(discussion_num, int) or discussion_num <= 0:
             raise ValueError("Invalid discussion_num, must be a positive integer")
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(
-                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
-            )
+            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
 
-        path = (
-            f"{self.endpoint}/api/{repo_type}s/{repo_id}/discussions/{discussion_num}"
-        )
+        path = f"{self.endpoint}/api/{repo_type}s/{repo_id}/discussions/{discussion_num}"
         headers = self._build_hf_headers(token=token)
         resp = get_session().get(path, params={"diff": "1"}, headers=headers)
         hf_raise_for_status(resp)
@@ -6348,17 +6149,9 @@ class HfApi:
         discussion_details = resp.json()
         is_pull_request = discussion_details["isPullRequest"]
 
-        target_branch = (
-            discussion_details["changes"]["base"] if is_pull_request else None
-        )
-        conflicting_files = (
-            discussion_details["filesWithConflicts"] if is_pull_request else None
-        )
-        merge_commit_oid = (
-            discussion_details["changes"].get("mergeCommitId", None)
-            if is_pull_request
-            else None
-        )
+        target_branch = discussion_details["changes"]["base"] if is_pull_request else None
+        conflicting_files = discussion_details["filesWithConflicts"] if is_pull_request else None
+        merge_commit_oid = discussion_details["changes"].get("mergeCommitId", None) if is_pull_request else None
 
         return DiscussionWithDetails(
             title=discussion_details["title"],
@@ -6434,9 +6227,7 @@ class HfApi:
 
         </Tip>"""
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(
-                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
-            )
+            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
 
@@ -6546,9 +6337,7 @@ class HfApi:
         if not isinstance(discussion_num, int) or discussion_num <= 0:
             raise ValueError("Invalid discussion_num, must be a positive integer")
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(
-                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
-            )
+            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
         repo_id = f"{repo_type}s/{repo_id}"
@@ -7008,9 +6797,7 @@ class HfApi:
         hf_raise_for_status(r)
 
     @validate_hf_hub_args
-    def delete_space_secret(
-        self, repo_id: str, key: str, *, token: Union[bool, str, None] = None
-    ) -> None:
+    def delete_space_secret(self, repo_id: str, key: str, *, token: Union[bool, str, None] = None) -> None:
         """Deletes a secret from a Space.
 
         Secrets allow to set secret keys or tokens to a Space without hardcoding them.
@@ -7035,9 +6822,7 @@ class HfApi:
         hf_raise_for_status(r)
 
     @validate_hf_hub_args
-    def get_space_variables(
-        self, repo_id: str, *, token: Union[bool, str, None] = None
-    ) -> Dict[str, SpaceVariable]:
+    def get_space_variables(self, repo_id: str, *, token: Union[bool, str, None] = None) -> Dict[str, SpaceVariable]:
         """Gets all variables from a Space.
 
         Variables allow to set environment variables to a Space without hardcoding them.
@@ -7129,9 +6914,7 @@ class HfApi:
         return {k: SpaceVariable(k, v) for k, v in r.json().items()}
 
     @validate_hf_hub_args
-    def get_space_runtime(
-        self, repo_id: str, *, token: Union[bool, str, None] = None
-    ) -> SpaceRuntime:
+    def get_space_runtime(self, repo_id: str, *, token: Union[bool, str, None] = None) -> SpaceRuntime:
         """Gets runtime information about a Space.
 
         Args:
@@ -7257,9 +7040,7 @@ class HfApi:
         return runtime
 
     @validate_hf_hub_args
-    def pause_space(
-        self, repo_id: str, *, token: Union[bool, str, None] = None
-    ) -> SpaceRuntime:
+    def pause_space(self, repo_id: str, *, token: Union[bool, str, None] = None) -> SpaceRuntime:
         """Pause your Space.
 
         A paused Space stops executing until manually restarted by its owner. This is different from the sleeping
@@ -7580,20 +7361,14 @@ class HfApi:
             user = self.whoami(token=token)
 
             # List personal endpoints first
-            endpoints: List[InferenceEndpoint] = list_inference_endpoints(
-                namespace=self._get_namespace(token=token)
-            )
+            endpoints: List[InferenceEndpoint] = list_inference_endpoints(namespace=self._get_namespace(token=token))
 
             # Then list endpoints for all orgs the user belongs to and ignore 401 errors (no billing or no access)
             for org in user.get("orgs", []):
                 try:
-                    endpoints += list_inference_endpoints(
-                        namespace=org["name"], token=token
-                    )
+                    endpoints += list_inference_endpoints(namespace=org["name"], token=token)
                 except HfHubHTTPError as error:
-                    if (
-                        error.response.status_code == 401
-                    ):  # Either no billing or user don't have access)
+                    if error.response.status_code == 401:  # Either no billing or user don't have access)
                         logger.debug(
                             "Cannot list Inference Endpoints for org '%s': %s",
                             org["name"],
@@ -7745,11 +7520,7 @@ class HfApi:
         """
         namespace = namespace or self._get_namespace(token=token)
 
-        image = (
-            {"custom": custom_image}
-            if custom_image is not None
-            else {"huggingface": {}}
-        )
+        image = {"custom": custom_image} if custom_image is not None else {"huggingface": {}}
         payload: Dict = {
             "accountId": account_id,
             "compute": {
@@ -7785,9 +7556,7 @@ class HfApi:
         )
         hf_raise_for_status(response)
 
-        return InferenceEndpoint.from_raw(
-            response.json(), namespace=namespace, token=token
-        )
+        return InferenceEndpoint.from_raw(response.json(), namespace=namespace, token=token)
 
     def get_inference_endpoint(
         self,
@@ -7838,9 +7607,7 @@ class HfApi:
         )
         hf_raise_for_status(response)
 
-        return InferenceEndpoint.from_raw(
-            response.json(), namespace=namespace, token=token
-        )
+        return InferenceEndpoint.from_raw(response.json(), namespace=namespace, token=token)
 
     def update_inference_endpoint(
         self,
@@ -7948,9 +7715,7 @@ class HfApi:
         )
         hf_raise_for_status(response)
 
-        return InferenceEndpoint.from_raw(
-            response.json(), namespace=namespace, token=token
-        )
+        return InferenceEndpoint.from_raw(response.json(), namespace=namespace, token=token)
 
     def delete_inference_endpoint(
         self,
@@ -8021,9 +7786,7 @@ class HfApi:
         )
         hf_raise_for_status(response)
 
-        return InferenceEndpoint.from_raw(
-            response.json(), namespace=namespace, token=token
-        )
+        return InferenceEndpoint.from_raw(response.json(), namespace=namespace, token=token)
 
     def resume_inference_endpoint(
         self,
@@ -8064,20 +7827,12 @@ class HfApi:
             hf_raise_for_status(response)
         except HfHubHTTPError as error:
             # If already running (and it's ok), then fetch current status and return
-            if (
-                running_ok
-                and error.response.status_code == 400
-                and "already running" in error.response.text
-            ):
-                return self.get_inference_endpoint(
-                    name, namespace=namespace, token=token
-                )
+            if running_ok and error.response.status_code == 400 and "already running" in error.response.text:
+                return self.get_inference_endpoint(name, namespace=namespace, token=token)
             # Otherwise, raise the error
             raise
 
-        return InferenceEndpoint.from_raw(
-            response.json(), namespace=namespace, token=token
-        )
+        return InferenceEndpoint.from_raw(response.json(), namespace=namespace, token=token)
 
     def scale_to_zero_inference_endpoint(
         self,
@@ -8116,9 +7871,7 @@ class HfApi:
         )
         hf_raise_for_status(response)
 
-        return InferenceEndpoint.from_raw(
-            response.json(), namespace=namespace, token=token
-        )
+        return InferenceEndpoint.from_raw(response.json(), namespace=namespace, token=token)
 
     def _get_namespace(self, token: Union[bool, str, None] = None) -> str:
         """Get the default namespace for the current user."""
@@ -8193,9 +7946,7 @@ class HfApi:
         for position, collection_data in enumerate(items):
             yield Collection(position=position, **collection_data)
 
-    def get_collection(
-        self, collection_slug: str, *, token: Union[bool, str, None] = None
-    ) -> Collection:
+    def get_collection(self, collection_slug: str, *, token: Union[bool, str, None] = None) -> Collection:
         """Gets information about a Collection on the Hub.
 
         Args:
@@ -8676,9 +8427,7 @@ class HfApi:
         >>> accept_access_request("meta-llama/Llama-2-7b", "clem")
         ```
         """
-        return self._list_access_requests(
-            repo_id, "pending", repo_type=repo_type, token=token
-        )
+        return self._list_access_requests(repo_id, "pending", repo_type=repo_type, token=token)
 
     @validate_hf_hub_args
     def list_accepted_access_requests(
@@ -8744,9 +8493,7 @@ class HfApi:
         ]
         ```
         """
-        return self._list_access_requests(
-            repo_id, "accepted", repo_type=repo_type, token=token
-        )
+        return self._list_access_requests(repo_id, "accepted", repo_type=repo_type, token=token)
 
     @validate_hf_hub_args
     def list_rejected_access_requests(
@@ -8812,9 +8559,7 @@ class HfApi:
         ]
         ```
         """
-        return self._list_access_requests(
-            repo_id, "rejected", repo_type=repo_type, token=token
-        )
+        return self._list_access_requests(repo_id, "rejected", repo_type=repo_type, token=token)
 
     def _list_access_requests(
         self,
@@ -8824,9 +8569,7 @@ class HfApi:
         token: Union[bool, str, None] = None,
     ) -> List[AccessRequest]:
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(
-                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
-            )
+            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
 
@@ -8890,9 +8633,7 @@ class HfApi:
             [`HTTPError`](https://requests.readthedocs.io/en/latest/api/#requests.HTTPError):
                 HTTP 404 if the user access request is already in the pending list.
         """
-        self._handle_access_request(
-            repo_id, user, "pending", repo_type=repo_type, token=token
-        )
+        self._handle_access_request(repo_id, user, "pending", repo_type=repo_type, token=token)
 
     @validate_hf_hub_args
     def accept_access_request(
@@ -8939,9 +8680,7 @@ class HfApi:
             [`HTTPError`](https://requests.readthedocs.io/en/latest/api/#requests.HTTPError):
                 HTTP 404 if the user access request is already in the accepted list.
         """
-        self._handle_access_request(
-            repo_id, user, "accepted", repo_type=repo_type, token=token
-        )
+        self._handle_access_request(repo_id, user, "accepted", repo_type=repo_type, token=token)
 
     @validate_hf_hub_args
     def reject_access_request(
@@ -9011,9 +8750,7 @@ class HfApi:
         token: Union[bool, str, None] = None,
     ) -> None:
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(
-                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
-            )
+            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
 
@@ -9021,9 +8758,7 @@ class HfApi:
 
         if rejection_reason is not None:
             if status != "rejected":
-                raise ValueError(
-                    "`rejection_reason` can only be passed when rejecting an access request."
-                )
+                raise ValueError("`rejection_reason` can only be passed when rejecting an access request.")
             payload["rejectionReason"] = rejection_reason
 
         response = get_session().post(
@@ -9077,9 +8812,7 @@ class HfApi:
                 HTTP 404 if the user does not exist on the Hub.
         """
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(
-                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
-            )
+            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
 
@@ -9096,9 +8829,7 @@ class HfApi:
     ###################
 
     @validate_hf_hub_args
-    def get_webhook(
-        self, webhook_id: str, *, token: Union[bool, str, None] = None
-    ) -> WebhookInfo:
+    def get_webhook(self, webhook_id: str, *, token: Union[bool, str, None] = None) -> WebhookInfo:
         """Get a webhook by its id.
 
         Args:
@@ -9135,10 +8866,7 @@ class HfApi:
         hf_raise_for_status(response)
         webhook_data = response.json()["webhook"]
 
-        watched_items = [
-            WebhookWatchedItem(type=item["type"], name=item["name"])
-            for item in webhook_data["watched"]
-        ]
+        watched_items = [WebhookWatchedItem(type=item["type"], name=item["name"]) for item in webhook_data["watched"]]
 
         webhook = WebhookInfo(
             id=webhook_data["id"],
@@ -9152,9 +8880,7 @@ class HfApi:
         return webhook
 
     @validate_hf_hub_args
-    def list_webhooks(
-        self, *, token: Union[bool, str, None] = None
-    ) -> List[WebhookInfo]:
+    def list_webhooks(self, *, token: Union[bool, str, None] = None) -> List[WebhookInfo]:
         """List all configured webhooks.
 
         Args:
@@ -9195,10 +8921,7 @@ class HfApi:
             WebhookInfo(
                 id=webhook["id"],
                 url=webhook["url"],
-                watched=[
-                    WebhookWatchedItem(type=item["type"], name=item["name"])
-                    for item in webhook["watched"]
-                ],
+                watched=[WebhookWatchedItem(type=item["type"], name=item["name"]) for item in webhook["watched"]],
                 domains=webhook["domains"],
                 secret=webhook.get("secret"),
                 disabled=webhook["disabled"],
@@ -9257,10 +8980,7 @@ class HfApi:
             )
             ```
         """
-        watched_dicts = [
-            asdict(item) if isinstance(item, WebhookWatchedItem) else item
-            for item in watched
-        ]
+        watched_dicts = [asdict(item) if isinstance(item, WebhookWatchedItem) else item for item in watched]
 
         response = get_session().post(
             f"{constants.ENDPOINT}/api/settings/webhooks",
@@ -9274,10 +8994,7 @@ class HfApi:
         )
         hf_raise_for_status(response)
         webhook_data = response.json()["webhook"]
-        watched_items = [
-            WebhookWatchedItem(type=item["type"], name=item["name"])
-            for item in webhook_data["watched"]
-        ]
+        watched_items = [WebhookWatchedItem(type=item["type"], name=item["name"]) for item in webhook_data["watched"]]
 
         webhook = WebhookInfo(
             id=webhook_data["id"],
@@ -9346,10 +9063,7 @@ class HfApi:
         """
         if watched is None:
             watched = []
-        watched_dicts = [
-            asdict(item) if isinstance(item, WebhookWatchedItem) else item
-            for item in watched
-        ]
+        watched_dicts = [asdict(item) if isinstance(item, WebhookWatchedItem) else item for item in watched]
 
         response = get_session().post(
             f"{constants.ENDPOINT}/api/settings/webhooks/{webhook_id}",
@@ -9364,10 +9078,7 @@ class HfApi:
         hf_raise_for_status(response)
         webhook_data = response.json()["webhook"]
 
-        watched_items = [
-            WebhookWatchedItem(type=item["type"], name=item["name"])
-            for item in webhook_data["watched"]
-        ]
+        watched_items = [WebhookWatchedItem(type=item["type"], name=item["name"]) for item in webhook_data["watched"]]
 
         webhook = WebhookInfo(
             id=webhook_data["id"],
@@ -9381,9 +9092,7 @@ class HfApi:
         return webhook
 
     @validate_hf_hub_args
-    def enable_webhook(
-        self, webhook_id: str, *, token: Union[bool, str, None] = None
-    ) -> WebhookInfo:
+    def enable_webhook(self, webhook_id: str, *, token: Union[bool, str, None] = None) -> WebhookInfo:
         """Enable a webhook (makes it "active").
 
         Args:
@@ -9420,10 +9129,7 @@ class HfApi:
         hf_raise_for_status(response)
         webhook_data = response.json()["webhook"]
 
-        watched_items = [
-            WebhookWatchedItem(type=item["type"], name=item["name"])
-            for item in webhook_data["watched"]
-        ]
+        watched_items = [WebhookWatchedItem(type=item["type"], name=item["name"]) for item in webhook_data["watched"]]
 
         webhook = WebhookInfo(
             id=webhook_data["id"],
@@ -9437,9 +9143,7 @@ class HfApi:
         return webhook
 
     @validate_hf_hub_args
-    def disable_webhook(
-        self, webhook_id: str, *, token: Union[bool, str, None] = None
-    ) -> WebhookInfo:
+    def disable_webhook(self, webhook_id: str, *, token: Union[bool, str, None] = None) -> WebhookInfo:
         """Disable a webhook (makes it "disabled").
 
         Args:
@@ -9476,10 +9180,7 @@ class HfApi:
         hf_raise_for_status(response)
         webhook_data = response.json()["webhook"]
 
-        watched_items = [
-            WebhookWatchedItem(type=item["type"], name=item["name"])
-            for item in webhook_data["watched"]
-        ]
+        watched_items = [WebhookWatchedItem(type=item["type"], name=item["name"]) for item in webhook_data["watched"]]
 
         webhook = WebhookInfo(
             id=webhook_data["id"],
@@ -9493,9 +9194,7 @@ class HfApi:
         return webhook
 
     @validate_hf_hub_args
-    def delete_webhook(
-        self, webhook_id: str, *, token: Union[bool, str, None] = None
-    ) -> None:
+    def delete_webhook(self, webhook_id: str, *, token: Union[bool, str, None] = None) -> None:
         """Delete a webhook.
 
         Args:
@@ -9569,29 +9268,21 @@ class HfApi:
             return []
 
         # List remote files
-        filenames = self.list_repo_files(
-            repo_id=repo_id, revision=revision, repo_type=repo_type, token=token
-        )
+        filenames = self.list_repo_files(repo_id=repo_id, revision=revision, repo_type=repo_type, token=token)
 
         # Compute relative path in repo
         if path_in_repo and path_in_repo not in (".", "./"):
             path_in_repo = path_in_repo.strip("/") + "/"  # harmonize
             relpath_to_abspath = {
-                file[len(path_in_repo) :]: file
-                for file in filenames
-                if file.startswith(path_in_repo)
+                file[len(path_in_repo) :]: file for file in filenames if file.startswith(path_in_repo)
             }
         else:
             relpath_to_abspath = {file: file for file in filenames}
 
         # Apply filter on relative paths and return
         return [
-            CommitOperationDelete(
-                path_in_repo=relpath_to_abspath[relpath], is_folder=False
-            )
-            for relpath in filter_repo_objects(
-                relpath_to_abspath.keys(), allow_patterns=delete_patterns
-            )
+            CommitOperationDelete(path_in_repo=relpath_to_abspath[relpath], is_folder=False)
+            for relpath in filter_repo_objects(relpath_to_abspath.keys(), allow_patterns=delete_patterns)
             if relpath_to_abspath[relpath] != ".gitattributes"
         ]
 
@@ -9697,16 +9388,9 @@ class HfApi:
         )
         # Handle warnings (example: empty metadata)
         response_content = response.json()
-        message = "\n".join(
-            [
-                f"- {warning.get('message')}"
-                for warning in response_content.get("warnings", [])
-            ]
-        )
+        message = "\n".join([f"- {warning.get('message')}" for warning in response_content.get("warnings", [])])
         if message:
-            warnings.warn(
-                f"Warnings while validating metadata in README.md:\n{message}"
-            )
+            warnings.warn(f"Warnings while validating metadata in README.md:\n{message}")
 
         # Raise on errors
         try:
@@ -9716,9 +9400,7 @@ class HfApi:
             message = "\n".join([f"- {error.get('message')}" for error in errors])
             raise ValueError(f"Invalid metadata in README.md.\n{message}") from e
 
-    def get_user_overview(
-        self, username: str, token: Union[bool, str, None] = None
-    ) -> User:
+    def get_user_overview(self, username: str, token: Union[bool, str, None] = None) -> User:
         """
         Get an overview of a user on the Hub.
 
@@ -9745,9 +9427,7 @@ class HfApi:
         hf_raise_for_status(r)
         return User(**r.json())
 
-    def list_organization_members(
-        self, organization: str, token: Union[bool, str, None] = None
-    ) -> Iterable[User]:
+    def list_organization_members(self, organization: str, token: Union[bool, str, None] = None) -> Iterable[User]:
         """
         List of members of an organization on the Hub.
 
@@ -9775,9 +9455,7 @@ class HfApi:
         ):
             yield User(**member)
 
-    def list_user_followers(
-        self, username: str, token: Union[bool, str, None] = None
-    ) -> Iterable[User]:
+    def list_user_followers(self, username: str, token: Union[bool, str, None] = None) -> Iterable[User]:
         """
         Get the list of followers of a user on the Hub.
 
@@ -9805,9 +9483,7 @@ class HfApi:
         ):
             yield User(**follower)
 
-    def list_user_following(
-        self, username: str, token: Union[bool, str, None] = None
-    ) -> Iterable[User]:
+    def list_user_following(self, username: str, token: Union[bool, str, None] = None) -> Iterable[User]:
         """
         Get the list of users followed by a user on the Hub.
 
@@ -9964,9 +9640,7 @@ class HfApi:
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(
-                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
-            )
+            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
         path = f"{self.endpoint}/api/{repo_type}s/{repo_id}/auth-check"
         r = get_session().get(path, headers=headers)
         hf_raise_for_status(r)
@@ -9983,9 +9657,7 @@ def _parse_revision_from_pr_url(pr_url:
     """
     re_match = re.match(_REGEX_DISCUSSION_URL, pr_url)
     if re_match is None:
-        raise RuntimeError(
-            f"Unexpected response from the hub, expected a Pull Request URL but got: '{pr_url}'"
-        )
+        raise RuntimeError(f"Unexpected response from the hub, expected a Pull Request URL but got: '{pr_url}'")
     return f"refs/pr/{re_match[1]}"
 
 
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/hf_file_system.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/hf_file_system.py
@@ -46,15 +46,11 @@ class HfFileSystemResolvedPath:
     _raw_revision: Optional[str] = field(default=None, repr=False)
 
     def unresolve(self) -> str:
-        repo_path = (
-            constants.REPO_TYPES_URL_PREFIXES.get(self.repo_type, "") + self.repo_id
-        )
+        repo_path = constants.REPO_TYPES_URL_PREFIXES.get(self.repo_type, "") + self.repo_id
         if self._raw_revision:
             return f"{repo_path}@{self._raw_revision}/{self.path_in_repo}".rstrip("/")
         elif self.revision != constants.DEFAULT_REVISION:
-            return f"{repo_path}@{safe_revision(self.revision)}/{self.path_in_repo}".rstrip(
-                "/"
-            )
+            return f"{repo_path}@{safe_revision(self.revision)}/{self.path_in_repo}".rstrip("/")
         else:
             return f"{repo_path}/{self.path_in_repo}".rstrip("/")
 
@@ -161,9 +157,7 @@ class HfFileSystem(fsspec.AbstractFileSy
                 )
         return self._repo_and_revision_exists_cache[(repo_type, repo_id, revision)]
 
-    def resolve_path(
-        self, path: str, revision: Optional[str] = None
-    ) -> HfFileSystemResolvedPath:
+    def resolve_path(self, path: str, revision: Optional[str] = None) -> HfFileSystemResolvedPath:
         """
         Resolve a Hugging Face file system path into its components.
 
@@ -199,15 +193,11 @@ class HfFileSystem(fsspec.AbstractFileSy
         path = self._strip_protocol(path)
         if not path:
             # can't list repositories at root
-            raise NotImplementedError(
-                "Access to repositories lists is not implemented."
-            )
+            raise NotImplementedError("Access to repositories lists is not implemented.")
         elif path.split("/")[0] + "/" in constants.REPO_TYPES_URL_PREFIXES.values():
             if "/" not in path:
                 # can't list repositories at the repository type level
-                raise NotImplementedError(
-                    "Access to repositories lists is not implemented."
-                )
+                raise NotImplementedError("Access to repositories lists is not implemented.")
             repo_type, path = path.split("/", 1)
             repo_type = constants.REPO_TYPES_MAPPING[repo_type]
         else:
@@ -219,20 +209,14 @@ class HfFileSystem(fsspec.AbstractFileSy
                     match = SPECIAL_REFS_REVISION_REGEX.search(revision_in_path)
                     if match is not None and revision in (None, match.group()):
                         # Handle `refs/convert/parquet` and PR revisions separately
-                        path_in_repo = SPECIAL_REFS_REVISION_REGEX.sub(
-                            "", revision_in_path
-                        ).lstrip("/")
+                        path_in_repo = SPECIAL_REFS_REVISION_REGEX.sub("", revision_in_path).lstrip("/")
                         revision_in_path = match.group()
                     else:
                         revision_in_path, path_in_repo = revision_in_path.split("/", 1)
                 else:
                     path_in_repo = ""
-                revision = _align_revision_in_path_with_revision(
-                    unquote(revision_in_path), revision
-                )
-                repo_and_revision_exist, err = self._repo_and_revision_exist(
-                    repo_type, repo_id, revision
-                )
+                revision = _align_revision_in_path_with_revision(unquote(revision_in_path), revision)
+                repo_and_revision_exist, err = self._repo_and_revision_exist(repo_type, repo_id, revision)
                 if not repo_and_revision_exist:
                     _raise_file_not_found(path, err)
             else:
@@ -243,16 +227,12 @@ class HfFileSystem(fsspec.AbstractFileSy
                 path_in_repo_without_namespace = "/".join(path.split("/")[1:])
                 repo_id = repo_id_with_namespace
                 path_in_repo = path_in_repo_with_namespace
-                repo_and_revision_exist, err = self._repo_and_revision_exist(
-                    repo_type, repo_id, revision
-                )
+                repo_and_revision_exist, err = self._repo_and_revision_exist(repo_type, repo_id, revision)
                 if not repo_and_revision_exist:
                     if isinstance(err, (RepositoryNotFoundError, HFValidationError)):
                         repo_id = repo_id_without_namespace
                         path_in_repo = path_in_repo_without_namespace
-                        repo_and_revision_exist, _ = self._repo_and_revision_exist(
-                            repo_type, repo_id, revision
-                        )
+                        repo_and_revision_exist, _ = self._repo_and_revision_exist(repo_type, repo_id, revision)
                         if not repo_and_revision_exist:
                             _raise_file_not_found(path, err)
                     else:
@@ -262,23 +242,15 @@ class HfFileSystem(fsspec.AbstractFileSy
             path_in_repo = ""
             if "@" in path:
                 repo_id, revision_in_path = path.split("@", 1)
-                revision = _align_revision_in_path_with_revision(
-                    unquote(revision_in_path), revision
-                )
+                revision = _align_revision_in_path_with_revision(unquote(revision_in_path), revision)
             else:
                 revision_in_path = None
-            repo_and_revision_exist, _ = self._repo_and_revision_exist(
-                repo_type, repo_id, revision
-            )
+            repo_and_revision_exist, _ = self._repo_and_revision_exist(repo_type, repo_id, revision)
             if not repo_and_revision_exist:
-                raise NotImplementedError(
-                    "Access to repositories lists is not implemented."
-                )
+                raise NotImplementedError("Access to repositories lists is not implemented.")
 
         revision = revision if revision is not None else constants.DEFAULT_REVISION
-        return HfFileSystemResolvedPath(
-            repo_type, repo_id, revision, path_in_repo, _raw_revision=revision_in_path
-        )
+        return HfFileSystemResolvedPath(repo_type, repo_id, revision, path_in_repo, _raw_revision=revision_in_path)
 
     def invalidate_cache(self, path: Optional[str] = None) -> None:
         """
@@ -303,9 +275,7 @@ class HfFileSystem(fsspec.AbstractFileSy
 
             # Only clear repo cache if path is to repo root
             if not resolved_path.path_in_repo:
-                self._repo_and_revision_exists_cache.pop(
-                    (resolved_path.repo_type, resolved_path.repo_id, None), None
-                )
+                self._repo_and_revision_exists_cache.pop((resolved_path.repo_type, resolved_path.repo_id, None), None)
                 self._repo_and_revision_exists_cache.pop(
                     (
                         resolved_path.repo_type,
@@ -388,18 +358,9 @@ class HfFileSystem(fsspec.AbstractFileSy
 
         """
         resolved_path = self.resolve_path(path, revision=revision)
-        paths = self.expand_path(
-            path, recursive=recursive, maxdepth=maxdepth, revision=revision
-        )
-        paths_in_repo = [
-            self.resolve_path(path).path_in_repo
-            for path in paths
-            if not self.isdir(path)
-        ]
-        operations = [
-            CommitOperationDelete(path_in_repo=path_in_repo)
-            for path_in_repo in paths_in_repo
-        ]
+        paths = self.expand_path(path, recursive=recursive, maxdepth=maxdepth, revision=revision)
+        paths_in_repo = [self.resolve_path(path).path_in_repo for path in paths if not self.isdir(path)]
+        operations = [CommitOperationDelete(path_in_repo=path_in_repo) for path_in_repo in paths_in_repo]
         commit_message = f"Delete {path} "
         commit_message += "recursively " if recursive else ""
         commit_message += f"up to depth {maxdepth} " if maxdepth is not None else ""
@@ -458,9 +419,7 @@ class HfFileSystem(fsspec.AbstractFileSy
             # Path could be a file
             if not resolved_path.path_in_repo:
                 _raise_file_not_found(path, None)
-            out = self._ls_tree(
-                self._parent(path), refresh=refresh, revision=revision, **kwargs
-            )
+            out = self._ls_tree(self._parent(path), refresh=refresh, revision=revision, **kwargs)
             out = [o for o in out if o["name"] == path]
             if len(out) == 0:
                 _raise_file_not_found(path, None)
@@ -493,11 +452,7 @@ class HfFileSystem(fsspec.AbstractFileSy
                 # Use BFS to traverse the cache and build the "recursive "output
                 # (The Hub uses a so-called "tree first" strategy for the tree endpoint but we sort the output to follow the spec so the result is (eventually) the same)
                 dirs_to_visit = deque(
-                    [
-                        path_info
-                        for path_info in cached_path_infos
-                        if path_info["type"] == "directory"
-                    ]
+                    [path_info for path_info in cached_path_infos if path_info["type"] == "directory"]
                 )
                 while dirs_to_visit:
                     dir_info = dirs_to_visit.popleft()
@@ -507,28 +462,18 @@ class HfFileSystem(fsspec.AbstractFileSy
                         cached_path_infos = self.dircache[dir_info["name"]]
                         out.extend(cached_path_infos)
                         dirs_to_visit.extend(
-                            [
-                                path_info
-                                for path_info in cached_path_infos
-                                if path_info["type"] == "directory"
-                            ]
+                            [path_info for path_info in cached_path_infos if path_info["type"] == "directory"]
                         )
 
             dirs_not_expanded = []
             if expand_info:
                 # Check if there are directories with non-expanded entries
-                dirs_not_expanded = [
-                    self._parent(o["name"]) for o in out if o["last_commit"] is None
-                ]
-
-            if (recursive and dirs_not_in_dircache) or (
-                expand_info and dirs_not_expanded
-            ):
+                dirs_not_expanded = [self._parent(o["name"]) for o in out if o["last_commit"] is None]
+
+            if (recursive and dirs_not_in_dircache) or (expand_info and dirs_not_expanded):
                 # If the dircache is incomplete, find the common path of the missing and non-expanded entries
                 # and extend the output with the result of `_ls_tree(common_path, recursive=True)`
-                common_prefix = os.path.commonprefix(
-                    dirs_not_in_dircache + dirs_not_expanded
-                )
+                common_prefix = os.path.commonprefix(dirs_not_in_dircache + dirs_not_expanded)
                 # Get the parent directory if the common prefix itself is not a directory
                 common_path = (
                     common_prefix.rstrip("/")
@@ -584,9 +529,7 @@ class HfFileSystem(fsspec.AbstractFileSy
                 out.append(cache_path_info)
         return out
 
-    def walk(
-        self, path: str, *args, **kwargs
-    ) -> Iterator[Tuple[str, List[str], List[str]]]:
+    def walk(self, path: str, *args, **kwargs) -> Iterator[Tuple[str, List[str], List[str]]]:
         """
         Return all files below the given path.
 
@@ -695,9 +638,7 @@ class HfFileSystem(fsspec.AbstractFileSy
         else:
             return {name: out[name] for name in names}
 
-    def cp_file(
-        self, path1: str, path2: str, revision: Optional[str] = None, **kwargs
-    ) -> None:
+    def cp_file(self, path1: str, path2: str, revision: Optional[str] = None, **kwargs) -> None:
         """
         Copy a file within or between repositories.
 
@@ -720,8 +661,7 @@ class HfFileSystem(fsspec.AbstractFileSy
         resolved_path2 = self.resolve_path(path2, revision=revision)
 
         same_repo = (
-            resolved_path1.repo_type == resolved_path2.repo_type
-            and resolved_path1.repo_id == resolved_path2.repo_id
+            resolved_path1.repo_type == resolved_path2.repo_type and resolved_path1.repo_id == resolved_path2.repo_id
         )
 
         if same_repo:
@@ -773,9 +713,7 @@ class HfFileSystem(fsspec.AbstractFileSy
         info = self.info(path, **kwargs)
         return info["last_commit"]["date"]
 
-    def info(
-        self, path: str, refresh: bool = False, revision: Optional[str] = None, **kwargs
-    ) -> Dict[str, Any]:
+    def info(self, path: str, refresh: bool = False, revision: Optional[str] = None, **kwargs) -> Dict[str, Any]:
         """
         Get information about a file or directory.
 
@@ -838,11 +776,7 @@ class HfFileSystem(fsspec.AbstractFileSy
                 if not out1:
                     _raise_file_not_found(path, None)
                 out = out1[0]
-            if (
-                refresh
-                or out is None
-                or (expand_info and out and out["last_commit"] is None)
-            ):
+            if refresh or out is None or (expand_info and out and out["last_commit"] is None):
                 paths_info = self._api.get_paths_info(
                     resolved_path.repo_id,
                     resolved_path.path_in_repo,
@@ -970,9 +904,7 @@ class HfFileSystem(fsspec.AbstractFileSy
             url = url.replace("/resolve/", "/tree/", 1)
         return url
 
-    def get_file(
-        self, rpath, lpath, callback=_DEFAULT_CALLBACK, outfile=None, **kwargs
-    ) -> None:
+    def get_file(self, rpath, lpath, callback=_DEFAULT_CALLBACK, outfile=None, **kwargs) -> None:
         """
         Copy single remote file to local.
 
@@ -995,15 +927,10 @@ class HfFileSystem(fsspec.AbstractFileSy
         """
         revision = kwargs.get("revision")
         unhandled_kwargs = set(kwargs.keys()) - {"revision"}
-        if (
-            not isinstance(callback, (NoOpCallback, TqdmCallback))
-            or len(unhandled_kwargs) > 0
-        ):
+        if not isinstance(callback, (NoOpCallback, TqdmCallback)) or len(unhandled_kwargs) > 0:
             # for now, let's not handle custom callbacks
             # and let's not handle custom kwargs
-            return super().get_file(
-                rpath, lpath, callback=callback, outfile=outfile, **kwargs
-            )
+            return super().get_file(rpath, lpath, callback=callback, outfile=outfile, **kwargs)
 
         # Taken from https://github.com/fsspec/filesystem_spec/blob/47b445ae4c284a82dd15e0287b1ffc410e8fc470/fsspec/spec.py#L883
         if isfilelike(lpath):
@@ -1012,9 +939,7 @@ class HfFileSystem(fsspec.AbstractFileSy
             os.makedirs(lpath, exist_ok=True)
             return None
 
-        if isinstance(
-            lpath, (str, Path)
-        ):  # otherwise, let's assume it's a file-like object
+        if isinstance(lpath, (str, Path)):  # otherwise, let's assume it's a file-like object
             os.makedirs(os.path.dirname(lpath), exist_ok=True)
 
         # Open file if not already open
@@ -1069,9 +994,7 @@ class HfFileSystem(fsspec.AbstractFileSy
 
 
 class HfFileSystemFile(fsspec.spec.AbstractBufferedFile):
-    def __init__(
-        self, fs: HfFileSystem, path: str, revision: Optional[str] = None, **kwargs
-    ):
+    def __init__(self, fs: HfFileSystem, path: str, revision: Optional[str] = None, **kwargs):
         try:
             self.resolved_path = fs.resolve_path(path, revision=revision)
         except FileNotFoundError as e:
@@ -1146,9 +1069,7 @@ class HfFileSystemFile(fsspec.spec.Abstr
         temporary file and read from there.
         """
         if self.mode == "rb" and (length is None or length == -1) and self.loc == 0:
-            with self.fs.open(
-                self.path, "rb", block_size=0
-            ) as f:  # block_size=0 enables fast streaming
+            with self.fs.open(self.path, "rb", block_size=0) as f:  # block_size=0 enables fast streaming
                 return f.read()
         return super().read(length)
 
@@ -1168,17 +1089,11 @@ class HfFileSystemStreamFile(fsspec.spec
         **kwargs,
     ):
         if block_size != 0:
-            raise ValueError(
-                f"HfFileSystemStreamFile only supports block_size=0 but got {block_size}"
-            )
+            raise ValueError(f"HfFileSystemStreamFile only supports block_size=0 but got {block_size}")
         if cache_type != "none":
-            raise ValueError(
-                f"HfFileSystemStreamFile only supports cache_type='none' but got {cache_type}"
-            )
+            raise ValueError(f"HfFileSystemStreamFile only supports cache_type='none' but got {cache_type}")
         if "w" in mode:
-            raise ValueError(
-                f"HfFileSystemStreamFile only supports reading but got mode='{mode}'"
-            )
+            raise ValueError(f"HfFileSystemStreamFile only supports reading but got mode='{mode}'")
         try:
             self.resolved_path = fs.resolve_path(path, revision=revision)
         except FileNotFoundError as e:
@@ -1272,11 +1187,7 @@ class HfFileSystemStreamFile(fsspec.spec
 
 
 def safe_revision(revision: str) -> str:
-    return (
-        revision
-        if SPECIAL_REFS_REVISION_REGEX.match(revision)
-        else safe_quote(revision)
-    )
+    return revision if SPECIAL_REFS_REVISION_REGEX.match(revision) else safe_quote(revision)
 
 
 def safe_quote(s: str) -> str:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/hub_mixin.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/hub_mixin.py
@@ -196,12 +196,8 @@ class ModelHubMixin:
     # ^ information about the library integrating ModelHubMixin (used to generate model card)
     _hub_mixin_inject_config: bool  # whether `_from_pretrained` expects `config` or not
     _hub_mixin_init_parameters: Dict[str, inspect.Parameter]  # __init__ parameters
-    _hub_mixin_jsonable_default_values: Dict[
-        str, Any
-    ]  # default values for __init__ parameters
-    _hub_mixin_jsonable_custom_types: Tuple[
-        Type, ...
-    ]  # custom types that can be encoded/decoded
+    _hub_mixin_jsonable_default_values: Dict[str, Any]  # default values for __init__ parameters
+    _hub_mixin_jsonable_custom_types: Tuple[Type, ...]  # custom types that can be encoded/decoded
     _hub_mixin_coders: Dict[Type, CODER_T]  # encoders/decoders for custom types
     # ^ internal values to handle config
 
@@ -237,9 +233,7 @@ class ModelHubMixin:
         tags.append("model_hub_mixin")
 
         # Initialize MixinInfo if not existent
-        info = MixinInfo(
-            model_card_template=model_card_template, model_card_data=ModelCardData()
-        )
+        info = MixinInfo(model_card_template=model_card_template, model_card_data=ModelCardData())
 
         # If parent class has a MixinInfo, inherit from it as a copy
         if hasattr(cls, "_hub_mixin_info"):
@@ -248,9 +242,7 @@ class ModelHubMixin:
                 info.model_card_template = cls._hub_mixin_info.model_card_template
 
             # Inherit from parent model card data
-            info.model_card_data = ModelCardData(
-                **cls._hub_mixin_info.model_card_data.to_dict()
-            )
+            info.model_card_data = ModelCardData(**cls._hub_mixin_info.model_card_data.to_dict())
 
             # Inherit other info
             info.docs_url = cls._hub_mixin_info.docs_url
@@ -258,10 +250,7 @@ class ModelHubMixin:
         cls._hub_mixin_info = info
 
         # Update MixinInfo with metadata
-        if (
-            model_card_template is not None
-            and model_card_template != DEFAULT_MODEL_CARD
-        ):
+        if model_card_template is not None and model_card_template != DEFAULT_MODEL_CARD:
             info.model_card_template = model_card_template
         if repo_url is not None:
             info.repo_url = repo_url
@@ -292,18 +281,13 @@ class ModelHubMixin:
         cls._hub_mixin_jsonable_custom_types = tuple(cls._hub_mixin_coders.keys())
 
         # Inspect __init__ signature to handle config
-        cls._hub_mixin_init_parameters = dict(
-            inspect.signature(cls.__init__).parameters
-        )
+        cls._hub_mixin_init_parameters = dict(inspect.signature(cls.__init__).parameters)
         cls._hub_mixin_jsonable_default_values = {
             param.name: cls._encode_arg(param.default)
             for param in cls._hub_mixin_init_parameters.values()
-            if param.default is not inspect.Parameter.empty
-            and cls._is_jsonable(param.default)
+            if param.default is not inspect.Parameter.empty and cls._is_jsonable(param.default)
         }
-        cls._hub_mixin_inject_config = (
-            "config" in inspect.signature(cls._from_pretrained).parameters
-        )
+        cls._hub_mixin_inject_config = "config" in inspect.signature(cls._from_pretrained).parameters
 
     def __new__(cls: Type[T], *args, **kwargs) -> T:
         """Create a new instance of the class and handle config.
@@ -345,9 +329,7 @@ class ModelHubMixin:
             **{
                 key: cls._encode_arg(value)  # Encode custom types as jsonable value
                 for key, value in passed_values.items()
-                if instance._is_jsonable(
-                    value
-                )  # Only if jsonable or we have a custom encoder
+                if instance._is_jsonable(value)  # Only if jsonable or we have a custom encoder
             },
         }
         passed_config = init_config.pop("config", {})
@@ -451,9 +433,7 @@ class ModelHubMixin:
         model_card_path = save_directory / "README.md"
         model_card_kwargs = model_card_kwargs if model_card_kwargs is not None else {}
         if not model_card_path.exists():  # do not overwrite if already exists
-            self.generate_model_card(**model_card_kwargs).save(
-                save_directory / "README.md"
-            )
+            self.generate_model_card(**model_card_kwargs).save(save_directory / "README.md")
 
         # push to the Hub if required
         if push_to_hub:
@@ -462,9 +442,7 @@ class ModelHubMixin:
                 kwargs["config"] = config
             if repo_id is None:
                 repo_id = save_directory.name  # Defaults to `save_directory` name
-            return self.push_to_hub(
-                repo_id=repo_id, model_card_kwargs=model_card_kwargs, **kwargs
-            )
+            return self.push_to_hub(repo_id=repo_id, model_card_kwargs=model_card_kwargs, **kwargs)
         return None
 
     def _save_pretrained(self, save_directory: Path) -> None:
@@ -526,9 +504,7 @@ class ModelHubMixin:
             if constants.CONFIG_NAME in os.listdir(model_id):
                 config_file = os.path.join(model_id, constants.CONFIG_NAME)
             else:
-                logger.warning(
-                    f"{constants.CONFIG_NAME} not found in {Path(model_id).resolve()}"
-                )
+                logger.warning(f"{constants.CONFIG_NAME} not found in {Path(model_id).resolve()}")
         else:
             try:
                 config_file = hf_hub_download(
@@ -543,9 +519,7 @@ class ModelHubMixin:
                     local_files_only=local_files_only,
                 )
             except HfHubHTTPError as e:
-                logger.info(
-                    f"{constants.CONFIG_NAME} not found on the HuggingFace Hub: {str(e)}"
-                )
+                logger.info(f"{constants.CONFIG_NAME} not found on the HuggingFace Hub: {str(e)}")
 
         # Read config
         config = None
@@ -566,10 +540,7 @@ class ModelHubMixin:
                     model_kwargs[param.name] = config[param.name]
 
             # Check if `config` argument was passed at init
-            if (
-                "config" in cls._hub_mixin_init_parameters
-                and "config" not in model_kwargs
-            ):
+            if "config" in cls._hub_mixin_init_parameters and "config" not in model_kwargs:
                 # Decode `config` argument if it was passed
                 config_annotation = cls._hub_mixin_init_parameters["config"].annotation
                 config = cls._decode_arg(config_annotation, config)
@@ -582,10 +553,7 @@ class ModelHubMixin:
                 for key in cls.__dataclass_fields__:
                     if key not in model_kwargs and key in config:
                         model_kwargs[key] = config[key]
-            elif any(
-                param.kind == inspect.Parameter.VAR_KEYWORD
-                for param in cls._hub_mixin_init_parameters.values()
-            ):
+            elif any(param.kind == inspect.Parameter.VAR_KEYWORD for param in cls._hub_mixin_init_parameters.values()):
                 for key, value in config.items():
                     if key not in model_kwargs:
                         model_kwargs[key] = value
@@ -608,9 +576,7 @@ class ModelHubMixin:
 
         # Implicitly set the config as instance attribute if not already set by the class
         # This way `config` will be available when calling `save_pretrained` or `push_to_hub`.
-        if config is not None and (
-            getattr(instance, "_hub_mixin_config", None) in (None, {})
-        ):
+        if config is not None and (getattr(instance, "_hub_mixin_config", None) in (None, {})):
             instance._hub_mixin_config = config
 
         return instance
@@ -715,16 +681,12 @@ class ModelHubMixin:
             The url of the commit of your model in the given repository.
         """
         api = HfApi(token=token)
-        repo_id = api.create_repo(
-            repo_id=repo_id, private=private, exist_ok=True
-        ).repo_id
+        repo_id = api.create_repo(repo_id=repo_id, private=private, exist_ok=True).repo_id
 
         # Push the files to the repo in a single commit
         with SoftTemporaryDirectory() as tmp:
             saved_path = Path(tmp) / repo_id
-            self.save_pretrained(
-                saved_path, config=config, model_card_kwargs=model_card_kwargs
-            )
+            self.save_pretrained(saved_path, config=config, model_card_kwargs=model_card_kwargs)
             return api.upload_folder(
                 repo_id=repo_id,
                 repo_type="model",
@@ -793,9 +755,7 @@ class PyTorchModelHubMixin(ModelHubMixin
     ```
     """
 
-    def __init_subclass__(
-        cls, *args, tags: Optional[List[str]] = None, **kwargs
-    ) -> None:
+    def __init_subclass__(cls, *args, tags: Optional[List[str]] = None, **kwargs) -> None:
         tags = tags or []
         tags.append("pytorch_model_hub_mixin")
         kwargs["tags"] = tags
@@ -804,9 +764,7 @@ class PyTorchModelHubMixin(ModelHubMixin
     def _save_pretrained(self, save_directory: Path) -> None:
         """Save weights from a Pytorch model to a local directory."""
         model_to_save = self.module if hasattr(self, "module") else self  # type: ignore
-        save_model_as_safetensor(
-            model_to_save, str(save_directory / constants.SAFETENSORS_SINGLE_FILE)
-        )
+        save_model_as_safetensor(model_to_save, str(save_directory / constants.SAFETENSORS_SINGLE_FILE))
 
     @classmethod
     def _from_pretrained(
@@ -859,20 +817,14 @@ class PyTorchModelHubMixin(ModelHubMixin
                 return cls._load_as_pickle(model, model_file, map_location, strict)
 
     @classmethod
-    def _load_as_pickle(
-        cls, model: T, model_file: str, map_location: str, strict: bool
-    ) -> T:
-        state_dict = torch.load(
-            model_file, map_location=torch.device(map_location), weights_only=True
-        )
+    def _load_as_pickle(cls, model: T, model_file: str, map_location: str, strict: bool) -> T:
+        state_dict = torch.load(model_file, map_location=torch.device(map_location), weights_only=True)
         model.load_state_dict(state_dict, strict=strict)  # type: ignore
         model.eval()  # type: ignore
         return model
 
     @classmethod
-    def _load_as_safetensor(
-        cls, model: T, model_file: str, map_location: str, strict: bool
-    ) -> T:
+    def _load_as_safetensor(cls, model: T, model_file: str, map_location: str, strict: bool) -> T:
         if packaging.version.parse(safetensors.__version__) < packaging.version.parse("0.4.3"):  # type: ignore [attr-defined]
             load_model_as_safetensor(model, model_file, strict=strict)  # type: ignore [arg-type]
             if map_location != "cpu":
@@ -893,6 +845,4 @@ def _load_dataclass(datacls: Type[Datacl
 
     Fields not expected by the dataclass are ignored.
     """
-    return datacls(
-        **{k: v for k, v in data.items() if k in datacls.__dataclass_fields__}
-    )
+    return datacls(**{k: v for k, v in data.items() if k in datacls.__dataclass_fields__})
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_client.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_client.py
@@ -126,9 +126,7 @@ if TYPE_CHECKING:
 logger = logging.getLogger(__name__)
 
 
-MODEL_KWARGS_NOT_USED_REGEX = re.compile(
-    r"The following `model_kwargs` are not used by the model: \[(.*?)\]"
-)
+MODEL_KWARGS_NOT_USED_REGEX = re.compile(r"The following `model_kwargs` are not used by the model: \[(.*?)\]")
 
 
 class InferenceClient:
@@ -225,7 +223,8 @@ class InferenceClient:
         model: Optional[str] = None,
         task: Optional[str] = None,
         stream: Literal[False] = ...,
-    ) -> bytes: ...
+    ) -> bytes:
+        ...
 
     @overload
     def post(  # type: ignore[misc]
@@ -236,7 +235,8 @@ class InferenceClient:
         model: Optional[str] = None,
         task: Optional[str] = None,
         stream: Literal[True] = ...,
-    ) -> Iterable[bytes]: ...
+    ) -> Iterable[bytes]:
+        ...
 
     @overload
     def post(
@@ -247,7 +247,8 @@ class InferenceClient:
         model: Optional[str] = None,
         task: Optional[str] = None,
         stream: bool = False,
-    ) -> Union[bytes, Iterable[bytes]]: ...
+    ) -> Union[bytes, Iterable[bytes]]:
+        ...
 
     @_deprecate_method(
         version="0.31.0",
@@ -296,27 +297,27 @@ class InferenceClient:
     @overload
     def _inner_post(  # type: ignore[misc]
         self, request_parameters: RequestParameters, *, stream: Literal[False] = ...
-    ) -> bytes: ...
+    ) -> bytes:
+        ...
 
     @overload
     def _inner_post(  # type: ignore[misc]
         self, request_parameters: RequestParameters, *, stream: Literal[True] = ...
-    ) -> Iterable[bytes]: ...
+    ) -> Iterable[bytes]:
+        ...
 
     @overload
     def _inner_post(
         self, request_parameters: RequestParameters, *, stream: bool = False
-    ) -> Union[bytes, Iterable[bytes]]: ...
+    ) -> Union[bytes, Iterable[bytes]]:
+        ...
 
     def _inner_post(
         self, request_parameters: RequestParameters, *, stream: bool = False
     ) -> Union[bytes, Iterable[bytes]]:
         """Make a request to the inference server."""
         # TODO: this should be handled in provider helpers directly
-        if (
-            request_parameters.task in TASKS_EXPECTING_IMAGES
-            and "Accept" not in request_parameters.headers
-        ):
+        if request_parameters.task in TASKS_EXPECTING_IMAGES and "Accept" not in request_parameters.headers:
             request_parameters.headers["Accept"] = "image/png"
 
         while True:
@@ -340,10 +341,7 @@ class InferenceClient:
                 hf_raise_for_status(response)
                 return response.iter_lines() if stream else response.content
             except HTTPError as error:
-                if (
-                    error.response.status_code == 422
-                    and request_parameters.task != "unknown"
-                ):
+                if error.response.status_code == 422 and request_parameters.task != "unknown":
                     msg = str(error.args[0])
                     if len(error.response.text) > 0:
                         msg += f"\n{error.response.text}\n"
@@ -395,9 +393,7 @@ class InferenceClient:
         ]
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="audio-classification"
-        )
+        provider_helper = get_provider_helper(self.provider, task="audio-classification")
         request_parameters = provider_helper.prepare_request(
             inputs=audio,
             parameters={"function_to_apply": function_to_apply, "top_k": top_k},
@@ -495,9 +491,7 @@ class InferenceClient:
         "hello world"
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="automatic-speech-recognition"
-        )
+        provider_helper = get_provider_helper(self.provider, task="automatic-speech-recognition")
         request_parameters = provider_helper.prepare_request(
             inputs=audio,
             parameters={**(extra_body or {})},
@@ -526,17 +520,14 @@ class InferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[
-            Union[
-                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
-            ]
-        ] = None,
+        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
         top_p: Optional[float] = None,
         extra_body: Optional[Dict] = None,
-    ) -> ChatCompletionOutput: ...
+    ) -> ChatCompletionOutput:
+        ...
 
     @overload
     def chat_completion(  # type: ignore
@@ -556,17 +547,14 @@ class InferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[
-            Union[
-                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
-            ]
-        ] = None,
+        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
         top_p: Optional[float] = None,
         extra_body: Optional[Dict] = None,
-    ) -> Iterable[ChatCompletionStreamOutput]: ...
+    ) -> Iterable[ChatCompletionStreamOutput]:
+        ...
 
     @overload
     def chat_completion(
@@ -586,17 +574,14 @@ class InferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[
-            Union[
-                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
-            ]
-        ] = None,
+        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
         top_p: Optional[float] = None,
         extra_body: Optional[Dict] = None,
-    ) -> Union[ChatCompletionOutput, Iterable[ChatCompletionStreamOutput]]: ...
+    ) -> Union[ChatCompletionOutput, Iterable[ChatCompletionStreamOutput]]:
+        ...
 
     def chat_completion(
         self,
@@ -616,11 +601,7 @@ class InferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[
-            Union[
-                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
-            ]
-        ] = None,
+        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
@@ -1068,9 +1049,7 @@ class InferenceClient:
         ```
         """
         inputs: Dict[str, Any] = {"question": question, "image": _b64_encode(image)}
-        provider_helper = get_provider_helper(
-            self.provider, task="document-question-answering"
-        )
+        provider_helper = get_provider_helper(self.provider, task="document-question-answering")
         request_parameters = provider_helper.prepare_request(
             inputs=inputs,
             parameters={
@@ -1255,9 +1234,7 @@ class InferenceClient:
         [ImageClassificationOutputElement(label='Blenheim spaniel', score=0.9779096841812134), ...]
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="image-classification"
-        )
+        provider_helper = get_provider_helper(self.provider, task="image-classification")
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={"function_to_apply": function_to_apply, "top_k": top_k},
@@ -1318,9 +1295,7 @@ class InferenceClient:
         [ImageSegmentationOutputElement(score=0.989008, label='LABEL_184', mask=<PIL.PngImagePlugin.PngImageFile image mode=L size=400x300 at 0x7FDD2B129CC0>), ...]
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="audio-classification"
-        )
+        provider_helper = get_provider_helper(self.provider, task="audio-classification")
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={
@@ -1414,9 +1389,7 @@ class InferenceClient:
         response = self._inner_post(request_parameters)
         return _bytes_to_image(response)
 
-    def image_to_text(
-        self, image: ContentT, *, model: Optional[str] = None
-    ) -> ImageToTextOutput:
+    def image_to_text(self, image: ContentT, *, model: Optional[str] = None) -> ImageToTextOutput:
         """
         Takes an input image and return text.
 
@@ -1758,9 +1731,7 @@ class InferenceClient:
         TableQuestionAnsweringOutputElement(answer='36542', coordinates=[[0, 1]], cells=['36542'], aggregator='AVERAGE')
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="table-question-answering"
-        )
+        provider_helper = get_provider_helper(self.provider, task="table-question-answering")
         request_parameters = provider_helper.prepare_request(
             inputs=None,
             parameters={
@@ -1777,9 +1748,7 @@ class InferenceClient:
         response = self._inner_post(request_parameters)
         return TableQuestionAnsweringOutputElement.parse_obj_as_instance(response)
 
-    def tabular_classification(
-        self, table: Dict[str, Any], *, model: Optional[str] = None
-    ) -> List[str]:
+    def tabular_classification(self, table: Dict[str, Any], *, model: Optional[str] = None) -> List[str]:
         """
         Classifying a target category (a group) based on a set of attributes.
 
@@ -1821,9 +1790,7 @@ class InferenceClient:
         ["5", "5", "5"]
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="tabular-classification"
-        )
+        provider_helper = get_provider_helper(self.provider, task="tabular-classification")
         request_parameters = provider_helper.prepare_request(
             inputs=None,
             extra_payload={"table": table},
@@ -1835,9 +1802,7 @@ class InferenceClient:
         response = self._inner_post(request_parameters)
         return _bytes_to_list(response)
 
-    def tabular_regression(
-        self, table: Dict[str, Any], *, model: Optional[str] = None
-    ) -> List[float]:
+    def tabular_regression(self, table: Dict[str, Any], *, model: Optional[str] = None) -> List[float]:
         """
         Predicting a numerical target value given a set of attributes/features in a table.
 
@@ -1971,7 +1936,8 @@ class InferenceClient:
         truncate: Optional[int] = None,
         typical_p: Optional[float] = None,
         watermark: Optional[bool] = None,
-    ) -> str: ...
+    ) -> str:
+        ...
 
     @overload
     def text_generation(  # type: ignore
@@ -2001,7 +1967,8 @@ class InferenceClient:
         truncate: Optional[int] = None,
         typical_p: Optional[float] = None,
         watermark: Optional[bool] = None,
-    ) -> TextGenerationOutput: ...
+    ) -> TextGenerationOutput:
+        ...
 
     @overload
     def text_generation(  # type: ignore
@@ -2031,7 +1998,8 @@ class InferenceClient:
         truncate: Optional[int] = None,
         typical_p: Optional[float] = None,
         watermark: Optional[bool] = None,
-    ) -> Iterable[str]: ...
+    ) -> Iterable[str]:
+        ...
 
     @overload
     def text_generation(  # type: ignore
@@ -2061,7 +2029,8 @@ class InferenceClient:
         truncate: Optional[int] = None,
         typical_p: Optional[float] = None,
         watermark: Optional[bool] = None,
-    ) -> Iterable[TextGenerationStreamOutput]: ...
+    ) -> Iterable[TextGenerationStreamOutput]:
+        ...
 
     @overload
     def text_generation(
@@ -2091,7 +2060,8 @@ class InferenceClient:
         truncate: Optional[int] = None,
         typical_p: Optional[float] = None,
         watermark: Optional[bool] = None,
-    ) -> Union[TextGenerationOutput, Iterable[TextGenerationStreamOutput]]: ...
+    ) -> Union[TextGenerationOutput, Iterable[TextGenerationStreamOutput]]:
+        ...
 
     def text_generation(
         self,
@@ -2120,9 +2090,7 @@ class InferenceClient:
         truncate: Optional[int] = None,
         typical_p: Optional[float] = None,
         watermark: Optional[bool] = None,
-    ) -> Union[
-        str, TextGenerationOutput, Iterable[str], Iterable[TextGenerationStreamOutput]
-    ]:
+    ) -> Union[str, TextGenerationOutput, Iterable[str], Iterable[TextGenerationStreamOutput]]:
         """
         Given a prompt, generate the following text.
 
@@ -2396,9 +2364,7 @@ class InferenceClient:
         except HTTPError as e:
             match = MODEL_KWARGS_NOT_USED_REGEX.search(str(e))
             if isinstance(e, BadRequestError) and match:
-                unused_params = [
-                    kwarg.strip("' ") for kwarg in match.group(1).split(",")
-                ]
+                unused_params = [kwarg.strip("' ") for kwarg in match.group(1).split(",")]
                 _set_unsupported_text_generation_kwargs(model, unused_params)
                 return self.text_generation(  # type: ignore
                     prompt=prompt,
@@ -2436,11 +2402,7 @@ class InferenceClient:
         if isinstance(data, list):
             data = data[0]
 
-        return (
-            TextGenerationOutput.parse_obj_as_instance(data)
-            if details
-            else data["generated_text"]
-        )
+        return TextGenerationOutput.parse_obj_as_instance(data) if details else data["generated_text"]
 
     def text_to_image(
         self,
@@ -2947,9 +2909,7 @@ class InferenceClient:
         ]
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="token-classification"
-        )
+        provider_helper = get_provider_helper(self.provider, task="token-classification")
         request_parameters = provider_helper.prepare_request(
             inputs=text,
             parameters={
@@ -3030,14 +2990,10 @@ class InferenceClient:
         """
         # Throw error if only one of `src_lang` and `tgt_lang` was given
         if src_lang is not None and tgt_lang is None:
-            raise ValueError(
-                "You cannot specify `src_lang` without specifying `tgt_lang`."
-            )
+            raise ValueError("You cannot specify `src_lang` without specifying `tgt_lang`.")
 
         if src_lang is None and tgt_lang is not None:
-            raise ValueError(
-                "You cannot specify `tgt_lang` without specifying `src_lang`."
-            )
+            raise ValueError("You cannot specify `tgt_lang` without specifying `src_lang`.")
 
         provider_helper = get_provider_helper(self.provider, task="translation")
         request_parameters = provider_helper.prepare_request(
@@ -3102,9 +3058,7 @@ class InferenceClient:
         ]
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="visual-question-answering"
-        )
+        provider_helper = get_provider_helper(self.provider, task="visual-question-answering")
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={"top_k": top_k},
@@ -3220,9 +3174,7 @@ class InferenceClient:
         elif candidate_labels is None:
             raise ValueError("Must specify `candidate_labels`")
 
-        provider_helper = get_provider_helper(
-            self.provider, task="zero-shot-classification"
-        )
+        provider_helper = get_provider_helper(self.provider, task="zero-shot-classification")
         request_parameters = provider_helper.prepare_request(
             inputs=text,
             parameters={
@@ -3237,9 +3189,7 @@ class InferenceClient:
         response = self._inner_post(request_parameters)
         output = _bytes_to_dict(response)
         return [
-            ZeroShotClassificationOutputElement.parse_obj_as_instance(
-                {"label": label, "score": score}
-            )
+            ZeroShotClassificationOutputElement.parse_obj_as_instance({"label": label, "score": score})
             for label, score in zip(output["labels"], output["scores"])
         ]
 
@@ -3310,9 +3260,7 @@ class InferenceClient:
         if len(candidate_labels) < 2:
             raise ValueError("You must specify at least 2 classes to compare.")
 
-        provider_helper = get_provider_helper(
-            self.provider, task="zero-shot-image-classification"
-        )
+        provider_helper = get_provider_helper(self.provider, task="zero-shot-image-classification")
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={
@@ -3385,9 +3333,7 @@ class InferenceClient:
         ```
         """
         if self.provider != "hf-inference":
-            raise ValueError(
-                f"Listing deployed models is not supported on '{self.provider}'."
-            )
+            raise ValueError(f"Listing deployed models is not supported on '{self.provider}'.")
 
         # Resolve which frameworks to check
         if frameworks is None:
@@ -3406,16 +3352,10 @@ class InferenceClient:
                 if framework == "sentence-transformers":
                     # Model running with the `sentence-transformers` framework can work with both tasks even if not
                     # branded as such in the API response
-                    models_by_task.setdefault("feature-extraction", []).append(
-                        model["model_id"]
-                    )
-                    models_by_task.setdefault("sentence-similarity", []).append(
-                        model["model_id"]
-                    )
+                    models_by_task.setdefault("feature-extraction", []).append(model["model_id"])
+                    models_by_task.setdefault("sentence-similarity", []).append(model["model_id"])
                 else:
-                    models_by_task.setdefault(model["task"], []).append(
-                        model["model_id"]
-                    )
+                    models_by_task.setdefault(model["task"], []).append(model["model_id"])
 
         for framework in frameworks:
             response = get_session().get(
@@ -3474,9 +3414,7 @@ class InferenceClient:
         ```
         """
         if self.provider != "hf-inference":
-            raise ValueError(
-                f"Getting endpoint info is not supported on '{self.provider}'."
-            )
+            raise ValueError(f"Getting endpoint info is not supported on '{self.provider}'.")
 
         model = model or self.model
         if model is None:
@@ -3565,17 +3503,13 @@ class InferenceClient:
         ```
         """
         if self.provider != "hf-inference":
-            raise ValueError(
-                f"Getting model status is not supported on '{self.provider}'."
-            )
+            raise ValueError(f"Getting model status is not supported on '{self.provider}'.")
 
         model = model or self.model
         if model is None:
             raise ValueError("Model id not provided.")
         if model.startswith("https://"):
-            raise NotImplementedError(
-                "Model status is only available for Inference API endpoints."
-            )
+            raise NotImplementedError("Model status is only available for Inference API endpoints.")
         url = f"{constants.INFERENCE_ENDPOINT}/status/{model}"
 
         response = get_session().get(url, headers=build_hf_headers(token=self.token))
--- /dev/null
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_client.py.save
@@ -0,0 +1,3555 @@
+# coding=utf-8
+# Copyright 2023-present, the HuggingFace Inc. team.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+# Related resources:
+#    https://huggingface.co/tasks
+#    https://huggingface.co/docs/huggingface.js/inference/README
+#    https://github.com/huggingface/huggingface.js/tree/main/packages/inference/src
+#    https://github.com/huggingface/text-generation-inference/tree/main/clients/python
+#    https://github.com/huggingface/text-generation-inference/blob/main/clients/python/text_generation/client.py
+#    https://huggingface.slack.com/archives/C03E4DQ9LAJ/p1680169099087869
+#    https://github.com/huggingface/unity-api#tasks
+#
+# Some TODO:
+# - add all tasks
+#
+# NOTE: the philosophy of this client is "let's make it as easy as possible to use it, even if less optimized". Some
+# examples of how it translates:
+# - Timeout / Server unavailable is handled by the client in a single "timeout" parameter.
+# - Files can be provided as bytes, file paths, or URLs and the client will try to "guess" the type.
+# - Images are parsed as PIL.Image for easier manipulation.
+# - Provides a "recommended model" for each task => suboptimal but user-wise quicker to get a first script running.
+# - Only the main parameters are publicly exposed. Power users can always read the docs for more options.
+import base64
+import logging
+import re
+import warnings
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Dict,
+    Iterable,
+    List,
+    Literal,
+    Optional,
+    Union,
+    overload,
+)
+
+from requests import HTTPError
+
+from huggingface_hub import constants
+from huggingface_hub.errors import BadRequestError, InferenceTimeoutError
+from huggingface_hub.inference._common import (
+    TASKS_EXPECTING_IMAGES,
+    ContentT,
+    ModelStatus,
+    RequestParameters,
+    _b64_encode,
+    _b64_to_image,
+    _bytes_to_dict,
+    _bytes_to_image,
+    _bytes_to_list,
+    _get_unsupported_text_generation_kwargs,
+    _import_numpy,
+    _open_as_binary,
+    _set_unsupported_text_generation_kwargs,
+    _stream_chat_completion_response,
+    _stream_text_generation_response,
+    raise_text_generation_error,
+)
+from huggingface_hub.inference._generated.types import (
+    AudioClassificationOutputElement,
+    AudioClassificationOutputTransform,
+    AudioToAudioOutputElement,
+    AutomaticSpeechRecognitionOutput,
+    ChatCompletionInputGrammarType,
+    ChatCompletionInputStreamOptions,
+    ChatCompletionInputTool,
+    ChatCompletionInputToolChoiceClass,
+    ChatCompletionInputToolChoiceEnum,
+    ChatCompletionOutput,
+    ChatCompletionStreamOutput,
+    DocumentQuestionAnsweringOutputElement,
+    FillMaskOutputElement,
+    ImageClassificationOutputElement,
+    ImageClassificationOutputTransform,
+    ImageSegmentationOutputElement,
+    ImageSegmentationSubtask,
+    ImageToImageTargetSize,
+    ImageToTextOutput,
+    ObjectDetectionOutputElement,
+    Padding,
+    QuestionAnsweringOutputElement,
+    SummarizationOutput,
+    SummarizationTruncationStrategy,
+    TableQuestionAnsweringOutputElement,
+    TextClassificationOutputElement,
+    TextClassificationOutputTransform,
+    TextGenerationInputGrammarType,
+    TextGenerationOutput,
+    TextGenerationStreamOutput,
+    TextToSpeechEarlyStoppingEnum,
+    TokenClassificationAggregationStrategy,
+    TokenClassificationOutputElement,
+    TranslationOutput,
+    TranslationTruncationStrategy,
+    VisualQuestionAnsweringOutputElement,
+    ZeroShotClassificationOutputElement,
+    ZeroShotImageClassificationOutputElement,
+)
+from huggingface_hub.inference._providers import (
+    PROVIDER_T,
+    HFInferenceTask,
+    get_provider_helper,
+)
+from huggingface_hub.utils import build_hf_headers, get_session, hf_raise_for_status
+from huggingface_hub.utils._deprecation import _deprecate_arguments, _deprecate_method
+
+
+if TYPE_CHECKING:
+    import numpy as np
+    from PIL.Image import Image
+
+logger = logging.getLogger(__name__)
+
+
+MODEL_KWARGS_NOT_USED_REGEX = re.compile(r"The following `model_kwargs` are not used by the model: \[(.*?)\]")
+
+
+class InferenceClient:
+    """
+    Initialize a new Inference Client.
+
+    [`InferenceClient`] aims to provide a unified experience to perform inference. The client can be used
+    seamlessly with either the (free) Inference API, self-hosted Inference Endpoints, or third-party Inference Providers.
+
+    Args:
+        model (`str`, `optional`):
+            The model to run inference with. Can be a model id hosted on the Hugging Face Hub, e.g. `meta-llama/Meta-Llama-3-8B-Instruct`
+            or a URL to a deployed Inference Endpoint. Defaults to None, in which case a recommended model is
+            automatically selected for the task.
+            Note: for better compatibility with OpenAI's client, `model` has been aliased as `base_url`. Those 2
+            arguments are mutually exclusive. If using `base_url` for chat completion, the `/chat/completions` suffix
+            path will be appended to the base URL (see the [TGI Messages API](https://huggingface.co/docs/text-generation-inference/en/messages_api)
+            documentation for details). When passing a URL as `model`, the client will not append any suffix path to it.
+        provider (`str`, *optional*):
+            Name of the provider to use for inference. Can be `"black-forest-labs"`, `"cerebras"`, `"cohere"`, `"fal-ai"`, `"fireworks-ai"`, `"hf-inference"`, `"hyperbolic"`, `"nebius"`, `"novita"`, `"replicate"`, "sambanova"` or `"together"`.
+            defaults to hf-inference (Hugging Face Serverless Inference API).
+            If model is a URL or `base_url` is passed, then `provider` is not used.
+        token (`str` or `bool`, *optional*):
+            Hugging Face token. Will default to the locally saved token if not provided.
+            Pass `token=False` if you don't want to send your token to the server.
+            Note: for better compatibility with OpenAI's client, `token` has been aliased as `api_key`. Those 2
+            arguments are mutually exclusive and have the exact same behavior.
+        timeout (`float`, `optional`):
+            The maximum number of seconds to wait for a response from the server. Loading a new model in Inference
+            API can take up to several minutes. Defaults to None, meaning it will loop until the server is available.
+        headers (`Dict[str, str]`, `optional`):
+            Additional headers to send to the server. By default only the authorization and user-agent headers are sent.
+            Values in this dictionary will override the default values.
+        cookies (`Dict[str, str]`, `optional`):
+            Additional cookies to send to the server.
+        proxies (`Any`, `optional`):
+            Proxies to use for the request.
+        base_url (`str`, `optional`):
+            Base URL to run inference. This is a duplicated argument from `model` to make [`InferenceClient`]
+            follow the same pattern as `openai.OpenAI` client. Cannot be used if `model` is set. Defaults to None.
+        api_key (`str`, `optional`):
+            Token to use for authentication. This is a duplicated argument from `token` to make [`InferenceClient`]
+            follow the same pattern as `openai.OpenAI` client. Cannot be used if `token` is set. Defaults to None.
+    """
+
+    def __init__(
+        self,
+        model: Optional[str] = None,
+        *,
+        provider: Optional[PROVIDER_T] = None,
+        token: Optional[str] = None,
+        timeout: Optional[float] = None,
+        headers: Optional[Dict[str, str]] = None,
+        cookies: Optional[Dict[str, str]] = None,
+        proxies: Optional[Any] = None,
+        # OpenAI compatibility
+        base_url: Optional[str] = None,
+        api_key: Optional[str] = None,
+    ) -> None:
+        if model is not None and base_url is not None:
+            raise ValueError(
+                "Received both `model` and `base_url` arguments. Please provide only one of them."
+                " `base_url` is an alias for `model` to make the API compatible with OpenAI's client."
+                " If using `base_url` for chat completion, the `/chat/completions` suffix path will be appended to the base url."
+                " When passing a URL as `model`, the client will not append any suffix path to it."
+            )
+        if token is not None and api_key is not None:
+            raise ValueError(
+                "Received both `token` and `api_key` arguments. Please provide only one of them."
+                " `api_key` is an alias for `token` to make the API compatible with OpenAI's client."
+                " It has the exact same behavior as `token`."
+            )
+
+        self.model: Optional[str] = base_url or model
+        self.token: Optional[str] = token if token is not None else api_key
+        self.headers = headers if headers is not None else {}
+
+        # Configure provider
+        self.provider = provider if provider is not None else "hf-inference"
+
+        self.cookies = cookies
+        self.timeout = timeout
+        self.proxies = proxies
+
+    def __repr__(self):
+        return f"<InferenceClient(model='{self.model if self.model else ''}', timeout={self.timeout})>"
+
+    @overload
+    def post(  # type: ignore[misc]
+        self,
+        *,
+        json: Optional[Union[str, Dict, List]] = None,
+        data: Optional[ContentT] = None,
+        model: Optional[str] = None,
+        task: Optional[str] = None,
+        stream: Literal[False] = ...,
+    ) -> bytes:
+        ...
+
+    @overload
+    def post(  # type: ignore[misc]
+        self,
+        *,
+        json: Optional[Union[str, Dict, List]] = None,
+        data: Optional[ContentT] = None,
+        model: Optional[str] = None,
+        task: Optional[str] = None,
+        stream: Literal[True] = ...,
+    ) -> Iterable[bytes]:
+        ...
+
+    @overload
+    def post(
+        self,
+        *,
+        json: Optional[Union[str, Dict, List]] = None,
+        data: Optional[ContentT] = None,
+        model: Optional[str] = None,
+        task: Optional[str] = None,
+        stream: bool = False,
+    ) -> Union[bytes, Iterable[bytes]]:
+        ...
+
+    @_deprecate_method(
+        version="0.31.0",
+        message=(
+            "Making direct POST requests to the inference server is not supported anymore. "
+            "Please use task methods instead (e.g. `InferenceClient.chat_completion`). "
+            "If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub."
+        ),
+    )
+    def post(
+        self,
+        *,
+        json: Optional[Union[str, Dict, List]] = None,
+        data: Optional[ContentT] = None,
+        model: Optional[str] = None,
+        task: Optional[str] = None,
+        stream: bool = False,
+    ) -> Union[bytes, Iterable[bytes]]:
+        """
+        Make a POST request to the inference server.
+
+        This method is deprecated and will be removed in the future.
+        Please use task methods instead (e.g. `InferenceClient.chat_completion`).
+        """
+        if self.provider != "hf-inference":
+            raise ValueError(
+                "Cannot use `post` with another provider than `hf-inference`. "
+                "`InferenceClient.post` is deprecated and should not be used directly anymore."
+            )
+        provider_helper = HFInferenceTask(task or "unknown")
+        mapped_model = provider_helper._prepare_mapped_model(model or self.model)
+        url = provider_helper._prepare_url(self.token, mapped_model)  # type: ignore[arg-type]
+        headers = provider_helper._prepare_headers(self.headers, self.token)  # type: ignore[arg-type]
+        return self._inner_post(
+            request_parameters=RequestParameters(
+                url=url,
+                task=task or "unknown",
+                model=model or "unknown",
+                json=json,
+                data=data,
+                headers=headers,
+            ),
+            stream=stream,
+        )
+
+    @overload
+    def _inner_post(  # type: ignore[misc]
+        self, request_parameters: RequestParameters, *, stream: Literal[False] = ...
+    ) -> bytes:
+        ...
+
+    @overload
+    def _inner_post(  # type: ignore[misc]
+        self, request_parameters: RequestParameters, *, stream: Literal[True] = ...
+    ) -> Iterable[bytes]:
+        ...
+
+    @overload
+    def _inner_post(
+        self, request_parameters: RequestParameters, *, stream: bool = False
+    ) -> Union[bytes, Iterable[bytes]]:
+        ...
+
+    def _inner_post(
+        self, request_parameters: RequestParameters, *, stream: bool = False
+    ) -> Union[bytes, Iterable[bytes]]:
+        """Make a request to the inference server."""
+        # TODO: this should be handled in provider helpers directly
+        if request_parameters.task in TASKS_EXPECTING_IMAGES and "Accept" not in request_parameters.headers:
+            request_parameters.headers["Accept"] = "image/png"
+
+        while True:
+            with _open_as_binary(request_parameters.data) as data_as_binary:
+                try:
+                    response = get_session().post(
+                        request_parameters.url,
+                        json=request_parameters.json,
+                        data=data_as_binary,
+                        headers=request_parameters.headers,
+                        cookies=self.cookies,
+                        timeout=self.timeout,
+                        stream=stream,
+                        proxies=self.proxies,
+                    )
+                except TimeoutError as error:
+                    # Convert any `TimeoutError` to a `InferenceTimeoutError`
+                    raise InferenceTimeoutError(f"Inference call timed out: {request_parameters.url}") from error  # type: ignore
+
+            try:
+                hf_raise_for_status(response)
+                return response.iter_lines() if stream else response.content
+            except HTTPError as error:
+                if error.response.status_code == 422 and request_parameters.task != "unknown":
+                    msg = str(error.args[0])
+                    if len(error.response.text) > 0:
+                        msg += f"\n{error.response.text}\n"
+                    error.args = (msg,) + error.args[1:]
+                raise
+
+    def audio_classification(
+        self,
+        audio: ContentT,
+        *,
+        model: Optional[str] = None,
+        top_k: Optional[int] = None,
+        function_to_apply: Optional["AudioClassificationOutputTransform"] = None,
+    ) -> List[AudioClassificationOutputElement]:
+        """
+        Perform audio classification on the provided audio content.
+
+        Args:
+            audio (Union[str, Path, bytes, BinaryIO]):
+                The audio content to classify. It can be raw audio bytes, a local audio file, or a URL pointing to an
+                audio file.
+            model (`str`, *optional*):
+                The model to use for audio classification. Can be a model ID hosted on the Hugging Face Hub
+                or a URL to a deployed Inference Endpoint. If not provided, the default recommended model for
+                audio classification will be used.
+            top_k (`int`, *optional*):
+                When specified, limits the output to the top K most probable classes.
+            function_to_apply (`"AudioClassificationOutputTransform"`, *optional*):
+                The function to apply to the model outputs in order to retrieve the scores.
+
+        Returns:
+            `List[AudioClassificationOutputElement]`: List of [`AudioClassificationOutputElement`] items containing the predicted labels and their confidence.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.audio_classification("audio.flac")
+        [
+            AudioClassificationOutputElement(score=0.4976358711719513, label='hap'),
+            AudioClassificationOutputElement(score=0.3677836060523987, label='neu'),
+            ...
+        ]
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="audio-classification")
+        request_parameters = provider_helper.prepare_request(
+            inputs=audio,
+            parameters={"function_to_apply": function_to_apply, "top_k": top_k},
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return AudioClassificationOutputElement.parse_obj_as_list(response)
+
+    def audio_to_audio(
+        self,
+        audio: ContentT,
+        *,
+        model: Optional[str] = None,
+    ) -> List[AudioToAudioOutputElement]:
+        """
+        Performs multiple tasks related to audio-to-audio depending on the model (eg: speech enhancement, source separation).
+
+        Args:
+            audio (Union[str, Path, bytes, BinaryIO]):
+                The audio content for the model. It can be raw audio bytes, a local audio file, or a URL pointing to an
+                audio file.
+            model (`str`, *optional*):
+                The model can be any model which takes an audio file and returns another audio file. Can be a model ID hosted on the Hugging Face Hub
+                or a URL to a deployed Inference Endpoint. If not provided, the default recommended model for
+                audio_to_audio will be used.
+
+        Returns:
+            `List[AudioToAudioOutputElement]`: A list of [`AudioToAudioOutputElement`] items containing audios label, content-type, and audio content in blob.
+
+        Raises:
+            `InferenceTimeoutError`:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> audio_output = client.audio_to_audio("audio.flac")
+        >>> for i, item in enumerate(audio_output):
+        >>>     with open(f"output_{i}.flac", "wb") as f:
+                    f.write(item.blob)
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="audio-to-audio")
+        request_parameters = provider_helper.prepare_request(
+            inputs=audio,
+            parameters={},
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        audio_output = AudioToAudioOutputElement.parse_obj_as_list(response)
+        for item in audio_output:
+            item.blob = base64.b64decode(item.blob)
+        return audio_output
+
+    def automatic_speech_recognition(
+        self,
+        audio: ContentT,
+        *,
+        model: Optional[str] = None,
+        extra_body: Optional[Dict] = None,
+    ) -> AutomaticSpeechRecognitionOutput:
+        """
+        Perform automatic speech recognition (ASR or audio-to-text) on the given audio content.
+
+        Args:
+            audio (Union[str, Path, bytes, BinaryIO]):
+                The content to transcribe. It can be raw audio bytes, local audio file, or a URL to an audio file.
+            model (`str`, *optional*):
+                The model to use for ASR. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
+                Inference Endpoint. If not provided, the default recommended model for ASR will be used.
+            extra_body (`Dict`, *optional*):
+                Additional provider-specific parameters to pass to the model. Refer to the provider's documentation
+                for supported parameters.
+        Returns:
+            [`AutomaticSpeechRecognitionOutput`]: An item containing the transcribed text and optionally the timestamp chunks.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.automatic_speech_recognition("hello_world.flac").text
+        "hello world"
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="automatic-speech-recognition")
+        request_parameters = provider_helper.prepare_request(
+            inputs=audio,
+            parameters={**(extra_body or {})},
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return AutomaticSpeechRecognitionOutput.parse_obj_as_instance(response)
+
+    @overload
+    def chat_completion(  # type: ignore
+        self,
+        messages: List[Dict],
+        *,
+        model: Optional[str] = None,
+        stream: Literal[False] = False,
+        frequency_penalty: Optional[float] = None,
+        logit_bias: Optional[List[float]] = None,
+        logprobs: Optional[bool] = None,
+        max_tokens: Optional[int] = None,
+        n: Optional[int] = None,
+        presence_penalty: Optional[float] = None,
+        response_format: Optional[ChatCompletionInputGrammarType] = None,
+        seed: Optional[int] = None,
+        stop: Optional[List[str]] = None,
+        stream_options: Optional[ChatCompletionInputStreamOptions] = None,
+        temperature: Optional[float] = None,
+        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
+        tool_prompt: Optional[str] = None,
+        tools: Optional[List[ChatCompletionInputTool]] = None,
+        top_logprobs: Optional[int] = None,
+        top_p: Optional[float] = None,
+        extra_body: Optional[Dict] = None,
+    ) -> ChatCompletionOutput:
+        ...
+
+    @overload
+    def chat_completion(  # type: ignore
+        self,
+        messages: List[Dict],
+        *,
+        model: Optional[str] = None,
+        stream: Literal[True] = True,
+        frequency_penalty: Optional[float] = None,
+        logit_bias: Optional[List[float]] = None,
+        logprobs: Optional[bool] = None,
+        max_tokens: Optional[int] = None,
+        n: Optional[int] = None,
+        presence_penalty: Optional[float] = None,
+        response_format: Optional[ChatCompletionInputGrammarType] = None,
+        seed: Optional[int] = None,
+        stop: Optional[List[str]] = None,
+        stream_options: Optional[ChatCompletionInputStreamOptions] = None,
+        temperature: Optional[float] = None,
+        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
+        tool_prompt: Optional[str] = None,
+        tools: Optional[List[ChatCompletionInputTool]] = None,
+        top_logprobs: Optional[int] = None,
+        top_p: Optional[float] = None,
+        extra_body: Optional[Dict] = None,
+    ) -> Iterable[ChatCompletionStreamOutput]:
+        ...
+
+    @overload
+    def chat_completion(
+        self,
+        messages: List[Dict],
+        *,
+        model: Optional[str] = None,
+        stream: bool = False,
+        frequency_penalty: Optional[float] = None,
+        logit_bias: Optional[List[float]] = None,
+        logprobs: Optional[bool] = None,
+        max_tokens: Optional[int] = None,
+        n: Optional[int] = None,
+        presence_penalty: Optional[float] = None,
+        response_format: Optional[ChatCompletionInputGrammarType] = None,
+        seed: Optional[int] = None,
+        stop: Optional[List[str]] = None,
+        stream_options: Optional[ChatCompletionInputStreamOptions] = None,
+        temperature: Optional[float] = None,
+        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
+        tool_prompt: Optional[str] = None,
+        tools: Optional[List[ChatCompletionInputTool]] = None,
+        top_logprobs: Optional[int] = None,
+        top_p: Optional[float] = None,
+        extra_body: Optional[Dict] = None,
+    ) -> Union[ChatCompletionOutput, Iterable[ChatCompletionStreamOutput]]:
+        ...
+
+    def chat_completion(
+        self,
+        messages: List[Dict],
+        *,
+        model: Optional[str] = None,
+        stream: bool = False,
+        # Parameters from ChatCompletionInput (handled manually)
+        frequency_penalty: Optional[float] = None,
+        logit_bias: Optional[List[float]] = None,
+        logprobs: Optional[bool] = None,
+        max_tokens: Optional[int] = None,
+        n: Optional[int] = None,
+        presence_penalty: Optional[float] = None,
+        response_format: Optional[ChatCompletionInputGrammarType] = None,
+        seed: Optional[int] = None,
+        stop: Optional[List[str]] = None,
+        stream_options: Optional[ChatCompletionInputStreamOptions] = None,
+        temperature: Optional[float] = None,
+        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
+        tool_prompt: Optional[str] = None,
+        tools: Optional[List[ChatCompletionInputTool]] = None,
+        top_logprobs: Optional[int] = None,
+        top_p: Optional[float] = None,
+        extra_body: Optional[Dict] = None,
+    ) -> Union[ChatCompletionOutput, Iterable[ChatCompletionStreamOutput]]:
+        """
+        A method for completing conversations using a specified language model.
+
+        <Tip>
+
+        The `client.chat_completion` method is aliased as `client.chat.completions.create` for compatibility with OpenAI's client.
+        Inputs and outputs are strictly the same and using either syntax will yield the same results.
+        Check out the [Inference guide](https://huggingface.co/docs/huggingface_hub/guides/inference#openai-compatibility)
+        for more details about OpenAI's compatibility.
+
+        </Tip>
+
+        <Tip>
+        You can pass provider-specific parameters to the model by using the `extra_body` argument.
+        </Tip>
+
+        Args:
+            messages (List of [`ChatCompletionInputMessage`]):
+                Conversation history consisting of roles and content pairs.
+            model (`str`, *optional*):
+                The model to use for chat-completion. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
+                Inference Endpoint. If not provided, the default recommended model for chat-based text-generation will be used.
+                See https://huggingface.co/tasks/text-generation for more details.
+                If `model` is a model ID, it is passed to the server as the `model` parameter. If you want to define a
+                custom URL while setting `model` in the request payload, you must set `base_url` when initializing [`InferenceClient`].
+            frequency_penalty (`float`, *optional*):
+                Penalizes new tokens based on their existing frequency
+                in the text so far. Range: [-2.0, 2.0]. Defaults to 0.0.
+            logit_bias (`List[float]`, *optional*):
+                Adjusts the likelihood of specific tokens appearing in the generated output.
+            logprobs (`bool`, *optional*):
+                Whether to return log probabilities of the output tokens or not. If true, returns the log
+                probabilities of each output token returned in the content of message.
+            max_tokens (`int`, *optional*):
+                Maximum number of tokens allowed in the response. Defaults to 100.
+            n (`int`, *optional*):
+                The number of completions to generate for each prompt.
+            presence_penalty (`float`, *optional*):
+                Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the
+                text so far, increasing the model's likelihood to talk about new topics.
+            response_format ([`ChatCompletionInputGrammarType`], *optional*):
+                Grammar constraints. Can be either a JSONSchema or a regex.
+            seed (Optional[`int`], *optional*):
+                Seed for reproducible control flow. Defaults to None.
+            stop (`List[str]`, *optional*):
+                Up to four strings which trigger the end of the response.
+                Defaults to None.
+            stream (`bool`, *optional*):
+                Enable realtime streaming of responses. Defaults to False.
+            stream_options ([`ChatCompletionInputStreamOptions`], *optional*):
+                Options for streaming completions.
+            temperature (`float`, *optional*):
+                Controls randomness of the generations. Lower values ensure
+                less random completions. Range: [0, 2]. Defaults to 1.0.
+            top_logprobs (`int`, *optional*):
+                An integer between 0 and 5 specifying the number of most likely tokens to return at each token
+                position, each with an associated log probability. logprobs must be set to true if this parameter is
+                used.
+            top_p (`float`, *optional*):
+                Fraction of the most likely next words to sample from.
+                Must be between 0 and 1. Defaults to 1.0.
+            tool_choice ([`ChatCompletionInputToolChoiceClass`] or [`ChatCompletionInputToolChoiceEnum`], *optional*):
+                The tool to use for the completion. Defaults to "auto".
+            tool_prompt (`str`, *optional*):
+                A prompt to be appended before the tools.
+            tools (List of [`ChatCompletionInputTool`], *optional*):
+                A list of tools the model may call. Currently, only functions are supported as a tool. Use this to
+                provide a list of functions the model may generate JSON inputs for.
+            extra_body (`Dict`, *optional*):
+                Additional provider-specific parameters to pass to the model. Refer to the provider's documentation
+                for supported parameters.
+        Returns:
+            [`ChatCompletionOutput`] or Iterable of [`ChatCompletionStreamOutput`]:
+            Generated text returned from the server:
+            - if `stream=False`, the generated text is returned as a [`ChatCompletionOutput`] (default).
+            - if `stream=True`, the generated text is returned token by token as a sequence of [`ChatCompletionStreamOutput`].
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> messages = [{"role": "user", "content": "What is the capital of France?"}]
+        >>> client = InferenceClient("meta-llama/Meta-Llama-3-8B-Instruct")
+        >>> client.chat_completion(messages, max_tokens=100)
+        ChatCompletionOutput(
+            choices=[
+                ChatCompletionOutputComplete(
+                    finish_reason='eos_token',
+                    index=0,
+                    message=ChatCompletionOutputMessage(
+                        role='assistant',
+                        content='The capital of France is Paris.',
+                        name=None,
+                        tool_calls=None
+                    ),
+                    logprobs=None
+                )
+            ],
+            created=1719907176,
+            id='',
+            model='meta-llama/Meta-Llama-3-8B-Instruct',
+            object='text_completion',
+            system_fingerprint='2.0.4-sha-f426a33',
+            usage=ChatCompletionOutputUsage(
+                completion_tokens=8,
+                prompt_tokens=17,
+                total_tokens=25
+            )
+        )
+        ```
+
+        Example using streaming:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> messages = [{"role": "user", "content": "What is the capital of France?"}]
+        >>> client = InferenceClient("meta-llama/Meta-Llama-3-8B-Instruct")
+        >>> for token in client.chat_completion(messages, max_tokens=10, stream=True):
+        ...     print(token)
+        ChatCompletionStreamOutput(choices=[ChatCompletionStreamOutputChoice(delta=ChatCompletionStreamOutputDelta(content='The', role='assistant'), index=0, finish_reason=None)], created=1710498504)
+        ChatCompletionStreamOutput(choices=[ChatCompletionStreamOutputChoice(delta=ChatCompletionStreamOutputDelta(content=' capital', role='assistant'), index=0, finish_reason=None)], created=1710498504)
+        (...)
+        ChatCompletionStreamOutput(choices=[ChatCompletionStreamOutputChoice(delta=ChatCompletionStreamOutputDelta(content=' may', role='assistant'), index=0, finish_reason=None)], created=1710498504)
+        ```
+
+        Example using OpenAI's syntax:
+        ```py
+        # instead of `from openai import OpenAI`
+        from huggingface_hub import InferenceClient
+
+        # instead of `client = OpenAI(...)`
+        client = InferenceClient(
+            base_url=...,
+            api_key=...,
+        )
+
+        output = client.chat.completions.create(
+            model="meta-llama/Meta-Llama-3-8B-Instruct",
+            messages=[
+                {"role": "system", "content": "You are a helpful assistant."},
+                {"role": "user", "content": "Count to 10"},
+            ],
+            stream=True,
+            max_tokens=1024,
+        )
+
+        for chunk in output:
+            print(chunk.choices[0].delta.content)
+        ```
+
+        Example using a third-party provider directly with extra (provider-specific) parameters. Usage will be billed on your Together AI account.
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient(
+        ...     provider="together",  # Use Together AI provider
+        ...     api_key="<together_api_key>",  # Pass your Together API key directly
+        ... )
+        >>> client.chat_completion(
+        ...     model="meta-llama/Meta-Llama-3-8B-Instruct",
+        ...     messages=[{"role": "user", "content": "What is the capital of France?"}],
+        ...     extra_body={"safety_model": "Meta-Llama/Llama-Guard-7b"},
+        ... )
+        ```
+
+        Example using a third-party provider through Hugging Face Routing. Usage will be billed on your Hugging Face account.
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient(
+        ...     provider="sambanova",  # Use Sambanova provider
+        ...     api_key="hf_...",  # Pass your HF token
+        ... )
+        >>> client.chat_completion(
+        ...     model="meta-llama/Meta-Llama-3-8B-Instruct",
+        ...     messages=[{"role": "user", "content": "What is the capital of France?"}],
+        ... )
+        ```
+
+        Example using Image + Text as input:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+
+        # provide a remote URL
+        >>> image_url ="https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"
+        # or a base64-encoded image
+        >>> image_path = "/path/to/image.jpeg"
+        >>> with open(image_path, "rb") as f:
+        ...     base64_image = base64.b64encode(f.read()).decode("utf-8")
+        >>> image_url = f"data:image/jpeg;base64,{base64_image}"
+
+        >>> client = InferenceClient("meta-llama/Llama-3.2-11B-Vision-Instruct")
+        >>> output = client.chat.completions.create(
+        ...     messages=[
+        ...         {
+        ...             "role": "user",
+        ...             "content": [
+        ...                 {
+        ...                     "type": "image_url",
+        ...                     "image_url": {"url": image_url},
+        ...                 },
+        ...                 {
+        ...                     "type": "text",
+        ...                     "text": "Describe this image in one sentence.",
+        ...                 },
+        ...             ],
+        ...         },
+        ...     ],
+        ... )
+        >>> output
+        The image depicts the iconic Statue of Liberty situated in New York Harbor, New York, on a clear day.
+        ```
+
+        Example using tools:
+        ```py
+        >>> client = InferenceClient("meta-llama/Meta-Llama-3-70B-Instruct")
+        >>> messages = [
+        ...     {
+        ...         "role": "system",
+        ...         "content": "Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.",
+        ...     },
+        ...     {
+        ...         "role": "user",
+        ...         "content": "What's the weather like the next 3 days in San Francisco, CA?",
+        ...     },
+        ... ]
+        >>> tools = [
+        ...     {
+        ...         "type": "function",
+        ...         "function": {
+        ...             "name": "get_current_weather",
+        ...             "description": "Get the current weather",
+        ...             "parameters": {
+        ...                 "type": "object",
+        ...                 "properties": {
+        ...                     "location": {
+        ...                         "type": "string",
+        ...                         "description": "The city and state, e.g. San Francisco, CA",
+        ...                     },
+        ...                     "format": {
+        ...                         "type": "string",
+        ...                         "enum": ["celsius", "fahrenheit"],
+        ...                         "description": "The temperature unit to use. Infer this from the users location.",
+        ...                     },
+        ...                 },
+        ...                 "required": ["location", "format"],
+        ...             },
+        ...         },
+        ...     },
+        ...     {
+        ...         "type": "function",
+        ...         "function": {
+        ...             "name": "get_n_day_weather_forecast",
+        ...             "description": "Get an N-day weather forecast",
+        ...             "parameters": {
+        ...                 "type": "object",
+        ...                 "properties": {
+        ...                     "location": {
+        ...                         "type": "string",
+        ...                         "description": "The city and state, e.g. San Francisco, CA",
+        ...                     },
+        ...                     "format": {
+        ...                         "type": "string",
+        ...                         "enum": ["celsius", "fahrenheit"],
+        ...                         "description": "The temperature unit to use. Infer this from the users location.",
+        ...                     },
+        ...                     "num_days": {
+        ...                         "type": "integer",
+        ...                         "description": "The number of days to forecast",
+        ...                     },
+        ...                 },
+        ...                 "required": ["location", "format", "num_days"],
+        ...             },
+        ...         },
+        ...     },
+        ... ]
+
+        >>> response = client.chat_completion(
+        ...     model="meta-llama/Meta-Llama-3-70B-Instruct",
+        ...     messages=messages,
+        ...     tools=tools,
+        ...     tool_choice="auto",
+        ...     max_tokens=500,
+        ... )
+        >>> response.choices[0].message.tool_calls[0].function
+        ChatCompletionOutputFunctionDefinition(
+            arguments={
+                'location': 'San Francisco, CA',
+                'format': 'fahrenheit',
+                'num_days': 3
+            },
+            name='get_n_day_weather_forecast',
+            description=None
+        )
+        ```
+
+        Example using response_format:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient("meta-llama/Meta-Llama-3-70B-Instruct")
+        >>> messages = [
+        ...     {
+        ...         "role": "user",
+        ...         "content": "I saw a puppy a cat and a raccoon during my bike ride in the park. What did I saw and when?",
+        ...     },
+        ... ]
+        >>> response_format = {
+        ...     "type": "json",
+        ...     "value": {
+        ...         "properties": {
+        ...             "location": {"type": "string"},
+        ...             "activity": {"type": "string"},
+        ...             "animals_seen": {"type": "integer", "minimum": 1, "maximum": 5},
+        ...             "animals": {"type": "array", "items": {"type": "string"}},
+        ...         },
+        ...         "required": ["location", "activity", "animals_seen", "animals"],
+        ...     },
+        ... }
+        >>> response = client.chat_completion(
+        ...     messages=messages,
+        ...     response_format=response_format,
+        ...     max_tokens=500,
+        )
+        >>> response.choices[0].message.content
+        '{\n\n"activity": "bike ride",\n"animals": ["puppy", "cat", "raccoon"],\n"animals_seen": 3,\n"location": "park"}'
+        ```
+        """
+        # Get the provider helper
+        provider_helper = get_provider_helper(self.provider, task="conversational")
+
+        # Since `chat_completion(..., model=xxx)` is also a payload parameter for the server, we need to handle 'model' differently.
+        # `self.model` takes precedence over 'model' argument for building URL.
+        # `model` takes precedence for payload value.
+        model_id_or_url = self.model or model
+        payload_model = model or self.model
+
+        # Prepare the payload
+        parameters = {
+            "model": payload_model,
+            "frequency_penalty": frequency_penalty,
+            "logit_bias": logit_bias,
+            "logprobs": logprobs,
+            "max_tokens": max_tokens,
+            "n": n,
+            "presence_penalty": presence_penalty,
+            "response_format": response_format,
+            "seed": seed,
+            "stop": stop,
+            "temperature": temperature,
+            "tool_choice": tool_choice,
+            "tool_prompt": tool_prompt,
+            "tools": tools,
+            "top_logprobs": top_logprobs,
+            "top_p": top_p,
+            "stream": stream,
+            "stream_options": stream_options,
+            **(extra_body or {}),
+        }
+        request_parameters = provider_helper.prepare_request(
+            inputs=messages,
+            parameters=parameters,
+            headers=self.headers,
+            model=model_id_or_url,
+            api_key=self.token,
+        )
+        data = self._inner_post(request_parameters, stream=stream)
+
+        if stream:
+            return _stream_chat_completion_response(data)  # type: ignore[arg-type]
+
+        return ChatCompletionOutput.parse_obj_as_instance(data)  # type: ignore[arg-type]
+
+    def document_question_answering(
+        self,
+        image: ContentT,
+        question: str,
+        *,
+        model: Optional[str] = None,
+        doc_stride: Optional[int] = None,
+        handle_impossible_answer: Optional[bool] = None,
+        lang: Optional[str] = None,
+        max_answer_len: Optional[int] = None,
+        max_question_len: Optional[int] = None,
+        max_seq_len: Optional[int] = None,
+        top_k: Optional[int] = None,
+        word_boxes: Optional[List[Union[List[float], str]]] = None,
+    ) -> List[DocumentQuestionAnsweringOutputElement]:
+        """
+        Answer questions on document images.
+
+        Args:
+            image (`Union[str, Path, bytes, BinaryIO]`):
+                The input image for the context. It can be raw bytes, an image file, or a URL to an online image.
+            question (`str`):
+                Question to be answered.
+            model (`str`, *optional*):
+                The model to use for the document question answering task. Can be a model ID hosted on the Hugging Face Hub or a URL to
+                a deployed Inference Endpoint. If not provided, the default recommended document question answering model will be used.
+                Defaults to None.
+            doc_stride (`int`, *optional*):
+                If the words in the document are too long to fit with the question for the model, it will be split in
+                several chunks with some overlap. This argument controls the size of that overlap.
+            handle_impossible_answer (`bool`, *optional*):
+                Whether to accept impossible as an answer
+            lang (`str`, *optional*):
+                Language to use while running OCR. Defaults to english.
+            max_answer_len (`int`, *optional*):
+                The maximum length of predicted answers (e.g., only answers with a shorter length are considered).
+            max_question_len (`int`, *optional*):
+                The maximum length of the question after tokenization. It will be truncated if needed.
+            max_seq_len (`int`, *optional*):
+                The maximum length of the total sentence (context + question) in tokens of each chunk passed to the
+                model. The context will be split in several chunks (using doc_stride as overlap) if needed.
+            top_k (`int`, *optional*):
+                The number of answers to return (will be chosen by order of likelihood). Can return less than top_k
+                answers if there are not enough options available within the context.
+            word_boxes (`List[Union[List[float], str`, *optional*):
+                A list of words and bounding boxes (normalized 0->1000). If provided, the inference will skip the OCR
+                step and use the provided bounding boxes instead.
+        Returns:
+            `List[DocumentQuestionAnsweringOutputElement]`: a list of [`DocumentQuestionAnsweringOutputElement`] items containing the predicted label, associated probability, word ids, and page number.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.document_question_answering(image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png", question="What is the invoice number?")
+        [DocumentQuestionAnsweringOutputElement(answer='us-001', end=16, score=0.9999666213989258, start=16)]
+        ```
+        """
+        inputs: Dict[str, Any] = {"question": question, "image": _b64_encode(image)}
+        provider_helper = get_provider_helper(self.provider, task="document-question-answering")
+        request_parameters = provider_helper.prepare_request(
+            inputs=inputs,
+            parameters={
+                "doc_stride": doc_stride,
+                "handle_impossible_answer": handle_impossible_answer,
+                "lang": lang,
+                "max_answer_len": max_answer_len,
+                "max_question_len": max_question_len,
+                "max_seq_len": max_seq_len,
+                "top_k": top_k,
+                "word_boxes": word_boxes,
+            },
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return DocumentQuestionAnsweringOutputElement.parse_obj_as_list(response)
+
+    def feature_extraction(
+        self,
+        text: str,
+        *,
+        normalize: Optional[bool] = None,
+        prompt_name: Optional[str] = None,
+        truncate: Optional[bool] = None,
+        truncation_direction: Optional[Literal["Left", "Right"]] = None,
+        model: Optional[str] = None,
+    ) -> "np.ndarray":
+        """
+        Generate embeddings for a given text.
+
+        Args:
+            text (`str`):
+                The text to embed.
+            model (`str`, *optional*):
+                The model to use for the conversational task. Can be a model ID hosted on the Hugging Face Hub or a URL to
+                a deployed Inference Endpoint. If not provided, the default recommended conversational model will be used.
+                Defaults to None.
+            normalize (`bool`, *optional*):
+                Whether to normalize the embeddings or not.
+                Only available on server powered by Text-Embedding-Inference.
+            prompt_name (`str`, *optional*):
+                The name of the prompt that should be used by for encoding. If not set, no prompt will be applied.
+                Must be a key in the `Sentence Transformers` configuration `prompts` dictionary.
+                For example if ``prompt_name`` is "query" and the ``prompts`` is {"query": "query: ",...},
+                then the sentence "What is the capital of France?" will be encoded as "query: What is the capital of France?"
+                because the prompt text will be prepended before any text to encode.
+            truncate (`bool`, *optional*):
+                Whether to truncate the embeddings or not.
+                Only available on server powered by Text-Embedding-Inference.
+            truncation_direction (`Literal["Left", "Right"]`, *optional*):
+                Which side of the input should be truncated when `truncate=True` is passed.
+
+        Returns:
+            `np.ndarray`: The embedding representing the input text as a float32 numpy array.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.feature_extraction("Hi, who are you?")
+        array([[ 2.424802  ,  2.93384   ,  1.1750331 , ...,  1.240499, -0.13776633, -0.7889173 ],
+        [-0.42943227, -0.6364878 , -1.693462  , ...,  0.41978157, -2.4336355 ,  0.6162071 ],
+        ...,
+        [ 0.28552425, -0.928395  , -1.2077185 , ...,  0.76810825, -2.1069427 ,  0.6236161 ]], dtype=float32)
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="feature-extraction")
+        request_parameters = provider_helper.prepare_request(
+            inputs=text,
+            parameters={
+                "normalize": normalize,
+                "prompt_name": prompt_name,
+                "truncate": truncate,
+                "truncation_direction": truncation_direction,
+            },
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        np = _import_numpy()
+        return np.array(_bytes_to_dict(response), dtype="float32")
+
+    def fill_mask(
+        self,
+        text: str,
+        *,
+        model: Optional[str] = None,
+        targets: Optional[List[str]] = None,
+        top_k: Optional[int] = None,
+    ) -> List[FillMaskOutputElement]:
+        """
+        Fill in a hole with a missing word (token to be precise).
+
+        Args:
+            text (`str`):
+                a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask).
+            model (`str`, *optional*):
+                The model to use for the fill mask task. Can be a model ID hosted on the Hugging Face Hub or a URL to
+                a deployed Inference Endpoint. If not provided, the default recommended fill mask model will be used.
+            targets (`List[str`, *optional*):
+                When passed, the model will limit the scores to the passed targets instead of looking up in the whole
+                vocabulary. If the provided targets are not in the model vocab, they will be tokenized and the first
+                resulting token will be used (with a warning, and that might be slower).
+            top_k (`int`, *optional*):
+                When passed, overrides the number of predictions to return.
+        Returns:
+            `List[FillMaskOutputElement]`: a list of [`FillMaskOutputElement`] items containing the predicted label, associated
+            probability, token reference, and completed text.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.fill_mask("The goal of life is <mask>.")
+        [
+            FillMaskOutputElement(score=0.06897063553333282, token=11098, token_str=' happiness', sequence='The goal of life is happiness.'),
+            FillMaskOutputElement(score=0.06554922461509705, token=45075, token_str=' immortality', sequence='The goal of life is immortality.')
+        ]
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="fill-mask")
+        request_parameters = provider_helper.prepare_request(
+            inputs=text,
+            parameters={"targets": targets, "top_k": top_k},
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return FillMaskOutputElement.parse_obj_as_list(response)
+
+    def image_classification(
+        self,
+        image: ContentT,
+        *,
+        model: Optional[str] = None,
+        function_to_apply: Optional["ImageClassificationOutputTransform"] = None,
+        top_k: Optional[int] = None,
+    ) -> List[ImageClassificationOutputElement]:
+        """
+        Perform image classification on the given image using the specified model.
+
+        Args:
+            image (`Union[str, Path, bytes, BinaryIO]`):
+                The image to classify. It can be raw bytes, an image file, or a URL to an online image.
+            model (`str`, *optional*):
+                The model to use for image classification. Can be a model ID hosted on the Hugging Face Hub or a URL to a
+                deployed Inference Endpoint. If not provided, the default recommended model for image classification will be used.
+            function_to_apply (`"ImageClassificationOutputTransform"`, *optional*):
+                The function to apply to the model outputs in order to retrieve the scores.
+            top_k (`int`, *optional*):
+                When specified, limits the output to the top K most probable classes.
+        Returns:
+            `List[ImageClassificationOutputElement]`: a list of [`ImageClassificationOutputElement`] items containing the predicted label and associated probability.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.image_classification("https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg")
+        [ImageClassificationOutputElement(label='Blenheim spaniel', score=0.9779096841812134), ...]
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="image-classification")
+        request_parameters = provider_helper.prepare_request(
+            inputs=image,
+            parameters={"function_to_apply": function_to_apply, "top_k": top_k},
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return ImageClassificationOutputElement.parse_obj_as_list(response)
+
+    def image_segmentation(
+        self,
+        image: ContentT,
+        *,
+        model: Optional[str] = None,
+        mask_threshold: Optional[float] = None,
+        overlap_mask_area_threshold: Optional[float] = None,
+        subtask: Optional["ImageSegmentationSubtask"] = None,
+        threshold: Optional[float] = None,
+    ) -> List[ImageSegmentationOutputElement]:
+        """
+        Perform image segmentation on the given image using the specified model.
+
+        <Tip warning={true}>
+
+        You must have `PIL` installed if you want to work with images (`pip install Pillow`).
+
+        </Tip>
+
+        Args:
+            image (`Union[str, Path, bytes, BinaryIO]`):
+                The image to segment. It can be raw bytes, an image file, or a URL to an online image.
+            model (`str`, *optional*):
+                The model to use for image segmentation. Can be a model ID hosted on the Hugging Face Hub or a URL to a
+                deployed Inference Endpoint. If not provided, the default recommended model for image segmentation will be used.
+            mask_threshold (`float`, *optional*):
+                Threshold to use when turning the predicted masks into binary values.
+            overlap_mask_area_threshold (`float`, *optional*):
+                Mask overlap threshold to eliminate small, disconnected segments.
+            subtask (`"ImageSegmentationSubtask"`, *optional*):
+                Segmentation task to be performed, depending on model capabilities.
+            threshold (`float`, *optional*):
+                Probability threshold to filter out predicted masks.
+        Returns:
+            `List[ImageSegmentationOutputElement]`: A list of [`ImageSegmentationOutputElement`] items containing the segmented masks and associated attributes.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.image_segmentation("cat.jpg")
+        [ImageSegmentationOutputElement(score=0.989008, label='LABEL_184', mask=<PIL.PngImagePlugin.PngImageFile image mode=L size=400x300 at 0x7FDD2B129CC0>), ...]
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="audio-classification")
+        request_parameters = provider_helper.prepare_request(
+            inputs=image,
+            parameters={
+                "mask_threshold": mask_threshold,
+                "overlap_mask_area_threshold": overlap_mask_area_threshold,
+                "subtask": subtask,
+                "threshold": threshold,
+            },
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        output = ImageSegmentationOutputElement.parse_obj_as_list(response)
+        for item in output:
+            item.mask = _b64_to_image(item.mask)  # type: ignore [assignment]
+        return output
+
+    def image_to_image(
+        self,
+        image: ContentT,
+        prompt: Optional[str] = None,
+        *,
+        negative_prompt: Optional[str] = None,
+        num_inference_steps: Optional[int] = None,
+        guidance_scale: Optional[float] = None,
+        model: Optional[str] = None,
+        target_size: Optional[ImageToImageTargetSize] = None,
+        **kwargs,
+    ) -> "Image":
+        """
+        Perform image-to-image translation using a specified model.
+
+        <Tip warning={true}>
+
+        You must have `PIL` installed if you want to work with images (`pip install Pillow`).
+
+        </Tip>
+
+        Args:
+            image (`Union[str, Path, bytes, BinaryIO]`):
+                The input image for translation. It can be raw bytes, an image file, or a URL to an online image.
+            prompt (`str`, *optional*):
+                The text prompt to guide the image generation.
+            negative_prompt (`str`, *optional*):
+                One prompt to guide what NOT to include in image generation.
+            num_inference_steps (`int`, *optional*):
+                For diffusion models. The number of denoising steps. More denoising steps usually lead to a higher
+                quality image at the expense of slower inference.
+            guidance_scale (`float`, *optional*):
+                For diffusion models. A higher guidance scale value encourages the model to generate images closely
+                linked to the text prompt at the expense of lower image quality.
+            model (`str`, *optional*):
+                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
+                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.
+            target_size (`ImageToImageTargetSize`, *optional*):
+                The size in pixel of the output image.
+
+        Returns:
+            `Image`: The translated image.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> image = client.image_to_image("cat.jpg", prompt="turn the cat into a tiger")
+        >>> image.save("tiger.jpg")
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="image-to-image")
+        request_parameters = provider_helper.prepare_request(
+            inputs=image,
+            parameters={
+                "prompt": prompt,
+                "negative_prompt": negative_prompt,
+                "target_size": target_size,
+                "num_inference_steps": num_inference_steps,
+                "guidance_scale": guidance_scale,
+                **kwargs,
+            },
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return _bytes_to_image(response)
+
+    def image_to_text(self, image: ContentT, *, model: Optional[str] = None) -> ImageToTextOutput:
+        """
+        Takes an input image and return text.
+
+        Models can have very different outputs depending on your use case (image captioning, optical character recognition
+        (OCR), Pix2Struct, etc). Please have a look to the model card to learn more about a model's specificities.
+
+        Args:
+            image (`Union[str, Path, bytes, BinaryIO]`):
+                The input image to caption. It can be raw bytes, an image file, or a URL to an online image..
+            model (`str`, *optional*):
+                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
+                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.
+
+        Returns:
+            [`ImageToTextOutput`]: The generated text.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.image_to_text("cat.jpg")
+        'a cat standing in a grassy field '
+        >>> client.image_to_text("https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg")
+        'a dog laying on the grass next to a flower pot '
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="image-to-text")
+        request_parameters = provider_helper.prepare_request(
+            inputs=image,
+            parameters={},
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        output = ImageToTextOutput.parse_obj(response)
+        return output[0] if isinstance(output, list) else output
+
+    def object_detection(
+        self,
+        image: ContentT,
+        *,
+        model: Optional[str] = None,
+        threshold: Optional[float] = None,
+    ) -> List[ObjectDetectionOutputElement]:
+        """
+        Perform object detection on the given image using the specified model.
+
+        <Tip warning={true}>
+
+        You must have `PIL` installed if you want to work with images (`pip install Pillow`).
+
+        </Tip>
+
+        Args:
+            image (`Union[str, Path, bytes, BinaryIO]`):
+                The image to detect objects on. It can be raw bytes, an image file, or a URL to an online image.
+            model (`str`, *optional*):
+                The model to use for object detection. Can be a model ID hosted on the Hugging Face Hub or a URL to a
+                deployed Inference Endpoint. If not provided, the default recommended model for object detection (DETR) will be used.
+            threshold (`float`, *optional*):
+                The probability necessary to make a prediction.
+        Returns:
+            `List[ObjectDetectionOutputElement]`: A list of [`ObjectDetectionOutputElement`] items containing the bounding boxes and associated attributes.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+            `ValueError`:
+                If the request output is not a List.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.object_detection("people.jpg")
+        [ObjectDetectionOutputElement(score=0.9486683011054993, label='person', box=ObjectDetectionBoundingBox(xmin=59, ymin=39, xmax=420, ymax=510)), ...]
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="object-detection")
+        request_parameters = provider_helper.prepare_request(
+            inputs=image,
+            parameters={"threshold": threshold},
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return ObjectDetectionOutputElement.parse_obj_as_list(response)
+
+    def question_answering(
+        self,
+        question: str,
+        context: str,
+        *,
+        model: Optional[str] = None,
+        align_to_words: Optional[bool] = None,
+        doc_stride: Optional[int] = None,
+        handle_impossible_answer: Optional[bool] = None,
+        max_answer_len: Optional[int] = None,
+        max_question_len: Optional[int] = None,
+        max_seq_len: Optional[int] = None,
+        top_k: Optional[int] = None,
+    ) -> Union[QuestionAnsweringOutputElement, List[QuestionAnsweringOutputElement]]:
+        """
+        Retrieve the answer to a question from a given text.
+
+        Args:
+            question (`str`):
+                Question to be answered.
+            context (`str`):
+                The context of the question.
+            model (`str`):
+                The model to use for the question answering task. Can be a model ID hosted on the Hugging Face Hub or a URL to
+                a deployed Inference Endpoint.
+            align_to_words (`bool`, *optional*):
+                Attempts to align the answer to real words. Improves quality on space separated languages. Might hurt
+                on non-space-separated languages (like Japanese or Chinese)
+            doc_stride (`int`, *optional*):
+                If the context is too long to fit with the question for the model, it will be split in several chunks
+                with some overlap. This argument controls the size of that overlap.
+            handle_impossible_answer (`bool`, *optional*):
+                Whether to accept impossible as an answer.
+            max_answer_len (`int`, *optional*):
+                The maximum length of predicted answers (e.g., only answers with a shorter length are considered).
+            max_question_len (`int`, *optional*):
+                The maximum length of the question after tokenization. It will be truncated if needed.
+            max_seq_len (`int`, *optional*):
+                The maximum length of the total sentence (context + question) in tokens of each chunk passed to the
+                model. The context will be split in several chunks (using docStride as overlap) if needed.
+            top_k (`int`, *optional*):
+                The number of answers to return (will be chosen by order of likelihood). Note that we return less than
+                topk answers if there are not enough options available within the context.
+
+        Returns:
+            Union[`QuestionAnsweringOutputElement`, List[`QuestionAnsweringOutputElement`]]:
+                When top_k is 1 or not provided, it returns a single `QuestionAnsweringOutputElement`.
+                When top_k is greater than 1, it returns a list of `QuestionAnsweringOutputElement`.
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.question_answering(question="What's my name?", context="My name is Clara and I live in Berkeley.")
+        QuestionAnsweringOutputElement(answer='Clara', end=16, score=0.9326565265655518, start=11)
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="question-answering")
+        request_parameters = provider_helper.prepare_request(
+            inputs=None,
+            parameters={
+                "align_to_words": align_to_words,
+                "doc_stride": doc_stride,
+                "handle_impossible_answer": handle_impossible_answer,
+                "max_answer_len": max_answer_len,
+                "max_question_len": max_question_len,
+                "max_seq_len": max_seq_len,
+                "top_k": top_k,
+            },
+            extra_payload={"question": question, "context": context},
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        # Parse the response as a single `QuestionAnsweringOutputElement` when top_k is 1 or not provided, or a list of `QuestionAnsweringOutputElement` to ensure backward compatibility.
+        output = QuestionAnsweringOutputElement.parse_obj(response)
+        return output
+
+    def sentence_similarity(
+        self, sentence: str, other_sentences: List[str], *, model: Optional[str] = None
+    ) -> List[float]:
+        """
+        Compute the semantic similarity between a sentence and a list of other sentences by comparing their embeddings.
+
+        Args:
+            sentence (`str`):
+                The main sentence to compare to others.
+            other_sentences (`List[str]`):
+                The list of sentences to compare to.
+            model (`str`, *optional*):
+                The model to use for the conversational task. Can be a model ID hosted on the Hugging Face Hub or a URL to
+                a deployed Inference Endpoint. If not provided, the default recommended conversational model will be used.
+                Defaults to None.
+
+        Returns:
+            `List[float]`: The embedding representing the input text.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.sentence_similarity(
+        ...     "Machine learning is so easy.",
+        ...     other_sentences=[
+        ...         "Deep learning is so straightforward.",
+        ...         "This is so difficult, like rocket science.",
+        ...         "I can't believe how much I struggled with this.",
+        ...     ],
+        ... )
+        [0.7785726189613342, 0.45876261591911316, 0.2906220555305481]
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="sentence-similarity")
+        request_parameters = provider_helper.prepare_request(
+            inputs=None,
+            parameters={},
+            extra_payload={"source_sentence": sentence, "sentences": other_sentences},
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return _bytes_to_list(response)
+
+    def summarization(
+        self,
+        text: str,
+        *,
+        model: Optional[str] = None,
+        clean_up_tokenization_spaces: Optional[bool] = None,
+        generate_parameters: Optional[Dict[str, Any]] = None,
+        truncation: Optional["SummarizationTruncationStrategy"] = None,
+    ) -> SummarizationOutput:
+        """
+        Generate a summary of a given text using a specified model.
+
+        Args:
+            text (`str`):
+                The input text to summarize.
+            model (`str`, *optional*):
+                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
+                Inference Endpoint. If not provided, the default recommended model for summarization will be used.
+            clean_up_tokenization_spaces (`bool`, *optional*):
+                Whether to clean up the potential extra spaces in the text output.
+            generate_parameters (`Dict[str, Any]`, *optional*):
+                Additional parametrization of the text generation algorithm.
+            truncation (`"SummarizationTruncationStrategy"`, *optional*):
+                The truncation strategy to use.
+        Returns:
+            [`SummarizationOutput`]: The generated summary text.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.summarization("The Eiffel tower...")
+        SummarizationOutput(generated_text="The Eiffel tower is one of the most famous landmarks in the world....")
+        ```
+        """
+        parameters = {
+            "clean_up_tokenization_spaces": clean_up_tokenization_spaces,
+            "generate_parameters": generate_parameters,
+            "truncation": truncation,
+        }
+        provider_helper = get_provider_helper(self.provider, task="summarization")
+        request_parameters = provider_helper.prepare_request(
+            inputs=text,
+            parameters=parameters,
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return SummarizationOutput.parse_obj_as_list(response)[0]
+
+    def table_question_answering(
+        self,
+        table: Dict[str, Any],
+        query: str,
+        *,
+        model: Optional[str] = None,
+        padding: Optional["Padding"] = None,
+        sequential: Optional[bool] = None,
+        truncation: Optional[bool] = None,
+    ) -> TableQuestionAnsweringOutputElement:
+        """
+        Retrieve the answer to a question from information given in a table.
+
+        Args:
+            table (`str`):
+                A table of data represented as a dict of lists where entries are headers and the lists are all the
+                values, all lists must have the same size.
+            query (`str`):
+                The query in plain text that you want to ask the table.
+            model (`str`):
+                The model to use for the table-question-answering task. Can be a model ID hosted on the Hugging Face
+                Hub or a URL to a deployed Inference Endpoint.
+            padding (`"Padding"`, *optional*):
+                Activates and controls padding.
+            sequential (`bool`, *optional*):
+                Whether to do inference sequentially or as a batch. Batching is faster, but models like SQA require the
+                inference to be done sequentially to extract relations within sequences, given their conversational
+                nature.
+            truncation (`bool`, *optional*):
+                Activates and controls truncation.
+
+        Returns:
+            [`TableQuestionAnsweringOutputElement`]: a table question answering output containing the answer, coordinates, cells and the aggregator used.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> query = "How many stars does the transformers repository have?"
+        >>> table = {"Repository": ["Transformers", "Datasets", "Tokenizers"], "Stars": ["36542", "4512", "3934"]}
+        >>> client.table_question_answering(table, query, model="google/tapas-base-finetuned-wtq")
+        TableQuestionAnsweringOutputElement(answer='36542', coordinates=[[0, 1]], cells=['36542'], aggregator='AVERAGE')
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="table-question-answering")
+        request_parameters = provider_helper.prepare_request(
+            inputs=None,
+            parameters={
+                "model": model,
+                "padding": padding,
+                "sequential": sequential,
+                "truncation": truncation,
+            },
+            extra_payload={"query": query, "table": table},
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return TableQuestionAnsweringOutputElement.parse_obj_as_instance(response)
+
+    def tabular_classification(self, table: Dict[str, Any], *, model: Optional[str] = None) -> List[str]:
+        """
+        Classifying a target category (a group) based on a set of attributes.
+
+        Args:
+            table (`Dict[str, Any]`):
+                Set of attributes to classify.
+            model (`str`, *optional*):
+                The model to use for the tabular classification task. Can be a model ID hosted on the Hugging Face Hub or a URL to
+                a deployed Inference Endpoint. If not provided, the default recommended tabular classification model will be used.
+                Defaults to None.
+
+        Returns:
+            `List`: a list of labels, one per row in the initial table.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> table = {
+        ...     "fixed_acidity": ["7.4", "7.8", "10.3"],
+        ...     "volatile_acidity": ["0.7", "0.88", "0.32"],
+        ...     "citric_acid": ["0", "0", "0.45"],
+        ...     "residual_sugar": ["1.9", "2.6", "6.4"],
+        ...     "chlorides": ["0.076", "0.098", "0.073"],
+        ...     "free_sulfur_dioxide": ["11", "25", "5"],
+        ...     "total_sulfur_dioxide": ["34", "67", "13"],
+        ...     "density": ["0.9978", "0.9968", "0.9976"],
+        ...     "pH": ["3.51", "3.2", "3.23"],
+        ...     "sulphates": ["0.56", "0.68", "0.82"],
+        ...     "alcohol": ["9.4", "9.8", "12.6"],
+        ... }
+        >>> client.tabular_classification(table=table, model="julien-c/wine-quality")
+        ["5", "5", "5"]
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="tabular-classification")
+        request_parameters = provider_helper.prepare_request(
+            inputs=None,
+            extra_payload={"table": table},
+            parameters={},
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return _bytes_to_list(response)
+
+    def tabular_regression(self, table: Dict[str, Any], *, model: Optional[str] = None) -> List[float]:
+        """
+        Predicting a numerical target value given a set of attributes/features in a table.
+
+        Args:
+            table (`Dict[str, Any]`):
+                Set of attributes stored in a table. The attributes used to predict the target can be both numerical and categorical.
+            model (`str`, *optional*):
+                The model to use for the tabular regression task. Can be a model ID hosted on the Hugging Face Hub or a URL to
+                a deployed Inference Endpoint. If not provided, the default recommended tabular regression model will be used.
+                Defaults to None.
+
+        Returns:
+            `List`: a list of predicted numerical target values.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> table = {
+        ...     "Height": ["11.52", "12.48", "12.3778"],
+        ...     "Length1": ["23.2", "24", "23.9"],
+        ...     "Length2": ["25.4", "26.3", "26.5"],
+        ...     "Length3": ["30", "31.2", "31.1"],
+        ...     "Species": ["Bream", "Bream", "Bream"],
+        ...     "Width": ["4.02", "4.3056", "4.6961"],
+        ... }
+        >>> client.tabular_regression(table, model="scikit-learn/Fish-Weight")
+        [110, 120, 130]
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="tabular-regression")
+        request_parameters = provider_helper.prepare_request(
+            inputs=None,
+            parameters={},
+            extra_payload={"table": table},
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return _bytes_to_list(response)
+
+    def text_classification(
+        self,
+        text: str,
+        *,
+        model: Optional[str] = None,
+        top_k: Optional[int] = None,
+        function_to_apply: Optional["TextClassificationOutputTransform"] = None,
+    ) -> List[TextClassificationOutputElement]:
+        """
+        Perform text classification (e.g. sentiment-analysis) on the given text.
+
+        Args:
+            text (`str`):
+                A string to be classified.
+            model (`str`, *optional*):
+                The model to use for the text classification task. Can be a model ID hosted on the Hugging Face Hub or a URL to
+                a deployed Inference Endpoint. If not provided, the default recommended text classification model will be used.
+                Defaults to None.
+            top_k (`int`, *optional*):
+                When specified, limits the output to the top K most probable classes.
+            function_to_apply (`"TextClassificationOutputTransform"`, *optional*):
+                The function to apply to the model outputs in order to retrieve the scores.
+
+        Returns:
+            `List[TextClassificationOutputElement]`: a list of [`TextClassificationOutputElement`] items containing the predicted label and associated probability.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.text_classification("I like you")
+        [
+            TextClassificationOutputElement(label='POSITIVE', score=0.9998695850372314),
+            TextClassificationOutputElement(label='NEGATIVE', score=0.0001304351753788069),
+        ]
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="text-classification")
+        request_parameters = provider_helper.prepare_request(
+            inputs=text,
+            parameters={
+                "function_to_apply": function_to_apply,
+                "top_k": top_k,
+            },
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return TextClassificationOutputElement.parse_obj_as_list(response)[0]  # type: ignore [return-value]
+
+    @overload
+    def text_generation(  # type: ignore
+        self,
+        prompt: str,
+        *,
+        details: Literal[False] = ...,
+        stream: Literal[False] = ...,
+        model: Optional[str] = None,
+        # Parameters from `TextGenerationInputGenerateParameters` (maintained manually)
+        adapter_id: Optional[str] = None,
+        best_of: Optional[int] = None,
+        decoder_input_details: Optional[bool] = None,
+        do_sample: Optional[bool] = False,  # Manual default value
+        frequency_penalty: Optional[float] = None,
+        grammar: Optional[TextGenerationInputGrammarType] = None,
+        max_new_tokens: Optional[int] = None,
+        repetition_penalty: Optional[float] = None,
+        return_full_text: Optional[bool] = False,  # Manual default value
+        seed: Optional[int] = None,
+        stop: Optional[List[str]] = None,
+        stop_sequences: Optional[List[str]] = None,  # Deprecated, use `stop` instead
+        temperature: Optional[float] = None,
+        top_k: Optional[int] = None,
+        top_n_tokens: Optional[int] = None,
+        top_p: Optional[float] = None,
+        truncate: Optional[int] = None,
+        typical_p: Optional[float] = None,
+        watermark: Optional[bool] = None,
+    ) -> str:
+        ...
+
+    @overload
+    def text_generation(  # type: ignore
+        self,
+        prompt: str,
+        *,
+        details: Literal[True] = ...,
+        stream: Literal[False] = ...,
+        model: Optional[str] = None,
+        # Parameters from `TextGenerationInputGenerateParameters` (maintained manually)
+        adapter_id: Optional[str] = None,
+        best_of: Optional[int] = None,
+        decoder_input_details: Optional[bool] = None,
+        do_sample: Optional[bool] = False,  # Manual default value
+        frequency_penalty: Optional[float] = None,
+        grammar: Optional[TextGenerationInputGrammarType] = None,
+        max_new_tokens: Optional[int] = None,
+        repetition_penalty: Optional[float] = None,
+        return_full_text: Optional[bool] = False,  # Manual default value
+        seed: Optional[int] = None,
+        stop: Optional[List[str]] = None,
+        stop_sequences: Optional[List[str]] = None,  # Deprecated, use `stop` instead
+        temperature: Optional[float] = None,
+        top_k: Optional[int] = None,
+        top_n_tokens: Optional[int] = None,
+        top_p: Optional[float] = None,
+        truncate: Optional[int] = None,
+        typical_p: Optional[float] = None,
+        watermark: Optional[bool] = None,
+    ) -> TextGenerationOutput:
+        ...
+
+    @overload
+    def text_generation(  # type: ignore
+        self,
+        prompt: str,
+        *,
+        details: Literal[False] = ...,
+        stream: Literal[True] = ...,
+        model: Optional[str] = None,
+        # Parameters from `TextGenerationInputGenerateParameters` (maintained manually)
+        adapter_id: Optional[str] = None,
+        best_of: Optional[int] = None,
+        decoder_input_details: Optional[bool] = None,
+        do_sample: Optional[bool] = False,  # Manual default value
+        frequency_penalty: Optional[float] = None,
+        grammar: Optional[TextGenerationInputGrammarType] = None,
+        max_new_tokens: Optional[int] = None,
+        repetition_penalty: Optional[float] = None,
+        return_full_text: Optional[bool] = False,  # Manual default value
+        seed: Optional[int] = None,
+        stop: Optional[List[str]] = None,
+        stop_sequences: Optional[List[str]] = None,  # Deprecated, use `stop` instead
+        temperature: Optional[float] = None,
+        top_k: Optional[int] = None,
+        top_n_tokens: Optional[int] = None,
+        top_p: Optional[float] = None,
+        truncate: Optional[int] = None,
+        typical_p: Optional[float] = None,
+        watermark: Optional[bool] = None,
+    ) -> Iterable[str]:
+        ...
+
+    @overload
+    def text_generation(  # type: ignore
+        self,
+        prompt: str,
+        *,
+        details: Literal[True] = ...,
+        stream: Literal[True] = ...,
+        model: Optional[str] = None,
+        # Parameters from `TextGenerationInputGenerateParameters` (maintained manually)
+        adapter_id: Optional[str] = None,
+        best_of: Optional[int] = None,
+        decoder_input_details: Optional[bool] = None,
+        do_sample: Optional[bool] = False,  # Manual default value
+        frequency_penalty: Optional[float] = None,
+        grammar: Optional[TextGenerationInputGrammarType] = None,
+        max_new_tokens: Optional[int] = None,
+        repetition_penalty: Optional[float] = None,
+        return_full_text: Optional[bool] = False,  # Manual default value
+        seed: Optional[int] = None,
+        stop: Optional[List[str]] = None,
+        stop_sequences: Optional[List[str]] = None,  # Deprecated, use `stop` instead
+        temperature: Optional[float] = None,
+        top_k: Optional[int] = None,
+        top_n_tokens: Optional[int] = None,
+        top_p: Optional[float] = None,
+        truncate: Optional[int] = None,
+        typical_p: Optional[float] = None,
+        watermark: Optional[bool] = None,
+    ) -> Iterable[TextGenerationStreamOutput]:
+        ...
+
+    @overload
+    def text_generation(
+        self,
+        prompt: str,
+        *,
+        details: Literal[True] = ...,
+        stream: bool = ...,
+        model: Optional[str] = None,
+        # Parameters from `TextGenerationInputGenerateParameters` (maintained manually)
+        adapter_id: Optional[str] = None,
+        best_of: Optional[int] = None,
+        decoder_input_details: Optional[bool] = None,
+        do_sample: Optional[bool] = False,  # Manual default value
+        frequency_penalty: Optional[float] = None,
+        grammar: Optional[TextGenerationInputGrammarType] = None,
+        max_new_tokens: Optional[int] = None,
+        repetition_penalty: Optional[float] = None,
+        return_full_text: Optional[bool] = False,  # Manual default value
+        seed: Optional[int] = None,
+        stop: Optional[List[str]] = None,
+        stop_sequences: Optional[List[str]] = None,  # Deprecated, use `stop` instead
+        temperature: Optional[float] = None,
+        top_k: Optional[int] = None,
+        top_n_tokens: Optional[int] = None,
+        top_p: Optional[float] = None,
+        truncate: Optional[int] = None,
+        typical_p: Optional[float] = None,
+        watermark: Optional[bool] = None,
+    ) -> Union[TextGenerationOutput, Iterable[TextGenerationStreamOutput]]:
+        
+...
+
+    def text_generation(
+        self,
+        prompt: str,
+        *,
+        details: bool = False,
+        stream: bool = False,
+        model: Optional[str] = None,
+        # Parameters from `TextGenerationInputGenerateParameters` (maintained manually)
+        adapter_id: Optional[str] = None,
+        best_of: Optional[int] = None,
+        decoder_input_details: Optional[bool] = None,
+        do_sample: Optional[bool] = False,  # Manual default value
+        frequency_penalty: Optional[float] = None,
+        grammar: Optional[TextGenerationInputGrammarType] = None,
+        max_new_tokens: Optional[int] = None,
+        repetition_penalty: Optional[float] = None,
+        return_full_text: Optional[bool] = False,  # Manual default value
+        seed: Optional[int] = None,
+        stop: Optional[List[str]] = None,
+        stop_sequences: Optional[List[str]] = None,  # Deprecated, use `stop` instead
+        temperature: Optional[float] = None,
+        top_k: Optional[int] = None,
+        top_n_tokens: Optional[int] = None,
+        top_p: Optional[float] = None,
+        truncate: Optional[int] = None,
+        typical_p: Optional[float] = None,
+        watermark: Optional[bool] = None,
+    ) -> Union[str, TextGenerationOutput, Iterable[str], Iterable[TextGenerationStreamOutput]]:
+        """
+        Given a prompt, generate the following text.
+
+        <Tip>
+
+        If you want to generate a response from chat messages, you should use the [`InferenceClient.chat_completion`] method.
+        It accepts a list of messages instead of a single text prompt and handles the chat templating for you.
+
+        </Tip>
+
+        Args:
+            prompt (`str`):
+                Input text.
+            details (`bool`, *optional*):
+                By default, text_generation returns a string. Pass `details=True` if you want a detailed output (tokens,
+                probabilities, seed, finish reason, etc.). Only available for models running on with the
+                `text-generation-inference` backend.
+            stream (`bool`, *optional*):
+                By default, text_generation returns the full generated text. Pass `stream=True` if you want a stream of
+                tokens to be returned. Only available for models running on with the `text-generation-inference`
+                backend.
+            model (`str`, *optional*):
+                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
+                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.
+            adapter_id (`str`, *optional*):
+                Lora adapter id.
+            best_of (`int`, *optional*):
+                Generate best_of sequences and return the one if the highest token logprobs.
+            decoder_input_details (`bool`, *optional*):
+                Return the decoder input token logprobs and ids. You must set `details=True` as well for it to be taken
+                into account. Defaults to `False`.
+            do_sample (`bool`, *optional*):
+                Activate logits sampling
+            frequency_penalty (`float`, *optional*):
+                Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in
+                the text so far, decreasing the model's likelihood to repeat the same line verbatim.
+            grammar ([`TextGenerationInputGrammarType`], *optional*):
+                Grammar constraints. Can be either a JSONSchema or a regex.
+            max_new_tokens (`int`, *optional*):
+                Maximum number of generated tokens. Defaults to 100.
+            repetition_penalty (`float`, *optional*):
+                The parameter for repetition penalty. 1.0 means no penalty. See [this
+                paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.
+            return_full_text (`bool`, *optional*):
+                Whether to prepend the prompt to the generated text
+            seed (`int`, *optional*):
+                Random sampling seed
+            stop (`List[str]`, *optional*):
+                Stop generating tokens if a member of `stop` is generated.
+            stop_sequences (`List[str]`, *optional*):
+                Deprecated argument. Use `stop` instead.
+            temperature (`float`, *optional*):
+                The value used to module the logits distribution.
+            top_n_tokens (`int`, *optional*):
+                Return information about the `top_n_tokens` most likely tokens at each generation step, instead of
+                just the sampled token.
+            top_k (`int`, *optional`):
+                The number of highest probability vocabulary tokens to keep for top-k-filtering.
+            top_p (`float`, *optional`):
+                If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or
+                higher are kept for generation.
+            truncate (`int`, *optional`):
+                Truncate inputs tokens to the given size.
+            typical_p (`float`, *optional`):
+                Typical Decoding mass
+                See [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666) for more information
+            watermark (`bool`, *optional`):
+                Watermarking with [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)
+
+        Returns:
+            `Union[str, TextGenerationOutput, Iterable[str], Iterable[TextGenerationStreamOutput]]`:
+            Generated text returned from the server:
+            - if `stream=False` and `details=False`, the generated text is returned as a `str` (default)
+            - if `stream=True` and `details=False`, the generated text is returned token by token as a `Iterable[str]`
+            - if `stream=False` and `details=True`, the generated text is returned with more details as a [`~huggingface_hub.TextGenerationOutput`]
+            - if `details=True` and `stream=True`, the generated text is returned token by token as a iterable of [`~huggingface_hub.TextGenerationStreamOutput`]
+
+        Raises:
+            `ValidationError`:
+                If input values are not valid. No HTTP call is made to the server.
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+
+        # Case 1: generate text
+        >>> client.text_generation("The huggingface_hub library is ", max_new_tokens=12)
+        '100% open source and built to be easy to use.'
+
+        # Case 2: iterate over the generated tokens. Useful for large generation.
+        >>> for token in client.text_generation("The huggingface_hub library is ", max_new_tokens=12, stream=True):
+        ...     print(token)
+        100
+        %
+        open
+        source
+        and
+        built
+        to
+        be
+        easy
+        to
+        use
+        .
+
+        # Case 3: get more details about the generation process.
+        >>> client.text_generation("The huggingface_hub library is ", max_new_tokens=12, details=True)
+        TextGenerationOutput(
+            generated_text='100% open source and built to be easy to use.',
+            details=TextGenerationDetails(
+                finish_reason='length',
+                generated_tokens=12,
+                seed=None,
+                prefill=[
+                    TextGenerationPrefillOutputToken(id=487, text='The', logprob=None),
+                    TextGenerationPrefillOutputToken(id=53789, text=' hugging', logprob=-13.171875),
+                    (...)
+                    TextGenerationPrefillOutputToken(id=204, text=' ', logprob=-7.0390625)
+                ],
+                tokens=[
+                    TokenElement(id=1425, text='100', logprob=-1.0175781, special=False),
+                    TokenElement(id=16, text='%', logprob=-0.0463562, special=False),
+                    (...)
+                    TokenElement(id=25, text='.', logprob=-0.5703125, special=False)
+                ],
+                best_of_sequences=None
+            )
+        )
+
+        # Case 4: iterate over the generated tokens with more details.
+        # Last object is more complete, containing the full generated text and the finish reason.
+        >>> for details in client.text_generation("The huggingface_hub library is ", max_new_tokens=12, details=True, stream=True):
+        ...     print(details)
+        ...
+        TextGenerationStreamOutput(token=TokenElement(id=1425, text='100', logprob=-1.0175781, special=False), generated_text=None, details=None)
+        TextGenerationStreamOutput(token=TokenElement(id=16, text='%', logprob=-0.0463562, special=False), generated_text=None, details=None)
+        TextGenerationStreamOutput(token=TokenElement(id=1314, text=' open', logprob=-1.3359375, special=False), generated_text=None, details=None)
+        TextGenerationStreamOutput(token=TokenElement(id=3178, text=' source', logprob=-0.28100586, special=False), generated_text=None, details=None)
+        TextGenerationStreamOutput(token=TokenElement(id=273, text=' and', logprob=-0.5961914, special=False), generated_text=None, details=None)
+        TextGenerationStreamOutput(token=TokenElement(id=3426, text=' built', logprob=-1.9423828, special=False), generated_text=None, details=None)
+        TextGenerationStreamOutput(token=TokenElement(id=271, text=' to', logprob=-1.4121094, special=False), generated_text=None, details=None)
+        TextGenerationStreamOutput(token=TokenElement(id=314, text=' be', logprob=-1.5224609, special=False), generated_text=None, details=None)
+        TextGenerationStreamOutput(token=TokenElement(id=1833, text=' easy', logprob=-2.1132812, special=False), generated_text=None, details=None)
+        TextGenerationStreamOutput(token=TokenElement(id=271, text=' to', logprob=-0.08520508, special=False), generated_text=None, details=None)
+        TextGenerationStreamOutput(token=TokenElement(id=745, text=' use', logprob=-0.39453125, special=False), generated_text=None, details=None)
+        TextGenerationStreamOutput(token=TokenElement(
+            id=25,
+            text='.',
+            logprob=-0.5703125,
+            special=False),
+            generated_text='100% open source and built to be easy to use.',
+            details=TextGenerationStreamOutputStreamDetails(finish_reason='length', generated_tokens=12, seed=None)
+        )
+
+        # Case 5: generate constrained output using grammar
+        >>> response = client.text_generation(
+        ...     prompt="I saw a puppy a cat and a raccoon during my bike ride in the park",
+        ...     model="HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1",
+        ...     max_new_tokens=100,
+        ...     repetition_penalty=1.3,
+        ...     grammar={
+        ...         "type": "json",
+        ...         "value": {
+        ...             "properties": {
+        ...                 "location": {"type": "string"},
+        ...                 "activity": {"type": "string"},
+        ...                 "animals_seen": {"type": "integer", "minimum": 1, "maximum": 5},
+        ...                 "animals": {"type": "array", "items": {"type": "string"}},
+        ...             },
+        ...             "required": ["location", "activity", "animals_seen", "animals"],
+        ...         },
+        ...     },
+        ... )
+        >>> json.loads(response)
+        {
+            "activity": "bike riding",
+            "animals": ["puppy", "cat", "raccoon"],
+            "animals_seen": 3,
+            "location": "park"
+        }
+        ```
+        """
+        if decoder_input_details and not details:
+            warnings.warn(
+                "`decoder_input_details=True` has been passed to the server but `details=False` is set meaning that"
+                " the output from the server will be truncated."
+            )
+            decoder_input_details = False
+
+        if stop_sequences is not None:
+            warnings.warn(
+                "`stop_sequences` is a deprecated argument for `text_generation` task"
+                " and will be removed in version '0.28.0'. Use `stop` instead.",
+                FutureWarning,
+            )
+        if stop is None:
+            stop = stop_sequences  # use deprecated arg if provided
+
+        # Build payload
+        parameters = {
+            "adapter_id": adapter_id,
+            "best_of": best_of,
+            "decoder_input_details": decoder_input_details,
+            "details": details,
+            "do_sample": do_sample,
+            "frequency_penalty": frequency_penalty,
+            "grammar": grammar,
+            "max_new_tokens": max_new_tokens,
+            "repetition_penalty": repetition_penalty,
+            "return_full_text": return_full_text,
+            "seed": seed,
+            "stop": stop if stop is not None else [],
+            "temperature": temperature,
+            "top_k": top_k,
+            "top_n_tokens": top_n_tokens,
+            "top_p": top_p,
+            "truncate": truncate,
+            "typical_p": typical_p,
+            "watermark": watermark,
+        }
+
+        # Remove some parameters if not a TGI server
+        unsupported_kwargs = _get_unsupported_text_generation_kwargs(model)
+        if len(unsupported_kwargs) > 0:
+            # The server does not support some parameters
+            # => means it is not a TGI server
+            # => remove unsupported parameters and warn the user
+
+            ignored_parameters = []
+            for key in unsupported_kwargs:
+                if parameters.get(key):
+                    ignored_parameters.append(key)
+                parameters.pop(key, None)
+            if len(ignored_parameters) > 0:
+                warnings.warn(
+                    "API endpoint/model for text-generation is not served via TGI. Ignoring following parameters:"
+                    f" {', '.join(ignored_parameters)}.",
+                    UserWarning,
+                )
+            if details:
+                warnings.warn(
+                    "API endpoint/model for text-generation is not served via TGI. Parameter `details=True` will"
+                    " be ignored meaning only the generated text will be returned.",
+                    UserWarning,
+                )
+                details = False
+            if stream:
+                raise ValueError(
+                    "API endpoint/model for text-generation is not served via TGI. Cannot return output as a stream."
+                    " Please pass `stream=False` as input."
+                )
+
+        provider_helper = get_provider_helper(self.provider, task="text-generation")
+        request_parameters = provider_helper.prepare_request(
+            inputs=prompt,
+            parameters=parameters,
+            extra_payload={"stream": stream},
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+
+        # Handle errors separately for more precise error messages
+        try:
+            bytes_output = self._inner_post(request_parameters, stream=stream)
+        except HTTPError as e:
+            match = MODEL_KWARGS_NOT_USED_REGEX.search(str(e))
+            if isinstance(e, BadRequestError) and match:
+                unused_params = [kwarg.strip("' ") for kwarg in match.group(1).split(",")]
+                _set_unsupported_text_generation_kwargs(model, unused_params)
+                return self.text_generation(  # type: ignore
+                    prompt=prompt,
+                    details=details,
+                    stream=stream,
+                    model=model or self.model,
+                    adapter_id=adapter_id,
+                    best_of=best_of,
+                    decoder_input_details=decoder_input_details,
+                    do_sample=do_sample,
+                    frequency_penalty=frequency_penalty,
+                    grammar=grammar,
+                    max_new_tokens=max_new_tokens,
+                    repetition_penalty=repetition_penalty,
+                    return_full_text=return_full_text,
+                    seed=seed,
+                    stop=stop,
+                    temperature=temperature,
+                    top_k=top_k,
+                    top_n_tokens=top_n_tokens,
+                    top_p=top_p,
+                    truncate=truncate,
+                    typical_p=typical_p,
+                    watermark=watermark,
+                )
+            raise_text_generation_error(e)
+
+        # Parse output
+        if stream:
+            return _stream_text_generation_response(bytes_output, details)  # type: ignore
+
+        data = _bytes_to_dict(bytes_output)  # type: ignore[arg-type]
+
+        # Data can be a single element (dict) or an iterable of dicts where we select the first element of.
+        if isinstance(data, list):
+            data = data[0]
+
+        return TextGenerationOutput.parse_obj_as_instance(data) if details else data["generated_text"]
+
+    def text_to_image(
+        self,
+        prompt: str,
+        *,
+        negative_prompt: Optional[str] = None,
+        height: Optional[int] = None,
+        width: Optional[int] = None,
+        num_inference_steps: Optional[int] = None,
+        guidance_scale: Optional[float] = None,
+        model: Optional[str] = None,
+        scheduler: Optional[str] = None,
+        seed: Optional[int] = None,
+        extra_body: Optional[Dict[str, Any]] = None,
+    ) -> "Image":
+        """
+        Generate an image based on a given text using a specified model.
+
+        <Tip warning={true}>
+
+        You must have `PIL` installed if you want to work with images (`pip install Pillow`).
+
+        </Tip>
+
+        <Tip>
+        You can pass provider-specific parameters to the model by using the `extra_body` argument.
+        </Tip>
+
+        Args:
+            prompt (`str`):
+                The prompt to generate an image from.
+            negative_prompt (`str`, *optional*):
+                One prompt to guide what NOT to include in image generation.
+            height (`int`, *optional*):
+                The height in pixels of the output image
+            width (`int`, *optional*):
+                The width in pixels of the output image
+            num_inference_steps (`int`, *optional*):
+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
+                expense of slower inference.
+            guidance_scale (`float`, *optional*):
+                A higher guidance scale value encourages the model to generate images closely linked to the text
+                prompt, but values too high may cause saturation and other artifacts.
+            model (`str`, *optional*):
+                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
+                Inference Endpoint. If not provided, the default recommended text-to-image model will be used.
+                Defaults to None.
+            scheduler (`str`, *optional*):
+                Override the scheduler with a compatible one.
+            seed (`int`, *optional*):
+                Seed for the random number generator.
+            extra_body (`Dict[str, Any]`, *optional*):
+                Additional provider-specific parameters to pass to the model. Refer to the provider's documentation
+                for supported parameters.
+
+        Returns:
+            `Image`: The generated image.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+
+        >>> image = client.text_to_image("An astronaut riding a horse on the moon.")
+        >>> image.save("astronaut.png")
+
+        >>> image = client.text_to_image(
+        ...     "An astronaut riding a horse on the moon.",
+        ...     negative_prompt="low resolution, blurry",
+        ...     model="stabilityai/stable-diffusion-2-1",
+        ... )
+        >>> image.save("better_astronaut.png")
+        ```
+        Example using a third-party provider directly. Usage will be billed on your fal.ai account.
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient(
+        ...     provider="fal-ai",  # Use fal.ai provider
+        ...     api_key="fal-ai-api-key",  # Pass your fal.ai API key
+        ... )
+        >>> image = client.text_to_image(
+        ...     "A majestic lion in a fantasy forest",
+        ...     model="black-forest-labs/FLUX.1-schnell",
+        ... )
+        >>> image.save("lion.png")
+        ```
+
+        Example using a third-party provider through Hugging Face Routing. Usage will be billed on your Hugging Face account.
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient(
+        ...     provider="replicate",  # Use replicate provider
+        ...     api_key="hf_...",  # Pass your HF token
+        ... )
+        >>> image = client.text_to_image(
+        ...     "An astronaut riding a horse on the moon.",
+        ...     model="black-forest-labs/FLUX.1-dev",
+        ... )
+        >>> image.save("astronaut.png")
+        ```
+
+        Example using Replicate provider with extra parameters
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient(
+        ...     provider="replicate",  # Use replicate provider
+        ...     api_key="hf_...",  # Pass your HF token
+        ... )
+        >>> image = client.text_to_image(
+        ...     "An astronaut riding a horse on the moon.",
+        ...     model="black-forest-labs/FLUX.1-schnell",
+        ...     extra_body={"output_quality": 100},
+        ... )
+        >>> image.save("astronaut.png")
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="text-to-image")
+        request_parameters = provider_helper.prepare_request(
+            inputs=prompt,
+            parameters={
+                "negative_prompt": negative_prompt,
+                "height": height,
+                "width": width,
+                "num_inference_steps": num_inference_steps,
+                "guidance_scale": guidance_scale,
+                "scheduler": scheduler,
+                "seed": seed,
+                **(extra_body or {}),
+            },
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        response = provider_helper.get_response(response)
+        return _bytes_to_image(response)
+
+    def text_to_video(
+        self,
+        prompt: str,
+        *,
+        model: Optional[str] = None,
+        guidance_scale: Optional[float] = None,
+        negative_prompt: Optional[List[str]] = None,
+        num_frames: Optional[float] = None,
+        num_inference_steps: Optional[int] = None,
+        seed: Optional[int] = None,
+        extra_body: Optional[Dict[str, Any]] = None,
+    ) -> bytes:
+        """
+        Generate a video based on a given text.
+
+        <Tip>
+        You can pass provider-specific parameters to the model by using the `extra_body` argument.
+        </Tip>
+
+        Args:
+            prompt (`str`):
+                The prompt to generate a video from.
+            model (`str`, *optional*):
+                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
+                Inference Endpoint. If not provided, the default recommended text-to-video model will be used.
+                Defaults to None.
+            guidance_scale (`float`, *optional*):
+                A higher guidance scale value encourages the model to generate videos closely linked to the text
+                prompt, but values too high may cause saturation and other artifacts.
+            negative_prompt (`List[str]`, *optional*):
+                One or several prompt to guide what NOT to include in video generation.
+            num_frames (`float`, *optional*):
+                The num_frames parameter determines how many video frames are generated.
+            num_inference_steps (`int`, *optional*):
+                The number of denoising steps. More denoising steps usually lead to a higher quality video at the
+                expense of slower inference.
+            seed (`int`, *optional*):
+                Seed for the random number generator.
+            extra_body (`Dict[str, Any]`, *optional*):
+                Additional provider-specific parameters to pass to the model. Refer to the provider's documentation
+                for supported parameters.
+
+        Returns:
+            `bytes`: The generated video.
+
+        Example:
+
+        Example using a third-party provider directly. Usage will be billed on your fal.ai account.
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient(
+        ...     provider="fal-ai",  # Using fal.ai provider
+        ...     api_key="fal-ai-api-key",  # Pass your fal.ai API key
+        ... )
+        >>> video = client.text_to_video(
+        ...     "A majestic lion running in a fantasy forest",
+        ...     model="tencent/HunyuanVideo",
+        ... )
+        >>> with open("lion.mp4", "wb") as file:
+        ...     file.write(video)
+        ```
+
+        Example using a third-party provider through Hugging Face Routing. Usage will be billed on your Hugging Face account.
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient(
+        ...     provider="replicate",  # Using replicate provider
+        ...     api_key="hf_...",  # Pass your HF token
+        ... )
+        >>> video = client.text_to_video(
+        ...     "A cat running in a park",
+        ...     model="genmo/mochi-1-preview",
+        ... )
+        >>> with open("cat.mp4", "wb") as file:
+        ...     file.write(video)
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="text-to-video")
+        request_parameters = provider_helper.prepare_request(
+            inputs=prompt,
+            parameters={
+                "guidance_scale": guidance_scale,
+                "negative_prompt": negative_prompt,
+                "num_frames": num_frames,
+                "num_inference_steps": num_inference_steps,
+                "seed": seed,
+                **(extra_body or {}),
+            },
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        response = provider_helper.get_response(response)
+        return response
+
+    def text_to_speech(
+        self,
+        text: str,
+        *,
+        model: Optional[str] = None,
+        do_sample: Optional[bool] = None,
+        early_stopping: Optional[Union[bool, "TextToSpeechEarlyStoppingEnum"]] = None,
+        epsilon_cutoff: Optional[float] = None,
+        eta_cutoff: Optional[float] = None,
+        max_length: Optional[int] = None,
+        max_new_tokens: Optional[int] = None,
+        min_length: Optional[int] = None,
+        min_new_tokens: Optional[int] = None,
+        num_beam_groups: Optional[int] = None,
+        num_beams: Optional[int] = None,
+        penalty_alpha: Optional[float] = None,
+        temperature: Optional[float] = None,
+        top_k: Optional[int] = None,
+        top_p: Optional[float] = None,
+        typical_p: Optional[float] = None,
+        use_cache: Optional[bool] = None,
+        extra_body: Optional[Dict[str, Any]] = None,
+    ) -> bytes:
+        """
+        Synthesize an audio of a voice pronouncing a given text.
+
+        <Tip>
+        You can pass provider-specific parameters to the model by using the `extra_body` argument.
+        </Tip>
+
+        Args:
+            text (`str`):
+                The text to synthesize.
+            model (`str`, *optional*):
+                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
+                Inference Endpoint. If not provided, the default recommended text-to-speech model will be used.
+                Defaults to None.
+            do_sample (`bool`, *optional*):
+                Whether to use sampling instead of greedy decoding when generating new tokens.
+            early_stopping (`Union[bool, "TextToSpeechEarlyStoppingEnum"]`, *optional*):
+                Controls the stopping condition for beam-based methods.
+            epsilon_cutoff (`float`, *optional*):
+                If set to float strictly between 0 and 1, only tokens with a conditional probability greater than
+                epsilon_cutoff will be sampled. In the paper, suggested values range from 3e-4 to 9e-4, depending on
+                the size of the model. See [Truncation Sampling as Language Model
+                Desmoothing](https://hf.co/papers/2210.15191) for more details.
+            eta_cutoff (`float`, *optional*):
+                Eta sampling is a hybrid of locally typical sampling and epsilon sampling. If set to float strictly
+                between 0 and 1, a token is only considered if it is greater than either eta_cutoff or sqrt(eta_cutoff)
+                * exp(-entropy(softmax(next_token_logits))). The latter term is intuitively the expected next token
+                probability, scaled by sqrt(eta_cutoff). In the paper, suggested values range from 3e-4 to 2e-3,
+                depending on the size of the model. See [Truncation Sampling as Language Model
+                Desmoothing](https://hf.co/papers/2210.15191) for more details.
+            max_length (`int`, *optional*):
+                The maximum length (in tokens) of the generated text, including the input.
+            max_new_tokens (`int`, *optional*):
+                The maximum number of tokens to generate. Takes precedence over max_length.
+            min_length (`int`, *optional*):
+                The minimum length (in tokens) of the generated text, including the input.
+            min_new_tokens (`int`, *optional*):
+                The minimum number of tokens to generate. Takes precedence over min_length.
+            num_beam_groups (`int`, *optional*):
+                Number of groups to divide num_beams into in order to ensure diversity among different groups of beams.
+                See [this paper](https://hf.co/papers/1610.02424) for more details.
+            num_beams (`int`, *optional*):
+                Number of beams to use for beam search.
+            penalty_alpha (`float`, *optional*):
+                The value balances the model confidence and the degeneration penalty in contrastive search decoding.
+            temperature (`float`, *optional*):
+                The value used to modulate the next token probabilities.
+            top_k (`int`, *optional*):
+                The number of highest probability vocabulary tokens to keep for top-k-filtering.
+            top_p (`float`, *optional*):
+                If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to
+                top_p or higher are kept for generation.
+            typical_p (`float`, *optional*):
+                Local typicality measures how similar the conditional probability of predicting a target token next is
+                to the expected conditional probability of predicting a random token next, given the partial text
+                already generated. If set to float < 1, the smallest set of the most locally typical tokens with
+                probabilities that add up to typical_p or higher are kept for generation. See [this
+                paper](https://hf.co/papers/2202.00666) for more details.
+            use_cache (`bool`, *optional*):
+                Whether the model should use the past last key/values attentions to speed up decoding
+            extra_body (`Dict[str, Any]`, *optional*):
+                Additional provider-specific parameters to pass to the model. Refer to the provider's documentation
+                for supported parameters.
+        Returns:
+            `bytes`: The generated audio.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from pathlib import Path
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+
+        >>> audio = client.text_to_speech("Hello world")
+        >>> Path("hello_world.flac").write_bytes(audio)
+        ```
+
+        Example using a third-party provider directly. Usage will be billed on your Replicate account.
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient(
+        ...     provider="replicate",
+        ...     api_key="your-replicate-api-key",  # Pass your Replicate API key directly
+        ... )
+        >>> audio = client.text_to_speech(
+        ...     text="Hello world",
+        ...     model="OuteAI/OuteTTS-0.3-500M",
+        ... )
+        >>> Path("hello_world.flac").write_bytes(audio)
+        ```
+
+        Example using a third-party provider through Hugging Face Routing. Usage will be billed on your Hugging Face account.
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient(
+        ...     provider="replicate",
+        ...     api_key="hf_...",  # Pass your HF token
+        ... )
+        >>> audio =client.text_to_speech(
+        ...     text="Hello world",
+        ...     model="OuteAI/OuteTTS-0.3-500M",
+        ... )
+        >>> Path("hello_world.flac").write_bytes(audio)
+        ```
+        Example using Replicate provider with extra parameters
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient(
+        ...     provider="replicate",  # Use replicate provider
+        ...     api_key="hf_...",  # Pass your HF token
+        ... )
+        >>> audio = client.text_to_speech(
+        ...     "Hello, my name is Kororo, an awesome text-to-speech model.",
+        ...     model="hexgrad/Kokoro-82M",
+        ...     extra_body={"voice": "af_nicole"},
+        ... )
+        >>> Path("hello.flac").write_bytes(audio)
+        ```
+
+        Example music-gen using "YuE-s1-7B-anneal-en-cot" on fal.ai
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> lyrics = '''
+        ... [verse]
+        ... In the town where I was born
+        ... Lived a man who sailed to sea
+        ... And he told us of his life
+        ... In the land of submarines
+        ... So we sailed on to the sun
+        ... 'Til we found a sea of green
+        ... And we lived beneath the waves
+        ... In our yellow submarine
+
+        ... [chorus]
+        ... We all live in a yellow submarine
+        ... Yellow submarine, yellow submarine
+        ... We all live in a yellow submarine
+        ... Yellow submarine, yellow submarine
+        ... '''
+        >>> genres = "pavarotti-style tenor voice"
+        >>> client = InferenceClient(
+        ...     provider="fal-ai",
+        ...     model="m-a-p/YuE-s1-7B-anneal-en-cot",
+        ...     api_key=...,
+        ... )
+        >>> audio = client.text_to_speech(lyrics, extra_body={"genres": genres})
+        >>> with open("output.mp3", "wb") as f:
+        ...     f.write(audio)
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="text-to-speech")
+        request_parameters = provider_helper.prepare_request(
+            inputs=text,
+            parameters={
+                "do_sample": do_sample,
+                "early_stopping": early_stopping,
+                "epsilon_cutoff": epsilon_cutoff,
+                "eta_cutoff": eta_cutoff,
+                "max_length": max_length,
+                "max_new_tokens": max_new_tokens,
+                "min_length": min_length,
+                "min_new_tokens": min_new_tokens,
+                "num_beam_groups": num_beam_groups,
+                "num_beams": num_beams,
+                "penalty_alpha": penalty_alpha,
+                "temperature": temperature,
+                "top_k": top_k,
+                "top_p": top_p,
+                "typical_p": typical_p,
+                "use_cache": use_cache,
+                **(extra_body or {}),
+            },
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        response = provider_helper.get_response(response)
+        return response
+
+    def token_classification(
+        self,
+        text: str,
+        *,
+        model: Optional[str] = None,
+        aggregation_strategy: Optional["TokenClassificationAggregationStrategy"] = None,
+        ignore_labels: Optional[List[str]] = None,
+        stride: Optional[int] = None,
+    ) -> List[TokenClassificationOutputElement]:
+        """
+        Perform token classification on the given text.
+        Usually used for sentence parsing, either grammatical, or Named Entity Recognition (NER) to understand keywords contained within text.
+
+        Args:
+            text (`str`):
+                A string to be classified.
+            model (`str`, *optional*):
+                The model to use for the token classification task. Can be a model ID hosted on the Hugging Face Hub or a URL to
+                a deployed Inference Endpoint. If not provided, the default recommended token classification model will be used.
+                Defaults to None.
+            aggregation_strategy (`"TokenClassificationAggregationStrategy"`, *optional*):
+                The strategy used to fuse tokens based on model predictions
+            ignore_labels (`List[str`, *optional*):
+                A list of labels to ignore
+            stride (`int`, *optional*):
+                The number of overlapping tokens between chunks when splitting the input text.
+
+        Returns:
+            `List[TokenClassificationOutputElement]`: List of [`TokenClassificationOutputElement`] items containing the entity group, confidence score, word, start and end index.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.token_classification("My name is Sarah Jessica Parker but you can call me Jessica")
+        [
+            TokenClassificationOutputElement(
+                entity_group='PER',
+                score=0.9971321225166321,
+                word='Sarah Jessica Parker',
+                start=11,
+                end=31,
+            ),
+            TokenClassificationOutputElement(
+                entity_group='PER',
+                score=0.9773476123809814,
+                word='Jessica',
+                start=52,
+                end=59,
+            )
+        ]
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="token-classification")
+        request_parameters = provider_helper.prepare_request(
+            inputs=text,
+            parameters={
+                "aggregation_strategy": aggregation_strategy,
+                "ignore_labels": ignore_labels,
+                "stride": stride,
+            },
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return TokenClassificationOutputElement.parse_obj_as_list(response)
+
+    def translation(
+        self,
+        text: str,
+        *,
+        model: Optional[str] = None,
+        src_lang: Optional[str] = None,
+        tgt_lang: Optional[str] = None,
+        clean_up_tokenization_spaces: Optional[bool] = None,
+        truncation: Optional["TranslationTruncationStrategy"] = None,
+        generate_parameters: Optional[Dict[str, Any]] = None,
+    ) -> TranslationOutput:
+        """
+        Convert text from one language to another.
+
+        Check out https://huggingface.co/tasks/translation for more information on how to choose the best model for
+        your specific use case. Source and target languages usually depend on the model.
+        However, it is possible to specify source and target languages for certain models. If you are working with one of these models,
+        you can use `src_lang` and `tgt_lang` arguments to pass the relevant information.
+
+        Args:
+            text (`str`):
+                A string to be translated.
+            model (`str`, *optional*):
+                The model to use for the translation task. Can be a model ID hosted on the Hugging Face Hub or a URL to
+                a deployed Inference Endpoint. If not provided, the default recommended translation model will be used.
+                Defaults to None.
+            src_lang (`str`, *optional*):
+                The source language of the text. Required for models that can translate from multiple languages.
+            tgt_lang (`str`, *optional*):
+                Target language to translate to. Required for models that can translate to multiple languages.
+            clean_up_tokenization_spaces (`bool`, *optional*):
+                Whether to clean up the potential extra spaces in the text output.
+            truncation (`"TranslationTruncationStrategy"`, *optional*):
+                The truncation strategy to use.
+            generate_parameters (`Dict[str, Any]`, *optional*):
+                Additional parametrization of the text generation algorithm.
+
+        Returns:
+            [`TranslationOutput`]: The generated translated text.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+            `ValueError`:
+                If only one of the `src_lang` and `tgt_lang` arguments are provided.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.translation("My name is Wolfgang and I live in Berlin")
+        'Mein Name ist Wolfgang und ich lebe in Berlin.'
+        >>> client.translation("My name is Wolfgang and I live in Berlin", model="Helsinki-NLP/opus-mt-en-fr")
+        TranslationOutput(translation_text='Je m'appelle Wolfgang et je vis à Berlin.')
+        ```
+
+        Specifying languages:
+        ```py
+        >>> client.translation("My name is Sarah Jessica Parker but you can call me Jessica", model="facebook/mbart-large-50-many-to-many-mmt", src_lang="en_XX", tgt_lang="fr_XX")
+        "Mon nom est Sarah Jessica Parker mais vous pouvez m'appeler Jessica"
+        ```
+        """
+        # Throw error if only one of `src_lang` and `tgt_lang` was given
+        if src_lang is not None and tgt_lang is None:
+            raise ValueError("You cannot specify `src_lang` without specifying `tgt_lang`.")
+
+        if src_lang is None and tgt_lang is not None:
+            raise ValueError("You cannot specify `tgt_lang` without specifying `src_lang`.")
+
+        provider_helper = get_provider_helper(self.provider, task="translation")
+        request_parameters = provider_helper.prepare_request(
+            inputs=text,
+            parameters={
+                "src_lang": src_lang,
+                "tgt_lang": tgt_lang,
+                "clean_up_tokenization_spaces": clean_up_tokenization_spaces,
+                "truncation": truncation,
+                "generate_parameters": generate_parameters,
+            },
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return TranslationOutput.parse_obj_as_list(response)[0]
+
+    def visual_question_answering(
+        self,
+        image: ContentT,
+        question: str,
+        *,
+        model: Optional[str] = None,
+        top_k: Optional[int] = None,
+    ) -> List[VisualQuestionAnsweringOutputElement]:
+        """
+        Answering open-ended questions based on an image.
+
+        Args:
+            image (`Union[str, Path, bytes, BinaryIO]`):
+                The input image for the context. It can be raw bytes, an image file, or a URL to an online image.
+            question (`str`):
+                Question to be answered.
+            model (`str`, *optional*):
+                The model to use for the visual question answering task. Can be a model ID hosted on the Hugging Face Hub or a URL to
+                a deployed Inference Endpoint. If not provided, the default recommended visual question answering model will be used.
+                Defaults to None.
+            top_k (`int`, *optional*):
+                The number of answers to return (will be chosen by order of likelihood). Note that we return less than
+                topk answers if there are not enough options available within the context.
+        Returns:
+            `List[VisualQuestionAnsweringOutputElement]`: a list of [`VisualQuestionAnsweringOutputElement`] items containing the predicted label and associated probability.
+
+        Raises:
+            `InferenceTimeoutError`:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.visual_question_answering(
+        ...     image="https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg",
+        ...     question="What is the animal doing?"
+        ... )
+        [
+            VisualQuestionAnsweringOutputElement(score=0.778609573841095, answer='laying down'),
+            VisualQuestionAnsweringOutputElement(score=0.6957435607910156, answer='sitting'),
+        ]
+        ```
+        """
+        provider_helper = get_provider_helper(self.provider, task="visual-question-answering")
+        request_parameters = provider_helper.prepare_request(
+            inputs=image,
+            parameters={"top_k": top_k},
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+            extra_payload={"question": question, "image": _b64_encode(image)},
+        )
+        response = self._inner_post(request_parameters)
+        return VisualQuestionAnsweringOutputElement.parse_obj_as_list(response)
+
+    @_deprecate_arguments(
+        version="0.30.0",
+        deprecated_args=["labels"],
+        custom_message="`labels`has been renamed to `candidate_labels` and will be removed in huggingface_hub>=0.30.0.",
+    )
+    def zero_shot_classification(
+        self,
+        text: str,
+        # temporarily keeping it optional for backward compatibility.
+        candidate_labels: List[str] = None,  # type: ignore
+        *,
+        multi_label: Optional[bool] = False,
+        hypothesis_template: Optional[str] = None,
+        model: Optional[str] = None,
+        # deprecated argument
+        labels: List[str] = None,  # type: ignore
+    ) -> List[ZeroShotClassificationOutputElement]:
+        """
+        Provide as input a text and a set of candidate labels to classify the input text.
+
+        Args:
+            text (`str`):
+                The input text to classify.
+            candidate_labels (`List[str]`):
+                The set of possible class labels to classify the text into.
+            labels (`List[str]`, *optional*):
+                (deprecated) List of strings. Each string is the verbalization of a possible label for the input text.
+            multi_label (`bool`, *optional*):
+                Whether multiple candidate labels can be true. If false, the scores are normalized such that the sum of
+                the label likelihoods for each sequence is 1. If true, the labels are considered independent and
+                probabilities are normalized for each candidate.
+            hypothesis_template (`str`, *optional*):
+                The sentence used in conjunction with `candidate_labels` to attempt the text classification by
+                replacing the placeholder with the candidate labels.
+            model (`str`, *optional*):
+                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
+                Inference Endpoint. This parameter overrides the model defined at the instance level. If not provided, the default recommended zero-shot classification model will be used.
+
+
+        Returns:
+            `List[ZeroShotClassificationOutputElement]`: List of [`ZeroShotClassificationOutputElement`] items containing the predicted labels and their confidence.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example with `multi_label=False`:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> text = (
+        ...     "A new model offers an explanation for how the Galilean satellites formed around the solar system's"
+        ...     "largest world. Konstantin Batygin did not set out to solve one of the solar system's most puzzling"
+        ...     " mysteries when he went for a run up a hill in Nice, France."
+        ... )
+        >>> labels = ["space & cosmos", "scientific discovery", "microbiology", "robots", "archeology"]
+        >>> client.zero_shot_classification(text, labels)
+        [
+            ZeroShotClassificationOutputElement(label='scientific discovery', score=0.7961668968200684),
+            ZeroShotClassificationOutputElement(label='space & cosmos', score=0.18570658564567566),
+            ZeroShotClassificationOutputElement(label='microbiology', score=0.00730885099619627),
+            ZeroShotClassificationOutputElement(label='archeology', score=0.006258360575884581),
+            ZeroShotClassificationOutputElement(label='robots', score=0.004559356719255447),
+        ]
+        >>> client.zero_shot_classification(text, labels, multi_label=True)
+        [
+            ZeroShotClassificationOutputElement(label='scientific discovery', score=0.9829297661781311),
+            ZeroShotClassificationOutputElement(label='space & cosmos', score=0.755190908908844),
+            ZeroShotClassificationOutputElement(label='microbiology', score=0.0005462635890580714),
+            ZeroShotClassificationOutputElement(label='archeology', score=0.00047131875180639327),
+            ZeroShotClassificationOutputElement(label='robots', score=0.00030448526376858354),
+        ]
+        ```
+
+        Example with `multi_label=True` and a custom `hypothesis_template`:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.zero_shot_classification(
+        ...    text="I really like our dinner and I'm very happy. I don't like the weather though.",
+        ...    labels=["positive", "negative", "pessimistic", "optimistic"],
+        ...    multi_label=True,
+        ...    hypothesis_template="This text is {} towards the weather"
+        ... )
+        [
+            ZeroShotClassificationOutputElement(label='negative', score=0.9231801629066467),
+            ZeroShotClassificationOutputElement(label='pessimistic', score=0.8760990500450134),
+            ZeroShotClassificationOutputElement(label='optimistic', score=0.0008674879791215062),
+            ZeroShotClassificationOutputElement(label='positive', score=0.0005250611575320363)
+        ]
+        ```
+        """
+        # handle deprecation
+        if labels is not None:
+            if candidate_labels is not None:
+                raise ValueError(
+                    "Cannot specify both `labels` and `candidate_labels`. Use `candidate_labels` instead."
+                )
+            candidate_labels = labels
+        elif candidate_labels is None:
+            raise ValueError("Must specify `candidate_labels`")
+
+        provider_helper = get_provider_helper(self.provider, task="zero-shot-classification")
+        request_parameters = provider_helper.prepare_request(
+            inputs=text,
+            parameters={
+                "candidate_labels": candidate_labels,
+                "multi_label": multi_label,
+                "hypothesis_template": hypothesis_template,
+            },
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        output = _bytes_to_dict(response)
+        return [
+            ZeroShotClassificationOutputElement.parse_obj_as_instance({"label": label, "score": score})
+            for label, score in zip(output["labels"], output["scores"])
+        ]
+
+    @_deprecate_arguments(
+        version="0.30.0",
+        deprecated_args=["labels"],
+        custom_message="`labels`has been renamed to `candidate_labels` and will be removed in huggingface_hub>=0.30.0.",
+    )
+    def zero_shot_image_classification(
+        self,
+        image: ContentT,
+        # temporarily keeping it optional for backward compatibility.
+        candidate_labels: List[str] = None,  # type: ignore
+        *,
+        model: Optional[str] = None,
+        hypothesis_template: Optional[str] = None,
+        # deprecated argument
+        labels: List[str] = None,  # type: ignore
+    ) -> List[ZeroShotImageClassificationOutputElement]:
+        """
+        Provide input image and text labels to predict text labels for the image.
+
+        Args:
+            image (`Union[str, Path, bytes, BinaryIO]`):
+                The input image to caption. It can be raw bytes, an image file, or a URL to an online image.
+            candidate_labels (`List[str]`):
+                The candidate labels for this image
+            labels (`List[str]`, *optional*):
+                (deprecated) List of string possible labels. There must be at least 2 labels.
+            model (`str`, *optional*):
+                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
+                Inference Endpoint. This parameter overrides the model defined at the instance level. If not provided, the default recommended zero-shot image classification model will be used.
+            hypothesis_template (`str`, *optional*):
+                The sentence used in conjunction with `candidate_labels` to attempt the image classification by
+                replacing the placeholder with the candidate labels.
+
+        Returns:
+            `List[ZeroShotImageClassificationOutputElement]`: List of [`ZeroShotImageClassificationOutputElement`] items containing the predicted labels and their confidence.
+
+        Raises:
+            [`InferenceTimeoutError`]:
+                If the model is unavailable or the request times out.
+            `HTTPError`:
+                If the request fails with an HTTP error status code other than HTTP 503.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+
+        >>> client.zero_shot_image_classification(
+        ...     "https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg",
+        ...     labels=["dog", "cat", "horse"],
+        ... )
+        [ZeroShotImageClassificationOutputElement(label='dog', score=0.956),...]
+        ```
+        """
+        # handle deprecation
+        if labels is not None:
+            if candidate_labels is not None:
+                raise ValueError(
+                    "Cannot specify both `labels` and `candidate_labels`. Use `candidate_labels` instead."
+                )
+            candidate_labels = labels
+        elif candidate_labels is None:
+            raise ValueError("Must specify `candidate_labels`")
+        # Raise ValueError if input is less than 2 labels
+        if len(candidate_labels) < 2:
+            raise ValueError("You must specify at least 2 classes to compare.")
+
+        provider_helper = get_provider_helper(self.provider, task="zero-shot-image-classification")
+        request_parameters = provider_helper.prepare_request(
+            inputs=image,
+            parameters={
+                "candidate_labels": candidate_labels,
+                "hypothesis_template": hypothesis_template,
+            },
+            headers=self.headers,
+            model=model or self.model,
+            api_key=self.token,
+        )
+        response = self._inner_post(request_parameters)
+        return ZeroShotImageClassificationOutputElement.parse_obj_as_list(response)
+
+    @_deprecate_method(
+        version="0.33.0",
+        message=(
+            "HF Inference API is getting revamped and will only support warm models in the future (no cold start allowed)."
+            " Use `HfApi.list_models(..., inference_provider='...')` to list warm models per provider."
+        ),
+    )
+    def list_deployed_models(
+        self, frameworks: Union[None, str, Literal["all"], List[str]] = None
+    ) -> Dict[str, List[str]]:
+        """
+        List models deployed on the HF Serverless Inference API service.
+
+        This helper checks deployed models framework by framework. By default, it will check the 4 main frameworks that
+        are supported and account for 95% of the hosted models. However, if you want a complete list of models you can
+        specify `frameworks="all"` as input. Alternatively, if you know before-hand which framework you are interested
+        in, you can also restrict to search to this one (e.g. `frameworks="text-generation-inference"`). The more
+        frameworks are checked, the more time it will take.
+
+        <Tip warning={true}>
+
+        This endpoint method does not return a live list of all models available for the HF Inference API service.
+        It searches over a cached list of models that were recently available and the list may not be up to date.
+        If you want to know the live status of a specific model, use [`~InferenceClient.get_model_status`].
+
+        </Tip>
+
+        <Tip>
+
+        This endpoint method is mostly useful for discoverability. If you already know which model you want to use and want to
+        check its availability, you can directly use [`~InferenceClient.get_model_status`].
+
+        </Tip>
+
+        Args:
+            frameworks (`Literal["all"]` or `List[str]` or `str`, *optional*):
+                The frameworks to filter on. By default only a subset of the available frameworks are tested. If set to
+                "all", all available frameworks will be tested. It is also possible to provide a single framework or a
+                custom set of frameworks to check.
+
+        Returns:
+            `Dict[str, List[str]]`: A dictionary mapping task names to a sorted list of model IDs.
+
+        Example:
+        ```python
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+
+        # Discover zero-shot-classification models currently deployed
+        >>> models = client.list_deployed_models()
+        >>> models["zero-shot-classification"]
+        ['Narsil/deberta-large-mnli-zero-cls', 'facebook/bart-large-mnli', ...]
+
+        # List from only 1 framework
+        >>> client.list_deployed_models("text-generation-inference")
+        {'text-generation': ['bigcode/starcoder', 'meta-llama/Llama-2-70b-chat-hf', ...], ...}
+        ```
+        """
+        if self.provider != "hf-inference":
+            raise ValueError(f"Listing deployed models is not supported on '{self.provider}'.")
+
+        # Resolve which frameworks to check
+        if frameworks is None:
+            frameworks = constants.MAIN_INFERENCE_API_FRAMEWORKS
+        elif frameworks == "all":
+            frameworks = constants.ALL_INFERENCE_API_FRAMEWORKS
+        elif isinstance(frameworks, str):
+            frameworks = [frameworks]
+        frameworks = list(set(frameworks))
+
+        # Fetch them iteratively
+        models_by_task: Dict[str, List[str]] = {}
+
+        def _unpack_response(framework: str, items: List[Dict]) -> None:
+            for model in items:
+                if framework == "sentence-transformers":
+                    # Model running with the `sentence-transformers` framework can work with both tasks even if not
+                    # branded as such in the API response
+                    models_by_task.setdefault("feature-extraction", []).append(model["model_id"])
+                    models_by_task.setdefault("sentence-similarity", []).append(model["model_id"])
+                else:
+                    models_by_task.setdefault(model["task"], []).append(model["model_id"])
+
+        for framework in frameworks:
+            response = get_session().get(
+                f"{constants.INFERENCE_ENDPOINT}/framework/{framework}",
+                headers=build_hf_headers(token=self.token),
+            )
+            hf_raise_for_status(response)
+            _unpack_response(framework, response.json())
+
+        # Sort alphabetically for discoverability and return
+        for task, models in models_by_task.items():
+            models_by_task[task] = sorted(set(models), key=lambda x: x.lower())
+        return models_by_task
+
+    def get_endpoint_info(self, *, model: Optional[str] = None) -> Dict[str, Any]:
+        """
+        Get information about the deployed endpoint.
+
+        This endpoint is only available on endpoints powered by Text-Generation-Inference (TGI) or Text-Embedding-Inference (TEI).
+        Endpoints powered by `transformers` return an empty payload.
+
+        Args:
+            model (`str`, *optional*):
+                The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
+                Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.
+
+        Returns:
+            `Dict[str, Any]`: Information about the endpoint.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient("meta-llama/Meta-Llama-3-70B-Instruct")
+        >>> client.get_endpoint_info()
+        {
+            'model_id': 'meta-llama/Meta-Llama-3-70B-Instruct',
+            'model_sha': None,
+            'model_dtype': 'torch.float16',
+            'model_device_type': 'cuda',
+            'model_pipeline_tag': None,
+            'max_concurrent_requests': 128,
+            'max_best_of': 2,
+            'max_stop_sequences': 4,
+            'max_input_length': 8191,
+            'max_total_tokens': 8192,
+            'waiting_served_ratio': 0.3,
+            'max_batch_total_tokens': 1259392,
+            'max_waiting_tokens': 20,
+            'max_batch_size': None,
+            'validation_workers': 32,
+            'max_client_batch_size': 4,
+            'version': '2.0.2',
+            'sha': 'dccab72549635c7eb5ddb17f43f0b7cdff07c214',
+            'docker_label': 'sha-dccab72'
+        }
+        ```
+        """
+        if self.provider != "hf-inference":
+            raise ValueError(f"Getting endpoint info is not supported on '{self.provider}'.")
+
+        model = model or self.model
+        if model is None:
+            raise ValueError("Model id not provided.")
+        if model.startswith(("http://", "https://")):
+            url = model.rstrip("/") + "/info"
+        else:
+            url = f"{constants.INFERENCE_ENDPOINT}/models/{model}/info"
+
+        response = get_session().get(url, headers=build_hf_headers(token=self.token))
+        hf_raise_for_status(response)
+        return response.json()
+
+    def health_check(self, model: Optional[str] = None) -> bool:
+        """
+        Check the health of the deployed endpoint.
+
+        Health check is only available with Inference Endpoints powered by Text-Generation-Inference (TGI) or Text-Embedding-Inference (TEI).
+        For Inference API, please use [`InferenceClient.get_model_status`] instead.
+
+        Args:
+            model (`str`, *optional*):
+                URL of the Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.
+
+        Returns:
+            `bool`: True if everything is working fine.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient("https://jzgu0buei5.us-east-1.aws.endpoints.huggingface.cloud")
+        >>> client.health_check()
+        True
+        ```
+        """
+        if self.provider != "hf-inference":
+            raise ValueError(f"Health check is not supported on '{self.provider}'.")
+
+        model = model or self.model
+        if model is None:
+            raise ValueError("Model id not provided.")
+        if not model.startswith(("http://", "https://")):
+            raise ValueError(
+                "Model must be an Inference Endpoint URL. For serverless Inference API, please use `InferenceClient.get_model_status`."
+            )
+        url = model.rstrip("/") + "/health"
+
+        response = get_session().get(url, headers=build_hf_headers(token=self.token))
+        return response.status_code == 200
+
+    @_deprecate_method(
+        version="0.33.0",
+        message=(
+            "HF Inference API is getting revamped and will only support warm models in the future (no cold start allowed)."
+            " Use `HfApi.model_info` to get the model status both with HF Inference API and external providers."
+        ),
+    )
+    def get_model_status(self, model: Optional[str] = None) -> ModelStatus:
+        """
+        Get the status of a model hosted on the HF Inference API.
+
+        <Tip>
+
+        This endpoint is mostly useful when you already know which model you want to use and want to check its
+        availability. If you want to discover already deployed models, you should rather use [`~InferenceClient.list_deployed_models`].
+
+        </Tip>
+
+        Args:
+            model (`str`, *optional*):
+                Identifier of the model for witch the status gonna be checked. If model is not provided,
+                the model associated with this instance of [`InferenceClient`] will be used. Only HF Inference API service can be checked so the
+                identifier cannot be a URL.
+
+
+        Returns:
+            [`ModelStatus`]: An instance of ModelStatus dataclass, containing information,
+                         about the state of the model: load, state, compute type and framework.
+
+        Example:
+        ```py
+        >>> from huggingface_hub import InferenceClient
+        >>> client = InferenceClient()
+        >>> client.get_model_status("meta-llama/Meta-Llama-3-8B-Instruct")
+        ModelStatus(loaded=True, state='Loaded', compute_type='gpu', framework='text-generation-inference')
+        ```
+        """
+        if self.provider != "hf-inference":
+            raise ValueError(f"Getting model status is not supported on '{self.provider}'.")
+
+        model = model or self.model
+        if model is None:
+            raise ValueError("Model id not provided.")
+        if model.startswith("https://"):
+            raise NotImplementedError("Model status is only available for Inference API endpoints.")
+        url = f"{constants.INFERENCE_ENDPOINT}/status/{model}"
+
+        response = get_session().get(url, headers=build_hf_headers(token=self.token))
+        hf_raise_for_status(response)
+        response_data = response.json()
+
+        if "error" in response_data:
+            raise ValueError(response_data["error"])
+
+        return ModelStatus(
+            loaded=response_data["loaded"],
+            state=response_data["state"],
+            compute_type=response_data["compute_type"],
+            framework=response_data["framework"],
+        )
+
+    @property
+    def chat(self) -> "ProxyClientChat":
+        return ProxyClientChat(self)
+
+
+class _ProxyClient:
+    """Proxy class to be able to call `client.chat.completion.create(...)` as OpenAI client."""
+
+    def __init__(self, client: InferenceClient):
+        self._client = client
+
+
+class ProxyClientChat(_ProxyClient):
+    """Proxy class to be able to call `client.chat.completion.create(...)` as OpenAI client."""
+
+    @property
+    def completions(self) -> "ProxyClientChatCompletions":
+        return ProxyClientChatCompletions(self._client)
+
+
+class ProxyClientChatCompletions(_ProxyClient):
+    """Proxy class to be able to call `client.chat.completion.create(...)` as OpenAI client."""
+
+    @property
+    def create(self):
+        return self._client.chat_completion
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_common.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_common.py
@@ -121,9 +121,7 @@ class ModelStatus:
 def _import_aiohttp():
     # Make sure `aiohttp` is installed on the machine.
     if not is_aiohttp_available():
-        raise ImportError(
-            "Please install aiohttp to use `AsyncInferenceClient` (`pip install aiohttp`)."
-        )
+        raise ImportError("Please install aiohttp to use `AsyncInferenceClient` (`pip install aiohttp`).")
     import aiohttp
 
     return aiohttp
@@ -132,9 +130,7 @@ def _import_aiohttp():
 def _import_numpy():
     """Make sure `numpy` is installed on the machine."""
     if not is_numpy_available():
-        raise ImportError(
-            "Please install numpy to use deal with embeddings (`pip install numpy`)."
-        )
+        raise ImportError("Please install numpy to use deal with embeddings (`pip install numpy`).")
     import numpy
 
     return numpy
@@ -158,13 +154,15 @@ def _import_pil_image():
 @overload
 def _open_as_binary(
     content: ContentT,
-) -> ContextManager[BinaryT]: ...  # means "if input is not None, output is not None"
+) -> ContextManager[BinaryT]:
+    ...  # means "if input is not None, output is not None"
 
 
 @overload
 def _open_as_binary(
     content: Literal[None],
-) -> ContextManager[Literal[None]]: ...  # means "if input is None, output is None"
+) -> ContextManager[Literal[None]]:
+    ...  # means "if input is None, output is None"
 
 
 @contextmanager  # type: ignore
@@ -182,9 +180,7 @@ def _open_as_binary(
     if isinstance(content, str):
         if content.startswith("https://") or content.startswith("http://"):
             logger.debug(f"Downloading content from {content}")
-            yield get_session().get(
-                content
-            ).content  # TODO: retrieve as stream and pipe to post request ?
+            yield get_session().get(content).content  # TODO: retrieve as stream and pipe to post request ?
             return
         content = Path(content)
         if not content.exists():
@@ -300,9 +296,7 @@ def _format_text_generation_stream_outpu
 
     # Either an error as being returned
     if json_payload.get("error") is not None:
-        raise _parse_text_generation_error(
-            json_payload["error"], json_payload.get("error_type")
-        )
+        raise _parse_text_generation_error(json_payload["error"], json_payload.get("error_type"))
 
     # Or parse token payload
     output = TextGenerationStreamOutput.parse_obj_as_instance(json_payload)
@@ -350,17 +344,13 @@ def _format_chat_completion_stream_outpu
 
     # Either an error as being returned
     if json_payload.get("error") is not None:
-        raise _parse_text_generation_error(
-            json_payload["error"], json_payload.get("error_type")
-        )
+        raise _parse_text_generation_error(json_payload["error"], json_payload.get("error_type"))
 
     # Or parse token payload
     return ChatCompletionStreamOutput.parse_obj_as_instance(json_payload)
 
 
-async def _async_yield_from(
-    client: "ClientSession", response: "ClientResponse"
-) -> AsyncIterable[bytes]:
+async def _async_yield_from(client: "ClientSession", response: "ClientResponse") -> AsyncIterable[bytes]:
     async for byte_payload in response.content:
         yield byte_payload.strip()
     await client.close()
@@ -387,9 +377,7 @@ async def _async_yield_from(
 _UNSUPPORTED_TEXT_GENERATION_KWARGS: Dict[Optional[str], List[str]] = {}
 
 
-def _set_unsupported_text_generation_kwargs(
-    model: Optional[str], unsupported_kwargs: List[str]
-) -> None:
+def _set_unsupported_text_generation_kwargs(model: Optional[str], unsupported_kwargs: List[str]) -> None:
     _UNSUPPORTED_TEXT_GENERATION_KWARGS.setdefault(model, []).extend(unsupported_kwargs)
 
 
@@ -416,10 +404,7 @@ def raise_text_generation_error(http_err
 
     try:
         # Hacky way to retrieve payload in case of aiohttp error
-        payload = (
-            getattr(http_error, "response_error_payload", None)
-            or http_error.response.json()
-        )
+        payload = getattr(http_error, "response_error_payload", None) or http_error.response.json()
         error = payload.get("error")
         error_type = payload.get("error_type")
     except Exception:  # no payload
@@ -434,9 +419,7 @@ def raise_text_generation_error(http_err
     raise http_error
 
 
-def _parse_text_generation_error(
-    error: Optional[str], error_type: Optional[str]
-) -> TextGenerationError:
+def _parse_text_generation_error(error: Optional[str], error_type: Optional[str]) -> TextGenerationError:
     if error_type == "generation":
         return GenerationError(error)  # type: ignore
     if error_type == "incomplete_generation":
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/_async_client.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/_async_client.py
@@ -115,9 +115,7 @@ if TYPE_CHECKING:
 logger = logging.getLogger(__name__)
 
 
-MODEL_KWARGS_NOT_USED_REGEX = re.compile(
-    r"The following `model_kwargs` are not used by the model: \[(.*?)\]"
-)
+MODEL_KWARGS_NOT_USED_REGEX = re.compile(r"The following `model_kwargs` are not used by the model: \[(.*?)\]")
 
 
 class AsyncInferenceClient:
@@ -221,7 +219,8 @@ class AsyncInferenceClient:
         model: Optional[str] = None,
         task: Optional[str] = None,
         stream: Literal[False] = ...,
-    ) -> bytes: ...
+    ) -> bytes:
+        ...
 
     @overload
     async def post(  # type: ignore[misc]
@@ -232,7 +231,8 @@ class AsyncInferenceClient:
         model: Optional[str] = None,
         task: Optional[str] = None,
         stream: Literal[True] = ...,
-    ) -> AsyncIterable[bytes]: ...
+    ) -> AsyncIterable[bytes]:
+        ...
 
     @overload
     async def post(
@@ -243,7 +243,8 @@ class AsyncInferenceClient:
         model: Optional[str] = None,
         task: Optional[str] = None,
         stream: bool = False,
-    ) -> Union[bytes, AsyncIterable[bytes]]: ...
+    ) -> Union[bytes, AsyncIterable[bytes]]:
+        ...
 
     @_deprecate_method(
         version="0.31.0",
@@ -292,17 +293,20 @@ class AsyncInferenceClient:
     @overload
     async def _inner_post(  # type: ignore[misc]
         self, request_parameters: RequestParameters, *, stream: Literal[False] = ...
-    ) -> bytes: ...
+    ) -> bytes:
+        ...
 
     @overload
     async def _inner_post(  # type: ignore[misc]
         self, request_parameters: RequestParameters, *, stream: Literal[True] = ...
-    ) -> AsyncIterable[bytes]: ...
+    ) -> AsyncIterable[bytes]:
+        ...
 
     @overload
     async def _inner_post(
         self, request_parameters: RequestParameters, *, stream: bool = False
-    ) -> Union[bytes, AsyncIterable[bytes]]: ...
+    ) -> Union[bytes, AsyncIterable[bytes]]:
+        ...
 
     async def _inner_post(
         self, request_parameters: RequestParameters, *, stream: bool = False
@@ -312,10 +316,7 @@ class AsyncInferenceClient:
         aiohttp = _import_aiohttp()
 
         # TODO: this should be handled in provider helpers directly
-        if (
-            request_parameters.task in TASKS_EXPECTING_IMAGES
-            and "Accept" not in request_parameters.headers
-        ):
+        if request_parameters.task in TASKS_EXPECTING_IMAGES and "Accept" not in request_parameters.headers:
             request_parameters.headers["Accept"] = "image/png"
 
         while True:
@@ -334,9 +335,7 @@ class AsyncInferenceClient:
                     response_error_payload = None
                     if response.status != 200:
                         try:
-                            response_error_payload = (
-                                await response.json()
-                            )  # get payload before connection closed
+                            response_error_payload = await response.json()  # get payload before connection closed
                         except Exception:
                             pass
                     response.raise_for_status()
@@ -430,9 +429,7 @@ class AsyncInferenceClient:
         ]
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="audio-classification"
-        )
+        provider_helper = get_provider_helper(self.provider, task="audio-classification")
         request_parameters = provider_helper.prepare_request(
             inputs=audio,
             parameters={"function_to_apply": function_to_apply, "top_k": top_k},
@@ -532,9 +529,7 @@ class AsyncInferenceClient:
         "hello world"
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="automatic-speech-recognition"
-        )
+        provider_helper = get_provider_helper(self.provider, task="automatic-speech-recognition")
         request_parameters = provider_helper.prepare_request(
             inputs=audio,
             parameters={**(extra_body or {})},
@@ -563,17 +558,14 @@ class AsyncInferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[
-            Union[
-                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
-            ]
-        ] = None,
+        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
         top_p: Optional[float] = None,
         extra_body: Optional[Dict] = None,
-    ) -> ChatCompletionOutput: ...
+    ) -> ChatCompletionOutput:
+        ...
 
     @overload
     async def chat_completion(  # type: ignore
@@ -593,17 +585,14 @@ class AsyncInferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[
-            Union[
-                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
-            ]
-        ] = None,
+        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
         top_p: Optional[float] = None,
         extra_body: Optional[Dict] = None,
-    ) -> AsyncIterable[ChatCompletionStreamOutput]: ...
+    ) -> AsyncIterable[ChatCompletionStreamOutput]:
+        ...
 
     @overload
     async def chat_completion(
@@ -623,17 +612,14 @@ class AsyncInferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[
-            Union[
-                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
-            ]
-        ] = None,
+        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
         top_p: Optional[float] = None,
         extra_body: Optional[Dict] = None,
-    ) -> Union[ChatCompletionOutput, AsyncIterable[ChatCompletionStreamOutput]]: ...
+    ) -> Union[ChatCompletionOutput, AsyncIterable[ChatCompletionStreamOutput]]:
+        ...
 
     async def chat_completion(
         self,
@@ -653,11 +639,7 @@ class AsyncInferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[
-            Union[
-                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
-            ]
-        ] = None,
+        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
@@ -1112,9 +1094,7 @@ class AsyncInferenceClient:
         ```
         """
         inputs: Dict[str, Any] = {"question": question, "image": _b64_encode(image)}
-        provider_helper = get_provider_helper(
-            self.provider, task="document-question-answering"
-        )
+        provider_helper = get_provider_helper(self.provider, task="document-question-answering")
         request_parameters = provider_helper.prepare_request(
             inputs=inputs,
             parameters={
@@ -1302,9 +1282,7 @@ class AsyncInferenceClient:
         [ImageClassificationOutputElement(label='Blenheim spaniel', score=0.9779096841812134), ...]
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="image-classification"
-        )
+        provider_helper = get_provider_helper(self.provider, task="image-classification")
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={"function_to_apply": function_to_apply, "top_k": top_k},
@@ -1366,9 +1344,7 @@ class AsyncInferenceClient:
         [ImageSegmentationOutputElement(score=0.989008, label='LABEL_184', mask=<PIL.PngImagePlugin.PngImageFile image mode=L size=400x300 at 0x7FDD2B129CC0>), ...]
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="audio-classification"
-        )
+        provider_helper = get_provider_helper(self.provider, task="audio-classification")
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={
@@ -1463,9 +1439,7 @@ class AsyncInferenceClient:
         response = await self._inner_post(request_parameters)
         return _bytes_to_image(response)
 
-    async def image_to_text(
-        self, image: ContentT, *, model: Optional[str] = None
-    ) -> ImageToTextOutput:
+    async def image_to_text(self, image: ContentT, *, model: Optional[str] = None) -> ImageToTextOutput:
         """
         Takes an input image and return text.
 
@@ -1813,9 +1787,7 @@ class AsyncInferenceClient:
         TableQuestionAnsweringOutputElement(answer='36542', coordinates=[[0, 1]], cells=['36542'], aggregator='AVERAGE')
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="table-question-answering"
-        )
+        provider_helper = get_provider_helper(self.provider, task="table-question-answering")
         request_parameters = provider_helper.prepare_request(
             inputs=None,
             parameters={
@@ -1832,9 +1804,7 @@ class AsyncInferenceClient:
         response = await self._inner_post(request_parameters)
         return TableQuestionAnsweringOutputElement.parse_obj_as_instance(response)
 
-    async def tabular_classification(
-        self, table: Dict[str, Any], *, model: Optional[str] = None
-    ) -> List[str]:
+    async def tabular_classification(self, table: Dict[str, Any], *, model: Optional[str] = None) -> List[str]:
         """
         Classifying a target category (a group) based on a set of attributes.
 
@@ -1877,9 +1847,7 @@ class AsyncInferenceClient:
         ["5", "5", "5"]
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="tabular-classification"
-        )
+        provider_helper = get_provider_helper(self.provider, task="tabular-classification")
         request_parameters = provider_helper.prepare_request(
             inputs=None,
             extra_payload={"table": table},
@@ -1891,9 +1859,7 @@ class AsyncInferenceClient:
         response = await self._inner_post(request_parameters)
         return _bytes_to_list(response)
 
-    async def tabular_regression(
-        self, table: Dict[str, Any], *, model: Optional[str] = None
-    ) -> List[float]:
+    async def tabular_regression(self, table: Dict[str, Any], *, model: Optional[str] = None) -> List[float]:
         """
         Predicting a numerical target value given a set of attributes/features in a table.
 
@@ -2029,7 +1995,8 @@ class AsyncInferenceClient:
         truncate: Optional[int] = None,
         typical_p: Optional[float] = None,
         watermark: Optional[bool] = None,
-    ) -> str: ...
+    ) -> str:
+        ...
 
     @overload
     async def text_generation(  # type: ignore
@@ -2059,7 +2026,8 @@ class AsyncInferenceClient:
         truncate: Optional[int] = None,
         typical_p: Optional[float] = None,
         watermark: Optional[bool] = None,
-    ) -> TextGenerationOutput: ...
+    ) -> TextGenerationOutput:
+        ...
 
     @overload
     async def text_generation(  # type: ignore
@@ -2089,7 +2057,8 @@ class AsyncInferenceClient:
         truncate: Optional[int] = None,
         typical_p: Optional[float] = None,
         watermark: Optional[bool] = None,
-    ) -> AsyncIterable[str]: ...
+    ) -> AsyncIterable[str]:
+        ...
 
     @overload
     async def text_generation(  # type: ignore
@@ -2119,7 +2088,8 @@ class AsyncInferenceClient:
         truncate: Optional[int] = None,
         typical_p: Optional[float] = None,
         watermark: Optional[bool] = None,
-    ) -> AsyncIterable[TextGenerationStreamOutput]: ...
+    ) -> AsyncIterable[TextGenerationStreamOutput]:
+        ...
 
     @overload
     async def text_generation(
@@ -2149,7 +2119,8 @@ class AsyncInferenceClient:
         truncate: Optional[int] = None,
         typical_p: Optional[float] = None,
         watermark: Optional[bool] = None,
-    ) -> Union[TextGenerationOutput, AsyncIterable[TextGenerationStreamOutput]]: ...
+    ) -> Union[TextGenerationOutput, AsyncIterable[TextGenerationStreamOutput]]:
+        ...
 
     async def text_generation(
         self,
@@ -2456,13 +2427,9 @@ class AsyncInferenceClient:
         try:
             bytes_output = await self._inner_post(request_parameters, stream=stream)
         except _import_aiohttp().ClientResponseError as e:
-            match = MODEL_KWARGS_NOT_USED_REGEX.search(
-                e.response_error_payload["error"]
-            )
+            match = MODEL_KWARGS_NOT_USED_REGEX.search(e.response_error_payload["error"])
             if e.status == 400 and match:
-                unused_params = [
-                    kwarg.strip("' ") for kwarg in match.group(1).split(",")
-                ]
+                unused_params = [kwarg.strip("' ") for kwarg in match.group(1).split(",")]
                 _set_unsupported_text_generation_kwargs(model, unused_params)
                 return await self.text_generation(  # type: ignore
                     prompt=prompt,
@@ -2500,11 +2467,7 @@ class AsyncInferenceClient:
         if isinstance(data, list):
             data = data[0]
 
-        return (
-            TextGenerationOutput.parse_obj_as_instance(data)
-            if details
-            else data["generated_text"]
-        )
+        return TextGenerationOutput.parse_obj_as_instance(data) if details else data["generated_text"]
 
     async def text_to_image(
         self,
@@ -3014,9 +2977,7 @@ class AsyncInferenceClient:
         ]
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="token-classification"
-        )
+        provider_helper = get_provider_helper(self.provider, task="token-classification")
         request_parameters = provider_helper.prepare_request(
             inputs=text,
             parameters={
@@ -3098,14 +3059,10 @@ class AsyncInferenceClient:
         """
         # Throw error if only one of `src_lang` and `tgt_lang` was given
         if src_lang is not None and tgt_lang is None:
-            raise ValueError(
-                "You cannot specify `src_lang` without specifying `tgt_lang`."
-            )
+            raise ValueError("You cannot specify `src_lang` without specifying `tgt_lang`.")
 
         if src_lang is None and tgt_lang is not None:
-            raise ValueError(
-                "You cannot specify `tgt_lang` without specifying `src_lang`."
-            )
+            raise ValueError("You cannot specify `tgt_lang` without specifying `src_lang`.")
 
         provider_helper = get_provider_helper(self.provider, task="translation")
         request_parameters = provider_helper.prepare_request(
@@ -3171,9 +3128,7 @@ class AsyncInferenceClient:
         ]
         ```
         """
-        provider_helper = get_provider_helper(
-            self.provider, task="visual-question-answering"
-        )
+        provider_helper = get_provider_helper(self.provider, task="visual-question-answering")
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={"top_k": top_k},
@@ -3291,9 +3246,7 @@ class AsyncInferenceClient:
         elif candidate_labels is None:
             raise ValueError("Must specify `candidate_labels`")
 
-        provider_helper = get_provider_helper(
-            self.provider, task="zero-shot-classification"
-        )
+        provider_helper = get_provider_helper(self.provider, task="zero-shot-classification")
         request_parameters = provider_helper.prepare_request(
             inputs=text,
             parameters={
@@ -3308,9 +3261,7 @@ class AsyncInferenceClient:
         response = await self._inner_post(request_parameters)
         output = _bytes_to_dict(response)
         return [
-            ZeroShotClassificationOutputElement.parse_obj_as_instance(
-                {"label": label, "score": score}
-            )
+            ZeroShotClassificationOutputElement.parse_obj_as_instance({"label": label, "score": score})
             for label, score in zip(output["labels"], output["scores"])
         ]
 
@@ -3382,9 +3333,7 @@ class AsyncInferenceClient:
         if len(candidate_labels) < 2:
             raise ValueError("You must specify at least 2 classes to compare.")
 
-        provider_helper = get_provider_helper(
-            self.provider, task="zero-shot-image-classification"
-        )
+        provider_helper = get_provider_helper(self.provider, task="zero-shot-image-classification")
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={
@@ -3458,9 +3407,7 @@ class AsyncInferenceClient:
         ```
         """
         if self.provider != "hf-inference":
-            raise ValueError(
-                f"Listing deployed models is not supported on '{self.provider}'."
-            )
+            raise ValueError(f"Listing deployed models is not supported on '{self.provider}'.")
 
         # Resolve which frameworks to check
         if frameworks is None:
@@ -3479,16 +3426,10 @@ class AsyncInferenceClient:
                 if framework == "sentence-transformers":
                     # Model running with the `sentence-transformers` framework can work with both tasks even if not
                     # branded as such in the API response
-                    models_by_task.setdefault("feature-extraction", []).append(
-                        model["model_id"]
-                    )
-                    models_by_task.setdefault("sentence-similarity", []).append(
-                        model["model_id"]
-                    )
+                    models_by_task.setdefault("feature-extraction", []).append(model["model_id"])
+                    models_by_task.setdefault("sentence-similarity", []).append(model["model_id"])
                 else:
-                    models_by_task.setdefault(model["task"], []).append(
-                        model["model_id"]
-                    )
+                    models_by_task.setdefault(model["task"], []).append(model["model_id"])
 
         for framework in frameworks:
             response = get_session().get(
@@ -3589,9 +3530,7 @@ class AsyncInferenceClient:
         ```
         """
         if self.provider != "hf-inference":
-            raise ValueError(
-                f"Getting endpoint info is not supported on '{self.provider}'."
-            )
+            raise ValueError(f"Getting endpoint info is not supported on '{self.provider}'.")
 
         model = model or self.model
         if model is None:
@@ -3601,9 +3540,7 @@ class AsyncInferenceClient:
         else:
             url = f"{constants.INFERENCE_ENDPOINT}/models/{model}/info"
 
-        async with self._get_client_session(
-            headers=build_hf_headers(token=self.token)
-        ) as client:
+        async with self._get_client_session(headers=build_hf_headers(token=self.token)) as client:
             response = await client.get(url, proxy=self.proxies)
             response.raise_for_status()
             return await response.json()
@@ -3643,9 +3580,7 @@ class AsyncInferenceClient:
             )
         url = model.rstrip("/") + "/health"
 
-        async with self._get_client_session(
-            headers=build_hf_headers(token=self.token)
-        ) as client:
+        async with self._get_client_session(headers=build_hf_headers(token=self.token)) as client:
             response = await client.get(url, proxy=self.proxies)
             return response.status == 200
 
@@ -3688,22 +3623,16 @@ class AsyncInferenceClient:
         ```
         """
         if self.provider != "hf-inference":
-            raise ValueError(
-                f"Getting model status is not supported on '{self.provider}'."
-            )
+            raise ValueError(f"Getting model status is not supported on '{self.provider}'.")
 
         model = model or self.model
         if model is None:
             raise ValueError("Model id not provided.")
         if model.startswith("https://"):
-            raise NotImplementedError(
-                "Model status is only available for Inference API endpoints."
-            )
+            raise NotImplementedError("Model status is only available for Inference API endpoints.")
         url = f"{constants.INFERENCE_ENDPOINT}/status/{model}"
 
-        async with self._get_client_session(
-            headers=build_hf_headers(token=self.token)
-        ) as client:
+        async with self._get_client_session(headers=build_hf_headers(token=self.token)) as client:
             response = await client.get(url, proxy=self.proxies)
             response.raise_for_status()
             response_data = await response.json()
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/types/automatic_speech_recognition.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/types/automatic_speech_recognition.py
@@ -17,9 +17,7 @@ class AutomaticSpeechRecognitionGenerati
 
     do_sample: Optional[bool] = None
     """Whether to use sampling instead of greedy decoding when generating new tokens."""
-    early_stopping: Optional[
-        Union[bool, "AutomaticSpeechRecognitionEarlyStoppingEnum"]
-    ] = None
+    early_stopping: Optional[Union[bool, "AutomaticSpeechRecognitionEarlyStoppingEnum"]] = None
     """Controls the stopping condition for beam-based methods."""
     epsilon_cutoff: Optional[float] = None
     """If set to float strictly between 0 and 1, only tokens with a conditional probability
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/types/base.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/types/base.py
@@ -56,9 +56,7 @@ class BaseInferenceType(dict):
         """
         output = cls.parse_obj(data)
         if not isinstance(output, list):
-            raise ValueError(
-                f"Invalid input data for {cls}. Expected a list, but got {type(output)}."
-            )
+            raise ValueError(f"Invalid input data for {cls}. Expected a list, but got {type(output)}.")
         return output
 
     @classmethod
@@ -69,15 +67,11 @@ class BaseInferenceType(dict):
         """
         output = cls.parse_obj(data)
         if isinstance(output, list):
-            raise ValueError(
-                f"Invalid input data for {cls}. Expected a single instance, but got a list."
-            )
+            raise ValueError(f"Invalid input data for {cls}. Expected a single instance, but got a list.")
         return output
 
     @classmethod
-    def parse_obj(
-        cls: Type[T], data: Union[bytes, str, List, Dict]
-    ) -> Union[List[T], T]:
+    def parse_obj(cls: Type[T], data: Union[bytes, str, List, Dict]) -> Union[List[T], T]:
         """Parse server response as a dataclass or list of dataclasses.
 
         To enable future-compatibility, we want to handle cases where the server return more fields than expected.
@@ -107,9 +101,7 @@ class BaseInferenceType(dict):
                     field_type = cls.__dataclass_fields__[key].type
 
                     # if `field_type` is a `BaseInferenceType`, parse it
-                    if inspect.isclass(field_type) and issubclass(
-                        field_type, BaseInferenceType
-                    ):
+                    if inspect.isclass(field_type) and issubclass(field_type, BaseInferenceType):
                         value = field_type.parse_obj(value)
 
                     # otherwise, recursively parse nested dataclasses (if possible)
@@ -121,9 +113,7 @@ class BaseInferenceType(dict):
                                 expected_type = get_args(expected_type)[
                                     0
                                 ]  # assume same type for all items in the list
-                            if inspect.isclass(expected_type) and issubclass(
-                                expected_type, BaseInferenceType
-                            ):
+                            if inspect.isclass(expected_type) and issubclass(expected_type, BaseInferenceType):
                                 value = expected_type.parse_obj(value)
                                 break
                 init_values[key] = value
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/types/chat_completion.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/types/chat_completion.py
@@ -140,9 +140,7 @@ class ChatCompletionInput(BaseInferenceT
     lower values like 0.2 will make it more focused and deterministic.
     We generally recommend altering this or `top_p` but not both.
     """
-    tool_choice: Optional[
-        Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]
-    ] = None
+    tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None
     tool_prompt: Optional[str] = None
     """A prompt to be appended before the tools"""
     tools: Optional[List[ChatCompletionInputTool]] = None
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/types/summarization.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/types/summarization.py
@@ -8,9 +8,7 @@ from typing import Any, Dict, Literal, O
 from .base import BaseInferenceType, dataclass_with_extra
 
 
-SummarizationTruncationStrategy = Literal[
-    "do_not_truncate", "longest_first", "only_first", "only_second"
-]
+SummarizationTruncationStrategy = Literal["do_not_truncate", "longest_first", "only_first", "only_second"]
 
 
 @dataclass_with_extra
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/types/text2text_generation.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/types/text2text_generation.py
@@ -8,9 +8,7 @@ from typing import Any, Dict, Literal, O
 from .base import BaseInferenceType, dataclass_with_extra
 
 
-Text2TextGenerationTruncationStrategy = Literal[
-    "do_not_truncate", "longest_first", "only_first", "only_second"
-]
+Text2TextGenerationTruncationStrategy = Literal["do_not_truncate", "longest_first", "only_first", "only_second"]
 
 
 @dataclass_with_extra
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/types/token_classification.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/types/token_classification.py
@@ -8,9 +8,7 @@ from typing import List, Literal, Option
 from .base import BaseInferenceType, dataclass_with_extra
 
 
-TokenClassificationAggregationStrategy = Literal[
-    "none", "simple", "first", "average", "max"
-]
+TokenClassificationAggregationStrategy = Literal["none", "simple", "first", "average", "max"]
 
 
 @dataclass_with_extra
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/types/translation.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/types/translation.py
@@ -8,9 +8,7 @@ from typing import Any, Dict, Literal, O
 from .base import BaseInferenceType, dataclass_with_extra
 
 
-TranslationTruncationStrategy = Literal[
-    "do_not_truncate", "longest_first", "only_first", "only_second"
-]
+TranslationTruncationStrategy = Literal["do_not_truncate", "longest_first", "only_first", "only_second"]
 
 
 @dataclass_with_extra
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/__init__.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/__init__.py
@@ -73,9 +73,7 @@ PROVIDERS: Dict[PROVIDER_T, Dict[str, Ta
         "text-classification": HFInferenceTask("text-classification"),
         "question-answering": HFInferenceTask("question-answering"),
         "audio-classification": HFInferenceBinaryInputTask("audio-classification"),
-        "automatic-speech-recognition": HFInferenceBinaryInputTask(
-            "automatic-speech-recognition"
-        ),
+        "automatic-speech-recognition": HFInferenceBinaryInputTask("automatic-speech-recognition"),
         "fill-mask": HFInferenceTask("fill-mask"),
         "feature-extraction": HFInferenceTask("feature-extraction"),
         "image-classification": HFInferenceBinaryInputTask("image-classification"),
@@ -84,9 +82,7 @@ PROVIDERS: Dict[PROVIDER_T, Dict[str, Ta
         "image-to-text": HFInferenceBinaryInputTask("image-to-text"),
         "object-detection": HFInferenceBinaryInputTask("object-detection"),
         "audio-to-audio": HFInferenceBinaryInputTask("audio-to-audio"),
-        "zero-shot-image-classification": HFInferenceBinaryInputTask(
-            "zero-shot-image-classification"
-        ),
+        "zero-shot-image-classification": HFInferenceBinaryInputTask("zero-shot-image-classification"),
         "zero-shot-classification": HFInferenceTask("zero-shot-classification"),
         "image-to-image": HFInferenceBinaryInputTask("image-to-image"),
         "sentence-similarity": HFInferenceTask("sentence-similarity"),
@@ -96,9 +92,7 @@ PROVIDERS: Dict[PROVIDER_T, Dict[str, Ta
         "token-classification": HFInferenceTask("token-classification"),
         "translation": HFInferenceTask("translation"),
         "summarization": HFInferenceTask("summarization"),
-        "visual-question-answering": HFInferenceBinaryInputTask(
-            "visual-question-answering"
-        ),
+        "visual-question-answering": HFInferenceBinaryInputTask("visual-question-answering"),
     },
     "hyperbolic": {
         "text-to-image": HyperbolicTextToImageTask(),
@@ -144,9 +138,7 @@ def get_provider_helper(provider: PROVID
         ValueError: If provider or task is not supported
     """
     if provider not in PROVIDERS:
-        raise ValueError(
-            f"Provider '{provider}' not supported. Available providers: {list(PROVIDERS.keys())}"
-        )
+        raise ValueError(f"Provider '{provider}' not supported. Available providers: {list(PROVIDERS.keys())}")
     if task not in PROVIDERS[provider]:
         raise ValueError(
             f"Task '{task}' not supported for provider '{provider}'. "
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/_common.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/_common.py
@@ -70,16 +70,12 @@ class TaskProviderHelper:
         url = self._prepare_url(api_key, mapped_model)
 
         # prepare payload (to customize in subclasses)
-        payload = self._prepare_payload_as_dict(
-            inputs, parameters, mapped_model=mapped_model
-        )
+        payload = self._prepare_payload_as_dict(inputs, parameters, mapped_model=mapped_model)
         if payload is not None:
             payload = recursive_merge(payload, extra_payload or {})
 
         # body data (to customize in subclasses)
-        data = self._prepare_payload_as_bytes(
-            inputs, parameters, mapped_model, extra_payload
-        )
+        data = self._prepare_payload_as_bytes(inputs, parameters, mapped_model, extra_payload)
 
         # check if both payload and data are set and return
         if payload is not None and data is not None:
@@ -119,9 +115,7 @@ class TaskProviderHelper:
 
         Usually not overwritten in subclasses."""
         if model is None:
-            raise ValueError(
-                f"Please provide an HF model ID supported by {self.provider}."
-            )
+            raise ValueError(f"Please provide an HF model ID supported by {self.provider}.")
 
         # hardcoded mapping for local testing
         if HARDCODED_MODEL_ID_MAPPING.get(self.provider, {}).get(model):
@@ -129,9 +123,7 @@ class TaskProviderHelper:
 
         provider_mapping = _fetch_inference_provider_mapping(model).get(self.provider)
         if provider_mapping is None:
-            raise ValueError(
-                f"Model {model} is not supported by provider {self.provider}."
-            )
+            raise ValueError(f"Model {model} is not supported by provider {self.provider}.")
 
         if provider_mapping.task != self.task:
             raise ValueError(
@@ -166,9 +158,7 @@ class TaskProviderHelper:
         Usually not overwritten in subclasses."""
         # Route to the proxy if the api_key is a HF TOKEN
         if api_key.startswith("hf_"):
-            logger.info(
-                f"Calling '{self.provider}' provider through Hugging Face router."
-            )
+            logger.info(f"Calling '{self.provider}' provider through Hugging Face router.")
             return constants.INFERENCE_PROXY_TEMPLATE.format(provider=self.provider)
         else:
             logger.info(f"Calling '{self.provider}' provider directly.")
@@ -181,9 +171,7 @@ class TaskProviderHelper:
         """
         return ""
 
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
         """Return the payload to use for the request, as a dict.
 
         Override this method in subclasses for customized payloads.
@@ -218,9 +206,7 @@ class BaseConversationalTask(TaskProvide
     def _prepare_route(self, mapped_model: str) -> str:
         return "/v1/chat/completions"
 
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
         return {"messages": inputs, **filter_none(parameters), "model": mapped_model}
 
 
@@ -236,9 +222,7 @@ class BaseTextGenerationTask(TaskProvide
     def _prepare_route(self, mapped_model: str) -> str:
         return "/v1/completions"
 
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
         return {"prompt": inputs, **filter_none(parameters), "model": mapped_model}
 
 
@@ -262,11 +246,7 @@ def recursive_merge(dict1: Dict, dict2:
         **{
             key: (
                 recursive_merge(dict1[key], value)
-                if (
-                    key in dict1
-                    and isinstance(dict1[key], dict)
-                    and isinstance(value, dict)
-                )
+                if (key in dict1 and isinstance(dict1[key], dict) and isinstance(value, dict))
                 else value
             )
             for key, value in dict2.items()
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/black_forest_labs.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/black_forest_labs.py
@@ -31,9 +31,7 @@ class BlackForestLabsTextToImageTask(Tas
     def _prepare_route(self, mapped_model: str) -> str:
         return mapped_model
 
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
         parameters = filter_none(parameters)
         if "num_inference_steps" in parameters:
             parameters["steps"] = parameters.pop("num_inference_steps")
@@ -69,6 +67,4 @@ class BlackForestLabsTextToImageTask(Tas
                 image_resp.raise_for_status()
                 return image_resp.content
 
-        raise TimeoutError(
-            f"Failed to get the image URL after {MAX_POLLING_ATTEMPTS} attempts."
-        )
+        raise TimeoutError(f"Failed to get the image URL after {MAX_POLLING_ATTEMPTS} attempts.")
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/fal_ai.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/fal_ai.py
@@ -25,9 +25,7 @@ class FalAIAutomaticSpeechRecognitionTas
     def __init__(self):
         super().__init__("automatic-speech-recognition")
 
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
         if isinstance(inputs, str) and inputs.startswith(("http://", "https://")):
             # If input is a URL, pass it directly
             audio_url = inputs
@@ -46,9 +44,7 @@ class FalAIAutomaticSpeechRecognitionTas
     def get_response(self, response: Union[bytes, Dict]) -> Any:
         text = _as_dict(response)["text"]
         if not isinstance(text, str):
-            raise ValueError(
-                f"Unexpected output format from FalAI API. Expected string, got {type(text)}."
-            )
+            raise ValueError(f"Unexpected output format from FalAI API. Expected string, got {type(text)}.")
         return text
 
 
@@ -56,9 +52,7 @@ class FalAITextToImageTask(FalAITask):
     def __init__(self):
         super().__init__("text-to-image")
 
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
         parameters = filter_none(parameters)
         if "width" in parameters and "height" in parameters:
             parameters["image_size"] = {
@@ -76,9 +70,7 @@ class FalAITextToSpeechTask(FalAITask):
     def __init__(self):
         super().__init__("text-to-speech")
 
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
         return {"lyrics": inputs, **filter_none(parameters)}
 
     def get_response(self, response: Union[bytes, Dict]) -> Any:
@@ -90,9 +82,7 @@ class FalAITextToVideoTask(FalAITask):
     def __init__(self):
         super().__init__("text-to-video")
 
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
         return {"prompt": inputs, **filter_none(parameters)}
 
     def get_response(self, response: Union[bytes, Dict]) -> Any:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/fireworks_ai.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/fireworks_ai.py
@@ -3,6 +3,4 @@ from ._common import BaseConversationalT
 
 class FireworksAIConversationalTask(BaseConversationalTask):
     def __init__(self):
-        super().__init__(
-            provider="fireworks-ai", base_url="https://api.fireworks.ai/inference"
-        )
+        super().__init__(provider="fireworks-ai", base_url="https://api.fireworks.ai/inference")
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/hf_inference.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/hf_inference.py
@@ -51,22 +51,16 @@ class HFInferenceTask(TaskProviderHelper
             else f"{self.base_url}/models/{mapped_model}"
         )
 
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
         if isinstance(inputs, bytes):
             raise ValueError(f"Unexpected binary input for task {self.task}.")
         if isinstance(inputs, Path):
-            raise ValueError(
-                f"Unexpected path input for task {self.task} (got {inputs})"
-            )
+            raise ValueError(f"Unexpected path input for task {self.task} (got {inputs})")
         return {"inputs": inputs, "parameters": filter_none(parameters)}
 
 
 class HFInferenceBinaryInputTask(HFInferenceTask):
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
         return None
 
     def _prepare_payload_as_bytes(
@@ -82,9 +76,7 @@ class HFInferenceBinaryInputTask(HFInfer
 
         # Raise if not a binary object or a local path or a URL.
         if not isinstance(inputs, (bytes, Path)) and not isinstance(inputs, str):
-            raise ValueError(
-                f"Expected binary inputs or a local path or a URL. Got {inputs}"
-            )
+            raise ValueError(f"Expected binary inputs or a local path or a URL. Got {inputs}")
 
         # Send inputs as raw content when no parameters are provided
         if not has_parameters:
@@ -93,18 +85,14 @@ class HFInferenceBinaryInputTask(HFInfer
                 return data_as_bytes
 
         # Otherwise encode as b64
-        return json.dumps(
-            {"inputs": _b64_encode(inputs), "parameters": parameters, **extra_payload}
-        ).encode("utf-8")
+        return json.dumps({"inputs": _b64_encode(inputs), "parameters": parameters, **extra_payload}).encode("utf-8")
 
 
 class HFInferenceConversational(HFInferenceTask):
     def __init__(self):
         super().__init__("text-generation")
 
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
         payload_model = parameters.get("model") or mapped_model
 
         if payload_model is None or payload_model.startswith(("http://", "https://")):
@@ -138,11 +126,6 @@ def _build_chat_completion_url(model_url
 
 @lru_cache(maxsize=1)
 def _fetch_recommended_models() -> Dict[str, Optional[str]]:
-    response = get_session().get(
-        f"{constants.ENDPOINT}/api/tasks", headers=build_hf_headers()
-    )
+    response = get_session().get(f"{constants.ENDPOINT}/api/tasks", headers=build_hf_headers())
     hf_raise_for_status(response)
-    return {
-        task: next(iter(details["widgetModels"]), None)
-        for task, details in response.json().items()
-    }
+    return {task: next(iter(details["widgetModels"]), None) for task, details in response.json().items()}
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/hyperbolic.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/hyperbolic.py
@@ -20,9 +20,7 @@ class HyperbolicTextToImageTask(TaskProv
     def _prepare_route(self, mapped_model: str) -> str:
         return "/v1/images/generations"
 
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
         parameters = filter_none(parameters)
         if "num_inference_steps" in parameters:
             parameters["steps"] = parameters.pop("num_inference_steps")
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/nebius.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/nebius.py
@@ -31,9 +31,7 @@ class NebiusTextToImageTask(TaskProvider
     def _prepare_route(self, mapped_model: str) -> str:
         return "/v1/images/generations"
 
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
         parameters = filter_none(parameters)
         if "guidance_scale" in parameters:
             parameters.pop("guidance_scale")
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/replicate.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/replicate.py
@@ -23,12 +23,8 @@ class ReplicateTask(TaskProviderHelper):
             return "/v1/predictions"
         return f"/v1/models/{mapped_model}/predictions"
 
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
-        payload: Dict[str, Any] = {
-            "input": {"prompt": inputs, **filter_none(parameters)}
-        }
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+        payload: Dict[str, Any] = {"input": {"prompt": inputs, **filter_none(parameters)}}
         if ":" in mapped_model:
             version = mapped_model.split(":", 1)[1]
             payload["version"] = version
@@ -42,9 +38,7 @@ class ReplicateTask(TaskProviderHelper):
                 "The model might be in cold state or starting up. Please try again later."
             )
         output_url = (
-            response_dict["output"]
-            if isinstance(response_dict["output"], str)
-            else response_dict["output"][0]
+            response_dict["output"] if isinstance(response_dict["output"], str) else response_dict["output"][0]
         )
         return get_session().get(output_url).content
 
@@ -53,11 +47,7 @@ class ReplicateTextToSpeechTask(Replicat
     def __init__(self):
         super().__init__("text-to-speech")
 
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
         payload: Dict = super()._prepare_payload_as_dict(inputs, parameters, mapped_model)  # type: ignore[assignment]
-        payload["input"]["text"] = payload["input"].pop(
-            "prompt"
-        )  # rename "prompt" to "text" for TTS
+        payload["input"]["text"] = payload["input"].pop("prompt")  # rename "prompt" to "text" for TTS
         return payload
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/together.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/together.py
@@ -45,9 +45,7 @@ class TogetherTextToImageTask(TogetherTa
     def __init__(self):
         super().__init__("text-to-image")
 
-    def _prepare_payload_as_dict(
-        self, inputs: Any, parameters: Dict, mapped_model: str
-    ) -> Optional[Dict]:
+    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
         parameters = filter_none(parameters)
         if "num_inference_steps" in parameters:
             parameters["steps"] = parameters.pop("num_inference_steps")
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference_api.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference_api.py
@@ -193,9 +193,7 @@ class InferenceApi:
             payload["parameters"] = params
 
         # Make API call
-        response = get_session().post(
-            self.api_url, headers=self.headers, json=payload, data=data
-        )
+        response = get_session().post(self.api_url, headers=self.headers, json=payload, data=data)
 
         # Let the user handle the response
         if raw_response:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/keras_mixin.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/keras_mixin.py
@@ -88,9 +88,7 @@ def _create_hyperparameter_table(model):
         optimizer_params = model.optimizer.get_config()
         # flatten the configuration
         optimizer_params = _flatten_dict(optimizer_params)
-        optimizer_params["training_precision"] = (
-            keras.mixed_precision.global_policy().name
-        )
+        optimizer_params["training_precision"] = keras.mixed_precision.global_policy().name
         table = "| Hyperparameters | Value |\n| :-- | :-- |\n"
         for key, value in optimizer_params.items():
             table += f"| {key} | {value} |\n"
@@ -191,9 +189,7 @@ def save_pretrained_keras(
             [`tf.keras.models.save_model()`](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model).
     """
     if keras is None:
-        raise ImportError(
-            "Called a Tensorflow-specific function but could not import it."
-        )
+        raise ImportError("Called a Tensorflow-specific function but could not import it.")
 
     if not model.built:
         raise ValueError("Model should be built before trying to save")
@@ -204,9 +200,7 @@ def save_pretrained_keras(
     # saving config
     if config:
         if not isinstance(config, dict):
-            raise RuntimeError(
-                f"Provided config to save_pretrained_keras should be a dict. Got: '{type(config)}'"
-            )
+            raise RuntimeError(f"Provided config to save_pretrained_keras should be a dict. Got: '{type(config)}'")
 
         with (save_directory / constants.CONFIG_NAME).open("w") as f:
             json.dump(config, f)
@@ -240,9 +234,7 @@ def save_pretrained_keras(
                 json.dump(model.history.history, f, indent=2, sort_keys=True)
 
     _create_model_card(model, save_directory, plot_model, metadata)
-    keras.models.save_model(
-        model, save_directory, include_optimizer=include_optimizer, **model_save_kwargs
-    )
+    keras.models.save_model(model, save_directory, include_optimizer=include_optimizer, **model_save_kwargs)
 
 
 def from_pretrained_keras(*args, **kwargs) -> "KerasModelHubMixin":
@@ -382,9 +374,7 @@ def push_to_hub_keras(
         The url of the commit of your model in the given repository.
     """
     api = HfApi(endpoint=api_endpoint)
-    repo_id = api.create_repo(
-        repo_id=repo_id, token=token, private=private, exist_ok=True
-    ).repo_id
+    repo_id = api.create_repo(repo_id=repo_id, token=token, private=private, exist_ok=True).repo_id
 
     # Push the files to the repo in a single commit
     with SoftTemporaryDirectory() as tmp:
@@ -487,9 +477,7 @@ class KerasModelHubMixin(ModelHubMixin):
                 snapshot_download instead of hf_hub_download.
         """
         if keras is None:
-            raise ImportError(
-                "Called a TensorFlow-specific function but could not import it."
-            )
+            raise ImportError("Called a TensorFlow-specific function but could not import it.")
 
         # Root is either a local filepath matching model_id or a cached snapshot
         if not os.path.isdir(model_id):
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/lfs.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/lfs.py
@@ -166,9 +166,7 @@ def post_lfs_batch_info(
         "hash_algo": "sha256",
     }
     if revision is not None:
-        payload["ref"] = {
-            "name": unquote(revision)
-        }  # revision has been previously 'quoted'
+        payload["ref"] = {"name": unquote(revision)}  # revision has been previously 'quoted'
 
     headers = {
         **LFS_HEADERS,
@@ -233,9 +231,7 @@ def lfs_upload(
     actions = lfs_batch_action.get("actions")
     if actions is None:
         # The file was already uploaded
-        logger.debug(
-            f"Content of file {operation.path_in_repo} is already present upstream - skipping upload"
-        )
+        logger.debug(f"Content of file {operation.path_in_repo} is already present upstream - skipping upload")
         return
 
     # 1. Validate server response (check required keys in dict)
@@ -285,10 +281,7 @@ def _validate_lfs_action(lfs_action: dic
     """validates response from the LFS batch endpoint"""
     if not (
         isinstance(lfs_action.get("href"), str)
-        and (
-            lfs_action.get("header") is None
-            or isinstance(lfs_action.get("header"), dict)
-        )
+        and (lfs_action.get("header") is None or isinstance(lfs_action.get("header"), dict))
     ):
         raise ValueError("lfs_action is improperly formatted")
     return lfs_action
@@ -296,10 +289,7 @@ def _validate_lfs_action(lfs_action: dic
 
 def _validate_batch_actions(lfs_batch_actions: dict):
     """validates response from the LFS batch endpoint"""
-    if not (
-        isinstance(lfs_batch_actions.get("oid"), str)
-        and isinstance(lfs_batch_actions.get("size"), int)
-    ):
+    if not (isinstance(lfs_batch_actions.get("oid"), str) and isinstance(lfs_batch_actions.get("size"), int)):
         raise ValueError("lfs_batch_actions is improperly formatted")
 
     upload_action = lfs_batch_actions.get("actions", {}).get("upload")
@@ -313,10 +303,7 @@ def _validate_batch_actions(lfs_batch_ac
 
 def _validate_batch_error(lfs_batch_error: dict):
     """validates response from the LFS batch endpoint"""
-    if not (
-        isinstance(lfs_batch_error.get("oid"), str)
-        and isinstance(lfs_batch_error.get("size"), int)
-    ):
+    if not (isinstance(lfs_batch_error.get("oid"), str) and isinstance(lfs_batch_error.get("size"), int)):
         raise ValueError("lfs_batch_error is improperly formatted")
     error_info = lfs_batch_error.get("error")
     if not (
@@ -346,22 +333,16 @@ def _upload_single_part(operation: "Comm
     """
     with operation.as_file(with_tqdm=True) as fileobj:
         # S3 might raise a transient 500 error -> let's retry if that happens
-        response = http_backoff(
-            "PUT", upload_url, data=fileobj, retry_on_status_codes=(500, 502, 503, 504)
-        )
+        response = http_backoff("PUT", upload_url, data=fileobj, retry_on_status_codes=(500, 502, 503, 504))
         hf_raise_for_status(response)
 
 
-def _upload_multi_part(
-    operation: "CommitOperationAdd", header: Dict, chunk_size: int, upload_url: str
-) -> None:
+def _upload_multi_part(operation: "CommitOperationAdd", header: Dict, chunk_size: int, upload_url: str) -> None:
     """
     Uploads file using HF multipart LFS transfer protocol.
     """
     # 1. Get upload URLs for each part
-    sorted_parts_urls = _get_sorted_parts_urls(
-        header=header, upload_info=operation.upload_info, chunk_size=chunk_size
-    )
+    sorted_parts_urls = _get_sorted_parts_urls(header=header, upload_info=operation.upload_info, chunk_size=chunk_size)
 
     # 2. Upload parts (either with hf_transfer or in pure Python)
     use_hf_transfer = constants.HF_HUB_ENABLE_HF_TRANSFER
@@ -393,17 +374,13 @@ def _upload_multi_part(
     # 3. Send completion request
     completion_res = get_session().post(
         upload_url,
-        json=_get_completion_payload(
-            response_headers, operation.upload_info.sha256.hex()
-        ),
+        json=_get_completion_payload(response_headers, operation.upload_info.sha256.hex()),
         headers=LFS_HEADERS,
     )
     hf_raise_for_status(completion_res)
 
 
-def _get_sorted_parts_urls(
-    header: Dict, upload_info: UploadInfo, chunk_size: int
-) -> List[str]:
+def _get_sorted_parts_urls(header: Dict, upload_info: UploadInfo, chunk_size: int) -> List[str]:
     sorted_part_upload_urls = [
         upload_url
         for _, upload_url in sorted(
@@ -421,16 +398,12 @@ def _get_sorted_parts_urls(
     return sorted_part_upload_urls
 
 
-def _get_completion_payload(
-    response_headers: List[Dict], oid: str
-) -> CompletionPayloadT:
+def _get_completion_payload(response_headers: List[Dict], oid: str) -> CompletionPayloadT:
     parts: List[PayloadPartT] = []
     for part_number, header in enumerate(response_headers):
         etag = header.get("etag")
         if etag is None or etag == "":
-            raise ValueError(
-                f"Invalid etag (`{etag}`) returned for part {part_number + 1}"
-            )
+            raise ValueError(f"Invalid etag (`{etag}`) returned for part {part_number + 1}")
         parts.append(
             {
                 "partNumber": part_number + 1,
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/repocard.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/repocard.py
@@ -28,9 +28,7 @@ logger = logging.get_logger(__name__)
 
 
 TEMPLATE_MODELCARD_PATH = Path(__file__).parent / "templates" / "modelcard_template.md"
-TEMPLATE_DATASETCARD_PATH = (
-    Path(__file__).parent / "templates" / "datasetcard_template.md"
-)
+TEMPLATE_DATASETCARD_PATH = Path(__file__).parent / "templates" / "datasetcard_template.md"
 
 # exact same regex as in the Hub server. Please keep in sync.
 # See https://github.com/huggingface/moon-landing/blob/main/server/lib/ViewMarkdown.ts#L18
@@ -107,15 +105,11 @@ class RepoCard:
                 raise ValueError("repo card metadata block should be a dict")
         else:
             # Model card without metadata... create empty metadata
-            logger.warning(
-                "Repo card metadata block was not found. Setting CardData to empty."
-            )
+            logger.warning("Repo card metadata block was not found. Setting CardData to empty.")
             data_dict = {}
             self.text = content
 
-        self.data = self.card_data_class(
-            **data_dict, ignore_metadata_errors=self.ignore_metadata_errors
-        )
+        self.data = self.card_data_class(**data_dict, ignore_metadata_errors=self.ignore_metadata_errors)
         self._original_order = list(data_dict.keys())
 
     def __str__(self):
@@ -189,9 +183,7 @@ class RepoCard:
                 )
             )
         else:
-            raise ValueError(
-                f"Cannot load RepoCard: path not found on disk ({repo_id_or_path})."
-            )
+            raise ValueError(f"Cannot load RepoCard: path not found on disk ({repo_id_or_path}).")
 
         # Preserve newlines in the existing file.
         with card_path.open(mode="r", newline="", encoding="utf-8") as f:
@@ -228,9 +220,7 @@ class RepoCard:
         headers = {"Accept": "text/plain"}
 
         try:
-            r = get_session().post(
-                "https://huggingface.co/api/validate-yaml", body, headers=headers
-            )
+            r = get_session().post("https://huggingface.co/api/validate-yaml", body, headers=headers)
             r.raise_for_status()
         except requests.exceptions.HTTPError as exc:
             if r.status_code == 400:
@@ -423,9 +413,7 @@ class ModelCard(RepoCard):
 
             ```
         """
-        return super().from_template(
-            card_data, template_path, template_str, **template_kwargs
-        )
+        return super().from_template(card_data, template_path, template_str, **template_kwargs)
 
 
 class DatasetCard(RepoCard):
@@ -490,9 +478,7 @@ class DatasetCard(RepoCard):
 
             ```
         """
-        return super().from_template(
-            card_data, template_path, template_str, **template_kwargs
-        )
+        return super().from_template(card_data, template_path, template_str, **template_kwargs)
 
 
 class SpaceCard(RepoCard):
@@ -561,11 +547,7 @@ def metadata_save(local_path: Union[str,
         # sort_keys: keep dict order
         match = REGEX_YAML_BLOCK.search(content)
         if match:
-            output = (
-                content[: match.start()]
-                + f"---{line_break}{data_yaml}---{line_break}"
-                + content[match.end() :]
-            )
+            output = content[: match.start()] + f"---{line_break}{data_yaml}---{line_break}" + content[match.end() :]
         else:
             output = f"---{line_break}{data_yaml}---{line_break}{content}"
 
@@ -769,11 +751,7 @@ def metadata_update(
 
         ```
     """
-    commit_message = (
-        commit_message
-        if commit_message is not None
-        else "Update metadata with huggingface_hub"
-    )
+    commit_message = commit_message if commit_message is not None else "Update metadata with huggingface_hub"
 
     # Card class given repo_type
     card_class: Type[RepoCard]
@@ -792,9 +770,7 @@ def metadata_update(
         card = card_class.load(repo_id, token=token, repo_type=repo_type)
     except EntryNotFoundError:
         if repo_type == "space":
-            raise ValueError(
-                "Cannot update metadata on a Space that doesn't contain a `README.md` file."
-            )
+            raise ValueError("Cannot update metadata on a Space that doesn't contain a `README.md` file.")
 
         # Initialize a ModelCard or DatasetCard from default template and no data.
         card = card_class.from_template(CardData())
@@ -836,11 +812,7 @@ def metadata_update(
                         card.data.eval_results.append(new_result)
         else:
             # Any metadata that is not a result metric
-            if (
-                card.data.get(key) is not None
-                and not overwrite
-                and card.data.get(key) != value
-            ):
+            if card.data.get(key) is not None and not overwrite and card.data.get(key) != value:
                 raise ValueError(
                     f"You passed a new value for the existing meta data field '{key}'."
                     " Set `overwrite=True` to overwrite existing metadata."
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/repocard_data.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/repocard_data.py
@@ -158,9 +158,7 @@ class EvalResult:
 
     def __post_init__(self) -> None:
         if self.source_name is not None and self.source_url is None:
-            raise ValueError(
-                "If `source_name` is provided, `source_url` must also be provided."
-            )
+            raise ValueError("If `source_name` is provided, `source_url` must also be provided.")
 
 
 @dataclass
@@ -197,9 +195,7 @@ class CardData:
         """
         pass
 
-    def to_yaml(
-        self, line_break=None, original_order: Optional[List[str]] = None
-    ) -> str:
+    def to_yaml(self, line_break=None, original_order: Optional[List[str]] = None) -> str:
         """Dumps CardData to a YAML block for inclusion in a README.md file.
 
         Args:
@@ -212,8 +208,7 @@ class CardData:
         if original_order:
             self.__dict__ = {
                 k: self.__dict__[k]
-                for k in original_order
-                + list(set(self.__dict__.keys()) - set(original_order))
+                for k in original_order + list(set(self.__dict__.keys()) - set(original_order))
                 if k in self.__dict__
             }
         return yaml_dump(self.to_dict(), sort_keys=False, line_break=line_break).strip()
@@ -353,9 +348,7 @@ class ModelCardData(CardData):
                 self.eval_results = eval_results
             except (KeyError, TypeError) as error:
                 if ignore_metadata_errors:
-                    logger.warning(
-                        "Invalid model-index. Not loading eval results into CardData."
-                    )
+                    logger.warning("Invalid model-index. Not loading eval results into CardData.")
                 else:
                     raise ValueError(
                         f"Invalid `model_index` in metadata cannot be parsed: {error.__class__} {error}. Pass"
@@ -369,16 +362,12 @@ class ModelCardData(CardData):
             if isinstance(self.eval_results, EvalResult):
                 self.eval_results = [self.eval_results]
             if self.model_name is None:
-                raise ValueError(
-                    "Passing `eval_results` requires `model_name` to be set."
-                )
+                raise ValueError("Passing `eval_results` requires `model_name` to be set.")
 
     def _to_dict(self, data_dict):
         """Format the internal data dict. In this case, we convert eval results to a valid model index"""
         if self.eval_results is not None:
-            data_dict["model-index"] = eval_results_to_model_index(
-                self.model_name, self.eval_results
-            )
+            data_dict["model-index"] = eval_results_to_model_index(self.model_name, self.eval_results)
             del data_dict["eval_results"], data_dict["model_name"]
 
 
@@ -656,18 +645,12 @@ def _remove_none(obj):
     if isinstance(obj, (list, tuple, set)):
         return type(obj)(_remove_none(x) for x in obj if x is not None)
     elif isinstance(obj, dict):
-        return type(obj)(
-            (_remove_none(k), _remove_none(v))
-            for k, v in obj.items()
-            if k is not None and v is not None
-        )
+        return type(obj)((_remove_none(k), _remove_none(v)) for k, v in obj.items() if k is not None and v is not None)
     else:
         return obj
 
 
-def eval_results_to_model_index(
-    model_name: str, eval_results: List[EvalResult]
-) -> List[Dict[str, Any]]:
+def eval_results_to_model_index(model_name: str, eval_results: List[EvalResult]) -> List[Dict[str, Any]]:
     """Takes in given model name and list of `huggingface_hub.EvalResult` and returns a
     valid model-index that will be compatible with the format expected by the
     Hugging Face Hub.
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/repository.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/repository.py
@@ -124,9 +124,7 @@ def is_git_repo(folder: Union[str, Path]
         otherwise.
     """
     folder_exists = os.path.exists(os.path.join(folder, ".git"))
-    git_branch = subprocess.run(
-        "git branch".split(), cwd=folder, stdout=subprocess.PIPE, stderr=subprocess.PIPE
-    )
+    git_branch = subprocess.run("git branch".split(), cwd=folder, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
     return folder_exists and git_branch.returncode == 0
 
 
@@ -234,17 +232,13 @@ def is_binary_file(filename: Union[str,
 
         # Code sample taken from the following stack overflow thread
         # https://stackoverflow.com/questions/898669/how-can-i-detect-if-a-file-is-binary-non-text-in-python/7392391#7392391
-        text_chars = bytearray(
-            {7, 8, 9, 10, 12, 13, 27} | set(range(0x20, 0x100)) - {0x7F}
-        )
+        text_chars = bytearray({7, 8, 9, 10, 12, 13, 27} | set(range(0x20, 0x100)) - {0x7F})
         return bool(content.translate(None, text_chars))
     except UnicodeDecodeError:
         return True
 
 
-def files_to_be_staged(
-    pattern: str = ".", folder: Union[str, Path, None] = None
-) -> List[str]:
+def files_to_be_staged(pattern: str = ".", folder: Union[str, Path, None] = None) -> List[str]:
     """
     Returns a list of filenames that are to be staged.
 
@@ -258,9 +252,7 @@ def files_to_be_staged(
         `List[str]`: List of files that are to be staged.
     """
     try:
-        p = run_subprocess(
-            "git ls-files --exclude-standard -mo".split() + [pattern], folder
-        )
+        p = run_subprocess("git ls-files --exclude-standard -mo".split() + [pattern], folder)
         if len(p.stdout.strip()):
             files = p.stdout.strip().split("\n")
         else:
@@ -544,13 +536,9 @@ class Repository:
             if is_git_repo(self.local_dir):
                 logger.debug("[Repository] is a valid git repo")
             else:
-                raise ValueError(
-                    "If not specifying `clone_from`, you need to pass Repository a valid git clone."
-                )
+                raise ValueError("If not specifying `clone_from`, you need to pass Repository a valid git clone.")
 
-        if self.huggingface_token is not None and (
-            git_email is None or git_user is None
-        ):
+        if self.huggingface_token is not None and (git_email is None or git_user is None):
             user = self.client.whoami(self.huggingface_token)
 
             if git_email is None:
@@ -581,9 +569,7 @@ class Repository:
             `str`: Current checked out branch.
         """
         try:
-            result = run_subprocess(
-                "git rev-parse --abbrev-ref HEAD", self.local_dir
-            ).stdout.strip()
+            result = run_subprocess("git rev-parse --abbrev-ref HEAD", self.local_dir).stdout.strip()
         except subprocess.CalledProcessError as exc:
             raise EnvironmentError(exc.stderr)
 
@@ -600,14 +586,10 @@ class Repository:
         try:
             git_version = run_subprocess("git --version", self.local_dir).stdout.strip()
         except FileNotFoundError:
-            raise EnvironmentError(
-                "Looks like you do not have git installed, please install."
-            )
+            raise EnvironmentError("Looks like you do not have git installed, please install.")
 
         try:
-            lfs_version = run_subprocess(
-                "git-lfs --version", self.local_dir
-            ).stdout.strip()
+            lfs_version = run_subprocess("git-lfs --version", self.local_dir).stdout.strip()
         except FileNotFoundError:
             raise EnvironmentError(
                 "Looks like you do not have git-lfs installed, please install."
@@ -668,12 +650,8 @@ class Repository:
             )
 
         hub_url = self.client.endpoint
-        if hub_url in repo_url or (
-            "http" not in repo_url and len(repo_url.split("/")) <= 2
-        ):
-            repo_type, namespace, repo_name = repo_type_and_id_from_hf_id(
-                repo_url, hub_url=hub_url
-            )
+        if hub_url in repo_url or ("http" not in repo_url and len(repo_url.split("/")) <= 2):
+            repo_type, namespace, repo_name = repo_type_and_id_from_hf_id(repo_url, hub_url=hub_url)
             repo_id = f"{namespace}/{repo_name}" if namespace is not None else repo_name
 
             if repo_type is not None:
@@ -733,9 +711,7 @@ class Repository:
                         " `repo.git_pull()`."
                     )
                 else:
-                    output = run_subprocess(
-                        "git remote get-url origin", self.local_dir, check=False
-                    )
+                    output = run_subprocess("git remote get-url origin", self.local_dir, check=False)
 
                     error_msg = (
                         f"Tried to clone {clean_repo_url} in an unrelated git"
@@ -743,18 +719,14 @@ class Repository:
                         f" a remote with the following URL: {clean_repo_url}."
                     )
                     if output.returncode == 0:
-                        clean_local_remote_url = re.sub(
-                            r"https://.*@", "https://", output.stdout
-                        )
+                        clean_local_remote_url = re.sub(r"https://.*@", "https://", output.stdout)
                         error_msg += f"\nLocal path has its origin defined as: {clean_local_remote_url}"
                     raise EnvironmentError(error_msg)
 
         except subprocess.CalledProcessError as exc:
             raise EnvironmentError(exc.stderr)
 
-    def git_config_username_and_email(
-        self, git_user: Optional[str] = None, git_email: Optional[str] = None
-    ):
+    def git_config_username_and_email(self, git_user: Optional[str] = None, git_email: Optional[str] = None):
         """
         Sets git username and email (only in the current repo).
 
@@ -766,14 +738,10 @@ class Repository:
         """
         try:
             if git_user is not None:
-                run_subprocess(
-                    "git config user.name".split() + [git_user], self.local_dir
-                )
+                run_subprocess("git config user.name".split() + [git_user], self.local_dir)
 
             if git_email is not None:
-                run_subprocess(
-                    f"git config user.email {git_email}".split(), self.local_dir
-                )
+                run_subprocess(f"git config user.email {git_email}".split(), self.local_dir)
         except subprocess.CalledProcessError as exc:
             raise EnvironmentError(exc.stderr)
 
@@ -856,14 +824,10 @@ class Repository:
         modified_files_statuses = [status.strip() for status in git_status.split("\n")]
 
         # Only keep files that are deleted using the D prefix
-        deleted_files_statuses = [
-            status for status in modified_files_statuses if "D" in status.split()[0]
-        ]
+        deleted_files_statuses = [status for status in modified_files_statuses if "D" in status.split()[0]]
 
         # Remove the D prefix and strip to keep only the relevant filename
-        deleted_files = [
-            status.split()[-1].strip() for status in deleted_files_statuses
-        ]
+        deleted_files = [status.split()[-1].strip() for status in deleted_files_statuses]
 
         return deleted_files
 
@@ -989,11 +953,7 @@ class Repository:
             path_to_file = os.path.join(os.getcwd(), self.local_dir, filename)
             size_in_mb = os.path.getsize(path_to_file) / (1024 * 1024)
 
-            if (
-                size_in_mb >= 10
-                and not is_tracked_with_lfs(path_to_file)
-                and not is_git_ignored(path_to_file)
-            ):
+            if size_in_mb >= 10 and not is_tracked_with_lfs(path_to_file) and not is_git_ignored(path_to_file):
                 self.lfs_track(filename)
                 files_to_be_tracked_with_lfs.append(filename)
 
@@ -1015,9 +975,7 @@ class Repository:
         """
         try:
             with _lfs_log_progress():
-                result = run_subprocess(
-                    f"git lfs prune {'--recent' if recent else ''}", self.local_dir
-                )
+                result = run_subprocess(f"git lfs prune {'--recent' if recent else ''}", self.local_dir)
                 logger.info(result.stdout)
         except subprocess.CalledProcessError as exc:
             raise EnvironmentError(exc.stderr)
@@ -1089,9 +1047,7 @@ class Repository:
                 The message attributed to the commit.
         """
         try:
-            result = run_subprocess(
-                "git commit -v -m".split() + [commit_message], self.local_dir
-            )
+            result = run_subprocess("git commit -v -m".split() + [commit_message], self.local_dir)
             logger.info(f"Committed:\n{result.stdout}\n")
         except subprocess.CalledProcessError as exc:
             if len(exc.stderr) > 0:
@@ -1135,9 +1091,7 @@ class Repository:
         number_of_commits = commits_to_push(self.local_dir, upstream)
 
         if number_of_commits > 1:
-            logger.warning(
-                f"Several commits ({number_of_commits}) will be pushed upstream."
-            )
+            logger.warning(f"Several commits ({number_of_commits}) will be pushed upstream.")
             if blocking:
                 logger.warning("The progress bars may be unreliable.")
 
@@ -1160,9 +1114,7 @@ class Repository:
                         logger.warning(stderr)
 
                     if return_code:
-                        raise subprocess.CalledProcessError(
-                            return_code, process.args, output=stdout, stderr=stderr
-                        )
+                        raise subprocess.CalledProcessError(return_code, process.args, output=stdout, stderr=stderr)
 
         except subprocess.CalledProcessError as exc:
             raise EnvironmentError(exc.stderr)
@@ -1217,9 +1169,7 @@ class Repository:
                 raise EnvironmentError(exc.stderr)
             else:
                 try:
-                    result = run_subprocess(
-                        f"git checkout -b {revision}", self.local_dir
-                    )
+                    result = run_subprocess(f"git checkout -b {revision}", self.local_dir)
                     logger.warning(
                         f"Revision `{revision}` does not exist. Created and checked out branch `{revision}`."
                     )
@@ -1243,9 +1193,7 @@ class Repository:
         """
         if remote:
             try:
-                result = run_subprocess(
-                    f"git ls-remote origin refs/tags/{tag_name}", self.local_dir
-                ).stdout.strip()
+                result = run_subprocess(f"git ls-remote origin refs/tags/{tag_name}", self.local_dir).stdout.strip()
             except subprocess.CalledProcessError as exc:
                 raise EnvironmentError(exc.stderr)
 
@@ -1284,25 +1232,19 @@ class Repository:
 
         if delete_locally:
             try:
-                run_subprocess(
-                    ["git", "tag", "-d", tag_name], self.local_dir
-                ).stdout.strip()
+                run_subprocess(["git", "tag", "-d", tag_name], self.local_dir).stdout.strip()
             except subprocess.CalledProcessError as exc:
                 raise EnvironmentError(exc.stderr)
 
         if remote and delete_remotely:
             try:
-                run_subprocess(
-                    f"git push {remote} --delete {tag_name}", self.local_dir
-                ).stdout.strip()
+                run_subprocess(f"git push {remote} --delete {tag_name}", self.local_dir).stdout.strip()
             except subprocess.CalledProcessError as exc:
                 raise EnvironmentError(exc.stderr)
 
         return True
 
-    def add_tag(
-        self, tag_name: str, message: Optional[str] = None, remote: Optional[str] = None
-    ):
+    def add_tag(self, tag_name: str, message: Optional[str] = None, remote: Optional[str] = None):
         """
         Add a tag at the current head and push it
 
@@ -1332,9 +1274,7 @@ class Repository:
 
         if remote:
             try:
-                run_subprocess(
-                    f"git push {remote} {tag_name}", self.local_dir
-                ).stdout.strip()
+                run_subprocess(f"git push {remote} {tag_name}", self.local_dir).stdout.strip()
             except subprocess.CalledProcessError as exc:
                 raise EnvironmentError(exc.stderr)
 
@@ -1346,9 +1286,7 @@ class Repository:
             `bool`: `True` if the git status is clean, `False` otherwise.
         """
         try:
-            git_status = run_subprocess(
-                "git status --porcelain", self.local_dir
-            ).stdout.strip()
+            git_status = run_subprocess("git status --porcelain", self.local_dir).stdout.strip()
         except subprocess.CalledProcessError as exc:
             raise EnvironmentError(exc.stderr)
 
@@ -1448,11 +1386,7 @@ class Repository:
         files_to_stage = files_to_be_staged(".", folder=self.local_dir)
 
         if len(files_to_stage):
-            files_in_msg = (
-                str(files_to_stage[:5])[:-1] + ", ...]"
-                if len(files_to_stage) > 5
-                else str(files_to_stage)
-            )
+            files_in_msg = str(files_to_stage[:5])[:-1] + ", ...]" if len(files_to_stage) > 5 else str(files_to_stage)
             logger.error(
                 "There exists some updated files in the local repository that are not"
                 f" committed: {files_in_msg}. This may lead to errors if checking out"
@@ -1467,9 +1401,7 @@ class Repository:
             logger.warning("Pulling changes ...")
             self.git_pull(rebase=True)
         else:
-            logger.warning(
-                f"The current branch has no upstream branch. Will push to 'origin {self.current_branch}'"
-            )
+            logger.warning(f"The current branch has no upstream branch. Will push to 'origin {self.current_branch}'")
 
         current_working_directory = os.getcwd()
         os.chdir(os.path.join(current_working_directory, self.local_dir))
@@ -1495,9 +1427,7 @@ class Repository:
             except OSError as e:
                 # If no changes are detected, there is nothing to commit.
                 if "could not read Username" in str(e):
-                    raise OSError(
-                        "Couldn't authenticate user for push. Did you set `token` to `True`?"
-                    ) from e
+                    raise OSError("Couldn't authenticate user for push. Did you set `token` to `True`?") from e
                 else:
                     raise e
 
@@ -1510,9 +1440,7 @@ class Repository:
         return None
 
     def repocard_metadata_save(self, data: Dict) -> None:
-        return metadata_save(
-            os.path.join(self.local_dir, constants.REPOCARD_NAME), data
-        )
+        return metadata_save(os.path.join(self.local_dir, constants.REPOCARD_NAME), data)
 
     @property
     def commands_failed(self):
@@ -1535,9 +1463,7 @@ class Repository:
         """
         index = 0
         for command_failed in self.commands_failed:
-            logger.error(
-                f"The {command_failed.title} command with PID {command_failed._process.pid} failed."
-            )
+            logger.error(f"The {command_failed.title} command with PID {command_failed._process.pid} failed.")
             logger.error(command_failed.stderr)
 
         while self.commands_in_progress:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/serialization/_base.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/serialization/_base.py
@@ -198,17 +198,13 @@ def parse_size_to_int(size_as_str: str)
     # Parse unit
     unit = size_as_str[-2:].upper()
     if unit not in SIZE_UNITS:
-        raise ValueError(
-            f"Unit '{unit}' not supported. Supported units are TB, GB, MB, KB. Got '{size_as_str}'."
-        )
+        raise ValueError(f"Unit '{unit}' not supported. Supported units are TB, GB, MB, KB. Got '{size_as_str}'.")
     multiplier = SIZE_UNITS[unit]
 
     # Parse value
     try:
         value = float(size_as_str[:-2].strip())
     except ValueError as e:
-        raise ValueError(
-            f"Could not parse the size value from '{size_as_str}': {e}"
-        ) from e
+        raise ValueError(f"Could not parse the size value from '{size_as_str}': {e}") from e
 
     return int(value * multiplier)
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/serialization/_dduf.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/serialization/_dduf.py
@@ -133,16 +133,12 @@ def read_dduf_file(dduf_path: Union[os.P
         for info in zf.infolist():
             logger.debug(f"Reading entry {info.filename}")
             if info.compress_type != zipfile.ZIP_STORED:
-                raise DDUFCorruptedFileError(
-                    "Data must not be compressed in DDUF file."
-                )
+                raise DDUFCorruptedFileError("Data must not be compressed in DDUF file.")
 
             try:
                 _validate_dduf_entry_name(info.filename)
             except DDUFInvalidEntryNameError as e:
-                raise DDUFCorruptedFileError(
-                    f"Invalid entry name in DDUF file: {info.filename}"
-                ) from e
+                raise DDUFCorruptedFileError(f"Invalid entry name in DDUF file: {info.filename}") from e
 
             offset = _get_data_offset(zf, info)
 
@@ -155,9 +151,7 @@ def read_dduf_file(dduf_path: Union[os.P
 
     # Consistency checks on the DDUF file
     if "model_index.json" not in entries:
-        raise DDUFCorruptedFileError(
-            "Missing required 'model_index.json' entry in DDUF file."
-        )
+        raise DDUFCorruptedFileError("Missing required 'model_index.json' entry in DDUF file.")
     index = json.loads(entries["model_index.json"].read_text())
     _validate_dduf_structure(index, entries.keys())
 
@@ -257,9 +251,7 @@ def export_entries_as_dduf(
     logger.info(f"Done writing DDUF file {dduf_path}")
 
 
-def export_folder_as_dduf(
-    dduf_path: Union[str, os.PathLike], folder_path: Union[str, os.PathLike]
-) -> None:
+def export_folder_as_dduf(dduf_path: Union[str, os.PathLike], folder_path: Union[str, os.PathLike]) -> None:
     """
     Export a folder as a DDUF file.
 
@@ -295,9 +287,7 @@ def export_folder_as_dduf(
     export_entries_as_dduf(dduf_path, _iterate_over_folder())
 
 
-def _dump_content_in_archive(
-    archive: zipfile.ZipFile, filename: str, content: Union[str, os.PathLike, bytes]
-) -> None:
+def _dump_content_in_archive(archive: zipfile.ZipFile, filename: str, content: Union[str, os.PathLike, bytes]) -> None:
     with archive.open(filename, "w", force_zip64=True) as archive_fh:
         if isinstance(content, (str, Path)):
             content_path = Path(content)
@@ -306,9 +296,7 @@ def _dump_content_in_archive(
         elif isinstance(content, bytes):
             archive_fh.write(content)
         else:
-            raise DDUFExportError(
-                f"Invalid content type for {filename}. Must be str, Path or bytes."
-            )
+            raise DDUFExportError(f"Invalid content type for {filename}. Must be str, Path or bytes.")
 
 
 def _load_content(content: Union[str, Path, bytes]) -> bytes:
@@ -321,23 +309,17 @@ def _load_content(content: Union[str, Pa
     elif isinstance(content, bytes):
         return content
     else:
-        raise DDUFExportError(
-            f"Invalid content type. Must be str, Path or bytes. Got {type(content)}."
-        )
+        raise DDUFExportError(f"Invalid content type. Must be str, Path or bytes. Got {type(content)}.")
 
 
 def _validate_dduf_entry_name(entry_name: str) -> str:
     if "." + entry_name.split(".")[-1] not in DDUF_ALLOWED_ENTRIES:
         raise DDUFInvalidEntryNameError(f"File type not allowed: {entry_name}")
     if "\\" in entry_name:
-        raise DDUFInvalidEntryNameError(
-            f"Entry names must use UNIX separators ('/'). Got {entry_name}."
-        )
+        raise DDUFInvalidEntryNameError(f"Entry names must use UNIX separators ('/'). Got {entry_name}.")
     entry_name = entry_name.strip("/")
     if entry_name.count("/") > 1:
-        raise DDUFInvalidEntryNameError(
-            f"DDUF only supports 1 level of directory. Got {entry_name}."
-        )
+        raise DDUFInvalidEntryNameError(f"DDUF only supports 1 level of directory. Got {entry_name}.")
     return entry_name
 
 
@@ -360,20 +342,13 @@ def _validate_dduf_structure(index: Any,
         - [`DDUFCorruptedFileError`]: If the DDUF file is corrupted (i.e. doesn't follow the DDUF format).
     """
     if not isinstance(index, dict):
-        raise DDUFCorruptedFileError(
-            f"Invalid 'model_index.json' content. Must be a dictionary. Got {type(index)}."
-        )
+        raise DDUFCorruptedFileError(f"Invalid 'model_index.json' content. Must be a dictionary. Got {type(index)}.")
 
     dduf_folders = {entry.split("/")[0] for entry in entry_names if "/" in entry}
     for folder in dduf_folders:
         if folder not in index:
-            raise DDUFCorruptedFileError(
-                f"Missing required entry '{folder}' in 'model_index.json'."
-            )
-        if not any(
-            f"{folder}/{required_entry}" in entry_names
-            for required_entry in DDUF_FOLDER_REQUIRED_ENTRIES
-        ):
+            raise DDUFCorruptedFileError(f"Missing required entry '{folder}' in 'model_index.json'.")
+        if not any(f"{folder}/{required_entry}" in entry_names for required_entry in DDUF_FOLDER_REQUIRED_ENTRIES):
             raise DDUFCorruptedFileError(
                 f"Missing required file in folder '{folder}'. Must contains at least one of {DDUF_FOLDER_REQUIRED_ENTRIES}."
             )
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/serialization/_torch.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/serialization/_torch.py
@@ -271,9 +271,7 @@ def save_torch_state_dict(
 
     # Only main process should clean up existing files to avoid race conditions in distributed environment
     if is_main_process:
-        existing_files_regex = re.compile(
-            filename_pattern.format(suffix=r"(-\d{5}-of-\d{5})?") + r"(\.index\.json)?"
-        )
+        existing_files_regex = re.compile(filename_pattern.format(suffix=r"(-\d{5}-of-\d{5})?") + r"(\.index\.json)?")
         for filename in os.listdir(save_directory):
             if existing_files_regex.match(filename):
                 try:
@@ -465,9 +463,7 @@ def load_torch_model(
     # 2. If not, checkpoint_path is a directory
     if filename_pattern is None:
         filename_pattern = constants.SAFETENSORS_WEIGHTS_FILE_PATTERN
-        index_path = checkpoint_path / (
-            filename_pattern.format(suffix="") + ".index.json"
-        )
+        index_path = checkpoint_path / (filename_pattern.format(suffix="") + ".index.json")
         # Only fallback to pickle format if safetensors index is not found and safe is False.
         if not index_path.is_file() and not safe:
             filename_pattern = constants.PYTORCH_WEIGHTS_FILE_PATTERN
@@ -653,16 +649,10 @@ def load_state_dict_from_file(
                 f"The safetensors archive passed at {checkpoint_file} does not contain the valid metadata. Make sure "
                 "you save your model with the `save_torch_model` method."
             )
-        device = (
-            str(map_location.type)
-            if map_location is not None and hasattr(map_location, "type")
-            else map_location
-        )
+        device = str(map_location.type) if map_location is not None and hasattr(map_location, "type") else map_location
         # meta device is not supported with safetensors, falling back to CPU
         if device == "meta":
-            logger.warning(
-                "Meta device is not supported with safetensors. Falling back to CPU device."
-            )
+            logger.warning("Meta device is not supported with safetensors. Falling back to CPU device.")
             device = "cpu"
         return load_file(checkpoint_file, device=device)  # type: ignore[arg-type]
     # Otherwise, load from pickle
@@ -709,9 +699,7 @@ def _validate_keys_for_strict_loading(
     loaded_keys_set = set(loaded_keys)
     model_keys = set(model.state_dict().keys())
     missing_keys = model_keys - loaded_keys_set  # Keys in model but not in checkpoint
-    unexpected_keys = (
-        loaded_keys_set - model_keys
-    )  # Keys in checkpoint but not in model
+    unexpected_keys = loaded_keys_set - model_keys  # Keys in checkpoint but not in model
 
     if missing_keys or unexpected_keys:
         error_message = f"Error(s) in loading state_dict for {model.__class__.__name__}"
@@ -860,9 +848,7 @@ def _clean_state_dict_for_safetensors(
 
     Taken from https://github.com/huggingface/safetensors/blob/079781fd0dc455ba0fe851e2b4507c33d0c0d407/bindings/python/py_src/safetensors/torch.py#L155.
     """
-    to_removes = _remove_duplicate_names(
-        state_dict, discard_names=shared_tensors_to_discard
-    )
+    to_removes = _remove_duplicate_names(state_dict, discard_names=shared_tensors_to_discard)
     for kept_name, to_remove_group in to_removes.items():
         for to_remove in to_remove_group:
             if metadata is None:
@@ -888,9 +874,7 @@ def _end_ptr(tensor: "torch.Tensor") ->
     return stop
 
 
-def _filter_shared_not_shared(
-    tensors: List[Set[str]], state_dict: Dict[str, "torch.Tensor"]
-) -> List[Set[str]]:
+def _filter_shared_not_shared(tensors: List[Set[str]], state_dict: Dict[str, "torch.Tensor"]) -> List[Set[str]]:
     """
     Taken from https://github.com/huggingface/safetensors/blob/079781fd0dc455ba0fe851e2b4507c33d0c0d407/bindings/python/py_src/safetensors/torch.py#L44
     """
@@ -926,11 +910,7 @@ def _find_shared_tensors(state_dict: Dic
 
     tensors_dict = defaultdict(set)
     for k, v in state_dict.items():
-        if (
-            v.device != torch.device("meta")
-            and storage_ptr(v) != 0
-            and get_torch_storage_size(v) != 0
-        ):
+        if v.device != torch.device("meta") and storage_ptr(v) != 0 and get_torch_storage_size(v) != 0:
             # Need to add device as key because of multiple GPU.
             tensors_dict[(v.device, storage_ptr(v), get_torch_storage_size(v))].add(k)
     tensors = list(sorted(tensors_dict.values()))
@@ -953,11 +933,9 @@ def _is_complete(tensor: "torch.Tensor")
         # for torch version less than 2.1, we can fallback to original implementation
         pass
 
-    return tensor.data_ptr() == storage_ptr(
-        tensor
-    ) and tensor.nelement() * _get_dtype_size(tensor.dtype) == get_torch_storage_size(
-        tensor
-    )
+    return tensor.data_ptr() == storage_ptr(tensor) and tensor.nelement() * _get_dtype_size(
+        tensor.dtype
+    ) == get_torch_storage_size(tensor)
 
 
 def _remove_duplicate_names(
@@ -979,9 +957,7 @@ def _remove_duplicate_names(
     shareds = _find_shared_tensors(state_dict)
     to_remove = defaultdict(list)
     for shared in shareds:
-        complete_names = set(
-            [name for name in shared if _is_complete(state_dict[name])]
-        )
+        complete_names = set([name for name in shared if _is_complete(state_dict[name])])
         if not complete_names:
             raise RuntimeError(
                 "Error while trying to find names to remove to save state dict, but found no suitable name to keep"
@@ -1038,9 +1014,7 @@ def _get_dtype_size(dtype: "torch.dtype"
     return _SIZE[dtype]
 
 
-class _IncompatibleKeys(
-    namedtuple("IncompatibleKeys", ["missing_keys", "unexpected_keys"])
-):
+class _IncompatibleKeys(namedtuple("IncompatibleKeys", ["missing_keys", "unexpected_keys"])):
     """
     This is used to report missing and unexpected keys in the state dict.
     Taken from https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/module.py#L52.
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_auth.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_auth.py
@@ -46,11 +46,7 @@ def get_token() -> Optional[str]:
     Returns:
         `str` or `None`: The token, `None` if it doesn't exist.
     """
-    return (
-        _get_token_from_google_colab()
-        or _get_token_from_environment()
-        or _get_token_from_file()
-    )
+    return _get_token_from_google_colab() or _get_token_from_environment() or _get_token_from_file()
 
 
 def _get_token_from_google_colab() -> Optional[str]:
@@ -119,9 +115,7 @@ def _get_token_from_google_colab() -> Op
 
 def _get_token_from_environment() -> Optional[str]:
     # `HF_TOKEN` has priority (keep `HUGGING_FACE_HUB_TOKEN` for backward compatibility)
-    return _clean_token(
-        os.environ.get("HF_TOKEN") or os.environ.get("HUGGING_FACE_HUB_TOKEN")
-    )
+    return _clean_token(os.environ.get("HF_TOKEN") or os.environ.get("HUGGING_FACE_HUB_TOKEN"))
 
 
 def _get_token_from_file() -> Optional[str]:
@@ -146,10 +140,7 @@ def get_stored_tokens() -> Dict[str, str
     config = configparser.ConfigParser()
     try:
         config.read(tokens_path)
-        stored_tokens = {
-            token_name: config.get(token_name, "hf_token")
-            for token_name in config.sections()
-        }
+        stored_tokens = {token_name: config.get(token_name, "hf_token") for token_name in config.sections()}
     except configparser.Error as e:
         logger.error(f"Error parsing stored tokens file: {e}")
         stored_tokens = {}
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_cache_assets.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_cache_assets.py
@@ -129,9 +129,7 @@ def cached_assets_path(
     try:
         path.mkdir(exist_ok=True, parents=True)
     except (FileExistsError, NotADirectoryError):
-        raise ValueError(
-            f"Corrupted assets folder: cannot create directory because of an existing file ({path})."
-        )
+        raise ValueError(f"Corrupted assets folder: cannot create directory because of an existing file ({path}).")
 
     # Return
     return path
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_cache_manager.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_cache_manager.py
@@ -422,9 +422,7 @@ class HFCacheInfo:
         """
         hashes_to_delete: Set[str] = set(revisions)
 
-        repos_with_revisions: Dict[CachedRepoInfo, Set[CachedRevisionInfo]] = (
-            defaultdict(set)
-        )
+        repos_with_revisions: Dict[CachedRepoInfo, Set[CachedRevisionInfo]] = defaultdict(set)
 
         for repo in self.repos:
             for revision in repo.revisions:
@@ -433,9 +431,7 @@ class HFCacheInfo:
                     hashes_to_delete.remove(revision.commit_hash)
 
         if len(hashes_to_delete) > 0:
-            logger.warning(
-                f"Revision(s) not found - cannot delete them: {', '.join(hashes_to_delete)}"
-            )
+            logger.warning(f"Revision(s) not found - cannot delete them: {', '.join(hashes_to_delete)}")
 
         delete_strategy_blobs: Set[Path] = set()
         delete_strategy_refs: Set[Path] = set()
@@ -569,9 +565,7 @@ class HFCacheInfo:
                         str(revision.snapshot_path),
                     ]
                     for repo in sorted(self.repos, key=lambda repo: repo.repo_path)
-                    for revision in sorted(
-                        repo.revisions, key=lambda revision: revision.commit_hash
-                    )
+                    for revision in sorted(repo.revisions, key=lambda revision: revision.commit_hash)
                 ],
                 headers=[
                     "REPO ID",
@@ -713,9 +707,7 @@ def _scan_cached_repo(repo_path: Path) -
         raise CorruptedCacheException(f"Repo path is not a directory: {repo_path}")
 
     if "--" not in repo_path.name:
-        raise CorruptedCacheException(
-            f"Repo path is not a valid HuggingFace cache directory: {repo_path}"
-        )
+        raise CorruptedCacheException(f"Repo path is not a valid HuggingFace cache directory: {repo_path}")
 
     repo_type, repo_id = repo_path.name.split("--", maxsplit=1)
     repo_type = repo_type[:-1]  # "models" -> "model"
@@ -732,9 +724,7 @@ def _scan_cached_repo(repo_path: Path) -
     refs_path = repo_path / "refs"
 
     if not snapshots_path.exists() or not snapshots_path.is_dir():
-        raise CorruptedCacheException(
-            f"Snapshots dir doesn't exist in cached repo: {snapshots_path}"
-        )
+        raise CorruptedCacheException(f"Snapshots dir doesn't exist in cached repo: {snapshots_path}")
 
     # Scan over `refs` directory
 
@@ -748,9 +738,7 @@ def _scan_cached_repo(repo_path: Path) -
         #         └── pr
         #             └── 1
         if refs_path.is_file():
-            raise CorruptedCacheException(
-                f"Refs directory cannot be a file: {refs_path}"
-            )
+            raise CorruptedCacheException(f"Refs directory cannot be a file: {refs_path}")
 
         for ref_path in refs_path.glob("**/*"):
             # glob("**/*") iterates over all files and directories -> skip directories
@@ -770,9 +758,7 @@ def _scan_cached_repo(repo_path: Path) -
         if revision_path.name in FILES_TO_IGNORE:
             continue
         if revision_path.is_file():
-            raise CorruptedCacheException(
-                f"Snapshots folder corrupted. Found a file: {revision_path}"
-            )
+            raise CorruptedCacheException(f"Snapshots folder corrupted. Found a file: {revision_path}")
 
         cached_files = set()
         for file_path in revision_path.glob("**/*"):
@@ -782,9 +768,7 @@ def _scan_cached_repo(repo_path: Path) -
 
             blob_path = Path(file_path).resolve()
             if not blob_path.exists():
-                raise CorruptedCacheException(
-                    f"Blob missing (broken symlink): {blob_path}"
-                )
+                raise CorruptedCacheException(f"Blob missing (broken symlink): {blob_path}")
 
             if blob_path not in blob_stats:
                 blob_stats[blob_path] = blob_path.stat()
@@ -803,9 +787,7 @@ def _scan_cached_repo(repo_path: Path) -
         # Last modified is either the last modified blob file or the revision folder
         # itself if it is empty
         if len(cached_files) > 0:
-            revision_last_modified = max(
-                blob_stats[file.blob_path].st_mtime for file in cached_files
-            )
+            revision_last_modified = max(blob_stats[file.blob_path].st_mtime for file in cached_files)
         else:
             revision_last_modified = revision_path.stat().st_mtime
 
@@ -815,8 +797,7 @@ def _scan_cached_repo(repo_path: Path) -
                 files=frozenset(cached_files),
                 refs=frozenset(refs_by_hash.pop(revision_path.name, set())),
                 size_on_disk=sum(
-                    blob_stats[blob_path].st_size
-                    for blob_path in set(file.blob_path for file in cached_files)
+                    blob_stats[blob_path].st_size for blob_path in set(file.blob_path for file in cached_files)
                 ),
                 snapshot_path=revision_path,
                 last_modified=revision_last_modified,
@@ -910,10 +891,6 @@ def _try_delete_path(path: Path, path_ty
         else:
             shutil.rmtree(path)
     except FileNotFoundError:
-        logger.warning(
-            f"Couldn't delete {path_type}: file not found ({path})", exc_info=True
-        )
+        logger.warning(f"Couldn't delete {path_type}: file not found ({path})", exc_info=True)
     except PermissionError:
-        logger.warning(
-            f"Couldn't delete {path_type}: permission denied ({path})", exc_info=True
-        )
+        logger.warning(f"Couldn't delete {path_type}: permission denied ({path})", exc_info=True)
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_datetime.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_datetime.py
@@ -59,9 +59,7 @@ def parse_datetime(date_string: str) ->
                 # fraction[:6] takes first 6 digits and :0<6 pads with zeros if less than 6 digits
                 date_string = f"{base}.{fraction[:6]:0<6}Z"
 
-        return datetime.strptime(date_string, "%Y-%m-%dT%H:%M:%S.%fZ").replace(
-            tzinfo=timezone.utc
-        )
+        return datetime.strptime(date_string, "%Y-%m-%dT%H:%M:%S.%fZ").replace(tzinfo=timezone.utc)
     except ValueError as e:
         raise ValueError(
             f"Cannot parse '{date_string}' as a datetime. Date string is expected to"
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_deprecation.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_deprecation.py
@@ -123,7 +123,9 @@ def _deprecate_method(*, version: str, m
 
         @wraps(f)
         def inner_f(*args, **kwargs):
-            warning_message = f"'{name}' (from '{f.__module__}') is deprecated and will be removed from version '{version}'."
+            warning_message = (
+                f"'{name}' (from '{f.__module__}') is deprecated and will be removed from version '{version}'."
+            )
             if message is not None:
                 warning_message += " " + message
             warnings.warn(warning_message, FutureWarning)
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_experimental.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_experimental.py
@@ -50,11 +50,7 @@ def experimental(fn: Callable) -> Callab
     ```
     """
     # For classes, put the "experimental" around the "__new__" method => __new__ will be removed in warning message
-    name = (
-        fn.__qualname__[: -len(".__new__")]
-        if fn.__qualname__.endswith(".__new__")
-        else fn.__qualname__
-    )
+    name = fn.__qualname__[: -len(".__new__")] if fn.__qualname__.endswith(".__new__") else fn.__qualname__
 
     @wraps(fn)
     def _inner_fn(*args, **kwargs):
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_fixes.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_fixes.py
@@ -57,9 +57,7 @@ def SoftTemporaryDirectory(
 
     See https://www.scivision.dev/python-tempfile-permission-error-windows/.
     """
-    tmpdir = tempfile.TemporaryDirectory(
-        prefix=prefix, suffix=suffix, dir=dir, **kwargs
-    )
+    tmpdir = tempfile.TemporaryDirectory(prefix=prefix, suffix=suffix, dir=dir, **kwargs)
     yield Path(tmpdir.name).resolve()
 
     try:
@@ -108,13 +106,7 @@ def WeakFileLock(
             raise Timeout(str(lock_file))
 
         try:
-            lock.acquire(
-                timeout=(
-                    min(log_interval, timeout - elapsed_time)
-                    if timeout
-                    else log_interval
-                )
-            )
+            lock.acquire(timeout=(min(log_interval, timeout - elapsed_time) if timeout else log_interval))
         except Timeout:
             logger.info(
                 f"Still waiting to acquire lock on {lock_file} (elapsed: {time.time() - start_time:.1f} seconds)"
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_git_credential.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_git_credential.py
@@ -54,9 +54,7 @@ def list_credential_helpers(folder: Opti
         raise EnvironmentError(exc.stderr)
 
 
-def set_git_credential(
-    token: str, username: str = "hf_user", folder: Optional[str] = None
-) -> None:
+def set_git_credential(token: str, username: str = "hf_user", folder: Optional[str] = None) -> None:
     """Save a username/token pair in git credential for HF Hub registry.
 
     Credentials are saved in all configured helpers (store, cache, macOS keychain,...).
@@ -75,15 +73,11 @@ def set_git_credential(
         stdin,
         _,
     ):
-        stdin.write(
-            f"url={ENDPOINT}\nusername={username.lower()}\npassword={token}\n\n"
-        )
+        stdin.write(f"url={ENDPOINT}\nusername={username.lower()}\npassword={token}\n\n")
         stdin.flush()
 
 
-def unset_git_credential(
-    username: str = "hf_user", folder: Optional[str] = None
-) -> None:
+def unset_git_credential(username: str = "hf_user", folder: Optional[str] = None) -> None:
     """Erase credentials from git credential for HF Hub registry.
 
     Credentials are erased from the configured helpers (store, cache, macOS
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_http.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_http.py
@@ -80,9 +80,7 @@ class UniqueRequestIdAdapter(HTTPAdapter
 
         # Add random request ID => easier for server-side debug
         if X_AMZN_TRACE_ID not in request.headers:
-            request.headers[X_AMZN_TRACE_ID] = request.headers.get(X_REQUEST_ID) or str(
-                uuid.uuid4()
-            )
+            request.headers[X_AMZN_TRACE_ID] = request.headers.get(X_REQUEST_ID) or str(uuid.uuid4())
 
         # Add debug log
         has_token = len(str(request.headers.get("authorization", ""))) > 0
@@ -193,9 +191,7 @@ def get_session() -> requests.Session:
     session = get_session()
     ```
     """
-    return _get_session_from_cache(
-        process_id=os.getpid(), thread_id=threading.get_ident()
-    )
+    return _get_session_from_cache(process_id=os.getpid(), thread_id=threading.get_ident())
 
 
 def reset_sessions() -> None:
@@ -318,9 +314,7 @@ def http_backoff(
                 return response
 
             # Wrong status code returned (HTTP 503 for instance)
-            logger.warning(
-                f"HTTP Error {response.status_code} thrown while requesting {method} {url}"
-            )
+            logger.warning(f"HTTP Error {response.status_code} thrown while requesting {method} {url}")
             if nb_tries > max_retries:
                 response.raise_for_status()  # Will raise uncaught exception
                 # We return response to avoid infinite loop in the corner case where the
@@ -360,9 +354,7 @@ def fix_hf_endpoint_in_url(url: str, end
     return url
 
 
-def hf_raise_for_status(
-    response: Response, endpoint_name: Optional[str] = None
-) -> None:
+def hf_raise_for_status(response: Response, endpoint_name: Optional[str] = None) -> None:
     """
     Internal version of `response.raise_for_status()` that will refine a
     potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.
@@ -425,26 +417,16 @@ def hf_raise_for_status(
         error_message = response.headers.get("X-Error-Message")
 
         if error_code == "RevisionNotFound":
-            message = (
-                f"{response.status_code} Client Error."
-                + "\n\n"
-                + f"Revision Not Found for url: {response.url}."
-            )
+            message = f"{response.status_code} Client Error." + "\n\n" + f"Revision Not Found for url: {response.url}."
             raise _format(RevisionNotFoundError, message, response) from e
 
         elif error_code == "EntryNotFound":
-            message = (
-                f"{response.status_code} Client Error."
-                + "\n\n"
-                + f"Entry Not Found for url: {response.url}."
-            )
+            message = f"{response.status_code} Client Error." + "\n\n" + f"Entry Not Found for url: {response.url}."
             raise _format(EntryNotFoundError, message, response) from e
 
         elif error_code == "GatedRepo":
             message = (
-                f"{response.status_code} Client Error."
-                + "\n\n"
-                + f"Cannot access gated repo for url {response.url}."
+                f"{response.status_code} Client Error." + "\n\n" + f"Cannot access gated repo for url {response.url}."
             )
             raise _format(GatedRepoError, message, response) from e
 
@@ -482,9 +464,7 @@ def hf_raise_for_status(
 
         elif response.status_code == 400:
             message = (
-                f"\n\nBad request for {endpoint_name} endpoint:"
-                if endpoint_name is not None
-                else "\n\nBad request:"
+                f"\n\nBad request for {endpoint_name} endpoint:" if endpoint_name is not None else "\n\nBad request:"
             )
             raise _format(BadRequestError, message, response) from e
 
@@ -506,9 +486,7 @@ def hf_raise_for_status(
         raise _format(HfHubHTTPError, str(e), response) from e
 
 
-def _format(
-    error_type: Type[HfHubHTTPError], custom_message: str, response: Response
-) -> HfHubHTTPError:
+def _format(error_type: Type[HfHubHTTPError], custom_message: str, response: Response) -> HfHubHTTPError:
     server_errors = []
 
     # Retrieve server error from header
@@ -573,9 +551,7 @@ def _format(
         if "\n" in final_error_message:
             newline_index = final_error_message.index("\n")
             final_error_message = (
-                final_error_message[:newline_index]
-                + request_id_message
-                + final_error_message[newline_index:]
+                final_error_message[:newline_index] + request_id_message + final_error_message[newline_index:]
             )
         else:
             final_error_message += request_id_message
@@ -630,9 +606,7 @@ def _curlify(request: requests.PreparedR
 RANGE_REGEX = re.compile(r"^\s*bytes\s*=\s*(\d*)\s*-\s*(\d*)\s*$", re.IGNORECASE)
 
 
-def _adjust_range_header(
-    original_range: Optional[str], resume_size: int
-) -> Optional[str]:
+def _adjust_range_header(original_range: Optional[str], resume_size: int) -> Optional[str]:
     """
     Adjust HTTP Range header to account for resume position.
     """
@@ -640,9 +614,7 @@ def _adjust_range_header(
         return f"bytes={resume_size}-"
 
     if "," in original_range:
-        raise ValueError(
-            f"Multiple ranges detected - {original_range!r}, not supported yet."
-        )
+        raise ValueError(f"Multiple ranges detected - {original_range!r}, not supported yet.")
 
     match = RANGE_REGEX.match(original_range)
     if not match:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_lfs.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_lfs.py
@@ -86,9 +86,7 @@ class SliceFileObj(AbstractContextManage
         if pos >= self._len:
             return b""
         remaining_amount = self._len - pos
-        data = self.fileobj.read(
-            remaining_amount if n < 0 else min(n, remaining_amount)
-        )
+        data = self.fileobj.read(remaining_amount if n < 0 else min(n, remaining_amount))
         return data
 
     def tell(self) -> int:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_paths.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_paths.py
@@ -117,9 +117,7 @@ def filter_repo_objects(
                 return item
             if isinstance(item, Path):
                 return str(item)
-            raise ValueError(
-                f"Please provide `key` argument in `filter_repo_objects`: `{item}` is not a string."
-            )
+            raise ValueError(f"Please provide `key` argument in `filter_repo_objects`: `{item}` is not a string.")
 
         key = _identity  # Items must be `str` or `Path`, otherwise raise ValueError
 
@@ -127,15 +125,11 @@ def filter_repo_objects(
         path = key(item)
 
         # Skip if there's an allowlist and path doesn't match any
-        if allow_patterns is not None and not any(
-            fnmatch(path, r) for r in allow_patterns
-        ):
+        if allow_patterns is not None and not any(fnmatch(path, r) for r in allow_patterns):
             continue
 
         # Skip if there's a denylist and path matches any
-        if ignore_patterns is not None and any(
-            fnmatch(path, r) for r in ignore_patterns
-        ):
+        if ignore_patterns is not None and any(fnmatch(path, r) for r in ignore_patterns):
             continue
 
         yield item
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_runtime.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_runtime.py
@@ -327,9 +327,7 @@ def dump_environment_info() -> Dict[str,
         info["Running in iPython ?"] = "No"
     info["Running in notebook ?"] = "Yes" if is_notebook() else "No"
     info["Running in Google Colab ?"] = "Yes" if is_google_colab() else "No"
-    info["Running in Google Colab Enterprise ?"] = (
-        "Yes" if is_colab_enterprise() else "No"
-    )
+    info["Running in Google Colab Enterprise ?"] = "Yes" if is_colab_enterprise() else "No"
     # Login info
     info["Token path ?"] = constants.HF_TOKEN_PATH
     info["Has saved token ?"] = token is not None
@@ -370,9 +368,7 @@ def dump_environment_info() -> Dict[str,
     info["HF_HUB_DISABLE_TELEMETRY"] = constants.HF_HUB_DISABLE_TELEMETRY
     info["HF_HUB_DISABLE_PROGRESS_BARS"] = constants.HF_HUB_DISABLE_PROGRESS_BARS
     info["HF_HUB_DISABLE_SYMLINKS_WARNING"] = constants.HF_HUB_DISABLE_SYMLINKS_WARNING
-    info["HF_HUB_DISABLE_EXPERIMENTAL_WARNING"] = (
-        constants.HF_HUB_DISABLE_EXPERIMENTAL_WARNING
-    )
+    info["HF_HUB_DISABLE_EXPERIMENTAL_WARNING"] = constants.HF_HUB_DISABLE_EXPERIMENTAL_WARNING
     info["HF_HUB_DISABLE_IMPLICIT_TOKEN"] = constants.HF_HUB_DISABLE_IMPLICIT_TOKEN
     info["HF_HUB_ENABLE_HF_TRANSFER"] = constants.HF_HUB_ENABLE_HF_TRANSFER
     info["HF_HUB_ETAG_TIMEOUT"] = constants.HF_HUB_ETAG_TIMEOUT
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_telemetry.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_telemetry.py
@@ -83,9 +83,7 @@ def _start_telemetry_thread():
 
     If the thread is interrupted, start a new one.
     """
-    with (
-        _TELEMETRY_THREAD_LOCK
-    ):  # avoid to start multiple threads if called concurrently
+    with _TELEMETRY_THREAD_LOCK:  # avoid to start multiple threads if called concurrently
         global _TELEMETRY_THREAD
         if _TELEMETRY_THREAD is None or not _TELEMETRY_THREAD.is_alive():
             _TELEMETRY_THREAD = Thread(target=_telemetry_worker, daemon=True)
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_typing.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_typing.py
@@ -60,10 +60,7 @@ def is_jsonable(obj: Any) -> bool:
         if isinstance(obj, (list, tuple)):
             return all(is_jsonable(item) for item in obj)
         if isinstance(obj, dict):
-            return all(
-                isinstance(key, _JSON_SERIALIZABLE_TYPES) and is_jsonable(value)
-                for key, value in obj.items()
-            )
+            return all(isinstance(key, _JSON_SERIALIZABLE_TYPES) and is_jsonable(value) for key, value in obj.items())
         if hasattr(obj, "__json__"):
             return True
         return False
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_validators.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_validators.py
@@ -93,9 +93,7 @@ def validate_hf_hub_args(fn: CallableT)
 
     # Should the validator switch `use_auth_token` values to `token`? In practice, always
     # True in `huggingface_hub`. Might not be the case in a downstream library.
-    check_use_auth_token = (
-        "use_auth_token" not in signature.parameters and "token" in signature.parameters
-    )
+    check_use_auth_token = "use_auth_token" not in signature.parameters and "token" in signature.parameters
 
     @wraps(fn)
     def _inner_fn(*args, **kwargs):
@@ -111,9 +109,7 @@ def validate_hf_hub_args(fn: CallableT)
                 has_token = True
 
         if check_use_auth_token:
-            kwargs = smoothly_deprecate_use_auth_token(
-                fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
-            )
+            kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)
 
         return fn(*args, **kwargs)
 
@@ -152,9 +148,7 @@ def validate_repo_id(repo_id: str) -> No
     """
     if not isinstance(repo_id, str):
         # Typically, a Path is not a repo_id
-        raise HFValidationError(
-            f"Repo id must be a string, not {type(repo_id)}: '{repo_id}'."
-        )
+        raise HFValidationError(f"Repo id must be a string, not {type(repo_id)}: '{repo_id}'.")
 
     if repo_id.count("/") > 1:
         raise HFValidationError(
@@ -176,9 +170,7 @@ def validate_repo_id(repo_id: str) -> No
         raise HFValidationError(f"Repo_id cannot end by '.git': '{repo_id}'.")
 
 
-def smoothly_deprecate_use_auth_token(
-    fn_name: str, has_token: bool, kwargs: Dict[str, Any]
-) -> Dict[str, Any]:
+def smoothly_deprecate_use_auth_token(fn_name: str, has_token: bool, kwargs: Dict[str, Any]) -> Dict[str, Any]:
     """Smoothly deprecate `use_auth_token` in the `huggingface_hub` codebase.
 
     The long-term goal is to remove any mention of `use_auth_token` in the codebase in
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/endpoint_helpers.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/endpoint_helpers.py
@@ -25,9 +25,7 @@ if TYPE_CHECKING:
     from ..hf_api import ModelInfo
 
 
-def _is_emission_within_threshold(
-    model_info: "ModelInfo", minimum_threshold: float, maximum_threshold: float
-) -> bool:
+def _is_emission_within_threshold(model_info: "ModelInfo", minimum_threshold: float, maximum_threshold: float) -> bool:
     """Checks if a model's emission is within a given threshold.
 
     Args:
@@ -42,9 +40,7 @@ def _is_emission_within_threshold(
         `bool`: Whether the model's emission is within the given threshold.
     """
     if minimum_threshold is None and maximum_threshold is None:
-        raise ValueError(
-            "Both `minimum_threshold` and `maximum_threshold` cannot both be `None`"
-        )
+        raise ValueError("Both `minimum_threshold` and `maximum_threshold` cannot both be `None`")
     if minimum_threshold is None:
         minimum_threshold = -1
     if maximum_threshold is None:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/tqdm.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/tqdm.py
@@ -131,9 +131,7 @@ def disable_progress_bars(name: Optional
         progress_bar_states.clear()
         progress_bar_states["_global"] = False
     else:
-        keys_to_remove = [
-            key for key in progress_bar_states if key.startswith(f"{name}.")
-        ]
+        keys_to_remove = [key for key in progress_bar_states if key.startswith(f"{name}.")]
         for key in keys_to_remove:
             del progress_bar_states[key]
         progress_bar_states[name] = False
@@ -165,9 +163,7 @@ def enable_progress_bars(name: Optional[
         progress_bar_states.clear()
         progress_bar_states["_global"] = True
     else:
-        keys_to_remove = [
-            key for key in progress_bar_states if key.startswith(f"{name}.")
-        ]
+        keys_to_remove = [key for key in progress_bar_states if key.startswith(f"{name}.")]
         for key in keys_to_remove:
             del progress_bar_states[key]
         progress_bar_states[name] = True
--- python3-huggingface-hub-0.29.3.orig/tests/conftest.py
+++ python3-huggingface-hub-0.29.3/tests/conftest.py
@@ -17,18 +17,10 @@ def patch_constants(mocker):
     with SoftTemporaryDirectory() as cache_dir:
         mocker.patch.object(constants, "HF_HOME", cache_dir)
         mocker.patch.object(constants, "HF_HUB_CACHE", os.path.join(cache_dir, "hub"))
-        mocker.patch.object(
-            constants, "HUGGINGFACE_HUB_CACHE", os.path.join(cache_dir, "hub")
-        )
-        mocker.patch.object(
-            constants, "HF_ASSETS_CACHE", os.path.join(cache_dir, "assets")
-        )
-        mocker.patch.object(
-            constants, "HF_TOKEN_PATH", os.path.join(cache_dir, "token")
-        )
-        mocker.patch.object(
-            constants, "HF_STORED_TOKENS_PATH", os.path.join(cache_dir, "stored_tokens")
-        )
+        mocker.patch.object(constants, "HUGGINGFACE_HUB_CACHE", os.path.join(cache_dir, "hub"))
+        mocker.patch.object(constants, "HF_ASSETS_CACHE", os.path.join(cache_dir, "assets"))
+        mocker.patch.object(constants, "HF_TOKEN_PATH", os.path.join(cache_dir, "token"))
+        mocker.patch.object(constants, "HF_STORED_TOKENS_PATH", os.path.join(cache_dir, "stored_tokens"))
         yield
 
 
@@ -76,9 +68,7 @@ def disable_symlinks_on_windows_ci(monke
 
 @pytest.fixture(autouse=True)
 def disable_experimental_warnings(monkeypatch: pytest.MonkeyPatch) -> None:
-    monkeypatch.setattr(
-        huggingface_hub.constants, "HF_HUB_DISABLE_EXPERIMENTAL_WARNING", True
-    )
+    monkeypatch.setattr(huggingface_hub.constants, "HF_HUB_DISABLE_EXPERIMENTAL_WARNING", True)
 
 
 @pytest.fixture(scope="module")
--- python3-huggingface-hub-0.29.3.orig/tests/test_auth_cli.py
+++ python3-huggingface-hub-0.29.3/tests/test_auth_cli.py
@@ -19,6 +19,7 @@ from .testing_constants import ENDPOINT_
 
 # fixtures & constants
 
+
 MOCK_TOKEN = "hf_1234"
 
 
@@ -63,18 +64,14 @@ def mock_stored_tokens():
         "active_token": "hf_9012",
     }
     with patch("huggingface_hub._login.get_stored_tokens", return_value=stored_tokens):
-        with patch(
-            "huggingface_hub.utils._auth.get_stored_tokens", return_value=stored_tokens
-        ):
+        with patch("huggingface_hub.utils._auth.get_stored_tokens", return_value=stored_tokens):
             yield stored_tokens
 
 
 def assert_in_logs(caplog: LogCaptureFixture, expected_output):
     """Helper to check if a message appears in logs."""
     log_text = "\n".join(record.message for record in caplog.records)
-    assert (
-        expected_output in log_text
-    ), f"Expected '{expected_output}' not found in logs"
+    assert expected_output in log_text, f"Expected '{expected_output}' not found in logs"
 
 
 def test_login_command_basic(mock_whoami_api_call, caplog: LogCaptureFixture):
@@ -97,16 +94,12 @@ def test_login_command_with_git(mock_who
     args = type("Args", (), {"token": MOCK_TOKEN, "add_to_git_credential": True})()
     cmd = LoginCommand(args)
 
-    with patch(
-        "huggingface_hub._login._is_git_credential_helper_configured", return_value=True
-    ):
+    with patch("huggingface_hub._login._is_git_credential_helper_configured", return_value=True):
         with patch("huggingface_hub.utils.set_git_credential"):
             cmd.run()
 
     assert_in_logs(caplog, "Login successful")
-    assert_in_logs(
-        caplog, "Your token has been saved in your configured git credential helpers"
-    )
+    assert_in_logs(caplog, "Your token has been saved in your configured git credential helpers")
 
 
 def test_logout_specific_token(mock_stored_tokens, caplog: LogCaptureFixture):
@@ -129,9 +122,7 @@ def test_logout_active_token(mock_stored
         cmd = LogoutCommand(args)
         cmd.run()
 
-        assert_in_logs(
-            caplog, "Successfully logged out from access token: active_token"
-        )
+        assert_in_logs(caplog, "Successfully logged out from access token: active_token")
         assert_in_logs(caplog, "Active token 'active_token' has been deleted")
 
 
@@ -159,9 +150,7 @@ def test_switch_token(mock_stored_tokens
 
 def test_switch_nonexistent_token(mock_stored_tokens):
     """Test switching to a non-existent token."""
-    args = type(
-        "Args", (), {"token_name": "nonexistent", "add_to_git_credential": False}
-    )()
+    args = type("Args", (), {"token_name": "nonexistent", "add_to_git_credential": False})()
     cmd = AuthSwitchCommand(args)
 
     with pytest.raises(ValueError, match="Access token nonexistent not found"):
--- python3-huggingface-hub-0.29.3.orig/tests/test_cache_layout.py
+++ python3-huggingface-hub-0.29.3/tests/test_cache_layout.py
@@ -42,9 +42,7 @@ class CacheFileLayoutHfHubDownload(unitt
                     revision=revision,
                 )
 
-                expected_directory_name = (
-                    f"models--{MODEL_IDENTIFIER.replace('/', '--')}"
-                )
+                expected_directory_name = f"models--{MODEL_IDENTIFIER.replace('/', '--')}"
                 expected_path = os.path.join(cache, expected_directory_name)
 
                 refs = os.listdir(os.path.join(expected_path, "refs"))
@@ -71,9 +69,7 @@ class CacheFileLayoutHfHubDownload(unitt
                 self.assertTrue(os.path.islink(snapshot_content_path))
 
                 resolved_blob_relative = os.readlink(snapshot_content_path)
-                resolved_blob_absolute = os.path.normpath(
-                    os.path.join(snapshot_path, resolved_blob_relative)
-                )
+                resolved_blob_absolute = os.path.normpath(os.path.join(snapshot_path, resolved_blob_relative))
 
                 with open(resolved_blob_absolute) as f:
                     blob_contents = f.readline().strip()
@@ -89,19 +85,13 @@ class CacheFileLayoutHfHubDownload(unitt
                 filename = "this_does_not_exist.txt"
                 with self.assertRaises(EntryNotFoundError):
                     # The file does not exist, so we get an exception.
-                    hf_hub_download(
-                        MODEL_IDENTIFIER, filename, cache_dir=cache, revision=revision
-                    )
+                    hf_hub_download(MODEL_IDENTIFIER, filename, cache_dir=cache, revision=revision)
 
-                expected_directory_name = (
-                    f"models--{MODEL_IDENTIFIER.replace('/', '--')}"
-                )
+                expected_directory_name = f"models--{MODEL_IDENTIFIER.replace('/', '--')}"
                 expected_path = os.path.join(cache, expected_directory_name)
 
                 refs = os.listdir(os.path.join(expected_path, "refs"))
-                no_exist_snapshots = os.listdir(
-                    os.path.join(expected_path, ".no_exist")
-                )
+                no_exist_snapshots = os.listdir(os.path.join(expected_path, ".no_exist"))
 
                 # Only reference should be `main`.
                 self.assertListEqual(refs, [expected_reference])
@@ -150,9 +140,7 @@ class CacheFileLayoutHfHubDownload(unitt
 
             time.sleep(2)
 
-            path = hf_hub_download(
-                MODEL_IDENTIFIER, "file_0.txt", cache_dir=cache, revision="file-2"
-            )
+            path = hf_hub_download(MODEL_IDENTIFIER, "file_0.txt", cache_dir=cache, revision="file-2")
             creation_time_1 = os.path.getmtime(path)
 
             self.assertEqual(creation_time_0, creation_time_1)
@@ -161,9 +149,7 @@ class CacheFileLayoutHfHubDownload(unitt
     def test_multiple_refs_for_same_file(self):
         with SoftTemporaryDirectory() as cache:
             hf_hub_download(MODEL_IDENTIFIER, "file_0.txt", cache_dir=cache)
-            hf_hub_download(
-                MODEL_IDENTIFIER, "file_0.txt", cache_dir=cache, revision="file-2"
-            )
+            hf_hub_download(MODEL_IDENTIFIER, "file_0.txt", cache_dir=cache, revision="file-2")
 
             expected_directory_name = f"models--{MODEL_IDENTIFIER.replace('/', '--')}"
             expected_path = os.path.join(cache, expected_directory_name)
@@ -177,19 +163,14 @@ class CacheFileLayoutHfHubDownload(unitt
             # Directory should contain two revisions
             self.assertListEqual(refs, ["file-2", "main"])
 
-            refs_contents = [
-                get_file_contents(os.path.join(expected_path, "refs", f)) for f in refs
-            ]
+            refs_contents = [get_file_contents(os.path.join(expected_path, "refs", f)) for f in refs]
             refs_contents.sort()
 
             # snapshots directory should contain two snapshots
             self.assertListEqual(refs_contents, snapshots)
 
             snapshot_links = [
-                os.readlink(
-                    os.path.join(expected_path, "snapshots", filename, "file_0.txt")
-                )
-                for filename in snapshots
+                os.readlink(os.path.join(expected_path, "snapshots", filename, "file_0.txt")) for filename in snapshots
             ]
 
             # All snapshot links should point to the same file.
@@ -214,9 +195,7 @@ class CacheFileLayoutSnapshotDownload(un
             # Directory should contain two revisions
             self.assertListEqual(refs, ["main"])
 
-            ref_content = get_file_contents(
-                os.path.join(expected_path, "refs", refs[0])
-            )
+            ref_content = get_file_contents(os.path.join(expected_path, "refs", refs[0]))
 
             # snapshots directory should contain two snapshots
             self.assertListEqual([ref_content], snapshots)
@@ -225,19 +204,11 @@ class CacheFileLayoutSnapshotDownload(un
 
             files_in_snapshot = os.listdir(snapshot_path)
 
-            snapshot_links = [
-                os.readlink(os.path.join(snapshot_path, filename))
-                for filename in files_in_snapshot
-            ]
+            snapshot_links = [os.readlink(os.path.join(snapshot_path, filename)) for filename in files_in_snapshot]
 
-            resolved_snapshot_links = [
-                os.path.normpath(os.path.join(snapshot_path, link))
-                for link in snapshot_links
-            ]
+            resolved_snapshot_links = [os.path.normpath(os.path.join(snapshot_path, link)) for link in snapshot_links]
 
-            self.assertTrue(
-                all([os.path.isfile(link) for link in resolved_snapshot_links])
-            )
+            self.assertTrue(all([os.path.isfile(link) for link in resolved_snapshot_links]))
 
     @xfail_on_windows(reason="Symlinks are deactivated in Windows tests.")
     def test_file_downloaded_in_cache_several_revisions(self):
@@ -257,28 +228,21 @@ class CacheFileLayoutSnapshotDownload(un
             # Directory should contain two revisions
             self.assertListEqual(refs, ["file-2", "file-3"])
 
-            refs_content = [
-                get_file_contents(os.path.join(expected_path, "refs", ref))
-                for ref in refs
-            ]
+            refs_content = [get_file_contents(os.path.join(expected_path, "refs", ref)) for ref in refs]
             refs_content.sort()
 
             # snapshots directory should contain two snapshots
             self.assertListEqual(refs_content, snapshots)
 
-            snapshots_paths = [
-                os.path.join(expected_path, "snapshots", s) for s in snapshots
-            ]
+            snapshots_paths = [os.path.join(expected_path, "snapshots", s) for s in snapshots]
 
             files_in_snapshots = {s: os.listdir(s) for s in snapshots_paths}
             links_in_snapshots = {
-                k: [os.readlink(os.path.join(k, _v)) for _v in v]
-                for k, v in files_in_snapshots.items()
+                k: [os.readlink(os.path.join(k, _v)) for _v in v] for k, v in files_in_snapshots.items()
             }
 
             resolved_snapshots_links = {
-                k: [os.path.normpath(os.path.join(k, link)) for link in v]
-                for k, v in links_in_snapshots.items()
+                k: [os.path.normpath(os.path.join(k, link)) for link in v] for k, v in links_in_snapshots.items()
             }
 
             all_links = [b for a in resolved_snapshots_links.values() for b in a]
@@ -337,9 +301,7 @@ class ReferenceUpdates(unittest.TestCase
                 # Directory should contain two revisions
                 self.assertListEqual(refs, ["main"])
 
-                initial_ref_content = get_file_contents(
-                    os.path.join(expected_path, "refs", refs[0])
-                )
+                initial_ref_content = get_file_contents(os.path.join(expected_path, "refs", refs[0]))
 
                 # Upload a new file on the same branch
                 self._api.upload_file(
@@ -350,36 +312,18 @@ class ReferenceUpdates(unittest.TestCase
 
                 hf_hub_download(repo_id, "file.txt", cache_dir=cache)
 
-                final_ref_content = get_file_contents(
-                    os.path.join(expected_path, "refs", refs[0])
-                )
+                final_ref_content = get_file_contents(os.path.join(expected_path, "refs", refs[0]))
 
                 # The `main` reference should point to two different, but existing snapshots which contain
                 # a 'file.txt'
                 self.assertNotEqual(initial_ref_content, final_ref_content)
+                self.assertTrue(os.path.isdir(os.path.join(expected_path, "snapshots", initial_ref_content)))
                 self.assertTrue(
-                    os.path.isdir(
-                        os.path.join(expected_path, "snapshots", initial_ref_content)
-                    )
-                )
-                self.assertTrue(
-                    os.path.isfile(
-                        os.path.join(
-                            expected_path, "snapshots", initial_ref_content, "file.txt"
-                        )
-                    )
-                )
-                self.assertTrue(
-                    os.path.isdir(
-                        os.path.join(expected_path, "snapshots", final_ref_content)
-                    )
+                    os.path.isfile(os.path.join(expected_path, "snapshots", initial_ref_content, "file.txt"))
                 )
+                self.assertTrue(os.path.isdir(os.path.join(expected_path, "snapshots", final_ref_content)))
                 self.assertTrue(
-                    os.path.isfile(
-                        os.path.join(
-                            expected_path, "snapshots", final_ref_content, "file.txt"
-                        )
-                    )
+                    os.path.isfile(os.path.join(expected_path, "snapshots", final_ref_content, "file.txt"))
                 )
         except Exception:
             raise
--- python3-huggingface-hub-0.29.3.orig/tests/test_cache_no_symlinks.py
+++ python3-huggingface-hub-0.29.3/tests/test_cache_no_symlinks.py
@@ -26,9 +26,7 @@ class TestCacheLayoutIfSymlinksNotSuppor
 
     @patch("huggingface_hub.file_download.os.symlink")
     @patch("huggingface_hub.file_download._are_symlinks_supported_in_dir", {})
-    def test_are_symlinks_supported_windows_specific_dir(
-        self, mock_symlink: Mock
-    ) -> None:
+    def test_are_symlinks_supported_windows_specific_dir(self, mock_symlink: Mock) -> None:
         mock_symlink.side_effect = [OSError(), None]  # First dir not supported then yes
         this_dir = Path(__file__).parent
 
@@ -48,9 +46,7 @@ class TestCacheLayoutIfSymlinksNotSuppor
             self.assertTrue(are_symlinks_supported())  # True
 
     @patch("huggingface_hub.file_download.are_symlinks_supported")
-    def test_download_no_symlink_new_file(
-        self, mock_are_symlinks_supported: Mock
-    ) -> None:
+    def test_download_no_symlink_new_file(self, mock_are_symlinks_supported: Mock) -> None:
         mock_are_symlinks_supported.return_value = False
         filepath = Path(
             hf_hub_download(
@@ -68,9 +64,7 @@ class TestCacheLayoutIfSymlinksNotSuppor
         self.assertEqual(len(list((Path(filepath).parents[2] / "blobs").glob("*"))), 0)
 
     @patch("huggingface_hub.file_download.are_symlinks_supported")
-    def test_download_no_symlink_existing_file(
-        self, mock_are_symlinks_supported: Mock
-    ) -> None:
+    def test_download_no_symlink_existing_file(self, mock_are_symlinks_supported: Mock) -> None:
         mock_are_symlinks_supported.return_value = True
         filepath = Path(
             hf_hub_download(
@@ -106,9 +100,7 @@ class TestCacheLayoutIfSymlinksNotSuppor
         self.assertTrue(blob_path.is_file())
 
     @patch("huggingface_hub.file_download.are_symlinks_supported")
-    def test_scan_and_delete_cache_no_symlinks(
-        self, mock_are_symlinks_supported: Mock
-    ) -> None:
+    def test_scan_and_delete_cache_no_symlinks(self, mock_are_symlinks_supported: Mock) -> None:
         """Test scan_cache_dir works as well when cache-system doesn't use symlinks."""
         OLDER_REVISION = "44c70f043cfe8162efc274ff531575e224a0e6f0"
 
@@ -183,9 +175,7 @@ class TestCacheLayoutIfSymlinksNotSuppor
         # Since files are not shared (README.md is duplicated in cache), the total size
         # of the repo is the sum of each revision size. If symlinks were used, the total
         # size of the repo would be lower.
-        self.assertEqual(
-            repo.size_on_disk, main_revision.size_on_disk + older_revision.size_on_disk
-        )
+        self.assertEqual(repo.size_on_disk, main_revision.size_on_disk + older_revision.size_on_disk)
 
         # Test delete repo strategy
         strategy_delete_repo = report.delete_revisions(main_ref, OLDER_REVISION)
@@ -201,9 +191,7 @@ class TestCacheLayoutIfSymlinksNotSuppor
             strategy_delete_revision.blobs,
             {file.blob_path for file in older_revision.files},
         )
-        self.assertEqual(
-            strategy_delete_revision.snapshots, {older_revision.snapshot_path}
-        )
+        self.assertEqual(strategy_delete_revision.snapshots, {older_revision.snapshot_path})
         self.assertEqual(len(strategy_delete_revision.refs), 0)
         self.assertEqual(len(strategy_delete_revision.repos), 0)
         strategy_delete_revision.execute()  # Execute without error
--- python3-huggingface-hub-0.29.3.orig/tests/test_cli.py
+++ python3-huggingface-hub-0.29.3/tests/test_cli.py
@@ -24,9 +24,7 @@ class TestCacheCommand(unittest.TestCase
         """
         Set up scan-cache/delete-cache commands as in `src/huggingface_hub/commands/huggingface_cli.py`.
         """
-        self.parser = ArgumentParser(
-            "huggingface-cli", usage="huggingface-cli <command> [<args>]"
-        )
+        self.parser = ArgumentParser("huggingface-cli", usage="huggingface-cli <command> [<args>]")
         commands_parser = self.parser.add_subparsers()
         ScanCacheCommand.register_subcommand(commands_parser)
         DeleteCacheCommand.register_subcommand(commands_parser)
@@ -71,17 +69,13 @@ class TestUploadCommand(unittest.TestCas
         """
         Set up CLI as in `src/huggingface_hub/commands/huggingface_cli.py`.
         """
-        self.parser = ArgumentParser(
-            "huggingface-cli", usage="huggingface-cli <command> [<args>]"
-        )
+        self.parser = ArgumentParser("huggingface-cli", usage="huggingface-cli <command> [<args>]")
         commands_parser = self.parser.add_subparsers()
         UploadCommand.register_subcommand(commands_parser)
 
     def test_upload_basic(self) -> None:
         """Test `huggingface-cli upload my-folder to dummy-repo`."""
-        cmd = UploadCommand(
-            self.parser.parse_args(["upload", DUMMY_MODEL_ID, "my-folder"])
-        )
+        cmd = UploadCommand(self.parser.parse_args(["upload", DUMMY_MODEL_ID, "my-folder"]))
         self.assertEqual(cmd.repo_id, DUMMY_MODEL_ID)
         self.assertEqual(cmd.local_path, "my-folder")
         self.assertEqual(cmd.path_in_repo, ".")  # implicit
@@ -171,9 +165,7 @@ class TestUploadCommand(unittest.TestCas
         with tmp_current_directory() as cache_dir:
             folder_path = Path(cache_dir) / "my-cool-model"
             folder_path.mkdir()
-            cmd = UploadCommand(
-                self.parser.parse_args(["upload", "my-cool-org/my-cool-model"])
-            )
+            cmd = UploadCommand(self.parser.parse_args(["upload", "my-cool-org/my-cool-model"]))
 
         # A folder with the same name as the repo exists => upload it at the root of the repo
         self.assertEqual(cmd.local_path, "my-cool-model")
@@ -189,65 +181,43 @@ class TestUploadCommand(unittest.TestCas
         with tmp_current_directory() as cache_dir:
             folder_path = Path(cache_dir) / "path" / "to" / "folder"
             folder_path.mkdir(parents=True, exist_ok=True)
-            cmd = UploadCommand(
-                self.parser.parse_args(["upload", "my-repo", "./path/to/folder"])
-            )
+            cmd = UploadCommand(self.parser.parse_args(["upload", "my-repo", "./path/to/folder"]))
         self.assertEqual(cmd.local_path, "./path/to/folder")
-        self.assertEqual(
-            cmd.path_in_repo, "."
-        )  # Always upload the folder at the root of the repo
+        self.assertEqual(cmd.path_in_repo, ".")  # Always upload the folder at the root of the repo
 
     def test_upload_explicit_local_path_to_file_implicit_path_in_repo(self) -> None:
         with tmp_current_directory() as cache_dir:
             file_path = Path(cache_dir) / "path" / "to" / "file.txt"
             file_path.parent.mkdir(parents=True, exist_ok=True)
             file_path.touch()
-            cmd = UploadCommand(
-                self.parser.parse_args(["upload", "my-repo", "./path/to/file.txt"])
-            )
+            cmd = UploadCommand(self.parser.parse_args(["upload", "my-repo", "./path/to/file.txt"]))
         self.assertEqual(cmd.local_path, "./path/to/file.txt")
-        self.assertEqual(
-            cmd.path_in_repo, "file.txt"
-        )  # If a file, upload it at the root of the repo and keep name
+        self.assertEqual(cmd.path_in_repo, "file.txt")  # If a file, upload it at the root of the repo and keep name
 
     def test_upload_explicit_paths(self) -> None:
-        cmd = UploadCommand(
-            self.parser.parse_args(["upload", "my-repo", "./path/to/folder", "data/"])
-        )
+        cmd = UploadCommand(self.parser.parse_args(["upload", "my-repo", "./path/to/folder", "data/"]))
         self.assertEqual(cmd.local_path, "./path/to/folder")
         self.assertEqual(cmd.path_in_repo, "data/")
 
     def test_every_must_be_positive(self) -> None:
         with self.assertRaises(ValueError):
-            UploadCommand(
-                self.parser.parse_args(["upload", DUMMY_MODEL_ID, ".", "--every", "0"])
-            )
+            UploadCommand(self.parser.parse_args(["upload", DUMMY_MODEL_ID, ".", "--every", "0"]))
 
         with self.assertRaises(ValueError):
-            UploadCommand(
-                self.parser.parse_args(
-                    ["upload", DUMMY_MODEL_ID, ".", "--every", "-10"]
-                )
-            )
+            UploadCommand(self.parser.parse_args(["upload", DUMMY_MODEL_ID, ".", "--every", "-10"]))
 
     def test_every_as_int(self) -> None:
-        cmd = UploadCommand(
-            self.parser.parse_args(["upload", DUMMY_MODEL_ID, ".", "--every", "10"])
-        )
+        cmd = UploadCommand(self.parser.parse_args(["upload", DUMMY_MODEL_ID, ".", "--every", "10"]))
         self.assertEqual(cmd.every, 10)
 
     def test_every_as_float(self) -> None:
-        cmd = UploadCommand(
-            self.parser.parse_args(["upload", DUMMY_MODEL_ID, ".", "--every", "0.5"])
-        )
+        cmd = UploadCommand(self.parser.parse_args(["upload", DUMMY_MODEL_ID, ".", "--every", "0.5"]))
         self.assertEqual(cmd.every, 0.5)
 
     @patch("huggingface_hub.commands.upload.HfApi.repo_info")
     @patch("huggingface_hub.commands.upload.HfApi.upload_folder")
     @patch("huggingface_hub.commands.upload.HfApi.create_repo")
-    def test_upload_folder_mock(
-        self, create_mock: Mock, upload_mock: Mock, repo_info_mock: Mock
-    ) -> None:
+    def test_upload_folder_mock(self, create_mock: Mock, upload_mock: Mock, repo_info_mock: Mock) -> None:
         with SoftTemporaryDirectory() as cache_dir:
             cache_path = cache_dir.absolute().as_posix()
             cmd = UploadCommand(
@@ -291,9 +261,7 @@ class TestUploadCommand(unittest.TestCas
     @patch("huggingface_hub.commands.upload.HfApi.repo_info")
     @patch("huggingface_hub.commands.upload.HfApi.upload_file")
     @patch("huggingface_hub.commands.upload.HfApi.create_repo")
-    def test_upload_file_mock(
-        self, create_mock: Mock, upload_mock: Mock, repo_info_mock: Mock
-    ) -> None:
+    def test_upload_file_mock(self, create_mock: Mock, upload_mock: Mock, repo_info_mock: Mock) -> None:
         with SoftTemporaryDirectory() as cache_dir:
             file_path = Path(cache_dir) / "file.txt"
             file_path.write_text("content")
@@ -333,17 +301,11 @@ class TestUploadCommand(unittest.TestCas
     @patch("huggingface_hub.commands.upload.HfApi.repo_info")
     @patch("huggingface_hub.commands.upload.HfApi.upload_file")
     @patch("huggingface_hub.commands.upload.HfApi.create_repo")
-    def test_upload_file_no_revision_mock(
-        self, create_mock: Mock, upload_mock: Mock, repo_info_mock: Mock
-    ) -> None:
+    def test_upload_file_no_revision_mock(self, create_mock: Mock, upload_mock: Mock, repo_info_mock: Mock) -> None:
         with SoftTemporaryDirectory() as cache_dir:
             file_path = Path(cache_dir) / "file.txt"
             file_path.write_text("content")
-            cmd = UploadCommand(
-                self.parser.parse_args(
-                    ["upload", "my-model", str(file_path), "logs/file.txt"]
-                )
-            )
+            cmd = UploadCommand(self.parser.parse_args(["upload", "my-model", str(file_path), "logs/file.txt"]))
             cmd.run()
             # Revision not specified => no need to check
             repo_info_mock.assert_not_called()
@@ -421,11 +383,7 @@ class TestUploadCommand(unittest.TestCas
 
     @patch("huggingface_hub.commands.upload.HfApi.create_repo")
     def test_upload_missing_path(self, create_mock: Mock) -> None:
-        cmd = UploadCommand(
-            self.parser.parse_args(
-                ["upload", "my-model", "/path/to/missing_file", "logs/file.txt"]
-            )
-        )
+        cmd = UploadCommand(self.parser.parse_args(["upload", "my-model", "/path/to/missing_file", "logs/file.txt"]))
         with self.assertRaises(FileNotFoundError):
             cmd.run()  # File/folder does not exist locally
 
@@ -438,9 +396,7 @@ class TestDownloadCommand(unittest.TestC
         """
         Set up CLI as in `src/huggingface_hub/commands/huggingface_cli.py`.
         """
-        self.parser = ArgumentParser(
-            "huggingface-cli", usage="huggingface-cli <command> [<args>]"
-        )
+        self.parser = ArgumentParser("huggingface-cli", usage="huggingface-cli <command> [<args>]")
         commands_parser = self.parser.add_subparsers()
         DownloadCommand.register_subcommand(commands_parser)
 
@@ -526,9 +482,7 @@ class TestDownloadCommand(unittest.TestC
         # Output path is printed to terminal once run is completed
         with capture_output() as output:
             DownloadCommand(args).run()
-        self.assertRegex(
-            output.getvalue(), r"<MagicMock name='hf_hub_download\(\)' id='\d+'>"
-        )
+        self.assertRegex(output.getvalue(), r"<MagicMock name='hf_hub_download\(\)' id='\d+'>")
 
         mock.assert_called_once_with(
             repo_id="author/dataset",
@@ -669,16 +623,12 @@ class TestTagCommands(unittest.TestCase)
         """
         Set up CLI as in `src/huggingface_hub/commands/huggingface_cli.py`.
         """
-        self.parser = ArgumentParser(
-            "huggingface-cli", usage="huggingface-cli <command> [<args>]"
-        )
+        self.parser = ArgumentParser("huggingface-cli", usage="huggingface-cli <command> [<args>]")
         commands_parser = self.parser.add_subparsers()
         TagCommands.register_subcommand(commands_parser)
 
     def test_tag_create_basic(self) -> None:
-        args = self.parser.parse_args(
-            ["tag", DUMMY_MODEL_ID, "1.0", "-m", "My tag message"]
-        )
+        args = self.parser.parse_args(["tag", DUMMY_MODEL_ID, "1.0", "-m", "My tag message"])
         self.assertEqual(args.repo_id, DUMMY_MODEL_ID)
         self.assertEqual(args.tag, "1.0")
         self.assertIsNotNone(args.message)
@@ -746,9 +696,7 @@ class TestRepoFilesCommand(unittest.Test
         """
         Set up CLI as in `src/huggingface_hub/commands/huggingface_cli.py`.
         """
-        self.parser = ArgumentParser(
-            "huggingface-cli", usage="huggingface-cli <command> [<args>]"
-        )
+        self.parser = ArgumentParser("huggingface-cli", usage="huggingface-cli <command> [<args>]")
         commands_parser = self.parser.add_subparsers()
         RepoFilesCommand.register_subcommand(commands_parser)
 
@@ -894,9 +842,7 @@ class TestRepoFilesCommand(unittest.Test
             with self.subTest(expected):
                 delete_files_args = expected["delete_files_args"]
 
-                cmd = DeleteFilesSubCommand(
-                    self.parser.parse_args(expected["input_args"])
-                )
+                cmd = DeleteFilesSubCommand(self.parser.parse_args(expected["input_args"]))
                 cmd.run()
 
                 if delete_files_args is None:
--- python3-huggingface-hub-0.29.3.orig/tests/test_command_delete_cache.py
+++ python3-huggingface-hub-0.29.3/tests/test_command_delete_cache.py
@@ -49,9 +49,7 @@ class TestDeleteCacheHelpers(unittest.Te
 
         # Dataset repo separator
         self.assertIsInstance(choices[1], Separator)
-        self.assertEqual(
-            choices[1]._line, "\nDataset dummy_dataset (8M, used 2 weeks ago)"
-        )
+        self.assertEqual(choices[1]._line, "\nDataset dummy_dataset (8M, used 2 weeks ago)")
 
         # Only revision of `dummy_dataset`
         self.assertIsInstance(choices[2], Choice)
@@ -65,9 +63,7 @@ class TestDeleteCacheHelpers(unittest.Te
 
         # Model `dummy_model` separator
         self.assertIsInstance(choices[3], Separator)
-        self.assertEqual(
-            choices[3]._line, "\nModel dummy_model (1.4K, used 2 years ago)"
-        )
+        self.assertEqual(choices[3]._line, "\nModel dummy_model (1.4K, used 2 years ago)")
 
         # Oldest revision of `dummy_model`
         self.assertIsInstance(choices[4], Choice)
@@ -88,9 +84,7 @@ class TestDeleteCacheHelpers(unittest.Te
         # Only revision of `gpt2`
         self.assertIsInstance(choices[7], Choice)
         self.assertEqual(choices[7].value, "abcdef123456789")
-        self.assertEqual(
-            choices[7].name, "abcdef12: main, refs/pr/1 # modified 2 years ago"
-        )
+        self.assertEqual(choices[7].name, "abcdef12: main, refs/pr/1 # modified 2 years ago")
         self.assertFalse(choices[7].enabled)
 
     def test_get_expectations_str_on_no_deletion_item(self) -> None:
@@ -183,9 +177,7 @@ class TestDeleteCacheHelpers(unittest.Te
             self.assertIn("#    recent_hash_id", content)
 
             # Select dataset revision
-            content = content.replace(
-                "#    dataset_revision_hash_id", "dataset_revision_hash_id"
-            )
+            content = content.replace("#    dataset_revision_hash_id", "dataset_revision_hash_id")
             # Deselect abcdef123456789
             content = content.replace("abcdef123456789", "# abcdef123456789")
             with open(tmp_path, "w") as f:
@@ -208,9 +200,7 @@ class TestDeleteCacheHelpers(unittest.Te
         self.assertFalse(os.path.isfile(tmp_path))  # now deleted
 
         # User changed the selection
-        self.assertListEqual(
-            selected_hashes, ["dataset_revision_hash_id", "older_hash_id"]
-        )
+        self.assertListEqual(selected_hashes, ["dataset_revision_hash_id", "older_hash_id"])
 
         # Check printed instructions
         printed = output.getvalue()
@@ -293,12 +283,8 @@ class TestMockedDeleteCacheCommand(unitt
         mock__manual_review_tui.assert_called_once_with(cache_mock, preselected=[])
 
         # Step 3: ask confirmation
-        mock__get_expectations_str.assert_called_once_with(
-            cache_mock, ["hash_1", "hash_2"]
-        )
-        mock_confirm.assert_called_once_with(
-            "Will delete A and B. Confirm deletion ?", default=True
-        )
+        mock__get_expectations_str.assert_called_once_with(cache_mock, ["hash_1", "hash_2"])
+        mock_confirm.assert_called_once_with("Will delete A and B. Confirm deletion ?", default=True)
         mock_confirm().execute.assert_called_once_with()
 
         # Step 4: delete
@@ -325,9 +311,7 @@ class TestMockedDeleteCacheCommand(unitt
         # Check output
         self.assertEqual(output.getvalue(), "Deletion is cancelled. Do nothing.\n")
 
-    def test_run_stuff_selected_but_cancel_item_as_well_with_tui(
-        self, mock__manual_review_tui: Mock
-    ) -> None:
+    def test_run_stuff_selected_but_cancel_item_as_well_with_tui(self, mock__manual_review_tui: Mock) -> None:
         """Test command run when some are selected but "cancel item" as well."""
         # Mock return value
         mock__manual_review_tui.return_value = [
@@ -371,12 +355,8 @@ class TestMockedDeleteCacheCommand(unitt
         mock__manual_review_no_tui.assert_called_once_with(cache_mock, preselected=[])
 
         # Step 3: ask confirmation
-        mock__get_expectations_str.assert_called_once_with(
-            cache_mock, ["hash_1", "hash_2"]
-        )
-        mock__ask_for_confirmation_no_tui.assert_called_once_with(
-            "Will delete A and B. Confirm deletion ?"
-        )
+        mock__get_expectations_str.assert_called_once_with(cache_mock, ["hash_1", "hash_2"])
+        mock__ask_for_confirmation_no_tui.assert_called_once_with("Will delete A and B. Confirm deletion ?")
 
         # Step 4: delete
         cache_mock.delete_revisions.assert_called_once_with("hash_1", "hash_2")
--- python3-huggingface-hub-0.29.3.orig/tests/test_commit_api.py
+++ python3-huggingface-hub-0.29.3/tests/test_commit_api.py
@@ -10,42 +10,22 @@ from huggingface_hub._commit_api import
 class TestCommitOperationDelete(unittest.TestCase):
     def test_implicit_file(self):
         self.assertFalse(CommitOperationDelete(path_in_repo="path/to/file").is_folder)
-        self.assertFalse(
-            CommitOperationDelete(path_in_repo="path/to/file.md").is_folder
-        )
+        self.assertFalse(CommitOperationDelete(path_in_repo="path/to/file.md").is_folder)
 
     def test_implicit_folder(self):
         self.assertTrue(CommitOperationDelete(path_in_repo="path/to/folder/").is_folder)
-        self.assertTrue(
-            CommitOperationDelete(path_in_repo="path/to/folder.md/").is_folder
-        )
+        self.assertTrue(CommitOperationDelete(path_in_repo="path/to/folder.md/").is_folder)
 
     def test_explicit_file(self):
         # Weird case: if user explicitly set as file (`is_folder`=False) but path has a
         # trailing "/" => user input has priority
-        self.assertFalse(
-            CommitOperationDelete(
-                path_in_repo="path/to/folder/", is_folder=False
-            ).is_folder
-        )
-        self.assertFalse(
-            CommitOperationDelete(
-                path_in_repo="path/to/folder.md/", is_folder=False
-            ).is_folder
-        )
+        self.assertFalse(CommitOperationDelete(path_in_repo="path/to/folder/", is_folder=False).is_folder)
+        self.assertFalse(CommitOperationDelete(path_in_repo="path/to/folder.md/", is_folder=False).is_folder)
 
     def test_explicit_folder(self):
         # No need for the trailing "/" is `is_folder` explicitly passed
-        self.assertTrue(
-            CommitOperationDelete(
-                path_in_repo="path/to/folder", is_folder=True
-            ).is_folder
-        )
-        self.assertTrue(
-            CommitOperationDelete(
-                path_in_repo="path/to/folder.md", is_folder=True
-            ).is_folder
-        )
+        self.assertTrue(CommitOperationDelete(path_in_repo="path/to/folder", is_folder=True).is_folder)
+        self.assertTrue(CommitOperationDelete(path_in_repo="path/to/folder.md", is_folder=True).is_folder)
 
     def test_is_folder_wrong_value(self):
         with self.assertRaises(ValueError):
@@ -65,14 +45,10 @@ class TestCommitOperationPathInRepo(unit
         for input, expected in self.valid_values.items():
             with self.subTest(f"Testing with valid input: '{input}'"):
                 self.assertEqual(
-                    CommitOperationAdd(
-                        path_in_repo=input, path_or_fileobj=b""
-                    ).path_in_repo,
+                    CommitOperationAdd(path_in_repo=input, path_or_fileobj=b"").path_in_repo,
                     expected,
                 )
-                self.assertEqual(
-                    CommitOperationDelete(path_in_repo=input).path_in_repo, expected
-                )
+                self.assertEqual(CommitOperationDelete(path_in_repo=input).path_in_repo, expected)
 
     def test_path_in_repo_invalid(self) -> None:
         for input in self.invalid_values:
@@ -134,9 +110,7 @@ class TestWarnOnOverwritingOperations(un
     add_file_ab = CommitOperationAdd(path_in_repo="a/b.txt", path_or_fileobj=b"data")
     add_file_abc = CommitOperationAdd(path_in_repo="a/b/c.md", path_or_fileobj=b"data")
     add_file_abd = CommitOperationAdd(path_in_repo="a/b/d.md", path_or_fileobj=b"data")
-    update_file_abc = CommitOperationAdd(
-        path_in_repo="a/b/c.md", path_or_fileobj=b"updated data"
-    )
+    update_file_abc = CommitOperationAdd(path_in_repo="a/b/c.md", path_or_fileobj=b"updated data")
     delete_file_abc = CommitOperationDelete(path_in_repo="a/b/c.md")
     delete_folder_a = CommitOperationDelete(path_in_repo="a/")
     delete_folder_e = CommitOperationDelete(path_in_repo="e/")
@@ -170,6 +144,4 @@ class TestWarnOnOverwritingOperations(un
         _warn_on_overwriting_operations([self.delete_file_abc, self.add_file_abc])
 
     def test_delete_folder_then_add(self) -> None:
-        _warn_on_overwriting_operations(
-            [self.delete_folder_a, self.add_file_ab, self.add_file_abc]
-        )
+        _warn_on_overwriting_operations([self.delete_folder_a, self.add_file_ab, self.add_file_abc])
--- python3-huggingface-hub-0.29.3.orig/tests/test_commit_scheduler.py
+++ python3-huggingface-hub-0.29.3/tests/test_commit_scheduler.py
@@ -46,9 +46,7 @@ class TestCommitScheduler(unittest.TestC
         self.assertGreater(len(push_to_hub_mock.call_args_list), 2)
 
         # Can get the last upload result
-        self.assertEqual(
-            self.scheduler.last_future.result(), push_to_hub_mock.return_value
-        )
+        self.assertEqual(self.scheduler.last_future.result(), push_to_hub_mock.return_value)
 
     def test_invalid_folder_path_is_a_file(self) -> None:
         """Test cannot scheduler upload of a single file."""
@@ -56,15 +54,11 @@ class TestCommitScheduler(unittest.TestC
         file_path.write_text("something")
 
         with self.assertRaises(ValueError):
-            CommitScheduler(
-                folder_path=file_path, repo_id=self.repo_name, hf_api=self.api
-            )
+            CommitScheduler(folder_path=file_path, repo_id=self.repo_name, hf_api=self.api)
 
     def test_missing_folder_is_created(self) -> None:
         folder_path = self.cache_dir / "folder" / "subfolder"
-        self.scheduler = CommitScheduler(
-            folder_path=folder_path, repo_id=self.repo_name, hf_api=self.api
-        )
+        self.scheduler = CommitScheduler(folder_path=folder_path, repo_id=self.repo_name, hf_api=self.api)
         self.assertTrue(folder_path.is_dir())
 
     def test_sync_local_folder(self) -> None:
@@ -171,9 +165,7 @@ class TestCommitScheduler(unittest.TestC
         # Branch history has been squashed
         commits = self.api.list_repo_commits(repo_id=self.scheduler.repo_id)
         self.assertEqual(len(commits), 1)
-        self.assertEqual(
-            commits[0].title, "Super-squash branch 'main' using huggingface_hub"
-        )
+        self.assertEqual(commits[0].title, "Super-squash branch 'main' using huggingface_hub")
 
     def test_context_manager(self) -> None:
         watched_folder = self.cache_dir / "watched_folder"
@@ -190,9 +182,7 @@ class TestCommitScheduler(unittest.TestC
                 f.write("first line\n")
 
         assert "file.txt" in self.api.list_repo_files(scheduler.repo_id)
-        assert (
-            scheduler._CommitScheduler__stopped
-        )  # means the scheduler has been stopped when exiting the context
+        assert scheduler._CommitScheduler__stopped  # means the scheduler has been stopped when exiting the context
 
 
 @pytest.mark.usefixtures("fx_cache_dir")
--- python3-huggingface-hub-0.29.3.orig/tests/test_dduf.py
+++ python3-huggingface-hub-0.29.3/tests/test_dduf.py
@@ -27,9 +27,7 @@ class TestDDUFEntry:
     def dummy_entry(self, tmp_path: Path) -> DDUFEntry:
         dummy_dduf = tmp_path / "dummy_dduf.dduf"
         dummy_dduf.write_bytes(b"somethingCONTENTsomething")
-        return DDUFEntry(
-            filename="dummy.json", length=7, offset=9, dduf_path=dummy_dduf
-        )
+        return DDUFEntry(filename="dummy.json", length=7, offset=9, dduf_path=dummy_dduf)
 
     def test_dataclass(self, dummy_entry: DDUFEntry):
         assert dummy_entry.filename == "dummy.json"
@@ -46,15 +44,11 @@ class TestDDUFEntry:
 
 
 class TestUtils:
-    @pytest.mark.parametrize(
-        "filename", ["dummy.txt", "dummy.json", "dummy.safetensors"]
-    )
+    @pytest.mark.parametrize("filename", ["dummy.txt", "dummy.json", "dummy.safetensors"])
     def test_entry_name_valid_extension(self, filename: str):
         assert _validate_dduf_entry_name(filename) == filename
 
-    @pytest.mark.parametrize(
-        "filename", ["dummy", "dummy.bin", "dummy.dduf", "dummy.gguf"]
-    )
+    @pytest.mark.parametrize("filename", ["dummy", "dummy.bin", "dummy.dduf", "dummy.gguf"])
     def test_entry_name_invalid_extension(self, filename: str):
         with pytest.raises(DDUFInvalidEntryNameError):
             _validate_dduf_entry_name(filename)
@@ -102,23 +96,17 @@ class TestUtils:
 
     def test_validate_dduf_structure_not_a_dict(self):
         with pytest.raises(DDUFCorruptedFileError, match="Must be a dictionary."):
-            _validate_dduf_structure(
-                ["not a dict"], {}
-            )  # content from 'model_index.json'
+            _validate_dduf_structure(["not a dict"], {})  # content from 'model_index.json'
 
     def test_validate_dduf_structure_missing_folder(self):
         with pytest.raises(
             DDUFCorruptedFileError,
             match="Missing required entry 'encoder' in 'model_index.json'.",
         ):
-            _validate_dduf_structure(
-                {}, {"encoder/config.json", "encoder/model.safetensors"}
-            )
+            _validate_dduf_structure({}, {"encoder/config.json", "encoder/model.safetensors"})
 
     def test_validate_dduf_structure_missing_config_file(self):
-        with pytest.raises(
-            DDUFCorruptedFileError, match="Missing required file in folder 'encoder'."
-        ):
+        with pytest.raises(DDUFCorruptedFileError, match="Missing required file in folder 'encoder'."):
             _validate_dduf_structure(
                 {"encoder": {}},
                 {
@@ -148,9 +136,7 @@ class TestExportFolder:
         return folder_path
 
     def test_export_folder(self, dummy_folder: Path, mocker: MockerFixture):
-        mock = mocker.patch(
-            "huggingface_hub.serialization._dduf.export_entries_as_dduf"
-        )
+        mock = mocker.patch("huggingface_hub.serialization._dduf.export_entries_as_dduf")
         export_folder_as_dduf("dummy.dduf", dummy_folder)
         mock.assert_called_once()
         args = mock.call_args_list[0].args
@@ -167,13 +153,9 @@ class TestExportFolder:
 
 class TestExportEntries:
     @pytest.fixture
-    def dummy_entries(
-        self, tmp_path: Path
-    ) -> Iterable[Tuple[str, Union[str, Path, bytes]]]:
+    def dummy_entries(self, tmp_path: Path) -> Iterable[Tuple[str, Union[str, Path, bytes]]]:
         (tmp_path / "model_index.json").write_text(json.dumps({"foo": "bar"}))
-        (tmp_path / "doesnt_have_to_be_same_name.safetensors").write_bytes(
-            b"this is safetensors content"
-        )
+        (tmp_path / "doesnt_have_to_be_same_name.safetensors").write_bytes(b"this is safetensors content")
 
         return [
             ("model_index.json", str(tmp_path / "model_index.json")),  # string path
@@ -190,13 +172,9 @@ class TestExportEntries:
         dummy_entries: Iterable[Tuple[str, Union[str, Path, bytes]]],
         mocker: MockerFixture,
     ):
-        mock = mocker.patch(
-            "huggingface_hub.serialization._dduf._validate_dduf_structure"
-        )
+        mock = mocker.patch("huggingface_hub.serialization._dduf._validate_dduf_structure")
         export_entries_as_dduf(tmp_path / "dummy.dduf", dummy_entries)
-        mock.assert_called_once_with(
-            {"foo": "bar"}, {"model_index.json", "model.safetensors", "hello.txt"}
-        )
+        mock.assert_called_once_with({"foo": "bar"}, {"model_index.json", "model.safetensors", "hello.txt"})
 
         with zipfile.ZipFile(tmp_path / "dummy.dduf", "r") as archive:
             assert archive.compression == zipfile.ZIP_STORED  # uncompressed!
@@ -211,9 +189,7 @@ class TestExportEntries:
 
     def test_export_entries_invalid_name(self, tmp_path: Path):
         with pytest.raises(DDUFExportError, match="Invalid entry name") as e:
-            export_entries_as_dduf(
-                tmp_path / "dummy.dduf", [("config", "model_index.json")]
-            )
+            export_entries_as_dduf(tmp_path / "dummy.dduf", [("config", "model_index.json")])
         assert isinstance(e.value.__cause__, DDUFInvalidEntryNameError)
 
     def test_export_entries_no_duplicate(self, tmp_path: Path):
@@ -227,12 +203,8 @@ class TestExportEntries:
             )
 
     def test_export_entries_model_index_required(self, tmp_path: Path):
-        with pytest.raises(
-            DDUFExportError, match="Missing required 'model_index.json' entry"
-        ):
-            export_entries_as_dduf(
-                tmp_path / "dummy.dduf", [("model.safetensors", b"content")]
-            )
+        with pytest.raises(DDUFExportError, match="Missing required 'model_index.json' entry"):
+            export_entries_as_dduf(tmp_path / "dummy.dduf", [("model.safetensors", b"content")])
 
 
 class TestReadDDUFFile:
@@ -245,9 +217,7 @@ class TestReadDDUFFile:
         return tmp_path / "dummy.dduf"
 
     def test_read_dduf_file(self, dummy_dduf_file: Path, mocker: MockerFixture):
-        mock = mocker.patch(
-            "huggingface_hub.serialization._dduf._validate_dduf_structure"
-        )
+        mock = mocker.patch("huggingface_hub.serialization._dduf._validate_dduf_structure")
 
         entries = read_dduf_file(dummy_dduf_file)
         assert len(entries) == 3
@@ -255,9 +225,7 @@ class TestReadDDUFFile:
         model_entry = entries["model.safetensors"]
         hello_entry = entries["hello.txt"]
 
-        mock.assert_called_once_with(
-            {"foo": "bar"}, {"model_index.json", "model.safetensors", "hello.txt"}
-        )
+        mock.assert_called_once_with({"foo": "bar"}, {"model_index.json", "model.safetensors", "hello.txt"})
 
         assert index_entry.filename == "model_index.json"
         assert index_entry.dduf_path == dummy_dduf_file
@@ -283,7 +251,5 @@ class TestReadDDUFFile:
     def test_model_index_required(self, tmp_path: Path):
         with zipfile.ZipFile(tmp_path / "dummy.dduf", "w") as archive:
             archive.writestr("model.safetensors", b"this is safetensors content")
-        with pytest.raises(
-            DDUFCorruptedFileError, match="Missing required 'model_index.json' entry"
-        ):
+        with pytest.raises(DDUFCorruptedFileError, match="Missing required 'model_index.json' entry"):
             read_dduf_file(tmp_path / "dummy.dduf")
--- python3-huggingface-hub-0.29.3.orig/tests/test_fastai_integration.py
+++ python3-huggingface-hub-0.29.3/tests/test_fastai_integration.py
@@ -19,9 +19,7 @@ from .testing_utils import repo_name
 
 
 WORKING_REPO_SUBDIR = f"fixtures/working_repo_{__name__.split('.')[-1]}"
-WORKING_REPO_DIR = os.path.join(
-    os.path.dirname(os.path.abspath(__file__)), WORKING_REPO_SUBDIR
-)
+WORKING_REPO_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), WORKING_REPO_SUBDIR)
 
 if is_fastai_available():
     from fastai.data.block import DataBlock
@@ -86,9 +84,7 @@ class TestFastaiUtils(TestCase):
         api = HfApi(endpoint=ENDPOINT_STAGING, token=TOKEN)
         repo_id = f"{USER}/{repo_name()}"
 
-        push_to_hub_fastai(
-            learner=dummy_model, repo_id=repo_id, token=TOKEN, config=dummy_config
-        )
+        push_to_hub_fastai(learner=dummy_model, repo_id=repo_id, token=TOKEN, config=dummy_config)
         model_info = api.model_info(repo_id)
         assert model_info.id == repo_id
         loaded_model = from_pretrained_fastai(repo_id)
--- python3-huggingface-hub-0.29.3.orig/tests/test_file_download.py
+++ python3-huggingface-hub-0.29.3/tests/test_file_download.py
@@ -93,9 +93,7 @@ class TestDiskUsageWarning(unittest.Test
         with warnings.catch_warnings(record=True) as w:
             # Cause all warnings to always be triggered.
             warnings.simplefilter("always")
-            _check_disk_space(
-                expected_size=self.expected_size, target_dir=disk_usage_mock
-            )
+            _check_disk_space(expected_size=self.expected_size, target_dir=disk_usage_mock)
             assert len(w) == 1
             assert issubclass(w[-1].category, UserWarning)
 
@@ -104,9 +102,7 @@ class TestDiskUsageWarning(unittest.Test
         with warnings.catch_warnings(record=True) as w:
             # Cause all warnings to always be triggered.
             warnings.simplefilter("always")
-            _check_disk_space(
-                expected_size=self.expected_size, target_dir=disk_usage_mock
-            )
+            _check_disk_space(expected_size=self.expected_size, target_dir=disk_usage_mock)
             assert len(w) == 0
 
     def test_disk_usage_warning_with_non_existent_path(self) -> None:
@@ -114,9 +110,7 @@ class TestDiskUsageWarning(unittest.Test
         with warnings.catch_warnings(record=True) as w:
             # Cause all warnings to always be triggered.
             warnings.simplefilter("always")
-            _check_disk_space(
-                expected_size=self.expected_size, target_dir="path/to/not_existent_path"
-            )
+            _check_disk_space(expected_size=self.expected_size, target_dir="path/to/not_existent_path")
             assert len(w) == 0
 
         # Test for not existent (relative) path
@@ -134,9 +128,7 @@ class StagingDownloadTests(unittest.Test
     _api = HfApi(endpoint=ENDPOINT_STAGING, token=TOKEN)
 
     @use_tmp_repo()
-    def test_download_from_a_gated_repo_with_hf_hub_download(
-        self, repo_url: RepoUrl
-    ) -> None:
+    def test_download_from_a_gated_repo_with_hf_hub_download(self, repo_url: RepoUrl) -> None:
         """Checks `hf_hub_download` outputs error on gated repo.
 
         Regression test for #1121.
@@ -166,9 +158,7 @@ class StagingDownloadTests(unittest.Test
                 )
 
     @use_tmp_repo()
-    def test_download_regular_file_from_private_renamed_repo(
-        self, repo_url: RepoUrl
-    ) -> None:
+    def test_download_regular_file_from_private_renamed_repo(self, repo_url: RepoUrl) -> None:
         """Regression test for #1999.
 
         See https://github.com/huggingface/huggingface_hub/pull/1999.
@@ -178,9 +168,7 @@ class StagingDownloadTests(unittest.Test
 
         # Make private + rename + upload regular file
         self._api.update_repo_settings(repo_id_before, private=True)
-        self._api.upload_file(
-            repo_id=repo_id_before, path_in_repo="file.txt", path_or_fileobj=b"content"
-        )
+        self._api.upload_file(repo_id=repo_id_before, path_in_repo="file.txt", path_or_fileobj=b"content")
         self._api.move_repo(repo_id_before, repo_id_after)
 
         # Download from private renamed repo
@@ -219,9 +207,7 @@ class CachedDownloadTests(unittest.TestC
 
     def test_private_repo_and_file_cached_locally(self):
         api = HfApi(endpoint=ENDPOINT_STAGING)
-        repo_id = api.create_repo(
-            repo_id=repo_name(), private=True, token=TOKEN
-        ).repo_id
+        repo_id = api.create_repo(repo_id=repo_name(), private=True, token=TOKEN).repo_id
         api.upload_file(
             path_or_fileobj=b"content",
             path_in_repo="config.json",
@@ -231,14 +217,10 @@ class CachedDownloadTests(unittest.TestC
 
         with SoftTemporaryDirectory() as tmpdir:
             # Download a first time with token => file is cached
-            filepath_1 = api.hf_hub_download(
-                repo_id, filename="config.json", cache_dir=tmpdir, token=TOKEN
-            )
+            filepath_1 = api.hf_hub_download(repo_id, filename="config.json", cache_dir=tmpdir, token=TOKEN)
 
             # Download without token => return cached file
-            filepath_2 = api.hf_hub_download(
-                repo_id, filename="config.json", cache_dir=tmpdir, token=False
-            )
+            filepath_2 = api.hf_hub_download(repo_id, filename="config.json", cache_dir=tmpdir, token=False)
 
             assert filepath_1 == filepath_2
 
@@ -250,17 +232,13 @@ class CachedDownloadTests(unittest.TestC
         # Valid file but missing locally and network is disabled.
         with SoftTemporaryDirectory() as tmpdir:
             # Download a first time to get the refs ok
-            hf_hub_download(
-                DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=tmpdir
-            )
+            hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=tmpdir)
 
             # Set read-only permission recursively
             _recursive_chmod(tmpdir, 0o555)
 
             # Get without write-access must succeed
-            hf_hub_download(
-                DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=tmpdir
-            )
+            hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=tmpdir)
 
             # Set permission back for cleanup
             _recursive_chmod(tmpdir, 0o777)
@@ -277,9 +255,7 @@ class CachedDownloadTests(unittest.TestC
             # Equivalent to umask u=rwx,g=r,o=
             previous_umask = os.umask(0o037)
             try:
-                filepath = hf_hub_download(
-                    DUMMY_RENAMED_OLD_MODEL_ID, "config.json", cache_dir=tmpdir
-                )
+                filepath = hf_hub_download(DUMMY_RENAMED_OLD_MODEL_ID, "config.json", cache_dir=tmpdir)
                 # Permissions are honored (640: u=rw,g=r,o=)
                 self.assertEqual(stat.S_IMODE(os.stat(filepath).st_mode), 0o640)
             finally:
@@ -292,9 +268,7 @@ class CachedDownloadTests(unittest.TestC
         https://github.com/huggingface/huggingface_hub/issues/981
         """
         with SoftTemporaryDirectory() as tmpdir:
-            filepath = hf_hub_download(
-                DUMMY_RENAMED_OLD_MODEL_ID, "config.json", cache_dir=tmpdir
-            )
+            filepath = hf_hub_download(DUMMY_RENAMED_OLD_MODEL_ID, "config.json", cache_dir=tmpdir)
             self.assertTrue(os.path.exists(filepath))
 
     def test_hf_hub_download_with_empty_subfolder(self):
@@ -350,9 +324,7 @@ class CachedDownloadTests(unittest.TestC
             assert "foo/bar" in headers["user-agent"]
 
         with SoftTemporaryDirectory() as cache_dir:
-            with patch(
-                "huggingface_hub.file_download._request_wrapper", wraps=_request_wrapper
-            ) as mock_request:
+            with patch("huggingface_hub.file_download._request_wrapper", wraps=_request_wrapper) as mock_request:
                 # First download
                 hf_hub_download(
                     DUMMY_MODEL_ID,
@@ -367,9 +339,7 @@ class CachedDownloadTests(unittest.TestC
                 for call in calls:
                     _check_user_agent(call.kwargs["headers"])
 
-            with patch(
-                "huggingface_hub.file_download._request_wrapper", wraps=_request_wrapper
-            ) as mock_request:
+            with patch("huggingface_hub.file_download._request_wrapper", wraps=_request_wrapper) as mock_request:
                 # Second download: no GET call
                 hf_hub_download(
                     DUMMY_MODEL_ID,
@@ -422,14 +392,10 @@ class CachedDownloadTests(unittest.TestC
         # Make sure the file is cached
         filepath = hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME)
 
-        new_file_path = try_to_load_from_cache(
-            DUMMY_MODEL_ID, filename=constants.CONFIG_NAME
-        )
+        new_file_path = try_to_load_from_cache(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME)
         self.assertEqual(filepath, new_file_path)
 
-        new_file_path = try_to_load_from_cache(
-            DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, revision="main"
-        )
+        new_file_path = try_to_load_from_cache(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, revision="main")
         self.assertEqual(filepath, new_file_path)
 
         # If file is not cached, returns None
@@ -443,27 +409,17 @@ class CachedDownloadTests(unittest.TestC
             )
         )
         # Same for uncached models
-        self.assertIsNone(
-            try_to_load_from_cache("bert-base", filename=constants.CONFIG_NAME)
-        )
+        self.assertIsNone(try_to_load_from_cache("bert-base", filename=constants.CONFIG_NAME))
 
     def test_try_to_load_from_cache_specific_pr_revision_exists(self):
         # Make sure the file is cached
-        file_path = hf_hub_download(
-            DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, revision="refs/pr/1"
-        )
+        file_path = hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, revision="refs/pr/1")
 
-        new_file_path = try_to_load_from_cache(
-            DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, revision="refs/pr/1"
-        )
+        new_file_path = try_to_load_from_cache(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, revision="refs/pr/1")
         self.assertEqual(file_path, new_file_path)
 
         # If file is not cached, returns None
-        self.assertIsNone(
-            try_to_load_from_cache(
-                DUMMY_MODEL_ID, filename="conf.json", revision="refs/pr/1"
-            )
-        )
+        self.assertIsNone(try_to_load_from_cache(DUMMY_MODEL_ID, filename="conf.json", revision="refs/pr/1"))
 
         # If revision does not exist, returns None
         self.assertIsNone(
@@ -482,9 +438,7 @@ class CachedDownloadTests(unittest.TestC
         new_file_path = try_to_load_from_cache(DUMMY_MODEL_ID, filename="dummy")
         self.assertEqual(new_file_path, _CACHED_NO_EXIST)
 
-        new_file_path = try_to_load_from_cache(
-            DUMMY_MODEL_ID, filename="dummy", revision="main"
-        )
+        new_file_path = try_to_load_from_cache(DUMMY_MODEL_ID, filename="dummy", revision="main")
         self.assertEqual(new_file_path, _CACHED_NO_EXIST)
 
         # If file non-existence is not cached, returns None
@@ -547,9 +501,7 @@ class CachedDownloadTests(unittest.TestC
         metadata = get_hf_file_metadata(url)
 
         # Metadata
-        self.assertEqual(
-            metadata.commit_hash, DUMMY_MODEL_ID_REVISION_ONE_SPECIFIC_COMMIT
-        )
+        self.assertEqual(metadata.commit_hash, DUMMY_MODEL_ID_REVISION_ONE_SPECIFIC_COMMIT)
         self.assertIsNotNone(metadata.etag)  # example: "85c2fc2dcdd86563aaa85ef4911..."
         self.assertEqual(metadata.location, url)  # no redirect
         self.assertEqual(metadata.size, 851)
@@ -648,34 +600,24 @@ class CachedDownloadTests(unittest.TestC
         """
         with SoftTemporaryDirectory() as tmpdir:
             # Download the file once
-            filepath = Path(
-                hf_hub_download(
-                    DUMMY_MODEL_ID, filename="pytorch_model.bin", cache_dir=tmpdir
-                )
-            )
+            filepath = Path(hf_hub_download(DUMMY_MODEL_ID, filename="pytorch_model.bin", cache_dir=tmpdir))
 
             # Fake tmp file
             incomplete_filepath = Path(str(filepath.resolve()) + ".incomplete")
-            incomplete_filepath.write_bytes(
-                filepath.read_bytes()
-            )  # fake a partial download
+            incomplete_filepath.write_bytes(filepath.read_bytes())  # fake a partial download
             filepath.resolve().unlink()
 
             # delete snapshot folder to re-trigger a download
             shutil.rmtree(filepath.parents[2] / "snapshots")
 
             # Download must not fail
-            hf_hub_download(
-                DUMMY_MODEL_ID, filename="pytorch_model.bin", cache_dir=tmpdir
-            )
+            hf_hub_download(DUMMY_MODEL_ID, filename="pytorch_model.bin", cache_dir=tmpdir)
 
     @unittest.skipIf(os.name == "nt", "Lock files are always deleted on Windows.")
     def test_keep_lock_file(self):
         """Lock files should not be deleted on Linux."""
         with SoftTemporaryDirectory() as tmpdir:
-            hf_hub_download(
-                DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=tmpdir
-            )
+            hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=tmpdir)
             lock_file_exist = False
             locks_dir = os.path.join(tmpdir, ".locks")
             for subdir, dirs, files in os.walk(locks_dir):
@@ -719,16 +661,10 @@ class HfHubDownloadToLocalDir(unittest.T
     def setUpClass(cls):
         cls.api = HfApi(endpoint=ENDPOINT_STAGING, token=TOKEN)
         cls.repo_id = cls.api.create_repo(repo_id=repo_name()).repo_id
-        commit_1 = cls.api.upload_file(
-            path_or_fileobj=b"content", path_in_repo=cls.file_name, repo_id=cls.repo_id
-        )
-        commit_2 = cls.api.upload_file(
-            path_or_fileobj=b"content", path_in_repo=cls.lfs_name, repo_id=cls.repo_id
-        )
+        commit_1 = cls.api.upload_file(path_or_fileobj=b"content", path_in_repo=cls.file_name, repo_id=cls.repo_id)
+        commit_2 = cls.api.upload_file(path_or_fileobj=b"content", path_in_repo=cls.lfs_name, repo_id=cls.repo_id)
 
-        info = cls.api.get_paths_info(
-            repo_id=cls.repo_id, paths=[cls.file_name, cls.lfs_name]
-        )
+        info = cls.api.get_paths_info(repo_id=cls.repo_id, paths=[cls.file_name, cls.lfs_name])
         info = {item.path: item for item in info}
         cls.commit_hash_1 = commit_1.oid
         cls.commit_hash_2 = commit_2.oid
@@ -741,9 +677,7 @@ class HfHubDownloadToLocalDir(unittest.T
 
     @contextmanager
     def with_patch_head(self):
-        with patch(
-            "huggingface_hub.file_download._get_metadata_or_catch_error"
-        ) as mock:
+        with patch("huggingface_hub.file_download._get_metadata_or_catch_error") as mock:
             yield mock
 
     @contextmanager
@@ -770,9 +704,7 @@ class HfHubDownloadToLocalDir(unittest.T
     def test_metadata_ok_and_revision_is_a_commit_hash_and_match(self):
         # File already exists + commit_hash matches (and etag not even required)
         self.file_path.write_text("content")
-        write_download_metadata(
-            self.local_dir, self.file_name, self.commit_hash_1, etag="..."
-        )
+        write_download_metadata(self.local_dir, self.file_name, self.commit_hash_1, etag="...")
 
         # Download to local dir => no HEAD call needed
         with self.with_patch_head() as mock:
@@ -788,9 +720,7 @@ class HfHubDownloadToLocalDir(unittest.T
         # 1 HEAD call + 1 download
         # File already exists + commit_hash mismatch
         self.file_path.write_text("content")
-        write_download_metadata(
-            self.local_dir, self.file_name, self.commit_hash_1, etag="..."
-        )
+        write_download_metadata(self.local_dir, self.file_name, self.commit_hash_1, etag="...")
 
         # Mismatch => download
         with self.with_patch_download() as mock:
@@ -828,9 +758,7 @@ class HfHubDownloadToLocalDir(unittest.T
             local_files_only=True,
         )
         assert Path(path) == self.file_path
-        assert (
-            self.file_path.read_text() == "content2"
-        )  # not overwritten even if wrong content
+        assert self.file_path.read_text() == "content2"  # not overwritten even if wrong content
 
     def test_local_files_only_and_file_missing(self):
         # must raise
@@ -845,37 +773,27 @@ class HfHubDownloadToLocalDir(unittest.T
     def test_metadata_ok_and_etag_match(self):
         # 1 HEAD call + return early
         self.file_path.write_text("something")
-        write_download_metadata(
-            self.local_dir, self.file_name, self.commit_hash_1, etag=self.file_etag
-        )
+        write_download_metadata(self.local_dir, self.file_name, self.commit_hash_1, etag=self.file_etag)
 
         with self.with_patch_download() as mock:
             # Download from main => commit_hash mismatch but etag match => return early
-            self.api.hf_hub_download(
-                self.repo_id, filename=self.file_name, local_dir=self.local_dir
-            )
+            self.api.hf_hub_download(self.repo_id, filename=self.file_name, local_dir=self.local_dir)
         mock.assert_not_called()
 
     def test_metadata_ok_and_etag_mismatch(self):
         # 1 HEAD call + 1 download
         self.file_path.write_text("something")
-        write_download_metadata(
-            self.local_dir, self.file_name, self.commit_hash_1, etag="some_other_etag"
-        )
+        write_download_metadata(self.local_dir, self.file_name, self.commit_hash_1, etag="some_other_etag")
 
         with self.with_patch_download() as mock:
             # Download from main => commit_hash mismatch but etag match => return early
-            self.api.hf_hub_download(
-                self.repo_id, filename=self.file_name, local_dir=self.local_dir
-            )
+            self.api.hf_hub_download(self.repo_id, filename=self.file_name, local_dir=self.local_dir)
         mock.assert_called_once()
 
     def test_metadata_ok_and_etag_match_and_force_download(self):
         # force_download=True takes precedence on any other rule
         self.file_path.write_text("something")
-        write_download_metadata(
-            self.local_dir, self.file_name, self.commit_hash_1, etag=self.file_etag
-        )
+        write_download_metadata(self.local_dir, self.file_name, self.commit_hash_1, etag=self.file_etag)
 
         with self.with_patch_download() as mock:
             self.api.hf_hub_download(
@@ -894,9 +812,7 @@ class HfHubDownloadToLocalDir(unittest.T
             # Download from main
             # => no metadata but it's an LFS file
             # => compute local hash => matches => return early
-            self.api.hf_hub_download(
-                self.repo_id, filename=self.lfs_name, local_dir=self.local_dir
-            )
+            self.api.hf_hub_download(self.repo_id, filename=self.lfs_name, local_dir=self.local_dir)
         mock.assert_not_called()
 
     def test_metadata_not_ok_and_lfs_file_and_sha256_mismatch(self):
@@ -906,18 +822,14 @@ class HfHubDownloadToLocalDir(unittest.T
         # Download from main
         # => no metadata but it's an LFS file
         # => compute local hash => mismatches => download
-        path = self.api.hf_hub_download(
-            self.repo_id, filename=self.lfs_name, local_dir=self.local_dir
-        )
+        path = self.api.hf_hub_download(self.repo_id, filename=self.lfs_name, local_dir=self.local_dir)
 
         # existing file overwritten
         assert Path(path).read_text() == "content"
 
     def test_file_exists_in_cache(self):
         # 1 HEAD call + return early
-        self.api.hf_hub_download(
-            self.repo_id, filename=self.file_name, cache_dir=self.hub_cache_dir
-        )
+        self.api.hf_hub_download(self.repo_id, filename=self.file_name, cache_dir=self.hub_cache_dir)
 
         with self.with_patch_download() as mock:
             # Download to local dir
@@ -937,36 +849,20 @@ class HfHubDownloadToLocalDir(unittest.T
     def test_file_exists_and_overwrites(self):
         # 1 HEAD call + 1 download
         self.file_path.write_text("another content")
-        self.api.hf_hub_download(
-            self.repo_id, filename=self.file_name, local_dir=self.local_dir
-        )
+        self.api.hf_hub_download(self.repo_id, filename=self.file_name, local_dir=self.local_dir)
         assert self.file_path.read_text() == "content"
 
     def test_resume_from_incomplete(self):
         # An incomplete file already exists => use it
-        incomplete_path = (
-            self.local_dir
-            / ".cache"
-            / "huggingface"
-            / "download"
-            / (self.file_name + ".incomplete")
-        )
+        incomplete_path = self.local_dir / ".cache" / "huggingface" / "download" / (self.file_name + ".incomplete")
         incomplete_path.parent.mkdir(parents=True, exist_ok=True)
         incomplete_path.write_text("XXXX")  # Here we put fake data to test the resume
-        self.api.hf_hub_download(
-            self.repo_id, filename=self.file_name, local_dir=self.local_dir
-        )
+        self.api.hf_hub_download(self.repo_id, filename=self.file_name, local_dir=self.local_dir)
         self.file_path.read_text() == "XXXXent"
 
     def test_do_not_resume_on_force_download(self):
         # An incomplete file already exists but force_download=True
-        incomplete_path = (
-            self.local_dir
-            / ".cache"
-            / "huggingface"
-            / "download"
-            / (self.file_name + ".incomplete")
-        )
+        incomplete_path = self.local_dir / ".cache" / "huggingface" / "download" / (self.file_name + ".incomplete")
         incomplete_path.parent.mkdir(parents=True, exist_ok=True)
         incomplete_path.write_text("XXXX")
         self.api.hf_hub_download(
@@ -987,18 +883,14 @@ class HfHubDownloadToLocalDir(unittest.T
         """
         # Download to local dir
         mock.reset_mock(return_value={})
-        self.api.hf_hub_download(
-            self.repo_id, filename=self.file_name, local_dir=self.local_dir, token=False
-        )
+        self.api.hf_hub_download(self.repo_id, filename=self.file_name, local_dir=self.local_dir, token=False)
         mock.assert_called()
         for call in mock.call_args_list:
             assert call.kwargs["token"] is False
 
         # Download to cache dir
         mock.reset_mock(return_value={})
-        self.api.hf_hub_download(
-            self.repo_id, filename=self.file_name, cache_dir=self.local_dir, token=False
-        )
+        self.api.hf_hub_download(self.repo_id, filename=self.file_name, cache_dir=self.local_dir, token=False)
         mock.assert_called()
         for call in mock.call_args_list:
             assert call.kwargs["token"] is False
@@ -1022,7 +914,9 @@ class StagingCachedDownloadOnAwfulFilena
     def setUpClass(cls):
         cls.api = HfApi(endpoint=ENDPOINT_STAGING, token=TOKEN)
         cls.repo_url = cls.api.create_repo(repo_id=repo_name("awful_filename"))
-        cls.expected_resolve_url = f"{cls.repo_url}/resolve/main/subfolder/to%3F/awful%3Ffilename%25you%3Ashould%2Cnever.give"
+        cls.expected_resolve_url = (
+            f"{cls.repo_url}/resolve/main/subfolder/to%3F/awful%3Ffilename%25you%3Ashould%2Cnever.give"
+        )
         cls.api.upload_file(
             path_or_fileobj=b"content",
             path_in_repo=cls.filepath,
@@ -1034,9 +928,7 @@ class StagingCachedDownloadOnAwfulFilena
         cls.api.delete_repo(repo_id=cls.repo_url.repo_id)
 
     def test_hf_hub_url_on_awful_filepath(self):
-        self.assertEqual(
-            hf_hub_url(self.repo_url.repo_id, self.filepath), self.expected_resolve_url
-        )
+        self.assertEqual(hf_hub_url(self.repo_url.repo_id, self.filepath), self.expected_resolve_url)
 
     def test_hf_hub_url_on_awful_subfolder_and_filename(self):
         self.assertEqual(
@@ -1046,9 +938,7 @@ class StagingCachedDownloadOnAwfulFilena
 
     @xfail_on_windows(reason="Windows paths cannot contain a '?'.")
     def test_hf_hub_download_on_awful_filepath(self):
-        local_path = hf_hub_download(
-            self.repo_url.repo_id, self.filepath, cache_dir=self.cache_dir
-        )
+        local_path = hf_hub_download(self.repo_url.repo_id, self.filepath, cache_dir=self.cache_dir)
         # Local path is not url-encoded
         self.assertTrue(local_path.endswith(self.filepath))
 
@@ -1090,17 +980,11 @@ class TestHfHubDownloadRelativePaths(uni
     def tearDownClass(cls) -> None:
         cls.api.delete_repo(repo_id=cls.repo_id)
 
-    @xfail_on_windows(
-        reason="Windows paths cannot contain '\\..\\'.", raises=ValueError
-    )
+    @xfail_on_windows(reason="Windows paths cannot contain '\\..\\'.", raises=ValueError)
     def test_download_folder_file_in_cache_dir(self) -> None:
-        hf_hub_download(
-            self.repo_id, "folder/..\\..\\..\\file", cache_dir=self.cache_dir
-        )
+        hf_hub_download(self.repo_id, "folder/..\\..\\..\\file", cache_dir=self.cache_dir)
 
-    @xfail_on_windows(
-        reason="Windows paths cannot contain '\\..\\'.", raises=ValueError
-    )
+    @xfail_on_windows(reason="Windows paths cannot contain '\\..\\'.", raises=ValueError)
     def test_download_folder_file_to_local_dir(self) -> None:
         with SoftTemporaryDirectory() as local_dir:
             hf_hub_download(
@@ -1119,11 +1003,7 @@ class TestHfHubDownloadRelativePaths(uni
 
     def test_get_pointer_path_but_invalid_relative_filename(self) -> None:
         # Cannot happen because of other protections, but just in case.
-        relative_filename = (
-            "folder\\..\\..\\..\\file.txt"
-            if os.name == "nt"
-            else "folder/../../../file.txt"
-        )
+        relative_filename = "folder\\..\\..\\..\\file.txt" if os.name == "nt" else "folder/../../../file.txt"
         with self.assertRaises(ValueError):
             _get_pointer_path("path/to/storage", "abcdef", relative_filename)
 
@@ -1212,9 +1092,7 @@ class TestHttpGet:
             ),
         ],
     )
-    def test_http_get_with_range_headers(
-        self, caplog, initial_range: str, expected_ranges: List[str]
-    ):
+    def test_http_get_with_range_headers(self, caplog, initial_range: str, expected_ranges: List[str]):
         def _iter_content_1() -> Iterable[bytes]:
             yield b"0" * 10
             yield b"0" * 10
@@ -1262,9 +1140,7 @@ class TestHttpGet:
 class CreateSymlinkTest(unittest.TestCase):
     @unittest.skipIf(os.name == "nt", "No symlinks on Windows")
     @patch("huggingface_hub.file_download.are_symlinks_supported")
-    def test_create_symlink_concurrent_access(
-        self, mock_are_symlinks_supported: Mock
-    ) -> None:
+    def test_create_symlink_concurrent_access(self, mock_are_symlinks_supported: Mock) -> None:
         with SoftTemporaryDirectory() as tmpdir:
             src = os.path.join(tmpdir, "source")
             other = os.path.join(tmpdir, "other")
@@ -1357,8 +1233,7 @@ class TestNormalizeEtag(unittest.TestCas
     def _get_etag_and_normalize(response: Response) -> str:
         response.raise_for_status()
         return _normalize_etag(
-            response.headers.get(constants.HUGGINGFACE_HEADER_X_LINKED_ETAG)
-            or response.headers.get("ETag")
+            response.headers.get(constants.HUGGINGFACE_HEADER_X_LINKED_ETAG) or response.headers.get("ETag")
         )
 
 
@@ -1373,9 +1248,7 @@ class TestEtagTimeoutConfig(unittest.Tes
                 "get_hf_file_metadata",
                 wraps=huggingface_hub.file_download.get_hf_file_metadata,
             ) as mock_etag_call:
-                hf_hub_download(
-                    DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=cache_dir
-                )
+                hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=cache_dir)
                 kwargs = mock_etag_call.call_args.kwargs
                 self.assertIn("timeout", kwargs)
                 self.assertEqual(kwargs["timeout"], 10)
@@ -1397,14 +1270,10 @@ class TestEtagTimeoutConfig(unittest.Tes
                 )
                 kwargs = mock_etag_call.call_args.kwargs
                 self.assertIn("timeout", kwargs)
-                self.assertEqual(
-                    kwargs["timeout"], 12
-                )  # passed as parameter, takes priority
+                self.assertEqual(kwargs["timeout"], 12)  # passed as parameter, takes priority
 
     @patch("huggingface_hub.file_download.constants.DEFAULT_ETAG_TIMEOUT", 10)
-    @patch(
-        "huggingface_hub.file_download.constants.HF_HUB_ETAG_TIMEOUT", 15
-    )  # takes priority
+    @patch("huggingface_hub.file_download.constants.HF_HUB_ETAG_TIMEOUT", 15)  # takes priority
     def test_etag_timeout_set_as_env_variable(self):
         with SoftTemporaryDirectory() as cache_dir:
             with patch.object(
@@ -1412,17 +1281,13 @@ class TestEtagTimeoutConfig(unittest.Tes
                 "get_hf_file_metadata",
                 wraps=huggingface_hub.file_download.get_hf_file_metadata,
             ) as mock_etag_call:
-                hf_hub_download(
-                    DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=cache_dir
-                )
+                hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=cache_dir)
                 kwargs = mock_etag_call.call_args.kwargs
                 self.assertIn("timeout", kwargs)
                 self.assertEqual(kwargs["timeout"], 15)
 
     @patch("huggingface_hub.file_download.constants.DEFAULT_ETAG_TIMEOUT", 10)
-    @patch(
-        "huggingface_hub.file_download.constants.HF_HUB_ETAG_TIMEOUT", 12
-    )  # takes priority
+    @patch("huggingface_hub.file_download.constants.HF_HUB_ETAG_TIMEOUT", 12)  # takes priority
     def test_etag_timeout_set_as_env_variable_parameter_ignored(self):
         with SoftTemporaryDirectory() as cache_dir:
             with patch.object(
@@ -1438,9 +1303,7 @@ class TestEtagTimeoutConfig(unittest.Tes
                 )
                 kwargs = mock_etag_call.call_args.kwargs
                 self.assertIn("timeout", kwargs)
-                self.assertEqual(
-                    kwargs["timeout"], 12
-                )  # passed value ignored, HF_HUB_ETAG_TIMEOUT takes priority
+                self.assertEqual(kwargs["timeout"], 12)  # passed value ignored, HF_HUB_ETAG_TIMEOUT takes priority
 
 
 def _recursive_chmod(path: str, mode: int) -> None:
--- python3-huggingface-hub-0.29.3.orig/tests/test_hf_api.py
+++ python3-huggingface-hub-0.29.3/tests/test_hf_api.py
@@ -111,9 +111,7 @@ from .testing_utils import (
 
 logger = logging.get_logger(__name__)
 
-WORKING_REPO_DIR = os.path.join(
-    os.path.dirname(os.path.abspath(__file__)), "fixtures/working_repo"
-)
+WORKING_REPO_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "fixtures/working_repo")
 LARGE_FILE_14MB = "https://cdn-media.huggingface.co/lfs-largefiles/progit.epub"
 LARGE_FILE_18MB = "https://cdn-media.huggingface.co/lfs-largefiles/progit.pdf"
 
@@ -138,9 +136,7 @@ class HfApiRepoFileExistsTest(HfApiCommo
     def setUp(self) -> None:
         super().setUp()
         self.repo_id = self._api.create_repo(repo_name(), private=True).repo_id
-        self.upload = self._api.upload_file(
-            repo_id=self.repo_id, path_in_repo="file.txt", path_or_fileobj=b"content"
-        )
+        self.upload = self._api.upload_file(repo_id=self.repo_id, path_in_repo="file.txt", path_or_fileobj=b"content")
 
     def tearDown(self) -> None:
         self._api.delete_repo(self.repo_id)
@@ -148,24 +144,14 @@ class HfApiRepoFileExistsTest(HfApiCommo
 
     def test_repo_exists(self):
         assert self._api.repo_exists(self.repo_id)
-        self.assertFalse(
-            self._api.repo_exists(self.repo_id, token=False)
-        )  # private repo
-        self.assertFalse(
-            self._api.repo_exists("repo-that-does-not-exist")
-        )  # missing repo
+        self.assertFalse(self._api.repo_exists(self.repo_id, token=False))  # private repo
+        self.assertFalse(self._api.repo_exists("repo-that-does-not-exist"))  # missing repo
 
     def test_revision_exists(self):
         assert self._api.revision_exists(self.repo_id, "main")
-        assert not self._api.revision_exists(
-            self.repo_id, "revision-that-does-not-exist"
-        )  # missing revision
-        assert not self._api.revision_exists(
-            self.repo_id, "main", token=False
-        )  # private repo
-        assert not self._api.revision_exists(
-            "repo-that-does-not-exist", "main"
-        )  # missing repo
+        assert not self._api.revision_exists(self.repo_id, "revision-that-does-not-exist")  # missing revision
+        assert not self._api.revision_exists(self.repo_id, "main", token=False)  # private repo
+        assert not self._api.revision_exists("repo-that-does-not-exist", "main")  # missing repo
 
     @patch("huggingface_hub.constants.ENDPOINT", "https://hub-ci.huggingface.co")
     @patch(
@@ -174,20 +160,12 @@ class HfApiRepoFileExistsTest(HfApiCommo
     )
     def test_file_exists(self):
         assert self._api.file_exists(self.repo_id, "file.txt")
+        self.assertFalse(self._api.file_exists("repo-that-does-not-exist", "file.txt"))  # missing repo
+        self.assertFalse(self._api.file_exists(self.repo_id, "file-does-not-exist"))  # missing file
         self.assertFalse(
-            self._api.file_exists("repo-that-does-not-exist", "file.txt")
-        )  # missing repo
-        self.assertFalse(
-            self._api.file_exists(self.repo_id, "file-does-not-exist")
-        )  # missing file
-        self.assertFalse(
-            self._api.file_exists(
-                self.repo_id, "file.txt", revision="revision-that-does-not-exist"
-            )
+            self._api.file_exists(self.repo_id, "file.txt", revision="revision-that-does-not-exist")
         )  # missing revision
-        self.assertFalse(
-            self._api.file_exists(self.repo_id, "file.txt", token=False)
-        )  # private repo
+        self.assertFalse(self._api.file_exists(self.repo_id, "file.txt", token=False))  # private repo
 
 
 class HfApiEndpointsTest(HfApiCommonTest):
@@ -256,12 +234,8 @@ class HfApiEndpointsTest(HfApiCommonTest
         self._api.create_repo(repo_id=repo_id_1)
         self._api.create_repo(repo_id=repo_id_2)
 
-        with pytest.raises(
-            HfHubHTTPError, match=r"A model repository called .* already exists"
-        ):
-            self._api.move_repo(
-                from_id=repo_id_1, to_id=repo_id_2, repo_type=constants.REPO_TYPE_MODEL
-            )
+        with pytest.raises(HfHubHTTPError, match=r"A model repository called .* already exists"):
+            self._api.move_repo(from_id=repo_id_1, to_id=repo_id_2, repo_type=constants.REPO_TYPE_MODEL)
 
         self._api.delete_repo(repo_id=repo_id_1)
         self._api.delete_repo(repo_id=repo_id_2)
@@ -280,9 +254,7 @@ class HfApiEndpointsTest(HfApiCommonTest
 
         for gated_value in ["auto", "manual", False]:
             for private_value in [True, False]:  # Test both private and public settings
-                self._api.update_repo_settings(
-                    repo_id=repo_id, gated=gated_value, private=private_value
-                )
+                self._api.update_repo_settings(repo_id=repo_id, gated=gated_value, private=private_value)
                 info = self._api.model_info(repo_id)
                 assert info.gated == gated_value
                 assert info.private == private_value  # Verify the private setting
@@ -348,9 +320,7 @@ class CommitApiTest(HfApiCommonTest):
             ):
                 CommitOperationAdd(path_or_fileobj=ftext, path_in_repo="README.md")  # type: ignore
 
-        with self.assertRaises(
-            ValueError, msg="path_or_fileobj is str but does not point to a file"
-        ):
+        with self.assertRaises(ValueError, msg="path_or_fileobj is str but does not point to a file"):
             CommitOperationAdd(
                 path_or_fileobj=os.path.join(self.tmp_dir, "nofile.pth"),
                 path_in_repo="README.md",
@@ -368,11 +338,7 @@ class CommitApiTest(HfApiCommonTest):
         self.assertIsInstance(return_val, CommitInfo)
 
         with SoftTemporaryDirectory() as cache_dir:
-            with open(
-                hf_hub_download(
-                    repo_id=repo_id, filename="temp/new_file.md", cache_dir=cache_dir
-                )
-            ) as f:
+            with open(hf_hub_download(repo_id=repo_id, filename="temp/new_file.md", cache_dir=cache_dir)) as f:
                 self.assertEqual(f.read(), self.tmp_file_content)
 
     @use_tmp_repo()
@@ -397,11 +363,7 @@ class CommitApiTest(HfApiCommonTest):
         self.assertEqual(return_val, f"{repo_url}/blob/main/temp/new_file.md")
 
         with SoftTemporaryDirectory() as cache_dir:
-            with open(
-                hf_hub_download(
-                    repo_id=repo_id, filename="temp/new_file.md", cache_dir=cache_dir
-                )
-            ) as f:
+            with open(hf_hub_download(repo_id=repo_id, filename="temp/new_file.md", cache_dir=cache_dir)) as f:
                 self.assertEqual(f.read(), self.tmp_file_content)
 
     @use_tmp_repo()
@@ -416,11 +378,7 @@ class CommitApiTest(HfApiCommonTest):
         self.assertEqual(return_val, f"{repo_url}/blob/main/temp/new_file.md")
 
         with SoftTemporaryDirectory() as cache_dir:
-            with open(
-                hf_hub_download(
-                    repo_id=repo_id, filename="temp/new_file.md", cache_dir=cache_dir
-                )
-            ) as f:
+            with open(hf_hub_download(repo_id=repo_id, filename="temp/new_file.md", cache_dir=cache_dir)) as f:
                 self.assertEqual(f.read(), content.getvalue().decode())
 
     @use_tmp_repo()
@@ -510,9 +468,7 @@ class CommitApiTest(HfApiCommonTest):
             path_in_repo="temp/new_file.md",
             repo_id=repo_url.repo_id,
         )
-        return_val = self._api.delete_file(
-            path_in_repo="temp/new_file.md", repo_id=repo_url.repo_id
-        )
+        return_val = self._api.delete_file(path_in_repo="temp/new_file.md", repo_id=repo_url.repo_id)
         self.assertIsInstance(return_val, CommitInfo)
 
         with self.assertRaises(EntryNotFoundError):
@@ -523,9 +479,7 @@ class CommitApiTest(HfApiCommonTest):
         repo_name_with_no_org = self._api.get_full_repo_name("model")
         self.assertEqual(repo_name_with_no_org, f"{USER}/model")
 
-        repo_name_with_no_org = self._api.get_full_repo_name(
-            "model", organization="org"
-        )
+        repo_name_with_no_org = self._api.get_full_repo_name("model", organization="org")
         self.assertEqual(repo_name_with_no_org, "org/model")
 
     @use_tmp_repo()
@@ -533,9 +487,7 @@ class CommitApiTest(HfApiCommonTest):
         repo_id = repo_url.repo_id
 
         # Upload folder
-        url = self._api.upload_folder(
-            folder_path=self.tmp_dir, path_in_repo="temp/dir", repo_id=repo_id
-        )
+        url = self._api.upload_folder(folder_path=self.tmp_dir, path_in_repo="temp/dir", repo_id=repo_id)
         self.assertEqual(
             url,
             f"{self._api.endpoint}/{repo_id}/tree/main/temp/dir",
@@ -560,9 +512,7 @@ class CommitApiTest(HfApiCommonTest):
             self.assertEqual(content, expected_content)
 
         # Re-uploading the same folder twice should be fine
-        return_val = self._api.upload_folder(
-            folder_path=self.tmp_dir, path_in_repo="temp/dir", repo_id=repo_id
-        )
+        return_val = self._api.upload_folder(folder_path=self.tmp_dir, path_in_repo="temp/dir", repo_id=repo_id)
         self.assertIsInstance(return_val, CommitInfo)
 
     @use_tmp_repo()
@@ -576,24 +526,18 @@ class CommitApiTest(HfApiCommonTest):
             repo_id=repo_id,
             create_pr=True,
         )
-        self.assertEqual(
-            return_val, f"{self._api.endpoint}/{repo_id}/tree/refs%2Fpr%2F1/temp/dir"
-        )
+        self.assertEqual(return_val, f"{self._api.endpoint}/{repo_id}/tree/refs%2Fpr%2F1/temp/dir")
 
         # Check files are uploaded
         for rpath in ["temp", "nested/file.bin"]:
             local_path = os.path.join(self.tmp_dir, rpath)
-            filepath = hf_hub_download(
-                repo_id=repo_id, filename=f"temp/dir/{rpath}", revision="refs/pr/1"
-            )
+            filepath = hf_hub_download(repo_id=repo_id, filename=f"temp/dir/{rpath}", revision="refs/pr/1")
             assert Path(local_path).read_bytes() == Path(filepath).read_bytes()
 
     def test_upload_folder_default_path_in_repo(self):
         REPO_NAME = repo_name("upload_folder_to_root")
         self._api.create_repo(repo_id=REPO_NAME, exist_ok=False)
-        url = self._api.upload_folder(
-            folder_path=self.tmp_dir, repo_id=f"{USER}/{REPO_NAME}"
-        )
+        url = self._api.upload_folder(folder_path=self.tmp_dir, repo_id=f"{USER}/{REPO_NAME}")
         # URL to root of repository
         self.assertEqual(url, f"{self._api.endpoint}/{USER}/{REPO_NAME}/tree/main/")
 
@@ -682,9 +626,7 @@ class CommitApiTest(HfApiCommonTest):
         self.assertIn("pr_revision='refs/pr/1'", repr(resp))
         self.assertIsInstance(commit_id, str)
         self.assertGreater(len(commit_id), 0)
-        self.assertEqual(
-            resp.commit_url, f"{self._api.endpoint}/{repo_id}/commit/{commit_id}"
-        )
+        self.assertEqual(resp.commit_url, f"{self._api.endpoint}/{repo_id}/commit/{commit_id}")
         self.assertEqual(resp.commit_message, "Test create_commit")
         self.assertEqual(resp.commit_description, "")
         self.assertEqual(resp.pr_url, f"{self._api.endpoint}/{repo_id}/discussions/1")
@@ -698,9 +640,7 @@ class CommitApiTest(HfApiCommonTest):
             self.assertEqual(ctx.exception.response.status_code, 404)
 
         # ...but exists on PR
-        filepath = self._api.hf_hub_download(
-            filename="buffer", repo_id=repo_id, revision="refs/pr/1"
-        )
+        filepath = self._api.hf_hub_download(filename="buffer", repo_id=repo_id, revision="refs/pr/1")
         with open(filepath, "rb") as downloaded_file:
             content = downloaded_file.read()
         self.assertEqual(content, b"Buffer data")
@@ -729,9 +669,7 @@ class CommitApiTest(HfApiCommonTest):
         with self.assertRaises(RevisionNotFoundError):
             self._api.create_commit(
                 operations=[
-                    CommitOperationAdd(
-                        path_in_repo="file.txt", path_or_fileobj=b"content"
-                    ),
+                    CommitOperationAdd(path_in_repo="file.txt", path_or_fileobj=b"content"),
                 ],
                 commit_message="PR against a oid",
                 repo_id=repo_id,
@@ -743,9 +681,7 @@ class CommitApiTest(HfApiCommonTest):
         with self.assertRaises(RevisionNotFoundError):
             self._api.create_commit(
                 operations=[
-                    CommitOperationAdd(
-                        path_in_repo="file.txt", path_or_fileobj=b"content"
-                    ),
+                    CommitOperationAdd(path_in_repo="file.txt", path_or_fileobj=b"content"),
                 ],
                 commit_message="PR against missing branch",
                 repo_id=repo_id,
@@ -764,12 +700,8 @@ class CommitApiTest(HfApiCommonTest):
 
         self._api.create_commit(
             operations=[
-                CommitOperationAdd(
-                    path_in_repo="regular.txt", path_or_fileobj=b"File content"
-                ),
-                CommitOperationAdd(
-                    path_in_repo="lfs.pkl", path_or_fileobj=b"File content"
-                ),
+                CommitOperationAdd(path_in_repo="regular.txt", path_or_fileobj=b"File content"),
+                CommitOperationAdd(path_in_repo="lfs.pkl", path_or_fileobj=b"File content"),
             ],
             commit_message="PR on foreign repo",
             repo_id=foreign_repo_url.repo_id,
@@ -789,9 +721,7 @@ class CommitApiTest(HfApiCommonTest):
         with open(self.tmp_file, "rb") as fileobj:
             operations = [
                 CommitOperationDelete(path_in_repo="temp/new_file.md"),
-                CommitOperationAdd(
-                    path_in_repo="buffer", path_or_fileobj=b"Buffer data"
-                ),
+                CommitOperationAdd(path_in_repo="buffer", path_or_fileobj=b"Buffer data"),
                 CommitOperationAdd(
                     path_in_repo="bytesio",
                     path_or_fileobj=BytesIO(b"BytesIO data"),
@@ -911,9 +841,7 @@ class CommitApiTest(HfApiCommonTest):
             # Upload a PNG file
             self._api.create_commit(
                 operations=[
-                    CommitOperationAdd(
-                        path_in_repo="image.png", path_or_fileobj=b"image data"
-                    ),
+                    CommitOperationAdd(path_in_repo="image.png", path_or_fileobj=b"image data"),
                 ],
                 commit_message="Test upload lfs file",
                 repo_id=repo_id,
@@ -1002,9 +930,7 @@ class CommitApiTest(HfApiCommonTest):
             commit_message="Add 1 regular and 1 LFs files.",
             operations=[
                 CommitOperationAdd(path_in_repo="file.txt", path_or_fileobj=b"content"),
-                CommitOperationAdd(
-                    path_in_repo="lfs.bin", path_or_fileobj=b"LFS content"
-                ),
+                CommitOperationAdd(path_in_repo="lfs.bin", path_or_fileobj=b"LFS content"),
             ],
         )
         repo_files = self._api.list_repo_files(repo_id=repo_id)
@@ -1019,33 +945,21 @@ class CommitApiTest(HfApiCommonTest):
         """
         repo_id = repo_url.repo_id
 
-        self._api.upload_file(
-            path_or_fileobj=b"content", repo_id=repo_id, path_in_repo="file.txt"
-        )
-        self._api.upload_file(
-            path_or_fileobj=b"LFS content", repo_id=repo_id, path_in_repo="lfs.bin"
-        )
+        self._api.upload_file(path_or_fileobj=b"content", repo_id=repo_id, path_in_repo="file.txt")
+        self._api.upload_file(path_or_fileobj=b"LFS content", repo_id=repo_id, path_in_repo="lfs.bin")
 
         self._api.create_commit(
             repo_id=repo_id,
             commit_message="Copy LFS file.",
             operations=[
-                CommitOperationCopy(
-                    src_path_in_repo="lfs.bin", path_in_repo="lfs Copy.bin"
-                ),
-                CommitOperationCopy(
-                    src_path_in_repo="lfs.bin", path_in_repo="lfs Copy (1).bin"
-                ),
+                CommitOperationCopy(src_path_in_repo="lfs.bin", path_in_repo="lfs Copy.bin"),
+                CommitOperationCopy(src_path_in_repo="lfs.bin", path_in_repo="lfs Copy (1).bin"),
             ],
         )
         self._api.create_commit(
             repo_id=repo_id,
             commit_message="Copy regular file.",
-            operations=[
-                CommitOperationCopy(
-                    src_path_in_repo="file.txt", path_in_repo="file Copy.txt"
-                )
-            ],
+            operations=[CommitOperationCopy(src_path_in_repo="file.txt", path_in_repo="file Copy.txt")],
         )
         with self.assertRaises(EntryNotFoundError):
             self._api.create_commit(
@@ -1068,9 +982,7 @@ class CommitApiTest(HfApiCommonTest):
         self.assertIn("lfs Copy (1).bin", repo_files)
 
         # Check same LFS file
-        repo_file1, repo_file2 = self._api.get_paths_info(
-            repo_id=repo_id, paths=["lfs.bin", "lfs Copy.bin"]
-        )
+        repo_file1, repo_file2 = self._api.get_paths_info(repo_id=repo_id, paths=["lfs.bin", "lfs Copy.bin"])
         self.assertEqual(repo_file1.lfs["sha256"], repo_file2.lfs["sha256"])
 
     @use_tmp_repo()
@@ -1089,9 +1001,7 @@ class CommitApiTest(HfApiCommonTest):
 
         assert operations[0]._is_committed
         assert operations[0]._is_uploaded  # LFS file
-        self.assertEqual(
-            operations[0].path_or_fileobj, b"content"
-        )  # not removed by default
+        self.assertEqual(operations[0].path_or_fileobj, b"content")  # not removed by default
         assert operations[1]._is_committed
         self.assertEqual(operations[1].path_or_fileobj, b"content")
 
@@ -1136,12 +1046,8 @@ class CommitApiTest(HfApiCommonTest):
                             path_in_repo="README.md",
                             path_or_fileobj=INVALID_MODELCARD.encode(),
                         ),
-                        CommitOperationAdd(
-                            path_in_repo="file.txt", path_or_fileobj=b"content"
-                        ),
-                        CommitOperationAdd(
-                            path_in_repo="lfs.bin", path_or_fileobj=b"content"
-                        ),
+                        CommitOperationAdd(path_in_repo="file.txt", path_or_fileobj=b"content"),
+                        CommitOperationAdd(path_in_repo="lfs.bin", path_or_fileobj=b"content"),
                     ],
                     commit_message="Test commit",
                 )
@@ -1152,21 +1058,13 @@ class CommitApiTest(HfApiCommonTest):
     @use_tmp_repo()
     def test_commit_modelcard_empty_metadata(self, repo_url: RepoUrl) -> None:
         modelcard = "This is a modelcard without metadata"
-        with self.assertWarnsRegex(
-            UserWarning, "Warnings while validating metadata in README.md"
-        ):
+        with self.assertWarnsRegex(UserWarning, "Warnings while validating metadata in README.md"):
             commit = self._api.create_commit(
                 repo_id=repo_url.repo_id,
                 operations=[
-                    CommitOperationAdd(
-                        path_in_repo="README.md", path_or_fileobj=modelcard.encode()
-                    ),
-                    CommitOperationAdd(
-                        path_in_repo="file.txt", path_or_fileobj=b"content"
-                    ),
-                    CommitOperationAdd(
-                        path_in_repo="lfs.bin", path_or_fileobj=b"content"
-                    ),
+                    CommitOperationAdd(path_in_repo="README.md", path_or_fileobj=modelcard.encode()),
+                    CommitOperationAdd(path_in_repo="file.txt", path_or_fileobj=b"content"),
+                    CommitOperationAdd(path_in_repo="lfs.bin", path_or_fileobj=b"content"),
                 ],
                 commit_message="Test commit",
             )
@@ -1184,17 +1082,13 @@ class CommitApiTest(HfApiCommonTest):
         """
         repo_id = self._api.create_repo(repo_id=repo_name()).repo_id
         with self.assertRaises(HfHubHTTPError) as cm:
-            self._api.upload_file(
-                path_or_fileobj=b"content", path_in_repo="..\\ddd", repo_id=repo_id
-            )
+            self._api.upload_file(path_or_fileobj=b"content", path_in_repo="..\\ddd", repo_id=repo_id)
         assert cm.exception.response.status_code == 422
 
     @use_tmp_repo()
     def test_prevent_empty_commit_if_no_op(self, repo_url: RepoUrl) -> None:
         with self.assertLogs("huggingface_hub", level="INFO") as logs:
-            self._api.create_commit(
-                repo_id=repo_url.repo_id, commit_message="Empty commit", operations=[]
-            )
+            self._api.create_commit(repo_id=repo_url.repo_id, commit_message="Empty commit", operations=[])
         assert (
             logs.records[0].message
             == "No files have been modified since last commit. Skipping to prevent empty commit."
@@ -1207,12 +1101,8 @@ class CommitApiTest(HfApiCommonTest):
             repo_id=repo_url.repo_id,
             commit_message="initial commit",
             operations=[
-                CommitOperationAdd(
-                    path_or_fileobj=b"Regular file content", path_in_repo="file.txt"
-                ),
-                CommitOperationAdd(
-                    path_or_fileobj=b"LFS content", path_in_repo="lfs.bin"
-                ),
+                CommitOperationAdd(path_or_fileobj=b"Regular file content", path_in_repo="file.txt"),
+                CommitOperationAdd(path_or_fileobj=b"LFS content", path_in_repo="lfs.bin"),
             ],
         )
         with self.assertLogs("huggingface_hub", level="INFO") as logs:
@@ -1220,18 +1110,11 @@ class CommitApiTest(HfApiCommonTest):
                 repo_id=repo_url.repo_id,
                 commit_message="Empty commit",
                 operations=[
-                    CommitOperationAdd(
-                        path_or_fileobj=b"Regular file content", path_in_repo="file.txt"
-                    ),
-                    CommitOperationAdd(
-                        path_or_fileobj=b"LFS content", path_in_repo="lfs.bin"
-                    ),
+                    CommitOperationAdd(path_or_fileobj=b"Regular file content", path_in_repo="file.txt"),
+                    CommitOperationAdd(path_or_fileobj=b"LFS content", path_in_repo="lfs.bin"),
                 ],
             )
-        assert (
-            logs.records[0].message
-            == "Removing 2 file(s) from commit that have not changed."
-        )
+        assert logs.records[0].message == "Removing 2 file(s) from commit that have not changed."
         assert logs.records[0].levelname == "INFO"
 
         assert (
@@ -1247,19 +1130,13 @@ class CommitApiTest(HfApiCommonTest):
             repo_id=repo_url.repo_id,
             commit_message="initial commit",
             operations=[
-                CommitOperationAdd(
-                    path_or_fileobj=b"Regular file content", path_in_repo="file.txt"
-                ),
+                CommitOperationAdd(path_or_fileobj=b"Regular file content", path_in_repo="file.txt"),
                 CommitOperationAdd(
                     path_or_fileobj=b"Regular file content",
                     path_in_repo="file_copy.txt",
                 ),
-                CommitOperationAdd(
-                    path_or_fileobj=b"LFS content", path_in_repo="lfs.bin"
-                ),
-                CommitOperationAdd(
-                    path_or_fileobj=b"LFS content", path_in_repo="lfs_copy.bin"
-                ),
+                CommitOperationAdd(path_or_fileobj=b"LFS content", path_in_repo="lfs.bin"),
+                CommitOperationAdd(path_or_fileobj=b"LFS content", path_in_repo="lfs_copy.bin"),
             ],
         )
         with self.assertLogs("huggingface_hub", level="INFO") as logs:
@@ -1267,18 +1144,11 @@ class CommitApiTest(HfApiCommonTest):
                 repo_id=repo_url.repo_id,
                 commit_message="Empty commit",
                 operations=[
-                    CommitOperationCopy(
-                        src_path_in_repo="file.txt", path_in_repo="file_copy.txt"
-                    ),
-                    CommitOperationCopy(
-                        src_path_in_repo="lfs.bin", path_in_repo="lfs_copy.bin"
-                    ),
+                    CommitOperationCopy(src_path_in_repo="file.txt", path_in_repo="file_copy.txt"),
+                    CommitOperationCopy(src_path_in_repo="lfs.bin", path_in_repo="lfs_copy.bin"),
                 ],
             )
-        assert (
-            logs.records[0].message
-            == "Removing 2 file(s) from commit that have not changed."
-        )
+        assert logs.records[0].message == "Removing 2 file(s) from commit that have not changed."
         assert logs.records[0].levelname == "INFO"
 
         assert (
@@ -1304,9 +1174,7 @@ class CommitApiTest(HfApiCommonTest):
                 revision=pr.git_reference,
             )
 
-        commits = self._api.list_repo_commits(
-            repo_id=repo_url.repo_id, revision=pr.git_reference
-        )
+        commits = self._api.list_repo_commits(repo_id=repo_url.repo_id, revision=pr.git_reference)
         assert len(commits) == 1  # no 2nd commit
         assert url.oid == commits[0].commit_id
 
@@ -1316,18 +1184,10 @@ class CommitApiTest(HfApiCommonTest):
             repo_id=repo_url.repo_id,
             commit_message="initial commit",
             operations=[
-                CommitOperationAdd(
-                    path_or_fileobj=b"content 1.0", path_in_repo="file.txt"
-                ),
-                CommitOperationAdd(
-                    path_or_fileobj=b"content 2.0", path_in_repo="file2.txt"
-                ),
-                CommitOperationAdd(
-                    path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs.bin"
-                ),
-                CommitOperationAdd(
-                    path_or_fileobj=b"LFS content 2.0", path_in_repo="lfs2.bin"
-                ),
+                CommitOperationAdd(path_or_fileobj=b"content 1.0", path_in_repo="file.txt"),
+                CommitOperationAdd(path_or_fileobj=b"content 2.0", path_in_repo="file2.txt"),
+                CommitOperationAdd(path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs.bin"),
+                CommitOperationAdd(path_or_fileobj=b"LFS content 2.0", path_in_repo="lfs2.bin"),
             ],
         )
         with self.assertLogs("huggingface_hub", level="DEBUG") as logs:
@@ -1336,43 +1196,25 @@ class CommitApiTest(HfApiCommonTest):
                 commit_message="second commit",
                 operations=[
                     # Did not change => will be removed from commit
-                    CommitOperationAdd(
-                        path_or_fileobj=b"content 1.0", path_in_repo="file.txt"
-                    ),
+                    CommitOperationAdd(path_or_fileobj=b"content 1.0", path_in_repo="file.txt"),
                     # Change => will be kept
-                    CommitOperationAdd(
-                        path_or_fileobj=b"content 2.1", path_in_repo="file2.txt"
-                    ),
+                    CommitOperationAdd(path_or_fileobj=b"content 2.1", path_in_repo="file2.txt"),
                     # New file => will be kept
-                    CommitOperationAdd(
-                        path_or_fileobj=b"content 3.0", path_in_repo="file3.txt"
-                    ),
+                    CommitOperationAdd(path_or_fileobj=b"content 3.0", path_in_repo="file3.txt"),
                     # Did not change => will be removed from commit
-                    CommitOperationAdd(
-                        path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs.bin"
-                    ),
+                    CommitOperationAdd(path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs.bin"),
                     # Change => will be kept
-                    CommitOperationAdd(
-                        path_or_fileobj=b"LFS content 2.1", path_in_repo="lfs2.bin"
-                    ),
+                    CommitOperationAdd(path_or_fileobj=b"LFS content 2.1", path_in_repo="lfs2.bin"),
                     # New file => will be kept
-                    CommitOperationAdd(
-                        path_or_fileobj=b"LFS content 3.0", path_in_repo="lfs3.bin"
-                    ),
+                    CommitOperationAdd(path_or_fileobj=b"LFS content 3.0", path_in_repo="lfs3.bin"),
                 ],
             )
         debug_logs = [log.message for log in logs.records if log.levelname == "DEBUG"]
         info_logs = [log.message for log in logs.records if log.levelname == "INFO"]
-        warning_logs = [
-            log.message for log in logs.records if log.levelname == "WARNING"
-        ]
+        warning_logs = [log.message for log in logs.records if log.levelname == "WARNING"]
 
-        assert (
-            "Skipping upload for 'file.txt' as the file has not changed." in debug_logs
-        )
-        assert (
-            "Skipping upload for 'lfs.bin' as the file has not changed." in debug_logs
-        )
+        assert "Skipping upload for 'file.txt' as the file has not changed." in debug_logs
+        assert "Skipping upload for 'lfs.bin' as the file has not changed." in debug_logs
         assert "Removing 2 file(s) from commit that have not changed." in info_logs
         assert len(warning_logs) == 0  # no warnings since the commit is not empty
 
@@ -1406,24 +1248,12 @@ class CommitApiTest(HfApiCommonTest):
             repo_id=repo_url.repo_id,
             commit_message="initial commit",
             operations=[
-                CommitOperationAdd(
-                    path_or_fileobj=b"content 1.0", path_in_repo="file.txt"
-                ),
-                CommitOperationAdd(
-                    path_or_fileobj=b"content 1.0", path_in_repo="file_copy.txt"
-                ),
-                CommitOperationAdd(
-                    path_or_fileobj=b"content 2.0", path_in_repo="file2.txt"
-                ),
-                CommitOperationAdd(
-                    path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs.bin"
-                ),
-                CommitOperationAdd(
-                    path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs_copy.bin"
-                ),
-                CommitOperationAdd(
-                    path_or_fileobj=b"LFS content 2.0", path_in_repo="lfs2.bin"
-                ),
+                CommitOperationAdd(path_or_fileobj=b"content 1.0", path_in_repo="file.txt"),
+                CommitOperationAdd(path_or_fileobj=b"content 1.0", path_in_repo="file_copy.txt"),
+                CommitOperationAdd(path_or_fileobj=b"content 2.0", path_in_repo="file2.txt"),
+                CommitOperationAdd(path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs.bin"),
+                CommitOperationAdd(path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs_copy.bin"),
+                CommitOperationAdd(path_or_fileobj=b"LFS content 2.0", path_in_repo="lfs2.bin"),
             ],
         )
         with self.assertLogs("huggingface_hub", level="DEBUG") as logs:
@@ -1432,36 +1262,22 @@ class CommitApiTest(HfApiCommonTest):
                 commit_message="second commit",
                 operations=[
                     # Did not change => will be removed from commit
-                    CommitOperationCopy(
-                        src_path_in_repo="file.txt", path_in_repo="file_copy.txt"
-                    ),
+                    CommitOperationCopy(src_path_in_repo="file.txt", path_in_repo="file_copy.txt"),
                     # Change => will be kept
-                    CommitOperationCopy(
-                        src_path_in_repo="file2.txt", path_in_repo="file.txt"
-                    ),
+                    CommitOperationCopy(src_path_in_repo="file2.txt", path_in_repo="file.txt"),
                     # New file => will be kept
-                    CommitOperationCopy(
-                        src_path_in_repo="file2.txt", path_in_repo="file3.txt"
-                    ),
+                    CommitOperationCopy(src_path_in_repo="file2.txt", path_in_repo="file3.txt"),
                     # Did not change => will be removed from commit
-                    CommitOperationCopy(
-                        src_path_in_repo="lfs.bin", path_in_repo="lfs_copy.bin"
-                    ),
+                    CommitOperationCopy(src_path_in_repo="lfs.bin", path_in_repo="lfs_copy.bin"),
                     # Change => will be kept
-                    CommitOperationCopy(
-                        src_path_in_repo="lfs2.bin", path_in_repo="lfs.bin"
-                    ),
+                    CommitOperationCopy(src_path_in_repo="lfs2.bin", path_in_repo="lfs.bin"),
                     # New file => will be kept
-                    CommitOperationCopy(
-                        src_path_in_repo="lfs2.bin", path_in_repo="lfs3.bin"
-                    ),
+                    CommitOperationCopy(src_path_in_repo="lfs2.bin", path_in_repo="lfs3.bin"),
                 ],
             )
         debug_logs = [log.message for log in logs.records if log.levelname == "DEBUG"]
         info_logs = [log.message for log in logs.records if log.levelname == "INFO"]
-        warning_logs = [
-            log.message for log in logs.records if log.levelname == "WARNING"
-        ]
+        warning_logs = [log.message for log in logs.records if log.levelname == "WARNING"]
 
         assert (
             "Skipping copy for 'file.txt' -> 'file_copy.txt' as the content of the source file is the same as the destination file."
@@ -1504,15 +1320,9 @@ class CommitApiTest(HfApiCommonTest):
             repo_id=repo_url.repo_id,
             commit_message="initial commit",
             operations=[
-                CommitOperationAdd(
-                    path_or_fileobj=b"content 1.0", path_in_repo="file.txt"
-                ),
-                CommitOperationAdd(
-                    path_or_fileobj=b"content 1.0", path_in_repo="file_copy.txt"
-                ),
-                CommitOperationAdd(
-                    path_or_fileobj=b"content 2.0", path_in_repo="file2.txt"
-                ),
+                CommitOperationAdd(path_or_fileobj=b"content 1.0", path_in_repo="file.txt"),
+                CommitOperationAdd(path_or_fileobj=b"content 1.0", path_in_repo="file_copy.txt"),
+                CommitOperationAdd(path_or_fileobj=b"content 2.0", path_in_repo="file2.txt"),
             ],
         )
         with self.assertLogs("huggingface_hub", level="DEBUG") as logs:
@@ -1521,26 +1331,18 @@ class CommitApiTest(HfApiCommonTest):
                 commit_message="second commit",
                 operations=[
                     # Did not change => will be removed from commit
-                    CommitOperationAdd(
-                        path_or_fileobj=b"content 1.0", path_in_repo="file.txt"
-                    ),
+                    CommitOperationAdd(path_or_fileobj=b"content 1.0", path_in_repo="file.txt"),
                     # identical to file.txt => will be removed from commit
-                    CommitOperationCopy(
-                        src_path_in_repo="file.txt", path_in_repo="file_copy.txt"
-                    ),
+                    CommitOperationCopy(src_path_in_repo="file.txt", path_in_repo="file_copy.txt"),
                     # Delete operation => kept in any case
                     CommitOperationDelete(path_in_repo="file2.txt"),
                 ],
             )
         debug_logs = [log.message for log in logs.records if log.levelname == "DEBUG"]
         info_logs = [log.message for log in logs.records if log.levelname == "INFO"]
-        warning_logs = [
-            log.message for log in logs.records if log.levelname == "WARNING"
-        ]
+        warning_logs = [log.message for log in logs.records if log.levelname == "WARNING"]
 
-        assert (
-            "Skipping upload for 'file.txt' as the file has not changed." in debug_logs
-        )
+        assert "Skipping upload for 'file.txt' as the file has not changed." in debug_logs
         assert (
             "Skipping copy for 'file.txt' -> 'file_copy.txt' as the content of the source file is the same as the destination file."
             in debug_logs
@@ -1568,9 +1370,7 @@ class HfApiUploadEmptyFileTest(HfApiComm
 
     def test_upload_empty_lfs_file(self) -> None:
         # Should have been an LFS file, but uploaded as regular (would fail otherwise)
-        self._api.upload_file(
-            repo_id=self.repo_id, path_in_repo="empty.pkl", path_or_fileobj=b""
-        )
+        self._api.upload_file(repo_id=self.repo_id, path_in_repo="empty.pkl", path_or_fileobj=b"")
         info = self._api.repo_info(repo_id=self.repo_id, files_metadata=True)
 
         repo_file = {file.rfilename: file for file in info.siblings}["empty.pkl"]
@@ -1638,9 +1438,7 @@ class HfApiListFilesInfoTest(HfApiCommon
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="file.md"),
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="lfs.bin"),
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="1/file_1.md"),
-                CommitOperationAdd(
-                    path_or_fileobj=b"data", path_in_repo="1/2/file_1_2.md"
-                ),
+                CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="1/2/file_1_2.md"),
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="2/file_2.md"),
             ],
         )
@@ -1649,9 +1447,7 @@ class HfApiListFilesInfoTest(HfApiCommon
             repo_id=cls.repo_id,
             commit_message="Another commit",
             operations=[
-                CommitOperationAdd(
-                    path_or_fileobj=b"data2", path_in_repo="3/file_3.md"
-                ),
+                CommitOperationAdd(path_or_fileobj=b"data2", path_in_repo="3/file_3.md"),
             ],
         )
 
@@ -1673,9 +1469,7 @@ class HfApiListRepoTreeTest(HfApiCommonT
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="file.md"),
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="lfs.bin"),
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="1/file_1.md"),
-                CommitOperationAdd(
-                    path_or_fileobj=b"data", path_in_repo="1/2/file_1_2.md"
-                ),
+                CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="1/2/file_1_2.md"),
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="2/file_2.md"),
             ],
         )
@@ -1684,9 +1478,7 @@ class HfApiListRepoTreeTest(HfApiCommonT
             repo_id=cls.repo_id,
             commit_message="Another commit",
             operations=[
-                CommitOperationAdd(
-                    path_or_fileobj=b"data2", path_in_repo="3/file_3.md"
-                ),
+                CommitOperationAdd(path_or_fileobj=b"data2", path_in_repo="3/file_3.md"),
             ],
         )
 
@@ -1732,16 +1524,8 @@ class HfApiListRepoTreeTest(HfApiCommonT
 
     def test_list_with_empty_path(self):
         self.assertEqual(
-            set(
-                tree_obj.path
-                for tree_obj in self._api.list_repo_tree(
-                    repo_id=self.repo_id, path_in_repo=""
-                )
-            ),
-            set(
-                tree_obj.path
-                for tree_obj in self._api.list_repo_tree(repo_id=self.repo_id)
-            ),
+            set(tree_obj.path for tree_obj in self._api.list_repo_tree(repo_id=self.repo_id, path_in_repo="")),
+            set(tree_obj.path for tree_obj in self._api.list_repo_tree(repo_id=self.repo_id)),
         )
 
     @with_production_testing
@@ -1756,21 +1540,15 @@ class HfApiListRepoTreeTest(HfApiCommonT
         assert len(tree) == 11
 
         # check last_commit and security are present for a file
-        model_ckpt = next(
-            tree_obj for tree_obj in tree if tree_obj.path == "openjourney-v4.ckpt"
-        )
+        model_ckpt = next(tree_obj for tree_obj in tree if tree_obj.path == "openjourney-v4.ckpt")
         assert model_ckpt.last_commit is not None
-        assert (
-            model_ckpt.last_commit["oid"] == "bda967fdb79a50844e4a02cccae3217a8ecc86cd"
-        )
+        assert model_ckpt.last_commit["oid"] == "bda967fdb79a50844e4a02cccae3217a8ecc86cd"
         assert model_ckpt.security is not None
         assert model_ckpt.security["safe"]
         assert isinstance(model_ckpt.security["av_scan"], dict)  # all details in here
 
         # check last_commit is present for a folder
-        feature_extractor = next(
-            tree_obj for tree_obj in tree if tree_obj.path == "feature_extractor"
-        )
+        feature_extractor = next(tree_obj for tree_obj in tree if tree_obj.path == "feature_extractor")
         self.assertIsNotNone(feature_extractor.last_commit)
         self.assertEqual(
             feature_extractor.last_commit["oid"],
@@ -1788,16 +1566,12 @@ class HfApiListRepoTreeTest(HfApiCommonT
         self.assertEqual(len(tree), 11)
 
         # check last_commit and security are missing for a file
-        model_ckpt = next(
-            tree_obj for tree_obj in tree if tree_obj.path == "openjourney-v4.ckpt"
-        )
+        model_ckpt = next(tree_obj for tree_obj in tree if tree_obj.path == "openjourney-v4.ckpt")
         self.assertIsNone(model_ckpt.last_commit)
         self.assertIsNone(model_ckpt.security)
 
         # check last_commit is missing for a folder
-        feature_extractor = next(
-            tree_obj for tree_obj in tree if tree_obj.path == "feature_extractor"
-        )
+        feature_extractor = next(tree_obj for tree_obj in tree if tree_obj.path == "feature_extractor")
         self.assertIsNone(feature_extractor.last_commit)
 
 
@@ -1805,9 +1579,7 @@ class HfApiTagEndpointTest(HfApiCommonTe
     @use_tmp_repo("model")
     def test_create_tag_on_main(self, repo_url: RepoUrl) -> None:
         """Check `create_tag` on default main branch works."""
-        self._api.create_tag(
-            repo_url.repo_id, tag="v0", tag_message="This is a tag message."
-        )
+        self._api.create_tag(repo_url.repo_id, tag="v0", tag_message="This is a tag message.")
 
         # Check tag  is on `main`
         tag_info = self._api.model_info(repo_url.repo_id, revision="v0")
@@ -1822,23 +1594,15 @@ class HfApiTagEndpointTest(HfApiCommonTe
             repo_id=repo_url.repo_id,
             create_pr=True,
             commit_message="upload readme",
-            operations=[
-                CommitOperationAdd(
-                    path_or_fileobj=b"this is a file content", path_in_repo="readme.md"
-                )
-            ],
+            operations=[CommitOperationAdd(path_or_fileobj=b"this is a file content", path_in_repo="readme.md")],
         )
 
         # Tag the PR
-        self._api.create_tag(
-            repo_url.repo_id, tag="v0", revision=commit_info.pr_revision
-        )
+        self._api.create_tag(repo_url.repo_id, tag="v0", revision=commit_info.pr_revision)
 
         # Check tag  is on `refs/pr/1`
         tag_info = self._api.model_info(repo_url.repo_id, revision="v0")
-        pr_info = self._api.model_info(
-            repo_url.repo_id, revision=commit_info.pr_revision
-        )
+        pr_info = self._api.model_info(repo_url.repo_id, revision=commit_info.pr_revision)
         main_info = self._api.model_info(repo_url.repo_id)
 
         self.assertEqual(tag_info.sha, pr_info.sha)
@@ -1855,21 +1619,13 @@ class HfApiTagEndpointTest(HfApiCommonTe
             repo_id=repo_url.repo_id,
             repo_type="dataset",
             commit_message="upload readme",
-            operations=[
-                CommitOperationAdd(
-                    path_or_fileobj=b"this is a file content", path_in_repo="readme.md"
-                )
-            ],
+            operations=[CommitOperationAdd(path_or_fileobj=b"this is a file content", path_in_repo="readme.md")],
         )
         commit_info_2: CommitInfo = self._api.create_commit(
             repo_id=repo_url.repo_id,
             repo_type="dataset",
             commit_message="upload config",
-            operations=[
-                CommitOperationAdd(
-                    path_or_fileobj=b"{'hello': 'world'}", path_in_repo="config.json"
-                )
-            ],
+            operations=[CommitOperationAdd(path_or_fileobj=b"{'hello': 'world'}", path_in_repo="config.json")],
         )
 
         # Tag commits
@@ -2017,21 +1773,15 @@ class HfApiBranchEndpointTest(HfApiCommo
         initial_commit = self._api.model_info(repo_url.repo_id).sha
         commit = self._api.create_commit(
             repo_url.repo_id,
-            operations=[
-                CommitOperationAdd(path_in_repo="app.py", path_or_fileobj=b"content")
-            ],
+            operations=[CommitOperationAdd(path_in_repo="app.py", path_or_fileobj=b"content")],
             commit_message="test commit",
         )
         latest_commit = commit.oid
 
         # Create branches
         self._api.create_branch(repo_url.repo_id, branch="from-head")
-        self._api.create_branch(
-            repo_url.repo_id, branch="from-initial", revision=initial_commit
-        )
-        self._api.create_branch(
-            repo_url.repo_id, branch="from-branch", revision="from-initial"
-        )
+        self._api.create_branch(repo_url.repo_id, branch="from-initial", revision=initial_commit)
+        self._api.create_branch(repo_url.repo_id, branch="from-branch", revision="from-initial")
         time.sleep(0.2)  # hack: wait for server to update cache?
 
         # Checks branches start from expected commits
@@ -2042,10 +1792,7 @@ class HfApiBranchEndpointTest(HfApiCommo
                 "from-initial": initial_commit,
                 "from-branch": initial_commit,
             },
-            {
-                ref.name: ref.target_commit
-                for ref in self._api.list_repo_refs(repo_id=repo_url.repo_id).branches
-            },
+            {ref.name: ref.target_commit for ref in self._api.list_repo_refs(repo_id=repo_url.repo_id).branches},
         )
 
 
@@ -2059,20 +1806,12 @@ class HfApiDeleteFilesTest(HfApiCommonTe
             operations=[
                 # Regular files
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="file.txt"),
-                CommitOperationAdd(
-                    path_or_fileobj=b"data", path_in_repo="nested/file.txt"
-                ),
-                CommitOperationAdd(
-                    path_or_fileobj=b"data", path_in_repo="nested/sub/file.txt"
-                ),
+                CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="nested/file.txt"),
+                CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="nested/sub/file.txt"),
                 # LFS files
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="lfs.bin"),
-                CommitOperationAdd(
-                    path_or_fileobj=b"data", path_in_repo="nested/lfs.bin"
-                ),
-                CommitOperationAdd(
-                    path_or_fileobj=b"data", path_in_repo="nested/sub/lfs.bin"
-                ),
+                CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="nested/lfs.bin"),
+                CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="nested/sub/lfs.bin"),
             ],
             commit_message="Init repo structure",
         )
@@ -2089,9 +1828,7 @@ class HfApiDeleteFilesTest(HfApiCommonTe
         assert "file.txt" not in self.remote_files()
 
     def test_delete_multiple_files(self):
-        self._api.delete_files(
-            repo_id=self.repo_id, delete_patterns=["file.txt", "lfs.bin"]
-        )
+        self._api.delete_files(repo_id=self.repo_id, delete_patterns=["file.txt", "lfs.bin"])
         files = self.remote_files()
         assert "file.txt" not in files
         assert "lfs.bin" not in files
@@ -2105,9 +1842,7 @@ class HfApiDeleteFilesTest(HfApiCommonTe
         assert self.remote_files() == {".gitattributes", "file.txt", "lfs.bin"}
 
     def test_unknown_path_do_not_raise(self):
-        self._api.delete_files(
-            repo_id=self.repo_id, delete_patterns=["not_existing", "nested/*"]
-        )
+        self._api.delete_files(repo_id=self.repo_id, delete_patterns=["not_existing", "nested/*"])
         assert self.remote_files() == {".gitattributes", "file.txt", "lfs.bin"}
 
     def test_delete_bin_files_with_patterns(self):
@@ -2171,11 +1906,7 @@ class HfApiPublicProductionTest(unittest
         # Let's list the 10 most recent models
         # with tags "bert" and "jax",
         # ordered by last modified date.
-        models = list(
-            self._api.list_models(
-                filter=("bert", "jax"), sort="last_modified", direction=-1, limit=10
-            )
-        )
+        models = list(self._api.list_models(filter=("bert", "jax"), sort="last_modified", direction=-1, limit=10))
         # we have at least 1 models
         assert len(models) > 1
         assert len(models) <= 10
@@ -2208,15 +1939,11 @@ class HfApiPublicProductionTest(unittest
         assert all(model.likes is not None for model in models)
 
     def test_list_models_with_config(self):
-        for model in self._api.list_models(
-            filter=("adapter-transformers", "bert"), fetch_config=True, limit=20
-        ):
+        for model in self._api.list_models(filter=("adapter-transformers", "bert"), fetch_config=True, limit=20):
             self.assertIsNotNone(model.config)
 
     def test_list_models_without_config(self):
-        for model in self._api.list_models(
-            filter=("adapter-transformers", "bert"), fetch_config=False, limit=20
-        ):
+        for model in self._api.list_models(filter=("adapter-transformers", "bert"), fetch_config=False, limit=20):
             self.assertIsNone(model.config)
 
     def test_list_models_expand_author(self):
@@ -2231,9 +1958,7 @@ class HfApiPublicProductionTest(unittest
 
     def test_list_models_expand_multiple(self):
         # Only the selected fields are returned
-        models = list(
-            self._api.list_models(expand=["author", "downloadsAllTime"], limit=5)
-        )
+        models = list(self._api.list_models(expand=["author", "downloadsAllTime"], limit=5))
         for model in models:
             assert model.author is not None
             assert model.downloads_all_time is not None
@@ -2264,16 +1989,12 @@ class HfApiPublicProductionTest(unittest
 
     @pytest.mark.skip("Inference parameter is being revamped")
     def test_list_models_inference_warm(self):
-        for model in self._api.list_models(
-            inference=["warm"], expand="inference", limit=5
-        ):
+        for model in self._api.list_models(inference=["warm"], expand="inference", limit=5):
             assert model.inference == "warm"
 
     @pytest.mark.skip("Inference parameter is being revamped")
     def test_list_models_inference_cold(self):
-        for model in self._api.list_models(
-            inference=["cold"], expand="inference", limit=5
-        ):
+        for model in self._api.list_models(inference=["cold"], expand="inference", limit=5):
             assert model.inference == "cold"
 
     def test_model_info(self):
@@ -2286,9 +2007,7 @@ class HfApiPublicProductionTest(unittest
         )
 
         # One particular commit (not the top of `main`)
-        model = self._api.model_info(
-            repo_id=DUMMY_MODEL_ID, revision=DUMMY_MODEL_ID_REVISION_ONE_SPECIFIC_COMMIT
-        )
+        model = self._api.model_info(repo_id=DUMMY_MODEL_ID, revision=DUMMY_MODEL_ID_REVISION_ONE_SPECIFIC_COMMIT)
         self.assertIsInstance(model, ModelInfo)
         self.assertEqual(model.sha, DUMMY_MODEL_ID_REVISION_ONE_SPECIFIC_COMMIT)
 
@@ -2301,9 +2020,7 @@ class HfApiPublicProductionTest(unittest
             securityStatus=True,
         )
         self.assertIsNotNone(model.security_repo_status)
-        self.assertEqual(
-            model.security_repo_status, {"scansDone": True, "filesWithIssues": []}
-        )
+        self.assertEqual(model.security_repo_status, {"scansDone": True, "filesWithIssues": []})
 
     def test_model_info_with_file_metadata(self):
         model = self._api.model_info(
@@ -2366,9 +2083,7 @@ class HfApiPublicProductionTest(unittest
 
     def test_model_info_expand_author(self):
         # Only the selected field is returned
-        model = self._api.model_info(
-            repo_id="HuggingFaceH4/zephyr-7b-beta", expand=["author"]
-        )
+        model = self._api.model_info(repo_id="HuggingFaceH4/zephyr-7b-beta", expand=["author"])
         assert model.author == "HuggingFaceH4"
         assert model.downloads is None
         assert model.created_at is None
@@ -2395,13 +2110,9 @@ class HfApiPublicProductionTest(unittest
     def test_model_info_expand_cannot_be_used_with_other_params(self):
         # `expand` cannot be used with other params
         with self.assertRaises(ValueError):
-            self._api.model_info(
-                "HuggingFaceH4/zephyr-7b-beta", expand=["author"], securityStatus=True
-            )
+            self._api.model_info("HuggingFaceH4/zephyr-7b-beta", expand=["author"], securityStatus=True)
         with self.assertRaises(ValueError):
-            self._api.model_info(
-                "HuggingFaceH4/zephyr-7b-beta", expand=["author"], files_metadata=True
-            )
+            self._api.model_info("HuggingFaceH4/zephyr-7b-beta", expand=["author"], files_metadata=True)
 
     def test_list_repo_files(self):
         files = self._api.list_repo_files(repo_id=DUMMY_MODEL_ID)
@@ -2423,11 +2134,7 @@ class HfApiPublicProductionTest(unittest
         self.assertIsInstance(datasets[0], DatasetInfo)
 
     def test_filter_datasets_by_author_and_name(self):
-        datasets = list(
-            self._api.list_datasets(
-                author="huggingface", dataset_name="DataMeasurementsFiles"
-            )
-        )
+        datasets = list(self._api.list_datasets(author="huggingface", dataset_name="DataMeasurementsFiles"))
         assert len(datasets) > 0
         assert "huggingface" in datasets[0].author
         assert "DataMeasurementsFiles" in datasets[0].id
@@ -2453,9 +2160,7 @@ class HfApiPublicProductionTest(unittest
         assert "language:fr" in datasets[0].tags
 
     def test_filter_datasets_by_multilinguality(self):
-        datasets = list(
-            self._api.list_datasets(multilinguality="multilingual", limit=10)
-        )
+        datasets = list(self._api.list_datasets(multilinguality="multilingual", limit=10))
         assert len(datasets) > 0
         assert "multilinguality:multilingual" in datasets[0].tags
 
@@ -2465,16 +2170,12 @@ class HfApiPublicProductionTest(unittest
         assert "size_categories:100K<n<1M" in datasets[0].tags
 
     def test_filter_datasets_by_task_categories(self):
-        datasets = list(
-            self._api.list_datasets(task_categories="audio-classification", limit=10)
-        )
+        datasets = list(self._api.list_datasets(task_categories="audio-classification", limit=10))
         assert len(datasets) > 0
         assert "task_categories:audio-classification" in datasets[0].tags
 
     def test_filter_datasets_by_task_ids(self):
-        datasets = list(
-            self._api.list_datasets(task_ids="natural-language-inference", limit=10)
-        )
+        datasets = list(self._api.list_datasets(task_ids="natural-language-inference", limit=10))
         assert len(datasets) > 0
         assert "task_ids:natural-language-inference" in datasets[0].tags
 
@@ -2507,9 +2208,7 @@ class HfApiPublicProductionTest(unittest
 
     def test_list_datasets_expand_multiple(self):
         # Only the selected fields are returned
-        datasets = list(
-            self._api.list_datasets(expand=["author", "downloadsAllTime"], limit=5)
-        )
+        datasets = list(self._api.list_datasets(expand=["author", "downloadsAllTime"], limit=5))
         for dataset in datasets:
             assert dataset.author is not None
             assert dataset.downloads_all_time is not None
@@ -2535,14 +2234,8 @@ class HfApiPublicProductionTest(unittest
             assert dataset.gated is False
 
     def test_filter_datasets_with_card_data(self):
-        assert any(
-            dataset.card_data is not None
-            for dataset in self._api.list_datasets(full=True, limit=50)
-        )
-        assert all(
-            dataset.card_data is None
-            for dataset in self._api.list_datasets(full=False, limit=50)
-        )
+        assert any(dataset.card_data is not None for dataset in self._api.list_datasets(full=True, limit=50))
+        assert all(dataset.card_data is None for dataset in self._api.list_datasets(full=False, limit=50))
 
     def test_filter_datasets_by_tag(self):
         for dataset in self._api.list_datasets(tags="fiftyone", limit=5):
@@ -2550,10 +2243,7 @@ class HfApiPublicProductionTest(unittest
 
     def test_dataset_info(self):
         dataset = self._api.dataset_info(repo_id=DUMMY_DATASET_ID)
-        assert (
-            isinstance(dataset.card_data, DatasetCardData)
-            and len(dataset.card_data) > 0
-        )
+        assert isinstance(dataset.card_data, DatasetCardData) and len(dataset.card_data) > 0
         assert isinstance(dataset.siblings, list) and len(dataset.siblings) > 0
         assert isinstance(dataset, DatasetInfo)
         assert dataset.sha != DUMMY_DATASET_ID_REVISION_ONE_SPECIFIC_COMMIT
@@ -2565,9 +2255,7 @@ class HfApiPublicProductionTest(unittest
         assert dataset.sha == DUMMY_DATASET_ID_REVISION_ONE_SPECIFIC_COMMIT
 
     def test_dataset_info_with_file_metadata(self):
-        dataset = self._api.dataset_info(
-            repo_id=SAMPLE_DATASET_IDENTIFIER, files_metadata=True
-        )
+        dataset = self._api.dataset_info(repo_id=SAMPLE_DATASET_IDENTIFIER, files_metadata=True)
         files = dataset.siblings
         assert files is not None
         self._check_siblings_metadata(files)
@@ -2586,9 +2274,7 @@ class HfApiPublicProductionTest(unittest
 
     def test_dataset_info_expand_author(self):
         # Only the selected field is returned
-        dataset = self._api.dataset_info(
-            repo_id="HuggingFaceH4/no_robots", expand=["author"]
-        )
+        dataset = self._api.dataset_info(repo_id="HuggingFaceH4/no_robots", expand=["author"])
         assert dataset.author == "HuggingFaceH4"
         assert dataset.downloads is None
         assert dataset.created_at is None
@@ -2596,9 +2282,7 @@ class HfApiPublicProductionTest(unittest
 
     def test_dataset_info_expand_multiple(self):
         # Only the selected fields are returned
-        dataset = self._api.dataset_info(
-            repo_id="HuggingFaceH4/no_robots", expand=["author", "downloadsAllTime"]
-        )
+        dataset = self._api.dataset_info(repo_id="HuggingFaceH4/no_robots", expand=["author", "downloadsAllTime"])
         assert dataset.author == "HuggingFaceH4"
         assert dataset.downloads is None
         assert dataset.downloads_all_time is not None
@@ -2614,9 +2298,7 @@ class HfApiPublicProductionTest(unittest
     def test_dataset_info_expand_cannot_be_used_with_files_metadata(self):
         # `expand` cannot be used with other `files_metadata`
         with self.assertRaises(ValueError):
-            self._api.dataset_info(
-                "HuggingFaceH4/no_robots", expand=["author"], files_metadata=True
-            )
+            self._api.dataset_info("HuggingFaceH4/no_robots", expand=["author"], files_metadata=True)
 
     def test_space_info(self) -> None:
         space = self._api.space_info(repo_id="HuggingFaceH4/zephyr-chat")
@@ -2626,18 +2308,14 @@ class HfApiPublicProductionTest(unittest
 
     def test_space_info_expand_author(self):
         # Only the selected field is returned
-        space = self._api.space_info(
-            repo_id="HuggingFaceH4/zephyr-chat", expand=["author"]
-        )
+        space = self._api.space_info(repo_id="HuggingFaceH4/zephyr-chat", expand=["author"])
         assert space.author == "HuggingFaceH4"
         assert space.created_at is None
         assert space.last_modified is None
 
     def test_space_info_expand_multiple(self):
         # Only the selected fields are returned
-        space = self._api.space_info(
-            repo_id="HuggingFaceH4/zephyr-chat", expand=["author", "likes"]
-        )
+        space = self._api.space_info(repo_id="HuggingFaceH4/zephyr-chat", expand=["author", "likes"])
         assert space.author == "HuggingFaceH4"
         assert space.created_at is None
         assert space.last_modified is None
@@ -2652,9 +2330,7 @@ class HfApiPublicProductionTest(unittest
     def test_space_info_expand_cannot_be_used_with_files_metadata(self):
         # `expand` cannot be used with other files_metadata
         with self.assertRaises(ValueError):
-            self._api.space_info(
-                "HuggingFaceH4/zephyr-chat", expand=["author"], files_metadata=True
-            )
+            self._api.space_info("HuggingFaceH4/zephyr-chat", expand=["author"], files_metadata=True)
 
     def test_filter_models_by_author(self):
         models = list(self._api.list_models(author="muellerzr"))
@@ -2672,24 +2348,14 @@ class HfApiPublicProductionTest(unittest
         assert len(models) == 0
 
     def test_filter_models_with_library(self):
-        models = list(
-            self._api.list_models(
-                author="microsoft", model_name="wavlm-base-sd", library="tensorflow"
-            )
-        )
+        models = list(self._api.list_models(author="microsoft", model_name="wavlm-base-sd", library="tensorflow"))
         assert len(models) == 0
 
-        models = list(
-            self._api.list_models(
-                author="microsoft", model_name="wavlm-base-sd", library="pytorch"
-            )
-        )
+        models = list(self._api.list_models(author="microsoft", model_name="wavlm-base-sd", library="pytorch"))
         assert len(models) > 0
 
     def test_filter_models_with_task(self):
-        models = list(
-            self._api.list_models(task="fill-mask", model_name="albert-base-v2")
-        )
+        models = list(self._api.list_models(task="fill-mask", model_name="albert-base-v2"))
         assert models[0].pipeline_tag == "fill-mask"
         assert "albert" in models[0].id
         assert "base" in models[0].id
@@ -2704,9 +2370,7 @@ class HfApiPublicProductionTest(unittest
                 assert language in model.tags
 
     def test_filter_models_with_tag(self):
-        models = list(
-            self._api.list_models(author="HuggingFaceBR4", tags=["tensorboard"])
-        )
+        models = list(self._api.list_models(author="HuggingFaceBR4", tags=["tensorboard"]))
         assert models[0].id.startswith("HuggingFaceBR4/")
         assert "tensorboard" in models[0].tags
 
@@ -2735,9 +2399,7 @@ class HfApiPublicProductionTest(unittest
     def test_filter_emissions_with_max(self):
         assert all(
             model.card_data["co2_eq_emissions"] <= 100
-            for model in self._api.list_models(
-                emissions_thresholds=(None, 100), cardData=True, limit=1000
-            )
+            for model in self._api.list_models(emissions_thresholds=(None, 100), cardData=True, limit=1000)
             if isinstance(model.card_data["co2_eq_emissions"], (float, int))
         )
 
@@ -2745,19 +2407,13 @@ class HfApiPublicProductionTest(unittest
         assert all(
             [
                 model.card_data["co2_eq_emissions"] >= 5
-                for model in self._api.list_models(
-                    emissions_thresholds=(5, None), cardData=True, limit=1000
-                )
+                for model in self._api.list_models(emissions_thresholds=(5, None), cardData=True, limit=1000)
                 if isinstance(model.card_data["co2_eq_emissions"], (float, int))
             ]
         )
 
     def test_filter_emissions_with_min_and_max(self):
-        models = list(
-            self._api.list_models(
-                emissions_thresholds=(5, 100), cardData=True, limit=1000
-            )
-        )
+        models = list(self._api.list_models(emissions_thresholds=(5, 100), cardData=True, limit=1000))
         assert all(
             [
                 model.card_data["co2_eq_emissions"] >= 5
@@ -2793,9 +2449,7 @@ class HfApiPublicProductionTest(unittest
 
     def test_list_spaces_sort_and_direction(self):
         # Descending order => first item has more likes than second
-        spaces_descending_likes = list(
-            self._api.list_spaces(sort="likes", direction=-1, limit=100)
-        )
+        spaces_descending_likes = list(self._api.list_spaces(sort="likes", direction=-1, limit=100))
         assert spaces_descending_likes[0].likes > spaces_descending_likes[1].likes
 
     def test_list_spaces_limit(self):
@@ -2813,19 +2467,11 @@ class HfApiPublicProductionTest(unittest
     def test_list_spaces_linked(self):
         space_id = "stabilityai/stable-diffusion"
 
-        spaces = [
-            space
-            for space in self._api.list_spaces(search=space_id)
-            if space.id == space_id
-        ]
+        spaces = [space for space in self._api.list_spaces(search=space_id) if space.id == space_id]
         assert spaces[0].models is None
         assert spaces[0].datasets is None
 
-        spaces = [
-            space
-            for space in self._api.list_spaces(search=space_id, linked=True)
-            if space.id == space_id
-        ]
+        spaces = [space for space in self._api.list_spaces(search=space_id, linked=True) if space.id == space_id]
         assert spaces[0].models is not None
         assert spaces[0].datasets is not None
 
@@ -2911,14 +2557,10 @@ class HfApiPublicProductionTest(unittest
 
     def test_not_a_safetensors_repo(self) -> None:
         with self.assertRaises(NotASafetensorsRepoError):
-            self._api.get_safetensors_metadata(
-                "huggingface-hub-ci/test_safetensors_metadata"
-            )
+            self._api.get_safetensors_metadata("huggingface-hub-ci/test_safetensors_metadata")
 
     def test_get_safetensors_metadata_from_revision(self) -> None:
-        info = self._api.get_safetensors_metadata(
-            "huggingface-hub-ci/test_safetensors_metadata", revision="refs/pr/1"
-        )
+        info = self._api.get_safetensors_metadata("huggingface-hub-ci/test_safetensors_metadata", revision="refs/pr/1")
         assert isinstance(info, SafetensorsRepoMetadata)
 
     def test_parse_safetensors_metadata(self) -> None:
@@ -2967,9 +2609,7 @@ class HfApiPrivateTest(HfApiCommonTest):
             ):
                 _ = self._api.model_info(repo_id=f"{USER}/{self.REPO_NAME}")
 
-            model_info = self._api.model_info(
-                repo_id=f"{USER}/{self.REPO_NAME}", use_auth_token=self._token
-            )
+            model_info = self._api.model_info(repo_id=f"{USER}/{self.REPO_NAME}", use_auth_token=self._token)
             self.assertIsInstance(model_info, ModelInfo)
 
     @patch("huggingface_hub.utils._headers.get_token", return_value=None)
@@ -2985,9 +2625,7 @@ class HfApiPrivateTest(HfApiCommonTest):
             ):
                 _ = self._api.dataset_info(repo_id=f"{USER}/{self.REPO_NAME}")
 
-            dataset_info = self._api.dataset_info(
-                repo_id=f"{USER}/{self.REPO_NAME}", use_auth_token=self._token
-            )
+            dataset_info = self._api.dataset_info(repo_id=f"{USER}/{self.REPO_NAME}", use_auth_token=self._token)
             self.assertIsInstance(dataset_info, DatasetInfo)
 
     def test_list_private_datasets(self):
@@ -3046,15 +2684,11 @@ class UploadFolderMockedTest(unittest.Te
         self.api.list_repo_files = self.repo_files_mock
 
         self.create_commit_mock = Mock()
-        self.create_commit_mock.return_value.commit_url = (
-            f"{ENDPOINT_STAGING}/username/repo_id/commit/dummy_sha"
-        )
+        self.create_commit_mock.return_value.commit_url = f"{ENDPOINT_STAGING}/username/repo_id/commit/dummy_sha"
         self.create_commit_mock.return_value.pr_url = None
         self.api.create_commit = self.create_commit_mock
 
-    def _upload_folder_alias(
-        self, **kwargs
-    ) -> List[Union[CommitOperationAdd, CommitOperationDelete]]:
+    def _upload_folder_alias(self, **kwargs) -> List[Union[CommitOperationAdd, CommitOperationDelete]]:
         """Alias to call `upload_folder` + retrieve the CommitOperation list passed to `create_commit`."""
         if "folder_path" not in kwargs:
             kwargs["folder_path"] = self.cache_dir
@@ -3067,9 +2701,7 @@ class UploadFolderMockedTest(unittest.Te
         assert {op.path_in_repo for op in operations} == self.all_local_files
 
     def test_allow_everything_in_subdir_no_trailing_slash(self):
-        operations = self._upload_folder_alias(
-            folder_path=self.cache_dir / "subdir", path_in_repo="subdir"
-        )
+        operations = self._upload_folder_alias(folder_path=self.cache_dir / "subdir", path_in_repo="subdir")
         assert all(isinstance(op, CommitOperationAdd) for op in operations)
         assert {op.path_in_repo for op in operations} == {
             # correct `path_in_repo`
@@ -3078,9 +2710,7 @@ class UploadFolderMockedTest(unittest.Te
         }
 
     def test_allow_everything_in_subdir_with_trailing_slash(self):
-        operations = self._upload_folder_alias(
-            folder_path=self.cache_dir / "subdir", path_in_repo="subdir/"
-        )
+        operations = self._upload_folder_alias(folder_path=self.cache_dir / "subdir", path_in_repo="subdir/")
         assert all(isinstance(op, CommitOperationAdd) for op in operations)
         self.assertEqual(
             {op.path_in_repo for op in operations},
@@ -3088,9 +2718,7 @@ class UploadFolderMockedTest(unittest.Te
         )
 
     def test_allow_txt_ignore_subdir(self):
-        operations = self._upload_folder_alias(
-            allow_patterns="*.txt", ignore_patterns="subdir/*"
-        )
+        operations = self._upload_folder_alias(allow_patterns="*.txt", ignore_patterns="subdir/*")
         assert all(isinstance(op, CommitOperationAdd) for op in operations)
         assert {op.path_in_repo for op in operations} == {
             "sub/file.txt",
@@ -3098,9 +2726,7 @@ class UploadFolderMockedTest(unittest.Te
         }  # only .txt files, not in subdir
 
     def test_allow_txt_not_root_ignore_subdir(self):
-        operations = self._upload_folder_alias(
-            allow_patterns="**/*.txt", ignore_patterns="subdir/*"
-        )
+        operations = self._upload_folder_alias(allow_patterns="**/*.txt", ignore_patterns="subdir/*")
         assert all(isinstance(op, CommitOperationAdd) for op in operations)
         assert {op.path_in_repo for op in operations} == {
             # only .txt files, not in subdir, not at root
@@ -3113,25 +2739,15 @@ class UploadFolderMockedTest(unittest.Te
         Using `path_in_repo="."` or `path_in_repo=None` should be equivalent.
         See https://github.com/huggingface/huggingface_hub/pull/1382.
         """
-        operation_with_dot = self._upload_folder_alias(
-            path_in_repo=".", allow_patterns=["file.txt"]
-        )[0]
-        operation_with_none = self._upload_folder_alias(
-            path_in_repo=None, allow_patterns=["file.txt"]
-        )[0]
+        operation_with_dot = self._upload_folder_alias(path_in_repo=".", allow_patterns=["file.txt"])[0]
+        operation_with_none = self._upload_folder_alias(path_in_repo=None, allow_patterns=["file.txt"])[0]
         assert operation_with_dot.path_in_repo == "file.txt"
         assert operation_with_none.path_in_repo == "file.txt"
 
     def test_delete_txt(self):
         operations = self._upload_folder_alias(delete_patterns="*.txt")
-        added_files = {
-            op.path_in_repo for op in operations if isinstance(op, CommitOperationAdd)
-        }
-        deleted_files = {
-            op.path_in_repo
-            for op in operations
-            if isinstance(op, CommitOperationDelete)
-        }
+        added_files = {op.path_in_repo for op in operations if isinstance(op, CommitOperationAdd)}
+        deleted_files = {op.path_in_repo for op in operations if isinstance(op, CommitOperationDelete)}
 
         assert added_files == self.all_local_files
         assert deleted_files == {"file1.txt", "sub/file1.txt"}
@@ -3146,14 +2762,8 @@ class UploadFolderMockedTest(unittest.Te
             folder_path=self.cache_dir / "sub",
             delete_patterns="*.txt",
         )
-        added_files = {
-            op.path_in_repo for op in operations if isinstance(op, CommitOperationAdd)
-        }
-        deleted_files = {
-            op.path_in_repo
-            for op in operations
-            if isinstance(op, CommitOperationDelete)
-        }
+        added_files = {op.path_in_repo for op in operations if isinstance(op, CommitOperationAdd)}
+        deleted_files = {op.path_in_repo for op in operations if isinstance(op, CommitOperationDelete)}
 
         assert added_files == {
             "sub/file.txt",
@@ -3168,14 +2778,8 @@ class UploadFolderMockedTest(unittest.Te
             ignore_patterns="file.txt",
             delete_patterns="*.txt",
         )
-        added_files = {
-            op.path_in_repo for op in operations if isinstance(op, CommitOperationAdd)
-        }
-        deleted_files = {
-            op.path_in_repo
-            for op in operations
-            if isinstance(op, CommitOperationDelete)
-        }
+        added_files = {op.path_in_repo for op in operations if isinstance(op, CommitOperationAdd)}
+        deleted_files = {op.path_in_repo for op in operations if isinstance(op, CommitOperationDelete)}
 
         # since "sub/file.txt" should be deleted and is not overwritten (ignore_patterns), we delete it explicitly
         assert added_files == {"sub/lfs_in_sub.bin"}  # no "sub/file.txt"
@@ -3183,14 +2787,8 @@ class UploadFolderMockedTest(unittest.Te
 
     def test_delete_if_path_in_repo(self):
         # Regression test for https://github.com/huggingface/huggingface_hub/pull/2129
-        operations = self._upload_folder_alias(
-            path_in_repo=".", folder_path=self.cache_dir, delete_patterns="*"
-        )
-        deleted_files = {
-            op.path_in_repo
-            for op in operations
-            if isinstance(op, CommitOperationDelete)
-        }
+        operations = self._upload_folder_alias(path_in_repo=".", folder_path=self.cache_dir, delete_patterns="*")
+        deleted_files = {op.path_in_repo for op in operations if isinstance(op, CommitOperationDelete)}
         assert deleted_files == {"file1.txt", "sub/file1.txt"}  # all the 'old' files
 
 
@@ -3203,9 +2801,7 @@ class HfLargefilesTest(HfApiCommonTest):
 
     def setup_local_clone(self) -> None:
         scheme = urlparse(self.repo_url).scheme
-        repo_url_auth = self.repo_url.replace(
-            f"{scheme}://", f"{scheme}://user:{TOKEN}@"
-        )
+        repo_url_auth = self.repo_url.replace(f"{scheme}://", f"{scheme}://user:{TOKEN}@")
 
         subprocess.run(
             ["git", "clone", repo_url_auth, str(self.cache_dir)],
@@ -3214,9 +2810,7 @@ class HfLargefilesTest(HfApiCommonTest):
             stderr=subprocess.PIPE,
         )
         subprocess.run(["git", "lfs", "track", "*.pdf"], check=True, cwd=self.cache_dir)
-        subprocess.run(
-            ["git", "lfs", "track", "*.epub"], check=True, cwd=self.cache_dir
-        )
+        subprocess.run(["git", "lfs", "track", "*.epub"], check=True, cwd=self.cache_dir)
 
     @require_git_lfs
     def test_end_to_end_thresh_6M(self):
@@ -3235,9 +2829,7 @@ class HfLargefilesTest(HfApiCommonTest):
             cwd=self.cache_dir,
         )
         subprocess.run(["git", "add", "*"], check=True, cwd=self.cache_dir)
-        subprocess.run(
-            ["git", "commit", "-m", "commit message"], check=True, cwd=self.cache_dir
-        )
+        subprocess.run(["git", "commit", "-m", "commit message"], check=True, cwd=self.cache_dir)
 
         # This will fail as we haven't set up our custom transfer agent yet.
         failed_process = subprocess.run(
@@ -3249,9 +2841,7 @@ class HfLargefilesTest(HfApiCommonTest):
         self.assertEqual(failed_process.returncode, 1)
         self.assertIn("cli lfs-enable-largefiles", failed_process.stderr.decode())
         # ^ Instructions on how to fix this are included in the error message.
-        subprocess.run(
-            ["huggingface-cli", "lfs-enable-largefiles", self.cache_dir], check=True
-        )
+        subprocess.run(["huggingface-cli", "lfs-enable-largefiles", self.cache_dir], check=True)
 
         start_time = time.time()
         subprocess.run(["git", "push"], check=True, cwd=self.cache_dir)
@@ -3300,9 +2890,7 @@ class HfLargefilesTest(HfApiCommonTest):
             check=True,
             cwd=self.cache_dir,
         )
-        subprocess.run(
-            ["huggingface-cli", "lfs-enable-largefiles", self.cache_dir], check=True
-        )
+        subprocess.run(["huggingface-cli", "lfs-enable-largefiles", self.cache_dir], check=True)
 
         start_time = time.time()
         subprocess.run(["git", "push"], check=True, cwd=self.cache_dir)
@@ -3376,9 +2964,7 @@ class HfApiDiscussionsTest(HfApiCommonTe
         self._api.delete_repo(repo_id=self.repo_id)
 
     def test_create_discussion(self):
-        discussion = self._api.create_discussion(
-            repo_id=self.repo_id, title=" Test discussion !  "
-        )
+        discussion = self._api.create_discussion(repo_id=self.repo_id, title=" Test discussion !  ")
         self.assertEqual(discussion.num, 3)
         self.assertEqual(discussion.author, USER)
         self.assertEqual(discussion.is_pull_request, False)
@@ -3391,15 +2977,11 @@ class HfApiDiscussionsTest(HfApiCommonTe
         Computed URL was malformed with `dataset` and `space` repo_types.
         See https://github.com/huggingface/huggingface_hub/issues/1463.
         """
-        discussion = self._api.create_discussion(
-            repo_id=repo_url.repo_id, repo_type="dataset", title="title"
-        )
+        discussion = self._api.create_discussion(repo_id=repo_url.repo_id, repo_type="dataset", title="title")
         self.assertEqual(discussion.url, f"{repo_url}/discussions/1")
 
     def test_create_pull_request(self):
-        discussion = self._api.create_discussion(
-            repo_id=self.repo_id, title=" Test PR !  ", pull_request=True
-        )
+        discussion = self._api.create_discussion(repo_id=self.repo_id, title=" Test PR !  ", pull_request=True)
         self.assertEqual(discussion.num, 3)
         self.assertEqual(discussion.author, USER)
         self.assertEqual(discussion.is_pull_request, True)
@@ -3417,25 +2999,15 @@ class HfApiDiscussionsTest(HfApiCommonTe
         )
 
     def test_get_repo_discussion_by_type(self):
-        discussions_generator = self._api.get_repo_discussions(
-            repo_id=self.repo_id, discussion_type="pull_request"
-        )
+        discussions_generator = self._api.get_repo_discussions(repo_id=self.repo_id, discussion_type="pull_request")
         self.assertIsInstance(discussions_generator, types.GeneratorType)
-        self.assertListEqual(
-            list([d.num for d in discussions_generator]), [self.pull_request.num]
-        )
+        self.assertListEqual(list([d.num for d in discussions_generator]), [self.pull_request.num])
 
-        discussions_generator = self._api.get_repo_discussions(
-            repo_id=self.repo_id, discussion_type="discussion"
-        )
+        discussions_generator = self._api.get_repo_discussions(repo_id=self.repo_id, discussion_type="discussion")
         self.assertIsInstance(discussions_generator, types.GeneratorType)
-        self.assertListEqual(
-            list([d.num for d in discussions_generator]), [self.discussion.num]
-        )
+        self.assertListEqual(list([d.num for d in discussions_generator]), [self.discussion.num])
 
-        discussions_generator = self._api.get_repo_discussions(
-            repo_id=self.repo_id, discussion_type="all"
-        )
+        discussions_generator = self._api.get_repo_discussions(repo_id=self.repo_id, discussion_type="all")
         self.assertIsInstance(discussions_generator, types.GeneratorType)
         self.assertListEqual(
             list([d.num for d in discussions_generator]),
@@ -3443,34 +3015,22 @@ class HfApiDiscussionsTest(HfApiCommonTe
         )
 
     def test_get_repo_discussion_by_author(self):
-        discussions_generator = self._api.get_repo_discussions(
-            repo_id=self.repo_id, author="unknown"
-        )
+        discussions_generator = self._api.get_repo_discussions(repo_id=self.repo_id, author="unknown")
         self.assertIsInstance(discussions_generator, types.GeneratorType)
         self.assertListEqual(list([d.num for d in discussions_generator]), [])
 
     def test_get_repo_discussion_by_status(self):
         self._api.change_discussion_status(self.repo_id, self.discussion.num, "closed")
 
-        discussions_generator = self._api.get_repo_discussions(
-            repo_id=self.repo_id, discussion_status="open"
-        )
+        discussions_generator = self._api.get_repo_discussions(repo_id=self.repo_id, discussion_status="open")
         self.assertIsInstance(discussions_generator, types.GeneratorType)
-        self.assertListEqual(
-            list([d.num for d in discussions_generator]), [self.pull_request.num]
-        )
+        self.assertListEqual(list([d.num for d in discussions_generator]), [self.pull_request.num])
 
-        discussions_generator = self._api.get_repo_discussions(
-            repo_id=self.repo_id, discussion_status="closed"
-        )
+        discussions_generator = self._api.get_repo_discussions(repo_id=self.repo_id, discussion_status="closed")
         self.assertIsInstance(discussions_generator, types.GeneratorType)
-        self.assertListEqual(
-            list([d.num for d in discussions_generator]), [self.discussion.num]
-        )
+        self.assertListEqual(list([d.num for d in discussions_generator]), [self.discussion.num])
 
-        discussions_generator = self._api.get_repo_discussions(
-            repo_id=self.repo_id, discussion_status="all"
-        )
+        discussions_generator = self._api.get_repo_discussions(repo_id=self.repo_id, discussion_status="all")
         self.assertIsInstance(discussions_generator, types.GeneratorType)
         self.assertListEqual(
             list([d.num for d in discussions_generator]),
@@ -3480,16 +3040,12 @@ class HfApiDiscussionsTest(HfApiCommonTe
     @with_production_testing
     def test_get_repo_discussion_pagination(self):
         discussions = list(
-            HfApi().get_repo_discussions(
-                repo_id="open-llm-leaderboard/open_llm_leaderboard", repo_type="space"
-            )
+            HfApi().get_repo_discussions(repo_id="open-llm-leaderboard/open_llm_leaderboard", repo_type="space")
         )
         assert len(discussions) > 50
 
     def test_get_discussion_details(self):
-        retrieved = self._api.get_discussion_details(
-            repo_id=self.repo_id, discussion_num=2
-        )
+        retrieved = self._api.get_discussion_details(repo_id=self.repo_id, discussion_num=2)
         self.assertEqual(retrieved, self.discussion)
 
     def test_edit_discussion_comment(self):
@@ -3502,13 +3058,9 @@ class HfApiDiscussionsTest(HfApiCommonTe
             comment_id=get_first_comment(self.pull_request).id,
             new_content="**Edited** comment 🤗",
         )
-        retrieved = self._api.get_discussion_details(
-            repo_id=self.repo_id, discussion_num=self.pull_request.num
-        )
+        retrieved = self._api.get_discussion_details(repo_id=self.repo_id, discussion_num=self.pull_request.num)
         self.assertEqual(get_first_comment(retrieved).edited, True)
-        self.assertEqual(
-            get_first_comment(retrieved).id, get_first_comment(self.pull_request).id
-        )
+        self.assertEqual(get_first_comment(retrieved).id, get_first_comment(self.pull_request).id)
         self.assertEqual(get_first_comment(retrieved).content, "**Edited** comment 🤗")
 
         self.assertEqual(get_first_comment(retrieved), edited_comment)
@@ -3524,9 +3076,7 @@ class HfApiDiscussionsTest(HfApiCommonTe
                 And even [links](http://hf.co)! 💥🤯
             """,
         )
-        retrieved = self._api.get_discussion_details(
-            repo_id=self.repo_id, discussion_num=self.discussion.num
-        )
+        retrieved = self._api.get_discussion_details(repo_id=self.repo_id, discussion_num=self.discussion.num)
         self.assertEqual(len(retrieved.events), 2)
         self.assertIn(new_comment.id, {event.id for event in retrieved.events})
 
@@ -3536,9 +3086,7 @@ class HfApiDiscussionsTest(HfApiCommonTe
             discussion_num=self.discussion.num,
             new_title="New title2",
         )
-        retrieved = self._api.get_discussion_details(
-            repo_id=self.repo_id, discussion_num=self.discussion.num
-        )
+        retrieved = self._api.get_discussion_details(repo_id=self.repo_id, discussion_num=self.discussion.num)
         self.assertIn(rename_event.id, (event.id for event in retrieved.events))
         self.assertEqual(rename_event.old_title, self.discussion.title)
         self.assertEqual(rename_event.new_title, "New title2")
@@ -3549,9 +3097,7 @@ class HfApiDiscussionsTest(HfApiCommonTe
             discussion_num=self.discussion.num,
             new_status="closed",
         )
-        retrieved = self._api.get_discussion_details(
-            repo_id=self.repo_id, discussion_num=self.discussion.num
-        )
+        retrieved = self._api.get_discussion_details(repo_id=self.repo_id, discussion_num=self.discussion.num)
         self.assertIn(status_change_event.id, (event.id for event in retrieved.events))
         self.assertEqual(status_change_event.new_status, "closed")
 
@@ -3566,9 +3112,7 @@ class HfApiDiscussionsTest(HfApiCommonTe
         self._api.create_commit(
             repo_id=self.repo_id,
             commit_message="Commit some file",
-            operations=[
-                CommitOperationAdd(path_in_repo="file.test", path_or_fileobj=b"Content")
-            ],
+            operations=[CommitOperationAdd(path_in_repo="file.test", path_or_fileobj=b"Content")],
             revision=self.pull_request.git_reference,
         )
         self._api.change_discussion_status(
@@ -3578,9 +3122,7 @@ class HfApiDiscussionsTest(HfApiCommonTe
         )
         self._api.merge_pull_request(self.repo_id, self.pull_request.num)
 
-        retrieved = self._api.get_discussion_details(
-            repo_id=self.repo_id, discussion_num=self.pull_request.num
-        )
+        retrieved = self._api.get_discussion_details(repo_id=self.repo_id, discussion_num=self.pull_request.num)
         self.assertEqual(retrieved.status, "merged")
         self.assertIsNotNone(retrieved.merge_commit_oid)
 
@@ -3608,9 +3150,7 @@ class ActivityApiTest(unittest.TestCase)
     def test_list_repo_likers(self) -> None:
         # a repo with > 5000 likes
         all_likers = list(
-            HfApi().list_repo_likers(
-                repo_id="open-llm-leaderboard/open_llm_leaderboard", repo_type="space"
-            )
+            HfApi().list_repo_likers(repo_id="open-llm-leaderboard/open_llm_leaderboard", repo_type="space")
         )
         self.assertIsInstance(all_likers[0], User)
         self.assertGreater(len(all_likers), 5000)
@@ -3619,9 +3159,7 @@ class ActivityApiTest(unittest.TestCase)
     def test_list_likes_on_production(self) -> None:
         # Test julien-c likes a lot of repos !
         likes = HfApi().list_liked_repos("julien-c")
-        self.assertEqual(
-            len(likes.models) + len(likes.datasets) + len(likes.spaces), likes.total
-        )
+        self.assertEqual(len(likes.models) + len(likes.datasets) + len(likes.spaces), likes.total)
         self.assertGreater(len(likes.models), 0)
         self.assertGreater(len(likes.datasets), 0)
         self.assertGreater(len(likes.spaces), 0)
@@ -3632,15 +3170,9 @@ class TestSquashHistory(HfApiCommonTest)
     def test_super_squash_history_on_branch(self, repo_url: RepoUrl) -> None:
         # Upload + update file on main
         repo_id = repo_url.repo_id
-        self._api.upload_file(
-            repo_id=repo_id, path_in_repo="file.txt", path_or_fileobj=b"content"
-        )
-        self._api.upload_file(
-            repo_id=repo_id, path_in_repo="lfs.bin", path_or_fileobj=b"content"
-        )
-        self._api.upload_file(
-            repo_id=repo_id, path_in_repo="file.txt", path_or_fileobj=b"another_content"
-        )
+        self._api.upload_file(repo_id=repo_id, path_in_repo="file.txt", path_or_fileobj=b"content")
+        self._api.upload_file(repo_id=repo_id, path_in_repo="lfs.bin", path_or_fileobj=b"content")
+        self._api.upload_file(repo_id=repo_id, path_in_repo="file.txt", path_or_fileobj=b"another_content")
 
         # Upload file on a new branch
         self._api.create_branch(repo_id=repo_id, branch="v0.1", exist_ok=True)
@@ -3655,30 +3187,20 @@ class TestSquashHistory(HfApiCommonTest)
         self._api.super_squash_history(repo_id=repo_id)
 
         # List history
-        squashed_main_commits = self._api.list_repo_commits(
-            repo_id=repo_id, revision="main"
-        )
+        squashed_main_commits = self._api.list_repo_commits(repo_id=repo_id, revision="main")
         branch_commits = self._api.list_repo_commits(repo_id=repo_id, revision="v0.1")
 
         # Main branch has been squashed but initial commits still exists on other branch
         assert len(squashed_main_commits) == 1
-        assert (
-            squashed_main_commits[0].title
-            == "Super-squash branch 'main' using huggingface_hub"
-        )
+        assert squashed_main_commits[0].title == "Super-squash branch 'main' using huggingface_hub"
         assert len(branch_commits) == 5
         assert branch_commits[-1].title == "initial commit"
 
         # Squash history on branch
         self._api.super_squash_history(repo_id=repo_id, branch="v0.1")
-        squashed_branch_commits = self._api.list_repo_commits(
-            repo_id=repo_id, revision="v0.1"
-        )
+        squashed_branch_commits = self._api.list_repo_commits(repo_id=repo_id, revision="v0.1")
         assert len(squashed_branch_commits) == 1
-        assert (
-            squashed_branch_commits[0].title
-            == "Super-squash branch 'v0.1' using huggingface_hub"
-        )
+        assert squashed_branch_commits[0].title == "Super-squash branch 'v0.1' using huggingface_hub"
 
     @use_tmp_repo()
     def test_super_squash_history_on_special_ref(self, repo_url: RepoUrl) -> None:
@@ -3688,9 +3210,7 @@ class TestSquashHistory(HfApiCommonTest)
         The only case where it's useful is for the dataset-viewer on refs/convert/parquet.
         """
         repo_id = repo_url.repo_id
-        pr = self._api.create_pull_request(
-            repo_id=repo_id, title="Test super squash on PR"
-        )
+        pr = self._api.create_pull_request(repo_id=repo_id, title="Test super squash on PR")
 
         # Upload + update file on PR
         self._api.upload_file(
@@ -3715,9 +3235,7 @@ class TestSquashHistory(HfApiCommonTest)
         # Squash history PR
         self._api.super_squash_history(repo_id=repo_id, branch=pr.git_reference)
 
-        squashed_branch_commits = self._api.list_repo_commits(
-            repo_id=repo_id, revision=pr.git_reference
-        )
+        squashed_branch_commits = self._api.list_repo_commits(repo_id=repo_id, revision=pr.git_reference)
         assert len(squashed_branch_commits) == 1
 
 
@@ -3752,9 +3270,7 @@ iface.launch()
         self.api = HfApi(token="hf_fake_token", endpoint=ENDPOINT_PRODUCTION)
 
         # Create a Space
-        self.api.create_repo(
-            repo_id=self.repo_id, repo_type="space", space_sdk="gradio", private=True
-        )
+        self.api.create_repo(repo_id=self.repo_id, repo_type="space", space_sdk="gradio", private=True)
         self.api.upload_file(
             path_or_fileobj=self._BASIC_APP_PY_TEMPLATE,
             repo_id=self.repo_id,
@@ -3773,20 +3289,14 @@ iface.launch()
         self.api.add_space_secret(self.repo_id, "gh_api_key", "******")
 
         # Add secret with optional description
-        self.api.add_space_secret(
-            self.repo_id, "bar", "123", description="This is a secret"
-        )
+        self.api.add_space_secret(self.repo_id, "bar", "123", description="This is a secret")
 
         # Update secret
         self.api.add_space_secret(self.repo_id, "foo", "456")
 
         # Update secret with optional description
-        self.api.add_space_secret(
-            self.repo_id, "foo", "789", description="This is a secret"
-        )
-        self.api.add_space_secret(
-            self.repo_id, "bar", "456", description="This is another secret"
-        )
+        self.api.add_space_secret(self.repo_id, "foo", "789", description="This is a secret")
+        self.api.add_space_secret(self.repo_id, "bar", "456", description="This is another secret")
 
         # Delete secret
         self.api.delete_space_secret(self.repo_id, "gh_api_key")
@@ -3803,17 +3313,13 @@ iface.launch()
         self.api.add_space_variable(self.repo_id, "MODEL_REPO_ID", "user/repo")
 
         # Add 1 variable with optional description
-        self.api.add_space_variable(
-            self.repo_id, "MODEL_PAPER", "arXiv", description="found it there"
-        )
+        self.api.add_space_variable(self.repo_id, "MODEL_PAPER", "arXiv", description="found it there")
 
         # Update variable
         self.api.add_space_variable(self.repo_id, "foo", "456")
 
         # Update variable with optional description
-        self.api.add_space_variable(
-            self.repo_id, "foo", "456", description="updated description"
-        )
+        self.api.add_space_variable(self.repo_id, "foo", "456", description="updated description")
 
         # Delete variable
         self.api.delete_space_variable(self.repo_id, "gh_api_key")
@@ -3934,15 +3440,11 @@ class TestCommitInBackground(HfApiCommon
     def test_run_as_future(self, repo_url: RepoUrl) -> None:
         repo_id = repo_url.repo_id
         # update repo visibility to private
-        self._api.run_as_future(
-            self._api.update_repo_settings, repo_id=repo_id, private=True
-        )
+        self._api.run_as_future(self._api.update_repo_settings, repo_id=repo_id, private=True)
         future_1 = self._api.run_as_future(self._api.model_info, repo_id=repo_id)
 
         # update repo visibility to public
-        self._api.run_as_future(
-            self._api.update_repo_settings, repo_id=repo_id, private=False
-        )
+        self._api.run_as_future(self._api.update_repo_settings, repo_id=repo_id, private=False)
         future_2 = self._api.run_as_future(self._api.model_info, repo_id=repo_id)
 
         self.assertIsInstance(future_1, Future)
@@ -4244,9 +3746,7 @@ class TestSpaceAPIMocked(unittest.TestCa
         )
 
     def test_request_space_hardware_with_sleep_time(self) -> None:
-        self.api.request_space_hardware(
-            self.repo_id, SpaceHardware.T4_MEDIUM, sleep_time=123
-        )
+        self.api.request_space_hardware(self.repo_id, SpaceHardware.T4_MEDIUM, sleep_time=123)
         self.post_mock.assert_called_once_with(
             f"{self.api.endpoint}/api/spaces/{self.repo_id}/hardware",
             headers=self.api._build_hf_headers(),
@@ -4262,9 +3762,7 @@ class TestSpaceAPIMocked(unittest.TestCa
         )
 
     def test_set_space_sleep_time_cpu_basic(self) -> None:
-        self.post_mock.return_value.json.return_value["hardware"][
-            "requested"
-        ] = "cpu-basic"
+        self.post_mock.return_value.json.return_value["hardware"]["requested"] = "cpu-basic"
         with self.assertWarns(UserWarning):
             self.api.set_space_sleep_time(self.repo_id, sleep_time=123)
 
@@ -4318,9 +3816,7 @@ class ListGitRefsTest(unittest.TestCase)
         main_branch = [branch for branch in refs.branches if branch.name == "main"][0]
         self.assertEqual(main_branch.ref, "refs/heads/main")
 
-        convert_branch = [
-            branch for branch in refs.converts if branch.name == "parquet"
-        ][0]
+        convert_branch = [branch for branch in refs.converts if branch.name == "parquet"][0]
         self.assertEqual(convert_branch.ref, "refs/convert/parquet")
 
         # Can get info by convert revision
@@ -4331,9 +3827,7 @@ class ListGitRefsTest(unittest.TestCase)
         )
 
     def test_list_refs_with_prs(self) -> None:
-        refs = self.api.list_repo_refs(
-            "openchat/openchat_3.5", include_pull_requests=True
-        )
+        refs = self.api.list_repo_refs("openchat/openchat_3.5", include_pull_requests=True)
         self.assertGreater(len(refs.pull_requests), 1)
         assert refs.pull_requests[0].ref.startswith("refs/pr/")
 
@@ -4346,9 +3840,7 @@ class ListGitCommitsTest(unittest.TestCa
         cls.repo_id = cls.api.create_repo(repo_name()).repo_id
 
         # Create a commit on `main` branch
-        cls.api.upload_file(
-            repo_id=cls.repo_id, path_or_fileobj=b"content", path_in_repo="content.txt"
-        )
+        cls.api.upload_file(repo_id=cls.repo_id, path_or_fileobj=b"content", path_in_repo="content.txt")
 
         # Create a commit in a PR
         cls.api.upload_file(
@@ -4359,9 +3851,7 @@ class ListGitCommitsTest(unittest.TestCa
         )
 
         # Create another commit on `main` branch
-        cls.api.upload_file(
-            repo_id=cls.repo_id, path_or_fileobj=b"on_main", path_in_repo="on_main.txt"
-        )
+        cls.api.upload_file(repo_id=cls.repo_id, path_or_fileobj=b"on_main", path_in_repo="on_main.txt")
         return super().setUpClass()
 
     @classmethod
@@ -4431,21 +3921,15 @@ class HfApiTokenAttributeTest(unittest.T
         HfApi()._build_hf_headers(token=None)
         self._assert_token_is(mock_build_hf_headers, None)
 
-    def _assert_token_is(
-        self, mock_build_hf_headers: Mock, expected_value: str
-    ) -> None:
+    def _assert_token_is(self, mock_build_hf_headers: Mock, expected_value: str) -> None:
         self.assertEqual(mock_build_hf_headers.call_args[1]["token"], expected_value)
 
-    def test_library_name_and_version_are_set(
-        self, mock_build_hf_headers: Mock
-    ) -> None:
+    def test_library_name_and_version_are_set(self, mock_build_hf_headers: Mock) -> None:
         HfApi(library_name="a", library_version="b")._build_hf_headers()
         self.assertEqual(mock_build_hf_headers.call_args[1]["library_name"], "a")
         self.assertEqual(mock_build_hf_headers.call_args[1]["library_version"], "b")
 
-    def test_library_name_and_version_are_overwritten(
-        self, mock_build_hf_headers: Mock
-    ) -> None:
+    def test_library_name_and_version_are_overwritten(self, mock_build_hf_headers: Mock) -> None:
         api = HfApi(library_name="a", library_version="b")
         api._build_hf_headers(library_name="A", library_version="B")
         self.assertEqual(mock_build_hf_headers.call_args[1]["library_name"], "A")
@@ -4587,13 +4071,11 @@ class HfApiDuplicateSpaceTest(HfApiCommo
             "style.css",
             "temp/new_file.md",
         ]
-        assert self._api.list_repo_files(
-            repo_id=to_repo_id, repo_type="space"
-        ) == self._api.list_repo_files(repo_id=from_repo_id, repo_type="space")
-
-        self._api.delete_repo(
-            repo_id=from_repo_id, repo_type="space", token=OTHER_TOKEN
+        assert self._api.list_repo_files(repo_id=to_repo_id, repo_type="space") == self._api.list_repo_files(
+            repo_id=from_repo_id, repo_type="space"
         )
+
+        self._api.delete_repo(repo_id=from_repo_id, repo_type="space", token=OTHER_TOKEN)
         self._api.delete_repo(repo_id=to_repo_id, repo_type="space")
 
     def test_duplicate_space_from_missing_repo(self) -> None:
@@ -4623,9 +4105,7 @@ class CollectionAPITest(HfApiCommonTest)
         item_id = "teknium/OpenHermes-2.5-Mistral-7B"
         item_type = "model"
         limit = 3
-        collections = HfApi().list_collections(
-            item=f"{item_type}s/{item_id}", limit=limit
-        )
+        collections = HfApi().list_collections(item=f"{item_type}s/{item_id}", limit=limit)
 
         # Check return type
         self.assertIsInstance(collections, Iterable)
@@ -4638,15 +4118,10 @@ class CollectionAPITest(HfApiCommonTest)
         for collection in collections:
             # all items are not necessarily returned when listing collections => retrieve complete one
             full_collection = HfApi().get_collection(collection.slug)
-            assert any(
-                item.item_id == item_id and item.item_type == item_type
-                for item in full_collection.items
-            )
+            assert any(item.item_id == item_id and item.item_type == item_type for item in full_collection.items)
 
     def test_create_collection_with_description(self) -> None:
-        collection = self._api.create_collection(
-            self.title, description="Contains a lot of cool stuff"
-        )
+        collection = self._api.create_collection(self.title, description="Contains a lot of cool stuff")
         self.slug = collection.slug
 
         self.assertIsInstance(collection, Collection)
@@ -4654,9 +4129,7 @@ class CollectionAPITest(HfApiCommonTest)
         self.assertEqual(collection.description, "Contains a lot of cool stuff")
         self.assertEqual(collection.items, [])
         assert collection.slug.startswith(self.slug_prefix)
-        self.assertEqual(
-            collection.url, f"{ENDPOINT_STAGING}/collections/{collection.slug}"
-        )
+        self.assertEqual(collection.url, f"{ENDPOINT_STAGING}/collections/{collection.slug}")
 
     @pytest.mark.skip("Creating duplicated collections work on staging")
     def test_create_collection_exists_ok(self) -> None:
@@ -4669,9 +4142,7 @@ class CollectionAPITest(HfApiCommonTest)
             self._api.create_collection(self.title)
 
         # Can ignore error
-        collection_2 = self._api.create_collection(
-            self.title, description="description", exists_ok=True
-        )
+        collection_2 = self._api.create_collection(self.title, description="description", exists_ok=True)
 
         self.assertEqual(collection_1.slug, collection_2.slug)
         self.assertIsNone(collection_1.description)
@@ -4684,9 +4155,7 @@ class CollectionAPITest(HfApiCommonTest)
         # Get private collection
         self._api.get_collection(collection.slug)  # no error
         with self.assertRaises(HTTPError):
-            self._api.get_collection(
-                collection.slug, token=OTHER_TOKEN
-            )  # not authorized
+            self._api.get_collection(collection.slug, token=OTHER_TOKEN)  # not authorized
 
         # Get public collection
         self._api.update_collection_metadata(collection.slug, private=False)
@@ -4715,17 +4184,11 @@ class CollectionAPITest(HfApiCommonTest)
         self.assertNotEqual(collection_1.slug, collection_2.slug)
 
         # Different slug, same id
-        self.assertEqual(
-            collection_1.slug.split("-")[-1], collection_2.slug.split("-")[-1]
-        )
+        self.assertEqual(collection_1.slug.split("-")[-1], collection_2.slug.split("-")[-1])
 
         # Works with both slugs, same collection returned
-        self.assertEqual(
-            self._api.get_collection(collection_1.slug).slug, collection_2.slug
-        )
-        self.assertEqual(
-            self._api.get_collection(collection_2.slug).slug, collection_2.slug
-        )
+        self.assertEqual(self._api.get_collection(collection_1.slug).slug, collection_2.slug)
+        self.assertEqual(self._api.get_collection(collection_2.slug).slug, collection_2.slug)
 
     def test_delete_collection(self) -> None:
         collection = self._api.create_collection(self.title)
@@ -4746,12 +4209,8 @@ class CollectionAPITest(HfApiCommonTest)
 
         # Create collection + add items to it
         collection = self._api.create_collection(self.title)
-        self._api.add_collection_item(
-            collection.slug, model_id, "model", note="This is my model"
-        )
-        self._api.add_collection_item(
-            collection.slug, dataset_id, "dataset"
-        )  # note is optional
+        self._api.add_collection_item(collection.slug, model_id, "model", note="This is my model")
+        self._api.add_collection_item(collection.slug, dataset_id, "dataset")  # note is optional
 
         # Check consistency
         collection = self._api.get_collection(collection.slug)
@@ -4767,9 +4226,7 @@ class CollectionAPITest(HfApiCommonTest)
         # Add existing item fails (except if ignore error)
         with self.assertRaises(HTTPError):
             self._api.add_collection_item(collection.slug, model_id, "model")
-        self._api.add_collection_item(
-            collection.slug, model_id, "model", exists_ok=True
-        )
+        self._api.add_collection_item(collection.slug, model_id, "model", exists_ok=True)
 
         # Add inexistent item fails
         with self.assertRaises(HTTPError):
@@ -4785,26 +4242,18 @@ class CollectionAPITest(HfApiCommonTest)
 
         # Check consistency
         collection = self._api.get_collection(collection.slug)
-        self.assertEqual(
-            collection.items[0].item_id, dataset_id
-        )  # position got updated
+        self.assertEqual(collection.items[0].item_id, dataset_id)  # position got updated
         self.assertEqual(collection.items[1].item_id, model_id)
         self.assertEqual(collection.items[1].note, "New note")  # note got updated
 
         # Delete last item
-        self._api.delete_collection_item(
-            collection.slug, collection.items[1].item_object_id
-        )
-        self._api.delete_collection_item(
-            collection.slug, collection.items[1].item_object_id, missing_ok=True
-        )
+        self._api.delete_collection_item(collection.slug, collection.items[1].item_object_id)
+        self._api.delete_collection_item(collection.slug, collection.items[1].item_object_id, missing_ok=True)
 
         # Check consistency
         collection = self._api.get_collection(collection.slug)
         self.assertEqual(len(collection.items), 1)  # only 1 item remaining
-        self.assertEqual(
-            collection.items[0].item_id, dataset_id
-        )  # position got updated
+        self.assertEqual(collection.items[0].item_id, dataset_id)  # position got updated
 
         # Delete everything
         self._api.delete_repo(model_id)
@@ -4859,9 +4308,7 @@ class AccessRequestAPITest(HfApiCommonTe
         assert requests[0].username == OTHER_USER
 
         # Reject access
-        self._api.reject_access_request(
-            self.repo_id, OTHER_USER, rejection_reason="This is a rejection reason"
-        )
+        self._api.reject_access_request(self.repo_id, OTHER_USER, rejection_reason="This is a rejection reason")
         requests = self._api.list_pending_access_requests(self.repo_id)
         assert len(requests) == 0  # not pending anymore
         requests = self._api.list_rejected_access_requests(self.repo_id)
@@ -4887,13 +4334,9 @@ class AccessRequestAPITest(HfApiCommonTe
             self._api.accept_access_request(self.repo_id, OTHER_USER)
 
         # Cannot reject to already rejected
-        self._api.reject_access_request(
-            self.repo_id, OTHER_USER, rejection_reason="This is a rejection reason"
-        )
+        self._api.reject_access_request(self.repo_id, OTHER_USER, rejection_reason="This is a rejection reason")
         with self.assertRaises(HTTPError):
-            self._api.reject_access_request(
-                self.repo_id, OTHER_USER, rejection_reason="This is a rejection reason"
-            )
+            self._api.reject_access_request(self.repo_id, OTHER_USER, rejection_reason="This is a rejection reason")
 
         # Cannot cancel to already cancelled
         self._api.cancel_access_request(self.repo_id, OTHER_USER)
@@ -4958,9 +4401,7 @@ class WebhookApiTest(HfApiCommonTest):
         super().setUp()
         self.webhook_url = "https://webhook.site/test"
         self.watched_items = [
-            WebhookWatchedItem(
-                type="user", name="julien-c"
-            ),  # can be either a dataclass
+            WebhookWatchedItem(type="user", name="julien-c"),  # can be either a dataclass
             {"type": "org", "name": "HuggingFaceH4"},  # or a simple dictionary
         ]
         self.domains = ["repo", "discussion"]
@@ -5053,26 +4494,16 @@ class TestExpandPropertyType(HfApiCommon
         property_type = (
             ExpandModelProperty_T
             if repo_type == "model"
-            else (
-                ExpandDatasetProperty_T
-                if repo_type == "dataset"
-                else ExpandSpaceProperty_T
-            )
+            else (ExpandDatasetProperty_T if repo_type == "dataset" else ExpandSpaceProperty_T)
         )
         property_type_name = (
             "ExpandModelProperty_T"
             if repo_type == "model"
-            else (
-                "ExpandDatasetProperty_T"
-                if repo_type == "dataset"
-                else "ExpandSpaceProperty_T"
-            )
+            else ("ExpandDatasetProperty_T" if repo_type == "dataset" else "ExpandSpaceProperty_T")
         )
 
         try:
-            self._api.repo_info(
-                repo_id=repo_id, repo_type=repo_type, expand=["does_not_exist"]
-            )
+            self._api.repo_info(repo_id=repo_id, repo_type=repo_type, expand=["does_not_exist"])
             raise Exception("Should have raised an exception")
         except HfHubHTTPError as e:
             assert e.response.status_code == 400
@@ -5080,9 +4511,7 @@ class TestExpandPropertyType(HfApiCommon
 
         assert message.startswith('"expand" must be one of ')
         defined_args = set(get_args(property_type))
-        expected_args = set(
-            message.replace('"expand" must be one of ', "").strip("[]").split(", ")
-        )
+        expected_args = set(message.replace('"expand" must be one of ', "").strip("[]").split(", "))
         expected_args.discard("gitalyUid")  # internal one, do not document
 
         if defined_args != expected_args:
@@ -5111,12 +4540,8 @@ class TestLargeUpload(HfApiCommonTest):
                 subfolder = folder / f"subfolder_{i}"
                 subfolder.mkdir(parents=True, exist_ok=True)
                 for j in range(N_FILES_PER_FOLDER):
-                    (subfolder / f"file_lfs_{i}_{j}.bin").write_bytes(
-                        f"content_lfs_{i}_{j}".encode()
-                    )
-                    (subfolder / f"file_regular_{i}_{j}.txt").write_bytes(
-                        f"content_regular_{i}_{j}".encode()
-                    )
+                    (subfolder / f"file_lfs_{i}_{j}.bin").write_bytes(f"content_lfs_{i}_{j}".encode())
+                    (subfolder / f"file_regular_{i}_{j}.txt").write_bytes(f"content_regular_{i}_{j}".encode())
 
             # Upload the folder
             self._api.upload_large_folder(
@@ -5127,9 +4552,7 @@ class TestLargeUpload(HfApiCommonTest):
             )
 
         # Check all files have been uploaded
-        uploaded_files = self._api.list_repo_files(
-            repo_url.repo_id, repo_type=repo_url.repo_type
-        )
+        uploaded_files = self._api.list_repo_files(repo_url.repo_id, repo_type=repo_url.repo_type)
         for i in range(N_FILES_PER_FOLDER):
             for j in range(N_FILES_PER_FOLDER):
                 assert f"subfolder_{i}/file_lfs_{i}_{j}.bin" in uploaded_files
--- python3-huggingface-hub-0.29.3.orig/tests/test_hf_file_system.py
+++ python3-huggingface-hub-0.29.3/tests/test_hf_file_system.py
@@ -31,9 +31,7 @@ class HfFileSystemTests(unittest.TestCas
             fsspec.register_implementation(HfFileSystem.protocol, HfFileSystem)
 
     def setUp(self):
-        self.hffs = HfFileSystem(
-            endpoint=ENDPOINT_STAGING, token=TOKEN, skip_instance_cache=True
-        )
+        self.hffs = HfFileSystem(endpoint=ENDPOINT_STAGING, token=TOKEN, skip_instance_cache=True)
         self.api = self.hffs._api
 
         # Create dummy repo
@@ -88,9 +86,7 @@ class HfFileSystemTests(unittest.TestCas
         self.assertIsNone(text_data_file["lfs"])
         self.assertIsNotNone(text_data_file["last_commit"])
         self.assertIsNotNone(text_data_file["blob_id"])
-        self.assertIn(
-            "security", text_data_file
-        )  # the staging endpoint does not run security checks
+        self.assertIn("security", text_data_file)  # the staging endpoint does not run security checks
 
         # cached info
         self.assertEqual(self.hffs.info(self.text_file), text_data_file)
@@ -135,15 +131,12 @@ class HfFileSystemTests(unittest.TestCas
             self.hffs.dircache[self.hf_path + "@main"][0]["last_commit"]
         )  # no detail -> no last_commit in cache
 
-        files = self.hffs.glob(
-            self.hf_path + "@main" + "/*", detail=True, expand_info=False
-        )
+        files = self.hffs.glob(self.hf_path + "@main" + "/*", detail=True, expand_info=False)
         self.assertIsInstance(files, dict)
         self.assertEqual(len(files), 2)
         keys = sorted(files)
         self.assertTrue(
-            files[keys[0]]["name"].endswith("/.gitattributes")
-            and files[keys[1]]["name"].endswith("/data")
+            files[keys[0]]["name"].endswith("/.gitattributes") and files[keys[1]]["name"].endswith("/data")
         )
         self.assertIsNone(
             self.hffs.dircache[self.hf_path + "@main"][0]["last_commit"]
@@ -154,8 +147,7 @@ class HfFileSystemTests(unittest.TestCas
         self.assertEqual(len(files), 2)
         keys = sorted(files)
         self.assertTrue(
-            files[keys[0]]["name"].endswith("/.gitattributes")
-            and files[keys[1]]["name"].endswith("/data")
+            files[keys[0]]["name"].endswith("/.gitattributes") and files[keys[1]]["name"].endswith("/data")
         )
         self.assertIsNotNone(files[keys[0]]["last_commit"])
 
@@ -171,13 +163,9 @@ class HfFileSystemTests(unittest.TestCas
 
     def test_file_type(self):
         self.assertTrue(
-            self.hffs.isdir(self.hf_path + "/data")
-            and not self.hffs.isdir(self.hf_path + "/.gitattributes")
-        )
-        self.assertTrue(
-            self.hffs.isfile(self.text_file)
-            and not self.hffs.isfile(self.hf_path + "/data")
+            self.hffs.isdir(self.hf_path + "/data") and not self.hffs.isdir(self.hf_path + "/.gitattributes")
         )
+        self.assertTrue(self.hffs.isfile(self.text_file) and not self.hffs.isfile(self.hf_path + "/data"))
 
     def test_remove_file(self):
         self.hffs.rm_file(self.text_file)
@@ -192,9 +180,7 @@ class HfFileSystemTests(unittest.TestCas
         self.hffs.rm(self.hf_path + "/data", recursive=True)
         self.assertNotIn(self.hf_path + "/data", self.hffs.ls(self.hf_path))
         self.hffs.rm(self.hf_path + "@refs/pr/1" + "/data", recursive=True)
-        self.assertNotIn(
-            self.hf_path + "@refs/pr/1" + "/data", self.hffs.ls(self.hf_path)
-        )
+        self.assertNotIn(self.hf_path + "@refs/pr/1" + "/data", self.hffs.ls(self.hf_path))
 
     def test_read_file(self):
         with self.hffs.open(self.text_file, "r") as f:
@@ -214,14 +200,10 @@ class HfFileSystemTests(unittest.TestCas
             # Simulate that streaming fails mid-way
             f.response.raw.read = None
             self.assertEqual(f.read(6), b"binary")
-            self.assertIsNotNone(
-                f.response.raw.read
-            )  # a new connection has been created
+            self.assertIsNotNone(f.response.raw.read)  # a new connection has been created
 
     def test_read_file_with_revision(self):
-        with self.hffs.open(
-            self.hf_path + "/data/binary_data_for_pr.bin", "rb", revision="refs/pr/1"
-        ) as f:
+        with self.hffs.open(self.hf_path + "/data/binary_data_for_pr.bin", "rb", revision="refs/pr/1") as f:
             self.assertEqual(f.read(), b"dummy binary data on pr")
 
     def test_write_file(self):
@@ -263,28 +245,20 @@ class HfFileSystemTests(unittest.TestCas
         self.hffs.cp_file(self.text_file, self.hf_path + "/data/text_data_copy.txt")
         with self.hffs.open(self.hf_path + "/data/text_data_copy.txt", "r") as f:
             self.assertEqual(f.read(), "dummy text data")
-        self.assertIsNone(
-            self.hffs.info(self.hf_path + "/data/text_data_copy.txt")["lfs"]
-        )
+        self.assertIsNone(self.hffs.info(self.hf_path + "/data/text_data_copy.txt")["lfs"])
         # LFS file
-        self.assertIsNotNone(
-            self.hffs.info(self.hf_path + "/data/binary_data.bin")["lfs"]
-        )
+        self.assertIsNotNone(self.hffs.info(self.hf_path + "/data/binary_data.bin")["lfs"])
         self.hffs.cp_file(
             self.hf_path + "/data/binary_data.bin",
             self.hf_path + "/data/binary_data_copy.bin",
         )
         with self.hffs.open(self.hf_path + "/data/binary_data_copy.bin", "rb") as f:
             self.assertEqual(f.read(), b"dummy binary data")
-        self.assertIsNotNone(
-            self.hffs.info(self.hf_path + "/data/binary_data_copy.bin")["lfs"]
-        )
+        self.assertIsNotNone(self.hffs.info(self.hf_path + "/data/binary_data_copy.bin")["lfs"])
 
     def test_modified_time(self):
         self.assertIsInstance(self.hffs.modified(self.text_file), datetime.datetime)
-        self.assertIsInstance(
-            self.hffs.modified(self.hf_path + "/data"), datetime.datetime
-        )
+        self.assertIsInstance(self.hffs.modified(self.hf_path + "/data"), datetime.datetime)
         # should fail on a non-existing file
         with self.assertRaises(FileNotFoundError):
             self.hffs.modified(self.hf_path + "/data/not_existing_file.txt")
@@ -311,9 +285,7 @@ class HfFileSystemTests(unittest.TestCas
         self.assertEqual(fs.token, TOKEN)
         self.assertEqual(paths, [self.text_file])
 
-        fs, _, paths = fsspec.get_fs_token_paths(
-            f"hf://{self.repo_id}/data/text_data.txt"
-        )
+        fs, _, paths = fsspec.get_fs_token_paths(f"hf://{self.repo_id}/data/text_data.txt")
         self.assertIsInstance(fs, HfFileSystem)
         self.assertEqual(paths, [f"{self.repo_id}/data/text_data.txt"])
 
@@ -332,9 +304,7 @@ class HfFileSystemTests(unittest.TestCas
         self.assertTrue(files[1]["name"].endswith("/.gitattributes"))
         self.assertIsNotNone(files[1]["last_commit"])
         self.assertIsNotNone(files[1]["blob_id"])
-        self.assertIn(
-            "security", files[1]
-        )  # the staging endpoint does not run security checks
+        self.assertIn("security", files[1])  # the staging endpoint does not run security checks
 
     def test_list_data_directory_no_revision(self):
         files = self.hffs.ls(self.hf_path + "/data")
@@ -349,9 +319,7 @@ class HfFileSystemTests(unittest.TestCas
         self.assertIn("pointer_size", files[0]["lfs"])
         self.assertIsNotNone(files[0]["last_commit"])
         self.assertIsNotNone(files[0]["blob_id"])
-        self.assertIn(
-            "security", files[0]
-        )  # the staging endpoint does not run security checks
+        self.assertIn("security", files[0])  # the staging endpoint does not run security checks
 
         self.assertEqual(files[1]["type"], "file")
         self.assertGreater(files[1]["size"], 0)  # not empty
@@ -359,9 +327,7 @@ class HfFileSystemTests(unittest.TestCas
         self.assertIsNone(files[1]["lfs"])
         self.assertIsNotNone(files[1]["last_commit"])
         self.assertIsNotNone(files[1]["blob_id"])
-        self.assertIn(
-            "security", files[1]
-        )  # the staging endpoint does not run security checks
+        self.assertIn("security", files[1])  # the staging endpoint does not run security checks
 
     def test_list_data_file_no_revision(self):
         files = self.hffs.ls(self.text_file)
@@ -373,17 +339,13 @@ class HfFileSystemTests(unittest.TestCas
         self.assertIsNone(files[0]["lfs"])
         self.assertIsNotNone(files[0]["last_commit"])
         self.assertIsNotNone(files[0]["blob_id"])
-        self.assertIn(
-            "security", files[0]
-        )  # the staging endpoint does not run security checks
+        self.assertIn("security", files[0])  # the staging endpoint does not run security checks
 
     def test_list_data_directory_with_revision(self):
         files = self.hffs.ls(self.hf_path + "@refs%2Fpr%2F1" + "/data")
 
         for test_name, files in {
-            "quoted_rev_in_path": self.hffs.ls(
-                self.hf_path + "@refs%2Fpr%2F1" + "/data"
-            ),
+            "quoted_rev_in_path": self.hffs.ls(self.hf_path + "@refs%2Fpr%2F1" + "/data"),
             "rev_in_path": self.hffs.ls(self.hf_path + "@refs/pr/1" + "/data"),
             "rev_as_arg": self.hffs.ls(self.hf_path + "/data", revision="refs/pr/1"),
             "quoted_rev_in_path_and_rev_as_arg": self.hffs.ls(
@@ -393,9 +355,7 @@ class HfFileSystemTests(unittest.TestCas
             with self.subTest(test_name):
                 self.assertEqual(len(files), 1)  # only one file in PR
                 self.assertEqual(files[0]["type"], "file")
-                self.assertTrue(
-                    files[0]["name"].endswith("/data/binary_data_for_pr.bin")
-                )  # PR file
+                self.assertTrue(files[0]["name"].endswith("/data/binary_data_for_pr.bin"))  # PR file
                 if "quoted_rev_in_path" in test_name:
                     self.assertIn("@refs%2Fpr%2F1", files[0]["name"])
                 elif "rev_in_path" in test_name:
@@ -404,27 +364,19 @@ class HfFileSystemTests(unittest.TestCas
     def test_list_root_directory_no_revision_no_detail_then_with_detail(self):
         files = self.hffs.ls(self.hf_path, detail=False)
         self.assertEqual(len(files), 2)
-        self.assertTrue(
-            files[0].endswith("/data") and files[1].endswith("/.gitattributes")
-        )
-        self.assertIsNone(
-            self.hffs.dircache[self.hf_path][0]["last_commit"]
-        )  # no detail -> no last_commit in cache
+        self.assertTrue(files[0].endswith("/data") and files[1].endswith("/.gitattributes"))
+        self.assertIsNone(self.hffs.dircache[self.hf_path][0]["last_commit"])  # no detail -> no last_commit in cache
 
         files = self.hffs.ls(self.hf_path, detail=True)
         self.assertEqual(len(files), 2)
-        self.assertTrue(
-            files[0]["name"].endswith("/data")
-            and files[1]["name"].endswith("/.gitattributes")
-        )
+        self.assertTrue(files[0]["name"].endswith("/data") and files[1]["name"].endswith("/.gitattributes"))
         self.assertIsNotNone(self.hffs.dircache[self.hf_path][0]["last_commit"])
 
     def test_find_root_directory_no_revision(self):
         files = self.hffs.find(self.hf_path, detail=False)
         self.assertEqual(
             files,
-            self.hffs.ls(self.hf_path, detail=False)[1:]
-            + self.hffs.ls(self.hf_path + "/data", detail=False),
+            self.hffs.ls(self.hf_path, detail=False)[1:] + self.hffs.ls(self.hf_path + "/data", detail=False),
         )
 
         files = self.hffs.find(self.hf_path, detail=True)
@@ -658,14 +610,10 @@ def test_resolve_path(
         ),  # not a "special revision" -> encode
     ],
 )
-def test_unresolve_path(
-    path: str, revision: Optional[str], expected_path: str, path_in_repo: str
-) -> None:
+def test_unresolve_path(path: str, revision: Optional[str], expected_path: str, path_in_repo: str) -> None:
     fs = HfFileSystem()
     path = path + "/" + path_in_repo if path_in_repo else path
-    expected_path = (
-        expected_path + "/" + path_in_repo if path_in_repo else expected_path
-    )
+    expected_path = expected_path + "/" + path_in_repo if path_in_repo else expected_path
 
     with mock_repo_info(fs):
         assert fs.resolve_path(path, revision=revision).unresolve() == expected_path
@@ -689,11 +637,7 @@ def mock_repo_info(fs: HfFileSystem):
     def _inner(repo_id: str, *, revision: str, repo_type: str, **kwargs):
         if repo_id not in ["gpt2", "squad", "username/my_dataset", "username/my_model"]:
             raise RepositoryNotFoundError(repo_id)
-        if (
-            revision is not None
-            and revision not in ["main", "dev", "refs"]
-            and not revision.startswith("refs/")
-        ):
+        if revision is not None and revision not in ["main", "dev", "refs"] and not revision.startswith("refs/"):
             raise RevisionNotFoundError(revision)
 
     return patch.object(fs._api, "repo_info", _inner)
@@ -719,9 +663,7 @@ def test_access_repositories_lists(not_s
 def test_exists_after_repo_deletion():
     """Test that exists() correctly reflects repository deletion."""
     # Initialize with staging endpoint and skip cache
-    hffs = HfFileSystem(
-        endpoint=ENDPOINT_STAGING, token=TOKEN, skip_instance_cache=True
-    )
+    hffs = HfFileSystem(endpoint=ENDPOINT_STAGING, token=TOKEN, skip_instance_cache=True)
     api = hffs._api
 
     # Create a new repo
--- python3-huggingface-hub-0.29.3.orig/tests/test_hub_mixin.py
+++ python3-huggingface-hub-0.29.3/tests/test_hub_mixin.py
@@ -91,9 +91,7 @@ class DummyModelFromPretrainedExpectsCon
         return cls(**kwargs)
 
 
-class BaseModelForInheritance(
-    ModelHubMixin, repo_url="https://hf.co/my-repo", library_name="my-cool-library"
-):
+class BaseModelForInheritance(ModelHubMixin, repo_url="https://hf.co/my-repo", library_name="my-cool-library"):
     pass
 
 
@@ -107,9 +105,7 @@ class DummyModelSavingConfig(ModelHubMix
 
         This file must not be overwritten by the default config saved by `ModelHubMixin`.
         """
-        (save_directory / "config.json").write_text(
-            json.dumps({"custom_config": "custom_config"})
-        )
+        (save_directory / "config.json").write_text(json.dumps({"custom_config": "custom_config"}))
 
 
 @dataclass
@@ -279,9 +275,7 @@ class HubMixinTest(unittest.TestCase):
         model.save_pretrained(self.cache_dir)
         assert model._hub_mixin_config == {"something": "else"}
 
-        with patch.object(
-            DummyModelWithKwargs, "__init__", return_value=None
-        ) as init_call_mock:
+        with patch.object(DummyModelWithKwargs, "__init__", return_value=None) as init_call_mock:
             DummyModelWithKwargs.from_pretrained(self.cache_dir)
 
         # 'something' is passed to __init__ both as kwarg and in config.
@@ -301,18 +295,12 @@ class HubMixinTest(unittest.TestCase):
         mocked_model.push_to_hub.assert_not_called()
 
         # Push to hub with repo_id (config is pushed)
-        mocked_model.save_pretrained(
-            save_directory, push_to_hub=True, repo_id="CustomID"
-        )
-        mocked_model.push_to_hub.assert_called_with(
-            repo_id="CustomID", config=CONFIG_AS_DICT, model_card_kwargs={}
-        )
+        mocked_model.save_pretrained(save_directory, push_to_hub=True, repo_id="CustomID")
+        mocked_model.push_to_hub.assert_called_with(repo_id="CustomID", config=CONFIG_AS_DICT, model_card_kwargs={})
 
         # Push to hub with default repo_id (based on dir name)
         mocked_model.save_pretrained(save_directory, push_to_hub=True)
-        mocked_model.push_to_hub.assert_called_with(
-            repo_id=repo_id, config=CONFIG_AS_DICT, model_card_kwargs={}
-        )
+        mocked_model.push_to_hub.assert_called_with(repo_id=repo_id, config=CONFIG_AS_DICT, model_card_kwargs={})
 
     @patch.object(DummyModelNoConfig, "_from_pretrained")
     def test_from_pretrained_model_id_only(self, from_pretrained_mock: Mock) -> None:
@@ -321,14 +309,10 @@ class HubMixinTest(unittest.TestCase):
         assert model is from_pretrained_mock.return_value
 
     @patch.object(DummyModelNoConfig, "_from_pretrained")
-    def test_from_pretrained_model_id_and_revision(
-        self, from_pretrained_mock: Mock
-    ) -> None:
+    def test_from_pretrained_model_id_and_revision(self, from_pretrained_mock: Mock) -> None:
         """Regression test for #1313.
         See https://github.com/huggingface/huggingface_hub/issues/1313."""
-        model = DummyModelNoConfig.from_pretrained(
-            "namespace/repo_name", revision="123456789"
-        )
+        model = DummyModelNoConfig.from_pretrained("namespace/repo_name", revision="123456789")
         from_pretrained_mock.assert_called_once_with(
             model_id="namespace/repo_name",
             revision="123456789",  # Revision is passed correctly!
@@ -344,33 +328,25 @@ class HubMixinTest(unittest.TestCase):
     def test_from_pretrained_from_relative_path(self):
         with SoftTemporaryDirectory(dir=Path(".")) as tmp_relative_dir:
             relative_save_directory = Path(tmp_relative_dir) / "model"
-            DummyModelConfigAsDataclass(config=CONFIG_AS_DATACLASS).save_pretrained(
-                relative_save_directory
-            )
+            DummyModelConfigAsDataclass(config=CONFIG_AS_DATACLASS).save_pretrained(relative_save_directory)
             model = DummyModelConfigAsDataclass.from_pretrained(relative_save_directory)
             assert model._hub_mixin_config == CONFIG_AS_DATACLASS
 
     def test_from_pretrained_from_absolute_path(self):
         save_directory = self.cache_dir / "subfolder"
-        DummyModelConfigAsDataclass(config=CONFIG_AS_DATACLASS).save_pretrained(
-            save_directory
-        )
+        DummyModelConfigAsDataclass(config=CONFIG_AS_DATACLASS).save_pretrained(save_directory)
         model = DummyModelConfigAsDataclass.from_pretrained(save_directory)
         assert model._hub_mixin_config == CONFIG_AS_DATACLASS
 
     def test_from_pretrained_from_absolute_string_path(self):
         save_directory = str(self.cache_dir / "subfolder")
-        DummyModelConfigAsDataclass(config=CONFIG_AS_DATACLASS).save_pretrained(
-            save_directory
-        )
+        DummyModelConfigAsDataclass(config=CONFIG_AS_DATACLASS).save_pretrained(save_directory)
         model = DummyModelConfigAsDataclass.from_pretrained(save_directory)
         assert model._hub_mixin_config == CONFIG_AS_DATACLASS
 
     def test_push_to_hub(self):
         repo_id = f"{USER}/{repo_name('push_to_hub')}"
-        DummyModelConfigAsDataclass(CONFIG_AS_DATACLASS).push_to_hub(
-            repo_id=repo_id, token=TOKEN
-        )
+        DummyModelConfigAsDataclass(CONFIG_AS_DATACLASS).push_to_hub(repo_id=repo_id, token=TOKEN)
 
         # Test model id exists
         self._api.model_info(repo_id)
@@ -393,16 +369,10 @@ class HubMixinTest(unittest.TestCase):
             "token": TOKEN,
         }
         for cls in (DummyModelConfigAsDataclass, DummyModelConfigAsOptionalDataclass):
-            assert (
-                cls.from_pretrained(**from_pretrained_kwargs)._hub_mixin_config
-                == CONFIG_AS_DATACLASS
-            )
+            assert cls.from_pretrained(**from_pretrained_kwargs)._hub_mixin_config == CONFIG_AS_DATACLASS
 
         for cls in (DummyModelConfigAsDict, DummyModelConfigAsOptionalDict):
-            assert (
-                cls.from_pretrained(**from_pretrained_kwargs)._hub_mixin_config
-                == CONFIG_AS_DICT
-            )
+            assert cls.from_pretrained(**from_pretrained_kwargs)._hub_mixin_config == CONFIG_AS_DICT
 
         # Delete repo
         self._api.delete_repo(repo_id=repo_id)
@@ -424,9 +394,7 @@ class HubMixinTest(unittest.TestCase):
         If a previously existing config file exists, it should be overwritten.
         """
         # Something existing in the cache dir
-        (self.cache_dir / "config.json").write_text(
-            json.dumps({"something_legacy": 123})
-        )
+        (self.cache_dir / "config.json").write_text(json.dumps({"something_legacy": 123}))
 
         # Save model
         model = DummyModelWithKwargs(a=1, b=2)
@@ -445,9 +413,7 @@ class HubMixinTest(unittest.TestCase):
 
         See https://github.com/huggingface/huggingface_hub/issues/2157.
         """
-        (self.cache_dir / "config.json").write_text(
-            '{"foo": 42, "bar": "baz", "other": "value"}'
-        )
+        (self.cache_dir / "config.json").write_text('{"foo": 42, "bar": "baz", "other": "value"}')
         model = DummyModelThatIsAlsoADataclass.from_pretrained(self.cache_dir)
         assert model.foo == 42
         assert model.bar == "baz"
@@ -480,10 +446,7 @@ class HubMixinTest(unittest.TestCase):
         assert model_reloaded.bar == "bar"
         assert model_reloaded.baz == 1.0
         assert model_reloaded.custom.value == "custom"
-        assert (
-            model_reloaded.optional_custom_1 is not None
-            and model_reloaded.optional_custom_1.value == "optional"
-        )
+        assert model_reloaded.optional_custom_1 is not None and model_reloaded.optional_custom_1.value == "optional"
         assert model_reloaded.optional_custom_2 is None
         assert model_reloaded.custom_default.value == "default"
 
@@ -512,9 +475,7 @@ a.dum""".strip()
         script = jedi.Script(source, path="example.py")
         source_lines = source.split("\n")
         completions = script.complete(len(source_lines), len(source_lines[-1]))
-        assert any(
-            completion.name == "dummy_example_for_test" for completion in completions
-        )
+        assert any(completion.name == "dummy_example_for_test" for completion in completions)
 
     def test_get_type_hints_works_as_expected(self):
         """
--- python3-huggingface-hub-0.29.3.orig/tests/test_hub_mixin_pytorch.py
+++ python3-huggingface-hub-0.29.3/tests/test_hub_mixin_pytorch.py
@@ -180,18 +180,12 @@ class PytorchHubMixinTest(unittest.TestC
         mocked_model.push_to_hub.assert_not_called()
 
         # Push to hub with repo_id
-        mocked_model.save_pretrained(
-            save_directory, push_to_hub=True, repo_id="CustomID", config=config
-        )
-        mocked_model.push_to_hub.assert_called_with(
-            repo_id="CustomID", config=config, model_card_kwargs={}
-        )
+        mocked_model.save_pretrained(save_directory, push_to_hub=True, repo_id="CustomID", config=config)
+        mocked_model.push_to_hub.assert_called_with(repo_id="CustomID", config=config, model_card_kwargs={})
 
         # Push to hub with default repo_id (based on dir name)
         mocked_model.save_pretrained(save_directory, push_to_hub=True, config=config)
-        mocked_model.push_to_hub.assert_called_with(
-            repo_id=repo_id, config=config, model_card_kwargs={}
-        )
+        mocked_model.push_to_hub.assert_called_with(repo_id=repo_id, config=config, model_card_kwargs={})
 
     @patch.object(DummyModel, "_from_pretrained")
     def test_from_pretrained_model_id_only(self, from_pretrained_mock: Mock) -> None:
@@ -206,9 +200,7 @@ class PytorchHubMixinTest(unittest.TestC
         return self.cache_dir / "model.safetensors"
 
     @patch("huggingface_hub.hub_mixin.hf_hub_download")
-    def test_from_pretrained_model_from_hub_prefer_safetensor(
-        self, hf_hub_download_mock: Mock
-    ) -> None:
+    def test_from_pretrained_model_from_hub_prefer_safetensor(self, hf_hub_download_mock: Mock) -> None:
         hf_hub_download_mock.side_effect = self.pretend_file_download
         model = DummyModel.from_pretrained("namespace/repo_name")
         hf_hub_download_mock.assert_any_call(
@@ -240,9 +232,7 @@ class PytorchHubMixinTest(unittest.TestC
         return self.cache_dir / constants.PYTORCH_WEIGHTS_NAME
 
     @patch("huggingface_hub.hub_mixin.hf_hub_download")
-    def test_from_pretrained_model_from_hub_fallback_pickle(
-        self, hf_hub_download_mock: Mock
-    ) -> None:
+    def test_from_pretrained_model_from_hub_fallback_pickle(self, hf_hub_download_mock: Mock) -> None:
         hf_hub_download_mock.side_effect = self.pretend_file_download_fallback
         model = DummyModel.from_pretrained("namespace/repo_name")
         hf_hub_download_mock.assert_any_call(
@@ -270,9 +260,7 @@ class PytorchHubMixinTest(unittest.TestC
         self.assertIsNotNone(model)
 
     @patch.object(DummyModel, "_from_pretrained")
-    def test_from_pretrained_model_id_and_revision(
-        self, from_pretrained_mock: Mock
-    ) -> None:
+    def test_from_pretrained_model_id_and_revision(self, from_pretrained_mock: Mock) -> None:
         """Regression test for #1313.
         See https://github.com/huggingface/huggingface_hub/issues/1313."""
         model = DummyModel.from_pretrained("namespace/repo_name", revision="123456789")
@@ -319,9 +307,7 @@ class PytorchHubMixinTest(unittest.TestC
             "`PyTorchModelHubMixin.from_pretrained` return type annotation is not a TypeVar.",
         )
         self.assertEqual(
-            DummyModel.from_pretrained.__annotations__[
-                "return"
-            ].__bound__.__forward_arg__,
+            DummyModel.from_pretrained.__annotations__["return"].__bound__.__forward_arg__,
             "ModelHubMixin",
             "`PyTorchModelHubMixin.from_pretrained` return type annotation is not a TypeVar bound by `ModelHubMixin`.",
         )
@@ -391,9 +377,7 @@ class PytorchHubMixinTest(unittest.TestC
         assert reloaded._hub_mixin_config == {"num_classes": 50, "state": "layernorm"}
 
         # Reload model with custom config => custom config is used
-        reloaded_with_default = DummyModelNoConfig.from_pretrained(
-            self.cache_dir, state="other"
-        )
+        reloaded_with_default = DummyModelNoConfig.from_pretrained(self.cache_dir, state="other")
         assert reloaded_with_default.num_classes == 50
         assert reloaded_with_default.state == "other"
         assert reloaded_with_default._hub_mixin_config == {
@@ -463,9 +447,7 @@ class PytorchHubMixinTest(unittest.TestC
 
     def test_save_pretrained_when_config_and_kwargs_are_passed(self):
         # Test creating model with config and kwargs => all values are saved together in config.json
-        model = DummyModelWithConfigAndKwargs(
-            num_classes=50, state="layernorm", config={"a": 1}, b=2, c=3
-        )
+        model = DummyModelWithConfigAndKwargs(num_classes=50, state="layernorm", config={"a": 1}, b=2, c=3)
         model.save_pretrained(self.cache_dir)
         assert model._hub_mixin_config == {
             "num_classes": 50,
--- python3-huggingface-hub-0.29.3.orig/tests/test_inference_api.py
+++ python3-huggingface-hub-0.29.3/tests/test_inference_api.py
@@ -33,9 +33,7 @@ class InferenceApiTest(unittest.TestCase
     @classmethod
     @with_production_testing
     def setUpClass(cls) -> None:
-        cls.image_file = hf_hub_download(
-            repo_id="Narsil/image_dummy", repo_type="dataset", filename="lena.png"
-        )
+        cls.image_file = hf_hub_download(repo_id="Narsil/image_dummy", repo_type="dataset", filename="lena.png")
         return super().setUpClass()
 
     @expect_deprecation("huggingface_hub.inference_api")
@@ -131,9 +129,7 @@ class InferenceApiTest(unittest.TestCase
 
     @expect_deprecation("huggingface_hub.inference_api")
     def test_inference_overriding_invalid_task(self):
-        with self.assertRaises(
-            ValueError, msg="Invalid task invalid-task. Make sure it's valid."
-        ):
+        with self.assertRaises(ValueError, msg="Invalid task invalid-task. Make sure it's valid."):
             InferenceApi("bert-base-uncased", task="invalid-task")
 
     @expect_deprecation("huggingface_hub.inference_api")
--- python3-huggingface-hub-0.29.3.orig/tests/test_inference_async_client.py
+++ python3-huggingface-hub-0.29.3/tests/test_inference_async_client.py
@@ -57,9 +57,7 @@ from .testing_utils import with_producti
 
 @pytest.fixture(autouse=True)
 def patch_non_tgi_server(monkeypatch: pytest.MonkeyPatch):
-    monkeypatch.setattr(
-        huggingface_hub.inference._common, "_UNSUPPORTED_TEXT_GENERATION_KWARGS", {}
-    )
+    monkeypatch.setattr(huggingface_hub.inference._common, "_UNSUPPORTED_TEXT_GENERATION_KWARGS", {})
 
 
 @pytest.fixture
@@ -80,18 +78,14 @@ async def test_async_generate_no_details
 @pytest.mark.asyncio
 @with_production_testing
 async def test_async_generate_with_details(tgi_client: AsyncInferenceClient) -> None:
-    response = await tgi_client.text_generation(
-        "test", details=True, max_new_tokens=1, decoder_input_details=True
-    )
+    response = await tgi_client.text_generation("test", details=True, max_new_tokens=1, decoder_input_details=True)
 
     assert response.generated_text == "."
     assert response.details.finish_reason == "length"
     assert response.details.generated_tokens == 1
     assert response.details.seed is None
     assert len(response.details.prefill) == 1
-    assert response.details.prefill[0] == TextGenerationOutputPrefillToken(
-        id=9288, logprob=None, text="test"
-    )
+    assert response.details.prefill[0] == TextGenerationOutputPrefillToken(id=9288, logprob=None, text="test")
     assert len(response.details.tokens) == 1
     assert response.details.tokens[0].id == 13
     assert response.details.tokens[0].text == "."
@@ -129,9 +123,7 @@ async def test_async_generate_validation
 
 @pytest.mark.vcr
 @pytest.mark.asyncio
-@pytest.mark.skip(
-    "skipping this test, as InferenceAPI seems to not throw an error when sending unsupported params"
-)
+@pytest.mark.skip("skipping this test, as InferenceAPI seems to not throw an error when sending unsupported params")
 async def test_async_generate_non_tgi_endpoint(
     tgi_client: AsyncInferenceClient,
 ) -> None:
@@ -146,22 +138,16 @@ async def test_async_generate_non_tgi_en
 
     # Watermark is ignored (+ warning)
     with pytest.warns(UserWarning):
-        await tgi_client.text_generation(
-            "4 5 6", model="gpt2", max_new_tokens=10, watermark=True
-        )
+        await tgi_client.text_generation("4 5 6", model="gpt2", max_new_tokens=10, watermark=True)
 
     # Return as detail even if details=True (+ warning)
     with pytest.warns(UserWarning):
-        text = await tgi_client.text_generation(
-            "0 1 2", model="gpt2", max_new_tokens=10, details=True
-        )
+        text = await tgi_client.text_generation("0 1 2", model="gpt2", max_new_tokens=10, details=True)
     assert isinstance(text, str)
 
     # Return as stream raises error
     with pytest.raises(ValueError):
-        await tgi_client.text_generation(
-            "0 1 2", model="gpt2", max_new_tokens=10, stream=True
-        )
+        await tgi_client.text_generation("0 1 2", model="gpt2", max_new_tokens=10, stream=True)
 
 
 @pytest.mark.vcr
@@ -171,10 +157,7 @@ async def test_async_generate_stream_no_
     tgi_client: AsyncInferenceClient,
 ) -> None:
     responses = [
-        response
-        async for response in await tgi_client.text_generation(
-            "test", max_new_tokens=1, stream=True
-        )
+        response async for response in await tgi_client.text_generation("test", max_new_tokens=1, stream=True)
     ]
 
     assert len(responses) == 1
@@ -192,9 +175,7 @@ async def test_async_generate_stream_wit
 ) -> None:
     responses = [
         response
-        async for response in await tgi_client.text_generation(
-            "test", max_new_tokens=1, stream=True, details=True
-        )
+        async for response in await tgi_client.text_generation("test", max_new_tokens=1, stream=True, details=True)
     ]
 
     assert len(responses) == 1
@@ -217,9 +198,7 @@ async def test_async_chat_completion_no_
         id="",
         model="HuggingFaceH4/zephyr-7b-beta",
         system_fingerprint="3.0.1-sha-bb9095a",
-        usage=ChatCompletionOutputUsage(
-            completion_tokens=10, prompt_tokens=46, total_tokens=56
-        ),
+        usage=ChatCompletionOutputUsage(completion_tokens=10, prompt_tokens=46, total_tokens=56),
         choices=[
             ChatCompletionOutputComplete(
                 finish_reason="length",
@@ -258,9 +237,7 @@ async def test_async_chat_completion_not
         id="",
         model="microsoft/DialoGPT-small",
         system_fingerprint="3.0.1-sha-bb9095a",
-        usage=ChatCompletionOutputUsage(
-            completion_tokens=10, prompt_tokens=13, total_tokens=23
-        ),
+        usage=ChatCompletionOutputUsage(completion_tokens=10, prompt_tokens=13, total_tokens=23),
     )
 
 
@@ -269,9 +246,7 @@ async def test_async_chat_completion_not
 @with_production_testing
 async def test_async_chat_completion_with_stream() -> None:
     async_client = AsyncInferenceClient(model=CHAT_COMPLETION_MODEL)
-    output = await async_client.chat_completion(
-        CHAT_COMPLETION_MESSAGES, max_tokens=10, stream=True
-    )
+    output = await async_client.chat_completion(CHAT_COMPLETION_MESSAGES, max_tokens=10, stream=True)
 
     all_items = []
     generated_text = ""
@@ -353,9 +328,7 @@ def test_sync_vs_async_signatures() -> N
 @pytest.mark.asyncio
 @pytest.mark.skip("Deprecated (get_model_status)")
 async def test_get_status_too_big_model() -> None:
-    model_status = await AsyncInferenceClient(token=False).get_model_status(
-        "facebook/nllb-moe-54b"
-    )
+    model_status = await AsyncInferenceClient(token=False).get_model_status("facebook/nllb-moe-54b")
     assert model_status.loaded is False
     assert model_status.state == "TooBig"
     assert model_status.compute_type == "cpu"
@@ -365,14 +338,10 @@ async def test_get_status_too_big_model(
 @pytest.mark.asyncio
 @pytest.mark.skip("Deprecated (get_model_status)")
 async def test_get_status_loaded_model() -> None:
-    model_status = await AsyncInferenceClient(token=False).get_model_status(
-        "bigscience/bloom"
-    )
+    model_status = await AsyncInferenceClient(token=False).get_model_status("bigscience/bloom")
     assert model_status.loaded is True
     assert model_status.state == "Loaded"
-    assert isinstance(
-        model_status.compute_type, dict
-    )  # e.g. {'gpu': {'gpu': 'a100', 'count': 8}}
+    assert isinstance(model_status.compute_type, dict)  # e.g. {'gpu': {'gpu': 'a100', 'count': 8}}
     assert model_status.framework == "text-generation-inference"
 
 
@@ -393,9 +362,7 @@ async def test_get_status_model_as_url()
 @pytest.mark.asyncio
 @pytest.mark.skip("Deprecated (list_deployed_models)")
 async def test_list_deployed_models_single_frameworks() -> None:
-    models_by_task = await AsyncInferenceClient().list_deployed_models(
-        "text-generation-inference"
-    )
+    models_by_task = await AsyncInferenceClient().list_deployed_models("text-generation-inference")
     assert isinstance(models_by_task, dict)
     for task, models in models_by_task.items():
         assert isinstance(task, str)
@@ -424,9 +391,7 @@ class CustomException(Exception):
 @patch("aiohttp.ClientSession.post", side_effect=CustomException())
 @patch("aiohttp.ClientSession.close")
 @pytest.mark.asyncio
-async def test_close_connection_on_post_error(
-    mock_close: Mock, mock_post: Mock
-) -> None:
+async def test_close_connection_on_post_error(mock_close: Mock, mock_post: Mock) -> None:
     async_client = AsyncInferenceClient()
 
     with pytest.warns(FutureWarning, match=".*'post'.*"):
@@ -489,9 +454,7 @@ async def test_openai_compatibility_with
     )
 
     chunked_text = [
-        chunk.choices[0].delta.content
-        async for chunk in output
-        if chunk.choices[0].delta.content is not None
+        chunk.choices[0].delta.content async for chunk in output if chunk.choices[0].delta.content is not None
     ]
     assert len(chunked_text) == 35
     output_text = "".join(chunked_text)
--- python3-huggingface-hub-0.29.3.orig/tests/test_inference_client.py
+++ python3-huggingface-hub-0.29.3/tests/test_inference_client.py
@@ -256,25 +256,19 @@ def client(request):
 @pytest.fixture(scope="module")
 @with_production_testing
 def audio_file():
-    return hf_hub_download(
-        repo_id="Narsil/image_dummy", repo_type="dataset", filename="sample1.flac"
-    )
+    return hf_hub_download(repo_id="Narsil/image_dummy", repo_type="dataset", filename="sample1.flac")
 
 
 @pytest.fixture(scope="module")
 @with_production_testing
 def image_file():
-    return hf_hub_download(
-        repo_id="Narsil/image_dummy", repo_type="dataset", filename="lena.png"
-    )
+    return hf_hub_download(repo_id="Narsil/image_dummy", repo_type="dataset", filename="lena.png")
 
 
 @pytest.fixture(scope="module")
 @with_production_testing
 def document_file():
-    return hf_hub_download(
-        repo_id="impira/docquery", repo_type="space", filename="contract.jpeg"
-    )
+    return hf_hub_download(repo_id="impira/docquery", repo_type="space", filename="contract.jpeg")
 
 
 class TestBase:
@@ -298,9 +292,7 @@ class TestBase:
 @pytest.mark.vcr
 @with_production_testing
 class TestInferenceClient(TestBase):
-    @pytest.mark.parametrize(
-        "client", list_clients("audio-classification"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("audio-classification"), indirect=True)
     def test_audio_classification(self, client: InferenceClient):
         output = client.audio_classification(self.audio_file)
         assert isinstance(output, list)
@@ -319,19 +311,12 @@ class TestInferenceClient(TestBase):
             assert isinstance(item.blob, bytes)
             assert item.content_type == "audio/flac"
 
-    @pytest.mark.parametrize(
-        "client", list_clients("automatic-speech-recognition"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("automatic-speech-recognition"), indirect=True)
     def test_automatic_speech_recognition(self, client: InferenceClient):
         output = client.automatic_speech_recognition(self.audio_file)
         # Remove punctuation from the output
-        normalized_output = output.text.translate(
-            str.maketrans("", "", string.punctuation)
-        )
-        assert (
-            normalized_output.lower().strip()
-            == "a man said to the universe sir i exist"
-        )
+        normalized_output = output.text.translate(str.maketrans("", "", string.punctuation))
+        assert normalized_output.lower().strip() == "a man said to the universe sir i exist"
 
     @pytest.mark.parametrize("client", list_clients("conversational"), indirect=True)
     def test_chat_completion_no_stream(self, client: InferenceClient):
@@ -461,13 +446,9 @@ class TestInferenceClient(TestBase):
                 model="meta-llama/Meta-Llama-3-70B-Instruct",
             )
 
-    @pytest.mark.parametrize(
-        "client", list_clients("document-question-answering"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("document-question-answering"), indirect=True)
     def test_document_question_answering(self, client: InferenceClient):
-        output = client.document_question_answering(
-            self.document_file, "What is the purchase amount?"
-        )
+        output = client.document_question_answering(self.document_file, "What is the purchase amount?")
         assert output == [
             DocumentQuestionAnsweringOutputElement(
                 answer="$1,0000,000,00",
@@ -477,20 +458,14 @@ class TestInferenceClient(TestBase):
             )
         ]
 
-    @pytest.mark.parametrize(
-        "client", list_clients("feature-extraction"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("feature-extraction"), indirect=True)
     def test_feature_extraction_with_transformers(self, client: InferenceClient):
         embedding = client.feature_extraction("Hi, who are you?")
         assert isinstance(embedding, np.ndarray)
         assert embedding.shape == (1, 8, 768)
 
-    @pytest.mark.parametrize(
-        "client", list_clients("feature-extraction"), indirect=True
-    )
-    def test_feature_extraction_with_sentence_transformers(
-        self, client: InferenceClient
-    ):
+    @pytest.mark.parametrize("client", list_clients("feature-extraction"), indirect=True)
+    def test_feature_extraction_with_sentence_transformers(self, client: InferenceClient):
         embedding = client.feature_extraction("Hi, who are you?")
         assert isinstance(embedding, np.ndarray)
         assert embedding.shape == (1, 8, 768)
@@ -539,9 +514,7 @@ class TestInferenceClient(TestBase):
     def test_hf_inference_get_recommended_model_has_recommendation(self) -> None:
         from huggingface_hub.inference._providers.hf_inference import HFInferenceTask
 
-        HFInferenceTask("feature-extraction")._prepare_mapped_model(
-            None
-        ) == "facebook/bart-base"
+        HFInferenceTask("feature-extraction")._prepare_mapped_model(None) == "facebook/bart-base"
         HFInferenceTask("translation")._prepare_mapped_model(None) == "t5-small"
 
     def test_hf_inference_get_recommended_model_no_recommendation(self) -> None:
@@ -550,32 +523,18 @@ class TestInferenceClient(TestBase):
         with pytest.raises(ValueError):
             HFInferenceTask("text-generation")._prepare_mapped_model(None)
 
-    @pytest.mark.parametrize(
-        "client", list_clients("image-classification"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("image-classification"), indirect=True)
     def test_image_classification(self, client: InferenceClient):
         output = client.image_classification(self.image_file)
         assert output == [
-            ImageClassificationOutputElement(
-                label="brassiere, bra, bandeau", score=0.11767438799142838
-            ),
-            ImageClassificationOutputElement(
-                label="sombrero", score=0.09572819620370865
-            ),
-            ImageClassificationOutputElement(
-                label="cowboy hat, ten-gallon hat", score=0.0900089293718338
-            ),
-            ImageClassificationOutputElement(
-                label="bonnet, poke bonnet", score=0.06615174561738968
-            ),
-            ImageClassificationOutputElement(
-                label="fur coat", score=0.061511047184467316
-            ),
+            ImageClassificationOutputElement(label="brassiere, bra, bandeau", score=0.11767438799142838),
+            ImageClassificationOutputElement(label="sombrero", score=0.09572819620370865),
+            ImageClassificationOutputElement(label="cowboy hat, ten-gallon hat", score=0.0900089293718338),
+            ImageClassificationOutputElement(label="bonnet, poke bonnet", score=0.06615174561738968),
+            ImageClassificationOutputElement(label="fur coat", score=0.061511047184467316),
         ]
 
-    @pytest.mark.parametrize(
-        "client", list_clients("image-segmentation"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("image-segmentation"), indirect=True)
     def test_image_segmentation(self, client: InferenceClient):
         output = client.image_segmentation(self.image_file)
         assert isinstance(output, list)
@@ -634,20 +593,12 @@ class TestInferenceClient(TestBase):
             ),
         ]
 
-    @pytest.mark.parametrize(
-        "client", list_clients("question-answering"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("question-answering"), indirect=True)
     def test_question_answering(self, client: InferenceClient):
-        output = client.question_answering(
-            question="What is the meaning of life?", context="42"
-        )
-        assert output == QuestionAnsweringOutputElement(
-            answer="42", end=2, score=1.4291124728060822e-08, start=0
-        )
+        output = client.question_answering(question="What is the meaning of life?", context="42")
+        assert output == QuestionAnsweringOutputElement(answer="42", end=2, score=1.4291124728060822e-08, start=0)
 
-    @pytest.mark.parametrize(
-        "client", list_clients("sentence-similarity"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("sentence-similarity"), indirect=True)
     def test_sentence_similarity(self, client: InferenceClient):
         scores = client.sentence_similarity(
             "Machine learning is so easy.",
@@ -665,9 +616,7 @@ class TestInferenceClient(TestBase):
         assert isinstance(summary.summary_text, str)
 
     @pytest.mark.skip(reason="This model is not available on InferenceAPI")
-    @pytest.mark.parametrize(
-        "client", list_clients("tabular-classification"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("tabular-classification"), indirect=True)
     def test_tabular_classification(self, client: InferenceClient):
         table = {
             "fixed_acidity": ["7.4", "7.8", "10.3"],
@@ -686,9 +635,7 @@ class TestInferenceClient(TestBase):
         assert output == ["5", "5", "5"]
 
     @pytest.mark.skip(reason="This model is not available on InferenceAPI")
-    @pytest.mark.parametrize(
-        "client", list_clients("tabular-regression"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("tabular-regression"), indirect=True)
     def test_tabular_regression(self, client: InferenceClient):
         table = {
             "Height": ["11.52", "12.48", "12.3778"],
@@ -701,9 +648,7 @@ class TestInferenceClient(TestBase):
         output = client.tabular_regression(table=table)
         assert output == [110, 120, 130]
 
-    @pytest.mark.parametrize(
-        "client", list_clients("table-question-answering"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("table-question-answering"), indirect=True)
     def test_table_question_answering(self, client: InferenceClient):
         table = {
             "Repository": ["Transformers", "Datasets", "Tokenizers"],
@@ -718,16 +663,12 @@ class TestInferenceClient(TestBase):
             aggregator="AVERAGE",
         )
 
-    @pytest.mark.parametrize(
-        "client", list_clients("text-classification"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("text-classification"), indirect=True)
     def test_text_classification(self, client: InferenceClient):
         output = client.text_classification("I like you")
         assert output == [
             TextClassificationOutputElement(label="POSITIVE", score=0.9998695850372314),
-            TextClassificationOutputElement(
-                label="NEGATIVE", score=0.00013043530634604394
-            ),
+            TextClassificationOutputElement(label="NEGATIVE", score=0.00013043530634604394),
         ]
 
     def test_text_generation(self) -> None:
@@ -738,14 +679,10 @@ class TestInferenceClient(TestBase):
         image = client.text_to_image("An astronaut riding a horse on the moon.")
         assert isinstance(image, Image.Image)
 
-    @pytest.mark.skip(
-        reason="Need to check why fal.ai doesn't take image_size into account"
-    )
+    @pytest.mark.skip(reason="Need to check why fal.ai doesn't take image_size into account")
     @pytest.mark.parametrize("client", list_clients("text-to-image"), indirect=True)
     def test_text_to_image_with_parameters(self, client: InferenceClient):
-        image = client.text_to_image(
-            "An astronaut riding a horse on the moon.", height=256, width=256
-        )
+        image = client.text_to_image("An astronaut riding a horse on the moon.", height=256, width=256)
         assert isinstance(image, Image.Image)
         assert image.height == 256
         assert image.width == 256
@@ -784,13 +721,9 @@ class TestInferenceClient(TestBase):
                 tgt_lang="en_XX",
             )
 
-    @pytest.mark.parametrize(
-        "client", list_clients("token-classification"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("token-classification"), indirect=True)
     def test_token_classification(self, client: InferenceClient):
-        output = client.token_classification(
-            text="My name is Sarah Jessica Parker but you can call me Jessica"
-        )
+        output = client.token_classification(text="My name is Sarah Jessica Parker but you can call me Jessica")
         assert output == [
             TokenClassificationOutputElement(
                 score=0.9991335868835449,
@@ -808,34 +741,18 @@ class TestInferenceClient(TestBase):
             ),
         ]
 
-    @pytest.mark.parametrize(
-        "client", list_clients("visual-question-answering"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("visual-question-answering"), indirect=True)
     def test_visual_question_answering(self, client: InferenceClient):
-        output = client.visual_question_answering(
-            image=self.image_file, question="Who's in the picture?"
-        )
+        output = client.visual_question_answering(image=self.image_file, question="Who's in the picture?")
         assert output == [
-            VisualQuestionAnsweringOutputElement(
-                score=0.9386942982673645, answer="woman"
-            ),
-            VisualQuestionAnsweringOutputElement(
-                score=0.3431190550327301, answer="girl"
-            ),
-            VisualQuestionAnsweringOutputElement(
-                score=0.08407800644636154, answer="lady"
-            ),
-            VisualQuestionAnsweringOutputElement(
-                score=0.05075192078948021, answer="female"
-            ),
-            VisualQuestionAnsweringOutputElement(
-                score=0.017771074548363686, answer="man"
-            ),
+            VisualQuestionAnsweringOutputElement(score=0.9386942982673645, answer="woman"),
+            VisualQuestionAnsweringOutputElement(score=0.3431190550327301, answer="girl"),
+            VisualQuestionAnsweringOutputElement(score=0.08407800644636154, answer="lady"),
+            VisualQuestionAnsweringOutputElement(score=0.05075192078948021, answer="female"),
+            VisualQuestionAnsweringOutputElement(score=0.017771074548363686, answer="man"),
         ]
 
-    @pytest.mark.parametrize(
-        "client", list_clients("zero-shot-classification"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("zero-shot-classification"), indirect=True)
     def test_zero_shot_classification_single_label(self, client: InferenceClient):
         output = client.zero_shot_classification(
             "A new model offers an explanation for how the Galilean satellites formed around the solar system's"
@@ -850,26 +767,14 @@ class TestInferenceClient(TestBase):
             ],
         )
         assert output == [
-            ZeroShotClassificationOutputElement(
-                label="scientific discovery", score=0.796166181564331
-            ),
-            ZeroShotClassificationOutputElement(
-                label="space & cosmos", score=0.18570725619792938
-            ),
-            ZeroShotClassificationOutputElement(
-                label="microbiology", score=0.007308819331228733
-            ),
-            ZeroShotClassificationOutputElement(
-                label="archeology", score=0.0062583745457232
-            ),
-            ZeroShotClassificationOutputElement(
-                label="robots", score=0.004559362772852182
-            ),
+            ZeroShotClassificationOutputElement(label="scientific discovery", score=0.796166181564331),
+            ZeroShotClassificationOutputElement(label="space & cosmos", score=0.18570725619792938),
+            ZeroShotClassificationOutputElement(label="microbiology", score=0.007308819331228733),
+            ZeroShotClassificationOutputElement(label="archeology", score=0.0062583745457232),
+            ZeroShotClassificationOutputElement(label="robots", score=0.004559362772852182),
         ]
 
-    @pytest.mark.parametrize(
-        "client", list_clients("zero-shot-classification"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("zero-shot-classification"), indirect=True)
     def test_zero_shot_classification_multi_label(self, client: InferenceClient):
         output = client.zero_shot_classification(
             text="A new model offers an explanation for how the Galilean satellites formed around the solar system's"
@@ -885,26 +790,14 @@ class TestInferenceClient(TestBase):
             multi_label=True,
         )
         assert output == [
-            ZeroShotClassificationOutputElement(
-                label="scientific discovery", score=0.9829296469688416
-            ),
-            ZeroShotClassificationOutputElement(
-                label="space & cosmos", score=0.7551906108856201
-            ),
-            ZeroShotClassificationOutputElement(
-                label="microbiology", score=0.0005462627159431577
-            ),
-            ZeroShotClassificationOutputElement(
-                label="archeology", score=0.0004713202069979161
-            ),
-            ZeroShotClassificationOutputElement(
-                label="robots", score=0.000304485292872414
-            ),
+            ZeroShotClassificationOutputElement(label="scientific discovery", score=0.9829296469688416),
+            ZeroShotClassificationOutputElement(label="space & cosmos", score=0.7551906108856201),
+            ZeroShotClassificationOutputElement(label="microbiology", score=0.0005462627159431577),
+            ZeroShotClassificationOutputElement(label="archeology", score=0.0004713202069979161),
+            ZeroShotClassificationOutputElement(label="robots", score=0.000304485292872414),
         ]
 
-    @pytest.mark.parametrize(
-        "client", list_clients("zero-shot-image-classification"), indirect=True
-    )
+    @pytest.mark.parametrize("client", list_clients("zero-shot-image-classification"), indirect=True)
     def test_zero_shot_image_classification(self, client: InferenceClient):
         output = client.zero_shot_image_classification(
             image=self.image_file, candidate_labels=["tree", "woman", "cat"]
@@ -936,9 +829,7 @@ class TestOpenAsBinary:
             assert isinstance(content, io.BufferedReader)
 
     def test_open_as_binary_from_url(self) -> None:
-        with _open_as_binary(
-            "https://huggingface.co/datasets/Narsil/image_dummy/resolve/main/tree.png"
-        ) as content:
+        with _open_as_binary("https://huggingface.co/datasets/Narsil/image_dummy/resolve/main/tree.png") as content:
             assert isinstance(content, bytes)
 
     def test_open_as_binary_opened_file(self) -> None:
@@ -955,9 +846,7 @@ class TestOpenAsBinary:
 
 class TestHeadersAndCookies(TestBase):
     def test_headers_and_cookies(self) -> None:
-        client = InferenceClient(
-            headers={"X-My-Header": "foo"}, cookies={"my-cookie": "bar"}
-        )
+        client = InferenceClient(headers={"X-My-Header": "foo"}, cookies={"my-cookie": "bar"})
         assert client.headers["X-My-Header"] == "foo"
         assert client.cookies["my-cookie"] == "bar"
 
@@ -971,9 +860,7 @@ class TestHeadersAndCookies(TestBase):
             cookies={"my-cookie": "bar"},
             proxies="custom proxies",
         )
-        response = client.post(
-            data=b"content", model="username/repo_name", task="text-classification"
-        )
+        response = client.post(data=b"content", model="username/repo_name", task="text-classification")
         assert response == get_session_mock().post.return_value.content
 
         expected_headers = build_hf_headers()
@@ -990,9 +877,7 @@ class TestHeadersAndCookies(TestBase):
 
     @patch("huggingface_hub.inference._client._bytes_to_image")
     @patch("huggingface_hub.inference._client.get_session")
-    def test_accept_header_image(
-        self, get_session_mock: MagicMock, bytes_to_image_mock: MagicMock
-    ) -> None:
+    def test_accept_header_image(self, get_session_mock: MagicMock, bytes_to_image_mock: MagicMock) -> None:
         """Test that Accept: image/png header is set for image tasks."""
         client = InferenceClient()
 
@@ -1019,9 +904,7 @@ class TestModelStatus(TestBase):
         model_status = client.get_model_status("bigscience/bloom")
         assert model_status.loaded
         assert model_status.state == "Loaded"
-        assert isinstance(
-            model_status.compute_type, dict
-        )  # e.g. {'gpu': {'gpu': 'a100', 'count': 8}}
+        assert isinstance(model_status.compute_type, dict)  # e.g. {'gpu': {'gpu': 'a100', 'count': 8}}
         assert model_status.framework == "text-generation-inference"
 
     @expect_deprecation("get_model_status")
@@ -1040,29 +923,19 @@ class TestModelStatus(TestBase):
 class TestListDeployedModels(TestBase):
     @expect_deprecation("list_deployed_models")
     @patch("huggingface_hub.inference._client.get_session")
-    def test_list_deployed_models_main_frameworks_mock(
-        self, get_session_mock: MagicMock
-    ) -> None:
+    def test_list_deployed_models_main_frameworks_mock(self, get_session_mock: MagicMock) -> None:
         InferenceClient().list_deployed_models()
-        assert len(get_session_mock.return_value.get.call_args_list) == len(
-            constants.MAIN_INFERENCE_API_FRAMEWORKS
-        )
+        assert len(get_session_mock.return_value.get.call_args_list) == len(constants.MAIN_INFERENCE_API_FRAMEWORKS)
 
     @expect_deprecation("list_deployed_models")
     @patch("huggingface_hub.inference._client.get_session")
-    def test_list_deployed_models_all_frameworks_mock(
-        self, get_session_mock: MagicMock
-    ) -> None:
+    def test_list_deployed_models_all_frameworks_mock(self, get_session_mock: MagicMock) -> None:
         InferenceClient().list_deployed_models("all")
-        assert len(get_session_mock.return_value.get.call_args_list) == len(
-            constants.ALL_INFERENCE_API_FRAMEWORKS
-        )
+        assert len(get_session_mock.return_value.get.call_args_list) == len(constants.ALL_INFERENCE_API_FRAMEWORKS)
 
     @expect_deprecation("list_deployed_models")
     def test_list_deployed_models_single_frameworks(self) -> None:
-        models_by_task = InferenceClient().list_deployed_models(
-            "text-generation-inference"
-        )
+        models_by_task = InferenceClient().list_deployed_models("text-generation-inference")
         assert isinstance(models_by_task, dict)
         for task, models in models_by_task.items():
             assert isinstance(task, str)
--- python3-huggingface-hub-0.29.3.orig/tests/test_inference_endpoints.py
+++ python3-huggingface-hub-0.29.3/tests/test_inference_endpoints.py
@@ -160,12 +160,8 @@ def test_from_raw_initialization():
     assert endpoint.type == "protected"
 
     # Datetime parsed correctly
-    assert endpoint.created_at == datetime(
-        2023, 10, 26, 12, 41, 53, 263078, tzinfo=timezone.utc
-    )
-    assert endpoint.updated_at == datetime(
-        2023, 10, 26, 12, 41, 53, 263079, tzinfo=timezone.utc
-    )
+    assert endpoint.created_at == datetime(2023, 10, 26, 12, 41, 53, 263078, tzinfo=timezone.utc)
+    assert endpoint.updated_at == datetime(2023, 10, 26, 12, 41, 53, 263079, tzinfo=timezone.utc)
 
     # Not initialized yet
     assert endpoint.url is None
@@ -198,16 +194,11 @@ def test_get_client_not_ready():
 
 def test_get_client_ready():
     """Test clients are created correctly when endpoint is ready."""
-    endpoint = InferenceEndpoint.from_raw(
-        MOCK_RUNNING, namespace="foo", token="my-token"
-    )
+    endpoint = InferenceEndpoint.from_raw(MOCK_RUNNING, namespace="foo", token="my-token")
 
     # Endpoint is ready
     assert endpoint.status == "running"
-    assert (
-        endpoint.url
-        == "https://vksrvs8pc1xnifhq.us-east-1.aws.endpoints.huggingface.cloud"
-    )
+    assert endpoint.url == "https://vksrvs8pc1xnifhq.us-east-1.aws.endpoints.huggingface.cloud"
 
     # => Client available
     client = endpoint.client
@@ -228,10 +219,7 @@ def test_fetch(mock_get: Mock):
     endpoint.fetch()
 
     assert endpoint.status == "running"
-    assert (
-        endpoint.url
-        == "https://vksrvs8pc1xnifhq.us-east-1.aws.endpoints.huggingface.cloud"
-    )
+    assert endpoint.url == "https://vksrvs8pc1xnifhq.us-east-1.aws.endpoints.huggingface.cloud"
 
 
 @patch("huggingface_hub._inference_endpoints.get_session")
@@ -328,6 +316,4 @@ def test_resume(mock: Mock):
     endpoint = InferenceEndpoint.from_raw(MOCK_RUNNING, namespace="foo")
     mock.return_value = InferenceEndpoint.from_raw(MOCK_INITIALIZING, namespace="foo")
     endpoint.resume()
-    mock.assert_called_once_with(
-        namespace="foo", name="my-endpoint-name", token=None, running_ok=True
-    )
+    mock.assert_called_once_with(namespace="foo", name="my-endpoint-name", token=None, running_ok=True)
--- python3-huggingface-hub-0.29.3.orig/tests/test_inference_providers.py
+++ python3-huggingface-hub-0.29.3/tests/test_inference_providers.py
@@ -97,9 +97,7 @@ class TestBlackForestLabsProvider:
     def test_get_response_success(self, mocker):
         """Test successful response handling with polling."""
         helper = BlackForestLabsTextToImageTask()
-        mock_session = mocker.patch(
-            "huggingface_hub.inference._providers.black_forest_labs.get_session"
-        )
+        mock_session = mocker.patch("huggingface_hub.inference._providers.black_forest_labs.get_session")
         mock_session.return_value.get.side_effect = [
             mocker.Mock(
                 json=lambda: {
@@ -163,17 +161,11 @@ class TestFalAIProvider:
 
     def test_automatic_speech_recognition_payload(self):
         helper = FalAIAutomaticSpeechRecognitionTask()
-        payload = helper._prepare_payload_as_dict(
-            "https://example.com/audio.mp3", {}, "username/repo_name"
-        )
+        payload = helper._prepare_payload_as_dict("https://example.com/audio.mp3", {}, "username/repo_name")
         assert payload == {"audio_url": "https://example.com/audio.mp3"}
 
-        payload = helper._prepare_payload_as_dict(
-            b"dummy_audio_data", {}, "username/repo_name"
-        )
-        assert payload == {
-            "audio_url": f"data:audio/mpeg;base64,{base64.b64encode(b'dummy_audio_data').decode()}"
-        }
+        payload = helper._prepare_payload_as_dict(b"dummy_audio_data", {}, "username/repo_name")
+        assert payload == {"audio_url": f"data:audio/mpeg;base64,{base64.b64encode(b'dummy_audio_data').decode()}"}
 
     def test_automatic_speech_recognition_response(self):
         helper = FalAIAutomaticSpeechRecognitionTask()
@@ -202,9 +194,7 @@ class TestFalAIProvider:
 
     def test_text_to_speech_payload(self):
         helper = FalAITextToSpeechTask()
-        payload = helper._prepare_payload_as_dict(
-            "Hello world", {}, "username/repo_name"
-        )
+        payload = helper._prepare_payload_as_dict("Hello world", {}, "username/repo_name")
         assert payload == {"lyrics": "Hello world"}
 
     def test_text_to_speech_response(self, mocker):
@@ -216,9 +206,7 @@ class TestFalAIProvider:
 
     def test_text_to_video_payload(self):
         helper = FalAITextToVideoTask()
-        payload = helper._prepare_payload_as_dict(
-            "a cat walking", {"num_frames": 16}, "username/repo_name"
-        )
+        payload = helper._prepare_payload_as_dict("a cat walking", {"num_frames": 16}, "username/repo_name")
         assert payload == {"prompt": "a cat walking", "num_frames": 16}
 
     def test_text_to_video_response(self, mocker):
@@ -251,12 +239,8 @@ class TestFireworksAIConversationalTask:
 class TestHFInferenceProvider:
     def test_prepare_mapped_model(self, mocker):
         helper = HFInferenceTask("text-classification")
-        assert (
-            helper._prepare_mapped_model("username/repo_name") == "username/repo_name"
-        )
-        assert (
-            helper._prepare_mapped_model("https://any-url.com") == "https://any-url.com"
-        )
+        assert helper._prepare_mapped_model("username/repo_name") == "username/repo_name"
+        assert helper._prepare_mapped_model("https://any-url.com") == "https://any-url.com"
 
         mocker.patch(
             "huggingface_hub.inference._providers.hf_inference._fetch_recommended_models",
@@ -264,9 +248,7 @@ class TestHFInferenceProvider:
         )
         assert helper._prepare_mapped_model(None) == "username/repo_name"
 
-        with pytest.raises(
-            ValueError, match="Task unknown-task has no recommended model"
-        ):
+        with pytest.raises(ValueError, match="Task unknown-task has no recommended model"):
             assert HFInferenceTask("unknown-task")._prepare_mapped_model(None)
 
     def test_prepare_url(self):
@@ -276,10 +258,7 @@ class TestHFInferenceProvider:
             == "https://router.huggingface.co/hf-inference/models/username/repo_name"
         )
 
-        assert (
-            helper._prepare_url("hf_test_token", "https://any-url.com")
-            == "https://any-url.com"
-        )
+        assert helper._prepare_url("hf_test_token", "https://any-url.com") == "https://any-url.com"
 
     def test_prepare_payload_as_dict(self):
         helper = HFInferenceTask("text-classification")
@@ -292,12 +271,8 @@ class TestHFInferenceProvider:
             "parameters": {"a": 1},
         }
 
-        with pytest.raises(
-            ValueError, match="Unexpected binary input for task text-classification."
-        ):
-            helper._prepare_payload_as_dict(
-                b"dummy binary data", {}, "username/repo_name"
-            )
+        with pytest.raises(ValueError, match="Unexpected binary input for task text-classification."):
+            helper._prepare_payload_as_dict(b"dummy binary data", {}, "username/repo_name")
 
     def test_prepare_payload_as_bytes(self):
         helper = HFInferenceBinaryInputTask("image-classification")
@@ -327,12 +302,8 @@ class TestHFInferenceProvider:
         helper._prepare_url(
             "hf_test_token", "username/repo_name"
         ) == "https://router.huggingface.co/hf-inference/models/username/repo_name/v1/chat/completions"
-        helper._prepare_url(
-            "hf_test_token", "https://any-url.com"
-        ) == "https://any-url.com/v1/chat/completions"
-        helper._prepare_url(
-            "hf_test_token", "https://any-url.com/v1"
-        ) == "https://any-url.com/v1/chat/completions"
+        helper._prepare_url("hf_test_token", "https://any-url.com") == "https://any-url.com/v1/chat/completions"
+        helper._prepare_url("hf_test_token", "https://any-url.com/v1") == "https://any-url.com/v1/chat/completions"
 
     def test_prepare_request(self):
         helper = HFInferenceTask("text-classification")
@@ -344,10 +315,7 @@ class TestHFInferenceProvider:
             api_key="hf_test_token",
         )
 
-        assert (
-            request.url
-            == "https://router.huggingface.co/hf-inference/models/username/repo_name"
-        )
+        assert request.url == "https://router.huggingface.co/hf-inference/models/username/repo_name"
         assert request.task == "text-classification"
         assert request.model == "username/repo_name"
         assert request.headers["authorization"] == "Bearer hf_test_token"
@@ -364,8 +332,7 @@ class TestHFInferenceProvider:
         )
 
         assert (
-            request.url
-            == "https://router.huggingface.co/hf-inference/models/username/repo_name/v1/chat/completions"
+            request.url == "https://router.huggingface.co/hf-inference/models/username/repo_name/v1/chat/completions"
         )
         assert request.task == "text-generation"
         assert request.model == "username/repo_name"
@@ -408,9 +375,7 @@ class TestHFInferenceProvider:
             ),
         ],
     )
-    def test_prepare_payload_as_dict_conversational(
-        self, mapped_model, parameters, expected_model
-    ):
+    def test_prepare_payload_as_dict_conversational(self, mapped_model, parameters, expected_model):
         helper = HFInferenceConversational()
         messages = [{"role": "user", "content": "Hello!"}]
 
@@ -478,9 +443,7 @@ class TestHyperbolicProvider:
         """Test response handling for text-to-image task."""
         helper = HyperbolicTextToImageTask()
         dummy_image = b"image_bytes"
-        response = helper.get_response(
-            {"images": [{"image": base64.b64encode(dummy_image).decode()}]}
-        )
+        response = helper.get_response({"images": [{"image": base64.b64encode(dummy_image).decode()}]})
         assert response == dummy_image
 
 
@@ -512,9 +475,7 @@ class TestNebiusProvider:
 
     def test_text_to_image_get_response(self):
         helper = NebiusTextToImageTask()
-        response = helper.get_response(
-            {"data": [{"b64_json": base64.b64encode(b"image_bytes").decode()}]}
-        )
+        response = helper.get_response({"data": [{"b64_json": base64.b64encode(b"image_bytes").decode()}]})
         assert response == b"image_bytes"
 
 
@@ -557,9 +518,7 @@ class TestReplicateProvider:
             {"num_inference_steps": 20},
             "black-forest-labs/FLUX.1-schnell",
         )
-        assert payload == {
-            "input": {"prompt": "a beautiful cat", "num_inference_steps": 20}
-        }
+        assert payload == {"input": {"prompt": "a beautiful cat", "num_inference_steps": 20}}
 
         # Model with specific version
         payload = helper._prepare_payload_as_dict(
@@ -586,18 +545,12 @@ class TestReplicateProvider:
 
     def test_get_response_timeout(self):
         helper = ReplicateTask("text-to-image")
-        with pytest.raises(
-            TimeoutError, match="Inference request timed out after 60 seconds."
-        ):
-            helper.get_response(
-                {"model": "black-forest-labs/FLUX.1-schnell"}
-            )  # no 'output' key
+        with pytest.raises(TimeoutError, match="Inference request timed out after 60 seconds."):
+            helper.get_response({"model": "black-forest-labs/FLUX.1-schnell"})  # no 'output' key
 
     def test_get_response_single_output(self, mocker):
         helper = ReplicateTask("text-to-image")
-        mock = mocker.patch(
-            "huggingface_hub.inference._providers.replicate.get_session"
-        )
+        mock = mocker.patch("huggingface_hub.inference._providers.replicate.get_session")
         response = helper.get_response({"output": "https://example.com/image.jpg"})
         mock.return_value.get.assert_called_once_with("https://example.com/image.jpg")
         assert response == mock.return_value.get.return_value.content
@@ -641,24 +594,18 @@ class TestTogetherProvider:
 
     def test_text_to_image_get_response(self):
         helper = TogetherTextToImageTask()
-        response = helper.get_response(
-            {"data": [{"b64_json": base64.b64encode(b"image_bytes").decode()}]}
-        )
+        response = helper.get_response({"data": [{"b64_json": base64.b64encode(b"image_bytes").decode()}]})
         assert response == b"image_bytes"
 
 
 class TestBaseConversationalTask:
     def test_prepare_route(self):
-        helper = BaseConversationalTask(
-            provider="test-provider", base_url="https://api.test.com"
-        )
+        helper = BaseConversationalTask(provider="test-provider", base_url="https://api.test.com")
         assert helper._prepare_route("dummy-model") == "/v1/chat/completions"
         assert helper.task == "conversational"
 
     def test_prepare_payload(self):
-        helper = BaseConversationalTask(
-            provider="test-provider", base_url="https://api.test.com"
-        )
+        helper = BaseConversationalTask(provider="test-provider", base_url="https://api.test.com")
         messages = [{"role": "user", "content": "Hello!"}]
         parameters = {"temperature": 0.7, "max_tokens": 100}
 
@@ -678,16 +625,12 @@ class TestBaseConversationalTask:
 
 class TestBaseTextGenerationTask:
     def test_prepare_route(self):
-        helper = BaseTextGenerationTask(
-            provider="test-provider", base_url="https://api.test.com"
-        )
+        helper = BaseTextGenerationTask(provider="test-provider", base_url="https://api.test.com")
         assert helper._prepare_route("dummy-model") == "/v1/completions"
         assert helper.task == "text-generation"
 
     def test_prepare_payload(self):
-        helper = BaseTextGenerationTask(
-            provider="test-provider", base_url="https://api.test.com"
-        )
+        helper = BaseTextGenerationTask(provider="test-provider", base_url="https://api.test.com")
         prompt = "Once upon a time"
         parameters = {"temperature": 0.7, "max_tokens": 100}
 
--- python3-huggingface-hub-0.29.3.orig/tests/test_inference_text_generation.py
+++ python3-huggingface-hub-0.29.3/tests/test_inference_text_generation.py
@@ -69,18 +69,14 @@ class TestTextGenerationClientVCR(unitte
         assert response == ""
 
     def test_generate_with_details(self):
-        response = self.client.text_generation(
-            "test", details=True, max_new_tokens=1, decoder_input_details=True
-        )
+        response = self.client.text_generation("test", details=True, max_new_tokens=1, decoder_input_details=True)
 
         assert response.generated_text == ""
         assert response.details.finish_reason == "length"
         assert response.details.generated_tokens == 1
         assert response.details.seed is None
         assert len(response.details.prefill) == 1
-        assert response.details.prefill[0] == TextGenerationOutputPrefillToken(
-            id=0, text="<pad>", logprob=None
-        )
+        assert response.details.prefill[0] == TextGenerationOutputPrefillToken(id=0, text="<pad>", logprob=None)
         assert len(response.details.tokens) == 1
         assert response.details.tokens[0].id == 3
         assert response.details.tokens[0].text == " "
@@ -107,10 +103,7 @@ class TestTextGenerationClientVCR(unitte
 
     def test_generate_stream_no_details(self):
         responses = [
-            response
-            for response in self.client.text_generation(
-                "test", max_new_tokens=1, stream=True, details=True
-            )
+            response for response in self.client.text_generation("test", max_new_tokens=1, stream=True, details=True)
         ]
 
         assert len(responses) == 1
@@ -123,10 +116,7 @@ class TestTextGenerationClientVCR(unitte
 
     def test_generate_stream_with_details(self):
         responses = [
-            response
-            for response in self.client.text_generation(
-                "test", max_new_tokens=1, stream=True, details=True
-            )
+            response for response in self.client.text_generation("test", max_new_tokens=1, stream=True, details=True)
         ]
 
         assert len(responses) == 1
@@ -144,28 +134,20 @@ class TestTextGenerationClientVCR(unitte
 
         # Watermark is ignored (+ warning)
         with self.assertWarns(UserWarning):
-            self.client.text_generation(
-                "4 5 6", model="gpt2", max_new_tokens=10, watermark=True
-            )
+            self.client.text_generation("4 5 6", model="gpt2", max_new_tokens=10, watermark=True)
 
         # Return as detail even if details=True (+ warning)
         with self.assertWarns(UserWarning):
-            text = self.client.text_generation(
-                "0 1 2", model="gpt2", max_new_tokens=10, details=True
-            )
+            text = self.client.text_generation("0 1 2", model="gpt2", max_new_tokens=10, details=True)
             self.assertIsInstance(text, str)
 
         # Return as stream raises error
         with self.assertRaises(ValueError):
-            self.client.text_generation(
-                "0 1 2", model="gpt2", max_new_tokens=10, stream=True
-            )
+            self.client.text_generation("0 1 2", model="gpt2", max_new_tokens=10, stream=True)
 
     def test_generate_non_tgi_endpoint_regression_test(self):
         # Regression test for https://github.com/huggingface/huggingface_hub/issues/2135
-        with self.assertWarnsRegex(
-            UserWarning, "Ignoring following parameters: return_full_text"
-        ):
+        with self.assertWarnsRegex(UserWarning, "Ignoring following parameters: return_full_text"):
             text = self.client.text_generation(
                 prompt="How are you today?",
                 max_new_tokens=20,
--- python3-huggingface-hub-0.29.3.orig/tests/test_inference_types.py
+++ python3-huggingface-hub-0.29.3/tests/test_inference_types.py
@@ -114,9 +114,7 @@ def test_all_fields_are_optional():
 def test_normalize_keys():
     # all fields are normalized in the dataclasses (by convention)
     # if server response uses different keys, they will be normalized
-    instance = DummyNestedType.parse_obj(
-        {"ItEm": DUMMY_AS_DICT, "Maybe-Items": [DUMMY_AS_DICT]}
-    )
+    instance = DummyNestedType.parse_obj({"ItEm": DUMMY_AS_DICT, "Maybe-Items": [DUMMY_AS_DICT]})
     assert isinstance(instance.item, DummyType)
     assert isinstance(instance.maybe_items, list)
     assert len(instance.maybe_items) == 1
@@ -128,9 +126,7 @@ def test_optional_are_set_to_none():
         parameters = inspect.signature(_type).parameters
         for parameter in parameters.values():
             if _is_optional(parameter.annotation):
-                assert (
-                    parameter.default is None
-                ), f"Parameter {parameter} of {_type} should be set to None"
+                assert parameter.default is None, f"Parameter {parameter} of {_type} should be set to None"
 
 
 def test_none_inferred():
@@ -151,23 +147,22 @@ def test_other_fields_are_set():
     )
     assert instance.extra == "value"
     assert instance.items[0].another_extra == "value"
+    assert str(instance.items[0]) == "DummyType(foo=42, bar='baz', another_extra='value')"  # extra field always last
     assert (
-        str(instance.items[0]) == "DummyType(foo=42, bar='baz', another_extra='value')"
-    )  # extra field always last
-    assert repr(instance) == (  # works both with __str__ and __repr__
-        "DummyNestedType("
-        "item=DummyType(foo=42, bar='baz'), "
-        "items=[DummyType(foo=42, bar='baz', another_extra='value')], "
-        "maybe_items=None, extra='value'"
-        ")"
+        repr(instance)
+        == (  # works both with __str__ and __repr__
+            "DummyNestedType("
+            "item=DummyType(foo=42, bar='baz'), "
+            "items=[DummyType(foo=42, bar='baz', another_extra='value')], "
+            "maybe_items=None, extra='value'"
+            ")"
+        )
     )
 
 
 def test_other_fields_not_proper_dataclass_fields():
     instance_1 = DummyType.parse_obj({"foo": 42, "bar": "baz", "extra": "value1"})
-    instance_2 = DummyType.parse_obj(
-        {"foo": 42, "bar": "baz", "extra": "value2", "another_extra": "value2.1"}
-    )
+    instance_2 = DummyType.parse_obj({"foo": 42, "bar": "baz", "extra": "value2", "another_extra": "value2.1"})
     assert instance_1.extra == "value1"
     assert instance_2.extra == "value2"
     assert instance_2.another_extra == "value2.1"
--- python3-huggingface-hub-0.29.3.orig/tests/test_init_lazy_loading.py
+++ python3-huggingface-hub-0.29.3/tests/test_init_lazy_loading.py
@@ -30,14 +30,8 @@ class TestHuggingfaceHubInit(unittest.Te
                 # Assert docstring is find. This means autocomplete can also provide
                 # the help section.
                 signature_list = goto_list[0].get_signatures()
-                self.assertEqual(
-                    len(signature_list), 2
-                )  # create_commit has 2 signatures (normal and `run_as_future`)
-                self.assertTrue(
-                    signature_list[0]
-                    .docstring()
-                    .startswith("create_commit(repo_id: str,")
-                )
+                self.assertEqual(len(signature_list), 2)  # create_commit has 2 signatures (normal and `run_as_future`)
+                self.assertTrue(signature_list[0].docstring().startswith("create_commit(repo_id: str,"))
                 break
         else:
             self.fail(
--- python3-huggingface-hub-0.29.3.orig/tests/test_keras_integration.py
+++ python3-huggingface-hub-0.29.3/tests/test_keras_integration.py
@@ -104,11 +104,7 @@ class HubMixinTestKeras(CommonKerasTest)
         # Check a new model's weights are not the same as the reloaded model's weights
         another_model = DummyModel()
         another_model(tf.ones([2, 2]))
-        self.assertFalse(
-            tf.reduce_all(tf.equal(new_model.weights[0], another_model.weights[0]))
-            .numpy()
-            .item()
-        )
+        self.assertFalse(tf.reduce_all(tf.equal(new_model.weights[0], another_model.weights[0])).numpy().item())
 
     def test_abs_path_from_pretrained(self):
         model = DummyModel()
@@ -123,9 +119,7 @@ class HubMixinTestKeras(CommonKerasTest)
         model = DummyModel()
         model(model.dummy_inputs)
 
-        model.push_to_hub(
-            repo_id=repo_id, token=TOKEN, config={"num": 7, "act": "gelu_fast"}
-        )
+        model.push_to_hub(repo_id=repo_id, token=TOKEN, config={"num": 7, "act": "gelu_fast"})
 
         # Test model id exists
         assert self._api.model_info(repo_id).id == repo_id
@@ -209,9 +203,7 @@ class HubKerasSequentialTest(CommonKeras
 
             # Check that there is no "Training Metrics" section in the model card.
             # This was done in an older version.
-            self.assertNotIn(
-                "Training Metrics", (self.cache_dir / "README.md").read_text()
-            )
+            self.assertNotIn("Training Metrics", (self.cache_dir / "README.md").read_text())
 
     def test_save_pretrained_optimizer_state(self):
         model = self.model_init()
@@ -230,11 +222,7 @@ class HubKerasSequentialTest(CommonKeras
         # Check a new model's weights are not the same as the reloaded model's weights
         another_model = DummyModel()
         another_model(tf.ones([2, 2]))
-        self.assertFalse(
-            tf.reduce_all(tf.equal(new_model.weights[0], another_model.weights[0]))
-            .numpy()
-            .item()
-        )
+        self.assertFalse(tf.reduce_all(tf.equal(new_model.weights[0], another_model.weights[0])).numpy().item())
 
     def test_save_pretrained_task_name_deprecation(self):
         model = self.model_init()
@@ -244,9 +232,7 @@ class HubKerasSequentialTest(CommonKeras
             FutureWarning,
             match="`task_name` input argument is deprecated. Pass `tags` instead.",
         ):
-            save_pretrained_keras(
-                model, self.cache_dir, tags=["test"], task_name="test", save_traces=True
-            )
+            save_pretrained_keras(model, self.cache_dir, tags=["test"], task_name="test", save_traces=True)
 
     def test_abs_path_from_pretrained(self):
         model = self.model_init()
@@ -267,9 +253,7 @@ class HubKerasSequentialTest(CommonKeras
         model = self.model_init()
         model = self.model_fit(model)
 
-        push_to_hub_keras(
-            model, repo_id=repo_id, token=TOKEN, api_endpoint=ENDPOINT_STAGING
-        )
+        push_to_hub_keras(model, repo_id=repo_id, token=TOKEN, api_endpoint=ENDPOINT_STAGING)
         assert self._api.model_info(repo_id).id == repo_id
         repo_files = self._api.list_repo_files(repo_id)
         assert "README.md" in repo_files
--- python3-huggingface-hub-0.29.3.orig/tests/test_lfs.py
+++ python3-huggingface-hub-0.29.3/tests/test_lfs.py
@@ -119,9 +119,7 @@ class TestSliceFileObj(unittest.TestCase
             with open(filepath, "rb") as fileobj:
                 prev_pos = fileobj.tell()
                 # Test read
-                with SliceFileObj(
-                    fileobj, seek_from=24, read_limit=18
-                ) as fileobj_slice:
+                with SliceFileObj(fileobj, seek_from=24, read_limit=18) as fileobj_slice:
                     self.assertEqual(fileobj_slice.tell(), 0)
                     self.assertEqual(fileobj_slice.read(), self.content[24:42])
                     self.assertEqual(fileobj_slice.tell(), 18)
@@ -130,9 +128,7 @@ class TestSliceFileObj(unittest.TestCase
 
                 self.assertEqual(fileobj.tell(), prev_pos)
 
-                with SliceFileObj(
-                    fileobj, seek_from=0, read_limit=990
-                ) as fileobj_slice:
+                with SliceFileObj(fileobj, seek_from=0, read_limit=990) as fileobj_slice:
                     self.assertEqual(fileobj_slice.tell(), 0)
                     self.assertEqual(fileobj_slice.read(200), self.content[0:200])
                     self.assertEqual(fileobj_slice.read(500), self.content[200:700])
@@ -141,9 +137,7 @@ class TestSliceFileObj(unittest.TestCase
                     self.assertEqual(fileobj_slice.read(200), b"")
 
                 # Test seek with whence = os.SEEK_SET
-                with SliceFileObj(
-                    fileobj, seek_from=100, read_limit=100
-                ) as fileobj_slice:
+                with SliceFileObj(fileobj, seek_from=100, read_limit=100) as fileobj_slice:
                     self.assertEqual(fileobj_slice.tell(), 0)
                     fileobj_slice.seek(2, os.SEEK_SET)
                     self.assertEqual(fileobj_slice.tell(), 2)
@@ -156,9 +150,7 @@ class TestSliceFileObj(unittest.TestCase
                     self.assertEqual(fileobj_slice.fileobj.tell(), 200)
 
                 # Test seek with whence = os.SEEK_CUR
-                with SliceFileObj(
-                    fileobj, seek_from=100, read_limit=100
-                ) as fileobj_slice:
+                with SliceFileObj(fileobj, seek_from=100, read_limit=100) as fileobj_slice:
                     self.assertEqual(fileobj_slice.tell(), 0)
                     fileobj_slice.seek(-5, os.SEEK_CUR)
                     self.assertEqual(fileobj_slice.tell(), 0)
@@ -174,9 +166,7 @@ class TestSliceFileObj(unittest.TestCase
                     self.assertEqual(fileobj_slice.fileobj.tell(), 100)
 
                 # Test seek with whence = os.SEEK_END
-                with SliceFileObj(
-                    fileobj, seek_from=100, read_limit=100
-                ) as fileobj_slice:
+                with SliceFileObj(fileobj, seek_from=100, read_limit=100) as fileobj_slice:
                     self.assertEqual(fileobj_slice.tell(), 0)
                     fileobj_slice.seek(-5, os.SEEK_END)
                     self.assertEqual(fileobj_slice.tell(), 95)
--- python3-huggingface-hub-0.29.3.orig/tests/test_local_folder.py
+++ python3-huggingface-hub-0.29.3/tests/test_local_folder.py
@@ -53,9 +53,7 @@ def test_creates_huggingface_dir_with_gi
 def test_gitignore_lock_timeout_is_ignored(tmp_path: Path):
     local_dir = tmp_path / "path" / "to" / "local"
 
-    threads = [
-        threading.Thread(target=_huggingface_dir, args=(local_dir,)) for _ in range(10)
-    ]
+    threads = [threading.Thread(target=_huggingface_dir, args=(local_dir,)) for _ in range(10)]
     for thread in threads:
         thread.start()
     for thread in threads:
@@ -72,25 +70,9 @@ def test_local_download_paths(tmp_path:
     assert isinstance(paths, LocalDownloadFilePaths)
     assert paths.file_path == tmp_path / "path" / "in" / "repo.txt"
     assert (
-        paths.metadata_path
-        == tmp_path
-        / ".cache"
-        / "huggingface"
-        / "download"
-        / "path"
-        / "in"
-        / "repo.txt.metadata"
-    )
-    assert (
-        paths.lock_path
-        == tmp_path
-        / ".cache"
-        / "huggingface"
-        / "download"
-        / "path"
-        / "in"
-        / "repo.txt.lock"
+        paths.metadata_path == tmp_path / ".cache" / "huggingface" / "download" / "path" / "in" / "repo.txt.metadata"
     )
+    assert paths.lock_path == tmp_path / ".cache" / "huggingface" / "download" / "path" / "in" / "repo.txt.lock"
 
     # Paths are usable (parent directories have been created)
     assert paths.file_path.parent.is_dir()
@@ -99,19 +81,14 @@ def test_local_download_paths(tmp_path:
 
     # Incomplete paths are etag-based
     incomplete_path = paths.incomplete_path("etag123")
-    assert (
-        incomplete_path.parent
-        == tmp_path / ".cache" / "huggingface" / "download" / "path" / "in"
-    )
+    assert incomplete_path.parent == tmp_path / ".cache" / "huggingface" / "download" / "path" / "in"
     assert incomplete_path.name.endswith(".etag123.incomplete")
     assert paths.incomplete_path("etag123").parent.is_dir()
 
     # Incomplete paths are unique per file per etag
     other_paths = get_local_download_paths(tmp_path, "path/in/repo_other.txt")
     other_incomplete_path = other_paths.incomplete_path("etag123")
-    assert (
-        incomplete_path != other_incomplete_path
-    )  # different .incomplete files to prevent concurrency issues
+    assert incomplete_path != other_incomplete_path  # different .incomplete files to prevent concurrency issues
 
 
 def test_local_download_paths_are_recreated_each_time(tmp_path: Path):
@@ -147,12 +124,8 @@ def test_local_download_paths_long_paths
 def test_write_download_metadata(tmp_path: Path):
     """Test download metadata content is valid."""
     # Write metadata
-    write_download_metadata(
-        tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789"
-    )
-    metadata_path = (
-        tmp_path / ".cache" / "huggingface" / "download" / "file.txt.metadata"
-    )
+    write_download_metadata(tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789")
+    metadata_path = tmp_path / ".cache" / "huggingface" / "download" / "file.txt.metadata"
     assert metadata_path.exists()
 
     # Metadata is valid
@@ -161,16 +134,12 @@ def test_write_download_metadata(tmp_pat
         assert f.readline() == "123456789\n"
         timestamp = float(f.readline().strip())
     assert timestamp <= time.time()  # in the past
-    assert (
-        timestamp >= time.time() - 1
-    )  # but less than 1 seconds ago (we're not that slow)
+    assert timestamp >= time.time() - 1  # but less than 1 seconds ago (we're not that slow)
 
     time.sleep(0.2)  # for deterministic tests
 
     # Overwriting works as expected
-    write_download_metadata(
-        tmp_path, filename="file.txt", commit_hash="commit_hash2", etag="987654321"
-    )
+    write_download_metadata(tmp_path, filename="file.txt", commit_hash="commit_hash2", etag="987654321")
     with metadata_path.open() as f:
         assert f.readline() == "commit_hash2\n"
         assert f.readline() == "987654321\n"
@@ -182,9 +151,7 @@ def test_read_download_metadata_valid_me
     """Test reading download metadata when metadata is valid."""
     # Create file + write correct metadata
     (tmp_path / "file.txt").write_text("content")
-    write_download_metadata(
-        tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789"
-    )
+    write_download_metadata(tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789")
 
     # Read metadata
     metadata = read_download_metadata(tmp_path, filename="file.txt")
@@ -201,14 +168,10 @@ def test_read_download_metadata_no_metad
     assert read_download_metadata(tmp_path, filename="file.txt") is None
 
 
-def test_read_download_metadata_corrupted_metadata(
-    tmp_path: Path, caplog: pytest.LogCaptureFixture
-):
+def test_read_download_metadata_corrupted_metadata(tmp_path: Path, caplog: pytest.LogCaptureFixture):
     """Test reading download metadata when metadata is corrupted."""
     # Write corrupted metadata
-    metadata_path = (
-        tmp_path / ".cache" / "huggingface" / "download" / "file.txt.metadata"
-    )
+    metadata_path = tmp_path / ".cache" / "huggingface" / "download" / "file.txt.metadata"
     metadata_path.parent.mkdir(parents=True, exist_ok=True)
     metadata_path.write_text("invalid content")
 
@@ -222,9 +185,7 @@ def test_read_download_metadata_corrupte
 def test_read_download_metadata_correct_metadata_missing_file(tmp_path: Path):
     """Test reading download metadata when metadata is correct but file is missing."""
     # Write correct metadata
-    write_download_metadata(
-        tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789"
-    )
+    write_download_metadata(tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789")
 
     # File missing => return None
     assert read_download_metadata(tmp_path, filename="file.txt") is None
@@ -233,9 +194,7 @@ def test_read_download_metadata_correct_
 def test_read_download_metadata_correct_metadata_but_outdated(tmp_path: Path):
     """Test reading download metadata when metadata is correct but outdated."""
     # Write correct metadata
-    write_download_metadata(
-        tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789"
-    )
+    write_download_metadata(tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789")
     time.sleep(2)  # We allow for a 1s difference in practice, so let's wait a bit
 
     # File is outdated => return None
@@ -250,26 +209,8 @@ def test_local_upload_paths(tmp_path: Pa
     # Correct paths (also sanitized on windows)
     assert isinstance(paths, LocalUploadFilePaths)
     assert paths.file_path == tmp_path / "path" / "in" / "repo.txt"
-    assert (
-        paths.metadata_path
-        == tmp_path
-        / ".cache"
-        / "huggingface"
-        / "upload"
-        / "path"
-        / "in"
-        / "repo.txt.metadata"
-    )
-    assert (
-        paths.lock_path
-        == tmp_path
-        / ".cache"
-        / "huggingface"
-        / "upload"
-        / "path"
-        / "in"
-        / "repo.txt.lock"
-    )
+    assert paths.metadata_path == tmp_path / ".cache" / "huggingface" / "upload" / "path" / "in" / "repo.txt.metadata"
+    assert paths.lock_path == tmp_path / ".cache" / "huggingface" / "upload" / "path" / "in" / "repo.txt.lock"
 
     # Paths are usable (parent directories have been created)
     assert paths.file_path.parent.is_dir()
--- python3-huggingface-hub-0.29.3.orig/tests/test_login_utils.py
+++ python3-huggingface-hub-0.29.3/tests/test_login_utils.py
@@ -12,9 +12,7 @@ class TestSetGlobalStore(unittest.TestCa
     def setUp(self) -> None:
         """Get current global config value."""
         try:
-            self.previous_config = run_subprocess(
-                "git config --global credential.helper"
-            ).stdout
+            self.previous_config = run_subprocess("git config --global credential.helper").stdout
         except subprocess.CalledProcessError:
             self.previous_config = None  # Means global credential.helper value not set
 
@@ -25,9 +23,7 @@ class TestSetGlobalStore(unittest.TestCa
         if self.previous_config is None:
             run_subprocess("git config --global --unset credential.helper")
         else:
-            run_subprocess(
-                f"git config --global credential.helper {self.previous_config}"
-            )
+            run_subprocess(f"git config --global credential.helper {self.previous_config}")
 
     def test_set_store_as_git_credential_helper_globally(self) -> None:
         """Test `_set_store_as_git_credential_helper_globally` works as expected.
--- python3-huggingface-hub-0.29.3.orig/tests/test_repocard.py
+++ python3-huggingface-hub-0.29.3/tests/test_repocard.py
@@ -297,9 +297,7 @@ class RepocardMetadataUpdateTest(unittes
             repo_id=self.repo_id,
             path_in_repo=constants.REPOCARD_NAME,
         )
-        self.existing_metadata = yaml.safe_load(
-            DUMMY_MODELCARD_EVAL_RESULT.strip().strip("-")
-        )
+        self.existing_metadata = yaml.safe_load(DUMMY_MODELCARD_EVAL_RESULT.strip().strip("-"))
 
     def tearDown(self) -> None:
         self.api.delete_repo(repo_id=self.repo_id)
@@ -319,9 +317,7 @@ class RepocardMetadataUpdateTest(unittes
 
     def test_update_existing_result_with_overwrite(self):
         new_metadata = copy.deepcopy(self.existing_metadata)
-        new_metadata["model-index"][0]["results"][0]["metrics"][0][
-            "value"
-        ] = 0.2862102282047272
+        new_metadata["model-index"][0]["results"][0]["metrics"][0]["value"] = 0.2862102282047272
         metadata_update(self.repo_id, new_metadata, token=self.token, overwrite=True)
 
         updated_metadata = metadata_load(self._get_remote_card())
@@ -333,9 +329,7 @@ class RepocardMetadataUpdateTest(unittes
         Regression test for https://github.com/huggingface/huggingface_hub/issues/1210
         """
         new_metadata = copy.deepcopy(self.existing_metadata)
-        new_metadata["model-index"][0]["results"][0]["metrics"][0][
-            "verifyToken"
-        ] = "1234"
+        new_metadata["model-index"][0]["results"][0]["metrics"][0]["verifyToken"] = "1234"
         metadata_update(self.repo_id, new_metadata, token=self.token, overwrite=True)
 
         updated_metadata = metadata_load(self._get_remote_card())
@@ -354,9 +348,7 @@ class RepocardMetadataUpdateTest(unittes
 
     def test_update_existing_result_without_overwrite(self):
         new_metadata = copy.deepcopy(self.existing_metadata)
-        new_metadata["model-index"][0]["results"][0]["metrics"][0][
-            "value"
-        ] = 0.2862102282047272
+        new_metadata["model-index"][0]["results"][0]["metrics"][0]["value"] = 0.2862102282047272
 
         with pytest.raises(
             ValueError,
@@ -365,9 +357,7 @@ class RepocardMetadataUpdateTest(unittes
                 " accuracy'. Set `overwrite=True` to overwrite existing metrics."
             ),
         ):
-            metadata_update(
-                self.repo_id, new_metadata, token=self.token, overwrite=False
-            )
+            metadata_update(self.repo_id, new_metadata, token=self.token, overwrite=False)
 
     def test_update_existing_field_without_overwrite(self):
         new_datasets_data = {"datasets": ["Open-Orca/OpenOrca"]}
@@ -381,9 +371,7 @@ class RepocardMetadataUpdateTest(unittes
             ),
         ):
             new_datasets_data = {"datasets": ["HuggingFaceH4/no_robots"]}
-            metadata_update(
-                self.repo_id, new_datasets_data, token=self.token, overwrite=False
-            )
+            metadata_update(self.repo_id, new_datasets_data, token=self.token, overwrite=False)
 
     def test_update_new_result_existing_dataset(self):
         new_result = metadata_eval_result(
@@ -430,9 +418,7 @@ class RepocardMetadataUpdateTest(unittes
         metadata_update(self.repo_id, new_result, token=self.token, overwrite=False)
 
         expected_metadata = copy.deepcopy(self.existing_metadata)
-        expected_metadata["model-index"][0]["results"].append(
-            new_result["model-index"][0]["results"][0]
-        )
+        expected_metadata["model-index"][0]["results"].append(new_result["model-index"][0]["results"][0])
 
         updated_metadata = metadata_load(self._get_remote_card())
         self.assertDictEqual(updated_metadata, expected_metadata)
@@ -458,15 +444,11 @@ class RepocardMetadataUpdateTest(unittes
     def test_update_with_existing_name(self):
         new_metadata = copy.deepcopy(self.existing_metadata)
         new_metadata["model-index"][0].pop("name")
-        new_metadata["model-index"][0]["results"][0]["metrics"][0][
-            "value"
-        ] = 0.2862102282047272
+        new_metadata["model-index"][0]["results"][0]["metrics"][0]["value"] = 0.2862102282047272
         metadata_update(self.repo_id, new_metadata, token=self.token, overwrite=True)
 
         card_data = ModelCard.load(self.repo_id)
-        self.assertEqual(
-            card_data.data.model_name, self.existing_metadata["model-index"][0]["name"]
-        )
+        self.assertEqual(card_data.data.model_name, self.existing_metadata["model-index"][0]["name"])
 
     def test_update_without_existing_name(self):
         # delete existing metadata
@@ -496,9 +478,7 @@ class RepocardMetadataUpdateTest(unittes
         )
         card = ModelCard.load(self.repo_id)
         metadata = card.data.to_dict()
-        metadata_update(
-            self.repo_id, metadata=metadata, overwrite=True, token=self.token
-        )
+        metadata_update(self.repo_id, metadata=metadata, overwrite=True, token=self.token)
 
         new_card = ModelCard.load(self.repo_id)
         self.assertEqual(len(new_card.data.eval_results), 2)
@@ -599,9 +579,7 @@ class RepoCardTest(TestCaseWithHfApi):
             card.save(updated_card_path)
 
             updated_card = RepoCard.load(updated_card_path)
-            self.assertEqual(
-                updated_card.data.language, ["fr"], "Card data not updated properly"
-            )
+            self.assertEqual(updated_card.data.language, ["fr"], "Card data not updated properly")
 
     @require_jinja
     def test_repo_card_from_default_template(self):
@@ -672,9 +650,7 @@ class RepoCardTest(TestCaseWithHfApi):
 
     def test_repo_card_data_must_be_dict(self):
         sample_path = SAMPLE_CARDS_DIR / "sample_invalid_card_data.md"
-        with pytest.raises(
-            ValueError, match="repo card metadata block should be a dict"
-        ):
+        with pytest.raises(ValueError, match="repo card metadata block should be a dict"):
             RepoCard(sample_path.read_text())
 
     def test_repo_card_without_metadata(self):
@@ -684,8 +660,7 @@ class RepoCardTest(TestCaseWithHfApi):
             card = RepoCard(sample_path.read_text())
         self.assertTrue(
             any(
-                "Repo card metadata block was not found. Setting CardData to empty."
-                in log
+                "Repo card metadata block was not found. Setting CardData to empty." in log
                 for log in warning_logs.output
             )
         )
@@ -806,10 +781,7 @@ class ModelCardTest(TestCaseWithHfApi):
         with self.assertLogs("huggingface_hub", level="WARNING") as warning_logs:
             card = ModelCard.load(sample_path, ignore_metadata_errors=True)
         self.assertTrue(
-            any(
-                "Invalid model-index. Not loading eval results into CardData." in log
-                for log in warning_logs.output
-            )
+            any("Invalid model-index. Not loading eval results into CardData." in log for log in warning_logs.output)
         )
         self.assertIsNone(card.data.eval_results)
 
@@ -892,9 +864,7 @@ class ModelCardTest(TestCaseWithHfApi):
         self.assertIsInstance(card, ModelCard)
         self.assertTrue(card.text.endswith("asdf"))
         self.assertTrue(card.data.to_dict().get("eval_results") is None)
-        self.assertEqual(
-            str(card)[: len(DUMMY_MODELCARD_EVAL_RESULT)], DUMMY_MODELCARD_EVAL_RESULT
-        )
+        self.assertEqual(str(card)[: len(DUMMY_MODELCARD_EVAL_RESULT)], DUMMY_MODELCARD_EVAL_RESULT)
 
     def test_preserve_order_load_save(self):
         model_card = ModelCard(DUMMY_MODELCARD)
@@ -946,9 +916,7 @@ class DatasetCardTest(TestCaseWithHfApi)
 
         # Here we pass the card data as kwargs as well so template picks up pretty_name.
         card = DatasetCard.from_template(card_data, **card_data.to_dict())
-        self.assertTrue(
-            card.text.strip().startswith("# Dataset Card for My Cool Dataset")
-        )
+        self.assertTrue(card.text.strip().startswith("# Dataset Card for My Cool Dataset"))
 
         self.assertIsInstance(card, DatasetCard)
 
@@ -970,18 +938,14 @@ class DatasetCardTest(TestCaseWithHfApi)
                 "in the dataset card template are working."
             ),
         )
-        self.assertTrue(
-            card.text.strip().startswith("# Dataset Card for My Cool Dataset")
-        )
+        self.assertTrue(card.text.strip().startswith("# Dataset Card for My Cool Dataset"))
         self.assertIsInstance(card, DatasetCard)
 
         matches = re.findall(
             r"Repository:\*\* https://github\.com/huggingface/huggingface_hub",
             str(card),
         )
-        self.assertEqual(
-            matches[0], "Repository:** https://github.com/huggingface/huggingface_hub"
-        )
+        self.assertEqual(matches[0], "Repository:** https://github.com/huggingface/huggingface_hub")
 
     @require_jinja
     def test_dataset_card_from_custom_template(self):
--- python3-huggingface-hub-0.29.3.orig/tests/test_repocard_data.py
+++ python3-huggingface-hub-0.29.3/tests/test_repocard_data.py
@@ -14,9 +14,7 @@ from huggingface_hub.repocard_data impor
 )
 
 
-OPEN_LLM_LEADERBOARD_URL = (
-    "https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard"
-)
+OPEN_LLM_LEADERBOARD_URL = "https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard"
 DUMMY_METADATA_WITH_MODEL_INDEX = """
 language: en
 license: mit
@@ -168,9 +166,7 @@ class ModelCardDataTest(unittest.TestCas
         self.assertEqual(eval_results[2].source_url, OPEN_LLM_LEADERBOARD_URL)
 
     def test_card_data_requires_model_name_for_eval_results(self):
-        with pytest.raises(
-            ValueError, match="`eval_results` requires `model_name` to be set."
-        ):
+        with pytest.raises(ValueError, match="`eval_results` requires `model_name` to be set."):
             ModelCardData(
                 eval_results=[
                     EvalResult(
@@ -199,9 +195,7 @@ class ModelCardDataTest(unittest.TestCas
         model_index = eval_results_to_model_index(data.model_name, data.eval_results)
 
         self.assertEqual(model_index[0]["name"], "my-cool-model")
-        self.assertEqual(
-            model_index[0]["results"][0]["task"]["type"], "image-classification"
-        )
+        self.assertEqual(model_index[0]["results"][0]["task"]["type"], "image-classification")
 
     def test_arbitrary_incoming_card_data(self):
         data = ModelCardData(
@@ -250,9 +244,7 @@ class ModelCardDataTest(unittest.TestCas
         assert data.tags == ["tag2", "tag1", "tag3"]
 
     def test_remove_top_level_none_values(self):
-        as_obj = ModelCardData(
-            tags=["tag1", None], foo={"bar": 3, "baz": None}, pipeline_tag=None
-        )
+        as_obj = ModelCardData(tags=["tag1", None], foo={"bar": 3, "baz": None}, pipeline_tag=None)
         as_dict = as_obj.to_dict()
 
         assert as_obj.tags == ["tag1", None]
--- python3-huggingface-hub-0.29.3.orig/tests/test_repository.py
+++ python3-huggingface-hub-0.29.3/tests/test_repository.py
@@ -107,9 +107,7 @@ class TestRepositoryShared(RepositoryTes
     @expect_deprecation("Repository")
     def test_clone_from_repo_name_no_namespace_fails(self):
         with self.assertRaises(EnvironmentError):
-            Repository(
-                self.repo_path, clone_from=self.repo_id.split("/")[1], token=TOKEN
-            )
+            Repository(self.repo_path, clone_from=self.repo_id.split("/")[1], token=TOKEN)
 
     @expect_deprecation("Repository")
     def test_clone_from_not_hf_url(self):
@@ -197,9 +195,7 @@ class TestRepositoryShared(RepositoryTes
     def test_push_errors_on_wrong_checkout(self):
         repo = Repository(self.repo_path, clone_from=self.repo_id)
 
-        head_commit_ref = run_subprocess(
-            "git show --oneline -s", folder=self.repo_path
-        ).stdout.split()[0]
+        head_commit_ref = run_subprocess("git show --oneline -s", folder=self.repo_path).stdout.split()[0]
 
         repo.git_checkout(head_commit_ref)
 
@@ -372,9 +368,7 @@ class TestRepositoryUniqueRepos(Reposito
 
     def test_clone_skip_lfs_files(self):
         # Upload LFS file
-        self._api.upload_file(
-            path_or_fileobj=b"Bin file", path_in_repo="file.bin", repo_id=self.repo_id
-        )
+        self._api.upload_file(path_or_fileobj=b"Bin file", path_in_repo="file.bin", repo_id=self.repo_id)
 
         repo = self.clone_repo(skip_lfs_files=True)
         file_bin = self.repo_path / "file.bin"
@@ -520,13 +514,9 @@ class TestRepositoryUniqueRepos(Reposito
                 f.write("Random string 2")
 
         root_directory = self.repo_path / ".git" / "lfs"
-        git_lfs_files_size = sum(
-            f.stat().st_size for f in root_directory.glob("**/*") if f.is_file()
-        )
+        git_lfs_files_size = sum(f.stat().st_size for f in root_directory.glob("**/*") if f.is_file())
         repo.lfs_prune()
-        post_prune_git_lfs_files_size = sum(
-            f.stat().st_size for f in root_directory.glob("**/*") if f.is_file()
-        )
+        post_prune_git_lfs_files_size = sum(f.stat().st_size for f in root_directory.glob("**/*") if f.is_file())
 
         # Size of the directory holding LFS files was reduced
         self.assertLess(post_prune_git_lfs_files_size, git_lfs_files_size)
@@ -538,9 +528,7 @@ class TestRepositoryUniqueRepos(Reposito
                 f.write("Random string 1")
 
         root_directory = self.repo_path / ".git" / "lfs"
-        git_lfs_files_size = sum(
-            f.stat().st_size for f in root_directory.glob("**/*") if f.is_file()
-        )
+        git_lfs_files_size = sum(f.stat().st_size for f in root_directory.glob("**/*") if f.is_file())
 
         with open(os.path.join(repo.local_dir, "file.bin"), "w+") as f:
             f.write("Random string 2")
@@ -549,9 +537,7 @@ class TestRepositoryUniqueRepos(Reposito
         repo.git_commit("New commit")
         repo.git_push(auto_lfs_prune=True)
 
-        post_prune_git_lfs_files_size = sum(
-            f.stat().st_size for f in root_directory.glob("**/*") if f.is_file()
-        )
+        post_prune_git_lfs_files_size = sum(f.stat().st_size for f in root_directory.glob("**/*") if f.is_file())
 
         # Size of the directory holding LFS files is the exact same
         self.assertEqual(post_prune_git_lfs_files_size, git_lfs_files_size)
@@ -783,9 +769,7 @@ class TestRepositoryOffline(RepositoryTe
         self.assertFalse(is_tracked_upstream(self.repo.local_dir))
 
     def test_no_branch_checked_out_raises(self):
-        head_commit_ref = run_subprocess(
-            "git show --oneline -s", folder=self.repo_path
-        ).stdout.split()[0]
+        head_commit_ref = run_subprocess("git show --oneline -s", folder=self.repo_path).stdout.split()[0]
 
         self.repo.git_checkout(head_commit_ref)
         self.assertRaises(OSError, is_tracked_upstream, self.repo.local_dir)
--- python3-huggingface-hub-0.29.3.orig/tests/test_serialization.py
+++ python3-huggingface-hub-0.29.3/tests/test_serialization.py
@@ -83,21 +83,11 @@ def dummy_model():
 
             def __init__(self):
                 super().__init__()
-                self.register_parameter(
-                    "layer_1", torch.nn.Parameter(torch.tensor([4.0]))
-                )
-                self.register_parameter(
-                    "layer_2", torch.nn.Parameter(torch.tensor([10.0]))
-                )
-                self.register_parameter(
-                    "layer_3", torch.nn.Parameter(torch.tensor([30.0]))
-                )
-                self.register_parameter(
-                    "layer_4", torch.nn.Parameter(torch.tensor([2.0]))
-                )
-                self.register_parameter(
-                    "layer_5", torch.nn.Parameter(torch.tensor([2.0]))
-                )
+                self.register_parameter("layer_1", torch.nn.Parameter(torch.tensor([4.0])))
+                self.register_parameter("layer_2", torch.nn.Parameter(torch.tensor([10.0])))
+                self.register_parameter("layer_3", torch.nn.Parameter(torch.tensor([30.0])))
+                self.register_parameter("layer_4", torch.nn.Parameter(torch.tensor([2.0])))
+                self.register_parameter("layer_5", torch.nn.Parameter(torch.tensor([2.0])))
 
         return DummyModel()
     except ImportError:
@@ -224,9 +214,7 @@ def test_tensor_same_storage():
             "layer_4": [2],
             "layer_5": [1],
         },
-        get_storage_id=lambda x: (
-            x[0]
-        ),  # dummy for test: storage id based on first element
+        get_storage_id=lambda x: (x[0]),  # dummy for test: storage id based on first element
         get_storage_size=_dummy_get_storage_size,
         max_shard_size=1,
         filename_pattern="model{suffix}.safetensors",
@@ -258,20 +246,12 @@ def test_get_tf_storage_size():
 def test_get_torch_storage_size():
     import torch  # type: ignore[import]
 
-    assert (
-        get_torch_storage_size(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float64))
-        == 5 * 8
-    )
-    assert (
-        get_torch_storage_size(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float16))
-        == 5 * 2
-    )
+    assert get_torch_storage_size(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float64)) == 5 * 8
+    assert get_torch_storage_size(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float16)) == 5 * 2
 
 
 @requires("torch")
-@pytest.mark.skipif(
-    not is_wrapper_tensor_subclass_available(), reason="requires torch 2.1 or higher"
-)
+@pytest.mark.skipif(not is_wrapper_tensor_subclass_available(), reason="requires torch 2.1 or higher")
 def test_get_torch_storage_size_wrapper_tensor_subclass():
     import torch  # type: ignore[import]
     from torch.testing._internal.two_tensor import TwoTensor  # type: ignore[import]
@@ -299,9 +279,7 @@ def test_parse_size_to_int():
 def test_save_torch_model(mocker: MockerFixture, tmp_path: Path) -> None:
     """Test `save_torch_model` is only a wrapper around `save_torch_state_dict`."""
     model_mock = Mock()
-    safe_state_dict_mock = mocker.patch(
-        "huggingface_hub.serialization._torch.save_torch_state_dict"
-    )
+    safe_state_dict_mock = mocker.patch("huggingface_hub.serialization._torch.save_torch_state_dict")
     save_torch_model(
         model_mock,
         save_directory=tmp_path,
@@ -326,18 +304,14 @@ def test_save_torch_model(mocker: Mocker
     )
 
 
-def test_save_torch_state_dict_not_sharded(
-    tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]
-) -> None:
+def test_save_torch_state_dict_not_sharded(tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]) -> None:
     """Save as safetensors without sharding."""
     save_torch_state_dict(torch_state_dict, tmp_path, max_shard_size="1GB")
     assert (tmp_path / "model.safetensors").is_file()
     assert not (tmp_path / "model.safetensors.index.json").is_file()
 
 
-def test_save_torch_state_dict_sharded(
-    tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]
-) -> None:
+def test_save_torch_state_dict_sharded(tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]) -> None:
     """Save as safetensors with sharding."""
     save_torch_state_dict(torch_state_dict, tmp_path, max_shard_size=30)
     assert not (tmp_path / "model.safetensors").is_file()
@@ -345,9 +319,7 @@ def test_save_torch_state_dict_sharded(
     assert (tmp_path / "model-00001-of-00002.safetensors").is_file()
     assert (tmp_path / "model-00001-of-00002.safetensors").is_file()
 
-    assert json.loads(
-        (tmp_path / "model.safetensors.index.json").read_text("utf-8")
-    ) == {
+    assert json.loads((tmp_path / "model.safetensors.index.json").read_text("utf-8")) == {
         "metadata": {"total_size": 40},
         "weight_map": {
             "layer_1": "model-00001-of-00002.safetensors",
@@ -366,18 +338,14 @@ def test_save_torch_state_dict_unsafe_no
 ) -> None:
     """Save as pickle without sharding."""
     with caplog.at_level("WARNING"):
-        save_torch_state_dict(
-            torch_state_dict, tmp_path, max_shard_size="1GB", safe_serialization=False
-        )
+        save_torch_state_dict(torch_state_dict, tmp_path, max_shard_size="1GB", safe_serialization=False)
     assert "we strongly recommend using safe serialization" in caplog.text
 
     assert (tmp_path / "pytorch_model.bin").is_file()
     assert not (tmp_path / "pytorch_model.bin.index.json").is_file()
 
 
-@pytest.mark.skipif(
-    not is_wrapper_tensor_subclass_available(), reason="requires torch 2.1 or higher"
-)
+@pytest.mark.skipif(not is_wrapper_tensor_subclass_available(), reason="requires torch 2.1 or higher")
 def test_save_torch_state_dict_tensor_subclass_unsafe_not_sharded(
     tmp_path: Path,
     caplog: pytest.LogCaptureFixture,
@@ -397,9 +365,7 @@ def test_save_torch_state_dict_tensor_su
     assert not (tmp_path / "pytorch_model.bin.index.json").is_file()
 
 
-@pytest.mark.skipif(
-    not is_wrapper_tensor_subclass_available(), reason="requires torch 2.1 or higher"
-)
+@pytest.mark.skipif(not is_wrapper_tensor_subclass_available(), reason="requires torch 2.1 or higher")
 def test_save_torch_state_dict_shared_layers_tensor_subclass_unsafe_not_sharded(
     tmp_path: Path,
     caplog: pytest.LogCaptureFixture,
@@ -427,9 +393,7 @@ def test_save_torch_state_dict_unsafe_sh
     """Save as pickle with sharding."""
     # Check logs
     with caplog.at_level("WARNING"):
-        save_torch_state_dict(
-            torch_state_dict, tmp_path, max_shard_size=30, safe_serialization=False
-        )
+        save_torch_state_dict(torch_state_dict, tmp_path, max_shard_size=30, safe_serialization=False)
     assert "we strongly recommend using safe serialization" in caplog.text
 
     assert not (tmp_path / "pytorch_model.bin").is_file()
@@ -437,9 +401,7 @@ def test_save_torch_state_dict_unsafe_sh
     assert (tmp_path / "pytorch_model-00001-of-00002.bin").is_file()
     assert (tmp_path / "pytorch_model-00001-of-00002.bin").is_file()
 
-    assert json.loads(
-        (tmp_path / "pytorch_model.bin.index.json").read_text("utf-8")
-    ) == {
+    assert json.loads((tmp_path / "pytorch_model.bin.index.json").read_text("utf-8")) == {
         "metadata": {"total_size": 40},
         "weight_map": {
             "layer_1": "pytorch_model-00001-of-00002.bin",
@@ -456,9 +418,7 @@ def test_save_torch_state_dict_shared_la
 ) -> None:
     from safetensors.torch import load_file
 
-    save_torch_state_dict(
-        torch_state_dict_shared_layers, tmp_path, safe_serialization=True
-    )
+    save_torch_state_dict(torch_state_dict_shared_layers, tmp_path, safe_serialization=True)
     safetensors_file = tmp_path / "model.safetensors"
     assert safetensors_file.is_file()
 
@@ -561,14 +521,10 @@ def test_split_torch_state_dict_into_sha
     assert state_dict_split.is_sharded
 
 
-def test_save_torch_state_dict_custom_filename(
-    tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]
-) -> None:
+def test_save_torch_state_dict_custom_filename(tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]) -> None:
     """Custom filename pattern is respected."""
     # Not sharded
-    save_torch_state_dict(
-        torch_state_dict, tmp_path, filename_pattern="model.variant{suffix}.safetensors"
-    )
+    save_torch_state_dict(torch_state_dict, tmp_path, filename_pattern="model.variant{suffix}.safetensors")
     assert (tmp_path / "model.variant.safetensors").is_file()
 
     # Sharded
@@ -641,9 +597,7 @@ def test_save_torch_state_dict_not_main_
 
 
 @requires("torch")
-def test_load_state_dict_from_file(
-    tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]
-):
+def test_load_state_dict_from_file(tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]):
     """Test saving and loading a state dict with both safetensors and pickle formats."""
     import torch  # type: ignore[import]
 
@@ -730,17 +684,13 @@ def test_load_state_dict_missing_file(sa
 
 def test_load_torch_model_directory_does_not_exist():
     """Test proper error handling when directory does not contain a valid checkpoint."""
-    with pytest.raises(
-        ValueError, match="Checkpoint path does_not_exist does not exist"
-    ):
+    with pytest.raises(ValueError, match="Checkpoint path does_not_exist does not exist"):
         load_torch_model(Mock(), "does_not_exist")
 
 
 def test_load_torch_model_directory_does_not_contain_checkpoint(tmp_path):
     """Test proper error handling when directory does not contain a valid checkpoint."""
-    with pytest.raises(
-        ValueError, match=r"Directory .* does not contain a valid checkpoint."
-    ):
+    with pytest.raises(ValueError, match=r"Directory .* does not contain a valid checkpoint."):
         load_torch_model(Mock(), tmp_path)
 
 
@@ -751,9 +701,7 @@ def test_load_torch_model_directory_does
         False,
     ],
 )
-def test_load_sharded_model_strict_mode(
-    tmp_path, torch_state_dict, dummy_model, strict
-):
+def test_load_sharded_model_strict_mode(tmp_path, torch_state_dict, dummy_model, strict):
     """Test loading model with strict mode behavior for both sharded and non-sharded checkpoints."""
 
     import torch
@@ -784,9 +732,7 @@ def test_load_sharded_model_strict_mode(
         assert "extra_key" in result.unexpected_keys
 
 
-def test_load_torch_model_with_filename_pattern(
-    tmp_path, torch_state_dict, dummy_model
-):
+def test_load_torch_model_with_filename_pattern(tmp_path, torch_state_dict, dummy_model):
     """Test loading a model with a custom filename pattern."""
 
     import torch
@@ -873,9 +819,7 @@ def test_load_torch_model_index_selectio
         (tmp_path / filename).touch()
 
     # Mock _load_sharded_checkpoint to capture the safe parameter
-    mock_load = mocker.patch(
-        "huggingface_hub.serialization._torch._load_sharded_checkpoint"
-    )
+    mock_load = mocker.patch("huggingface_hub.serialization._torch._load_sharded_checkpoint")
     load_torch_model(model, tmp_path, safe=safe, filename_pattern=filename_pattern)
     mock_load.assert_called_once()
     assert mock_load.call_args.kwargs["filename_pattern"] == expected_filename_pattern
--- python3-huggingface-hub-0.29.3.orig/tests/test_snapshot_download.py
+++ python3-huggingface-hub-0.29.3/tests/test_snapshot_download.py
@@ -24,9 +24,7 @@ class SnapshotDownloadTests(unittest.Tes
         cls.first_commit_hash = cls.api.create_commit(
             repo_id=cls.repo_id,
             operations=[
-                CommitOperationAdd(
-                    path_in_repo="dummy_file.txt", path_or_fileobj=b"v1"
-                ),
+                CommitOperationAdd(path_in_repo="dummy_file.txt", path_or_fileobj=b"v1"),
                 CommitOperationAdd(
                     path_in_repo="subpath/file.txt",
                     path_or_fileobj=b"content in subpath",
@@ -39,12 +37,8 @@ class SnapshotDownloadTests(unittest.Tes
         cls.second_commit_hash = cls.api.create_commit(
             repo_id=cls.repo_id,
             operations=[
-                CommitOperationAdd(
-                    path_in_repo="dummy_file.txt", path_or_fileobj=b"v2"
-                ),
-                CommitOperationAdd(
-                    path_in_repo="dummy_file_2.txt", path_or_fileobj=b"v3"
-                ),
+                CommitOperationAdd(path_in_repo="dummy_file.txt", path_or_fileobj=b"v2"),
+                CommitOperationAdd(path_in_repo="dummy_file_2.txt", path_or_fileobj=b"v3"),
             ],
             commit_message="Add file to main branch",
         ).oid
@@ -54,9 +48,7 @@ class SnapshotDownloadTests(unittest.Tes
         cls.third_commit_hash = cls.api.create_commit(
             repo_id=cls.repo_id,
             operations=[
-                CommitOperationAdd(
-                    path_in_repo="dummy_file_2.txt", path_or_fileobj=b"v4"
-                ),
+                CommitOperationAdd(path_in_repo="dummy_file_2.txt", path_or_fileobj=b"v4"),
             ],
             commit_message="Add file to other branch",
             revision="other",
@@ -69,9 +61,7 @@ class SnapshotDownloadTests(unittest.Tes
     def test_download_model(self):
         # Test `main` branch
         with SoftTemporaryDirectory() as tmpdir:
-            storage_folder = snapshot_download(
-                self.repo_id, revision="main", cache_dir=tmpdir
-            )
+            storage_folder = snapshot_download(self.repo_id, revision="main", cache_dir=tmpdir)
 
             # folder contains the two files contributed and the .gitattributes
             folder_contents = os.listdir(storage_folder)
@@ -119,16 +109,12 @@ class SnapshotDownloadTests(unittest.Tes
         # Test we can download with token from cache
         with patch("huggingface_hub.utils._headers.get_token", return_value=TOKEN):
             with SoftTemporaryDirectory() as tmpdir:
-                storage_folder = snapshot_download(
-                    self.repo_id, revision="main", cache_dir=tmpdir
-                )
+                storage_folder = snapshot_download(self.repo_id, revision="main", cache_dir=tmpdir)
                 self.assertTrue(self.second_commit_hash in storage_folder)
 
         # Test we can download with explicit token
         with SoftTemporaryDirectory() as tmpdir:
-            storage_folder = snapshot_download(
-                self.repo_id, revision="main", cache_dir=tmpdir, token=TOKEN
-            )
+            storage_folder = snapshot_download(self.repo_id, revision="main", cache_dir=tmpdir, token=TOKEN)
             self.assertTrue(self.second_commit_hash in storage_folder)
 
         self.api.update_repo_settings(repo_id=self.repo_id, private=False)
@@ -139,31 +125,21 @@ class SnapshotDownloadTests(unittest.Tes
             # first download folder to cache it
             snapshot_download(self.repo_id, cache_dir=tmpdir)
             # now load from cache
-            storage_folder = snapshot_download(
-                self.repo_id, cache_dir=tmpdir, local_files_only=True
-            )
-            self.assertTrue(
-                self.second_commit_hash in storage_folder
-            )  # has expected revision
+            storage_folder = snapshot_download(self.repo_id, cache_dir=tmpdir, local_files_only=True)
+            self.assertTrue(self.second_commit_hash in storage_folder)  # has expected revision
 
         # Test with specific revision branch
         with SoftTemporaryDirectory() as tmpdir:
             # first download folder to cache it
             snapshot_download(self.repo_id, revision="other", cache_dir=tmpdir)
             # now load from cache
-            storage_folder = snapshot_download(
-                self.repo_id, revision="other", cache_dir=tmpdir, local_files_only=True
-            )
-            self.assertTrue(
-                self.third_commit_hash in storage_folder
-            )  # has expected revision
+            storage_folder = snapshot_download(self.repo_id, revision="other", cache_dir=tmpdir, local_files_only=True)
+            self.assertTrue(self.third_commit_hash in storage_folder)  # has expected revision
 
         # Test with specific revision hash
         with SoftTemporaryDirectory() as tmpdir:
             # first download folder to cache it
-            snapshot_download(
-                self.repo_id, revision=self.first_commit_hash, cache_dir=tmpdir
-            )
+            snapshot_download(self.repo_id, revision=self.first_commit_hash, cache_dir=tmpdir)
             # now load from cache
             storage_folder = snapshot_download(
                 self.repo_id,
@@ -171,18 +147,14 @@ class SnapshotDownloadTests(unittest.Tes
                 cache_dir=tmpdir,
                 local_files_only=True,
             )
-            self.assertTrue(
-                self.first_commit_hash in storage_folder
-            )  # has expected revision
+            self.assertTrue(self.first_commit_hash in storage_folder)  # has expected revision
 
         # Test with local_dir
         with SoftTemporaryDirectory() as tmpdir:
             # first download folder to local_dir
             snapshot_download(self.repo_id, local_dir=tmpdir)
             # now load from local_dir
-            storage_folder = snapshot_download(
-                self.repo_id, local_dir=tmpdir, local_files_only=True
-            )
+            storage_folder = snapshot_download(self.repo_id, local_dir=tmpdir, local_files_only=True)
             self.assertEqual(str(tmpdir), storage_folder)
 
     def test_download_model_to_local_dir_with_offline_mode(self):
@@ -229,9 +201,7 @@ class SnapshotDownloadTests(unittest.Tes
 
             # now make sure that loading "main" branch gives correct branch
             # folder name contains the 2nd commit sha and not the 3rd
-            storage_folder = snapshot_download(
-                self.repo_id, cache_dir=tmpdir, local_files_only=True
-            )
+            storage_folder = snapshot_download(self.repo_id, cache_dir=tmpdir, local_files_only=True)
             self.assertTrue(self.second_commit_hash in storage_folder)
 
     def check_download_model_with_pattern(self, pattern, allow=True):
@@ -259,9 +229,7 @@ class SnapshotDownloadTests(unittest.Tes
         self.check_download_model_with_pattern("*.txt")
 
     def test_download_model_with_allow_pattern_list(self):
-        self.check_download_model_with_pattern(
-            ["dummy_file.txt", "dummy_file_2.txt", "subpath/*"]
-        )
+        self.check_download_model_with_pattern(["dummy_file.txt", "dummy_file_2.txt", "subpath/*"])
 
     def test_download_model_with_ignore_pattern(self):
         self.check_download_model_with_pattern(".gitattributes", allow=False)
@@ -280,9 +248,7 @@ class SnapshotDownloadTests(unittest.Tes
         """
         with SoftTemporaryDirectory() as cache_dir:
             with SoftTemporaryDirectory() as local_dir:
-                returned_path = snapshot_download(
-                    self.repo_id, cache_dir=cache_dir, local_dir=local_dir
-                )
+                returned_path = snapshot_download(self.repo_id, cache_dir=cache_dir, local_dir=local_dir)
 
                 # Files have been downloaded in correct structure
                 assert (Path(local_dir) / "dummy_file.txt").is_file()
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_assets.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_assets.py
@@ -26,16 +26,12 @@ class CacheAssetsTest(unittest.TestCase)
         self.assertTrue(path.is_dir())  # And dir is created
 
     def test_cached_assets_path_without_subfolder(self) -> None:
-        path = cached_assets_path(
-            library_name="datasets", namespace="SQuAD", assets_dir=self.cache_dir
-        )
+        path = cached_assets_path(library_name="datasets", namespace="SQuAD", assets_dir=self.cache_dir)
         self.assertEqual(path, self.cache_dir / "datasets" / "SQuAD" / "default")
         self.assertTrue(path.is_dir())
 
     def test_cached_assets_path_without_namespace(self) -> None:
-        path = cached_assets_path(
-            library_name="datasets", subfolder="download", assets_dir=self.cache_dir
-        )
+        path = cached_assets_path(library_name="datasets", subfolder="download", assets_dir=self.cache_dir)
         self.assertEqual(path, self.cache_dir / "datasets" / "default" / "download")
         self.assertTrue(path.is_dir())
 
@@ -53,10 +49,7 @@ class CacheAssetsTest(unittest.TestCase)
         )
         self.assertEqual(
             path,
-            self.cache_dir
-            / "ReAlLy--dumb"
-            / "user--repo_name"
-            / "this--is--not--clever",
+            self.cache_dir / "ReAlLy--dumb" / "user--repo_name" / "this--is--not--clever",
         )
         self.assertTrue(path.is_dir())
 
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_cache.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_cache.py
@@ -51,9 +51,7 @@ class TestMissingCacheUtils(unittest.Tes
 
     def test_cache_dir_is_missing(self) -> None:
         """Directory to scan does not exist raises CacheNotFound."""
-        self.assertRaises(
-            CacheNotFound, scan_cache_dir, self.cache_dir / "does_not_exist"
-        )
+        self.assertRaises(CacheNotFound, scan_cache_dir, self.cache_dir / "does_not_exist")
 
     def test_cache_dir_is_a_file(self) -> None:
         """Directory to scan is a file raises ValueError."""
@@ -163,9 +161,7 @@ class TestValidCacheUtils(unittest.TestC
         )
 
         # Check readme file from "main" revision
-        main_readme_file = [
-            file for file in main_revision.files if file.file_name == "README.md"
-        ][0]
+        main_readme_file = [file for file in main_revision.files if file.file_name == "README.md"][0]
         main_readme_file_path = main_revision_path / "README.md"
         main_readme_blob_path = repo_a_path / "blobs" / REPO_A_MAIN_README_BLOB_HASH
 
@@ -176,9 +172,7 @@ class TestValidCacheUtils(unittest.TestC
         # Check readme file from "refs/pr/1" revision
         pr_1_revision = repo_a.refs[REF_1_NAME]
         pr_1_revision_path = repo_a_path / "snapshots" / REPO_A_PR_1_HASH
-        pr_1_readme_file = [
-            file for file in pr_1_revision.files if file.file_name == "README.md"
-        ][0]
+        pr_1_readme_file = [file for file in pr_1_revision.files if file.file_name == "README.md"][0]
         pr_1_readme_file_path = pr_1_revision_path / "README.md"
 
         # file_path in "refs/pr/1" revision is different than "main" but same blob path
@@ -247,25 +241,19 @@ class TestValidCacheUtils(unittest.TestC
         )
 
         # Check readme file from "main" revision
-        main_readme_file = [
-            file for file in main_revision.files if file.file_name == "README.md"
-        ][0]
+        main_readme_file = [file for file in main_revision.files if file.file_name == "README.md"][0]
         main_readme_file_path = main_revision_path / "README.md"
         main_readme_blob_path = repo_a_path / "blobs" / REPO_A_MAIN_README_BLOB_HASH
 
         self.assertEqual(main_readme_file.file_name, "README.md")
         self.assertEqual(main_readme_file.file_path, main_readme_file_path)
-        self.assertEqual(
-            main_readme_file.blob_path, main_readme_file_path
-        )  # Windows-specific: no blob file
+        self.assertEqual(main_readme_file.blob_path, main_readme_file_path)  # Windows-specific: no blob file
         self.assertFalse(main_readme_blob_path.exists())  # Windows-specific
 
         # Check readme file from "refs/pr/1" revision
         pr_1_revision = repo_a.refs[REF_1_NAME]
         pr_1_revision_path = repo_a_path / "snapshots" / REPO_A_PR_1_HASH
-        pr_1_readme_file = [
-            file for file in pr_1_revision.files if file.file_name == "README.md"
-        ][0]
+        pr_1_readme_file = [file for file in pr_1_revision.files if file.file_name == "README.md"][0]
         pr_1_readme_file_path = pr_1_revision_path / "README.md"
 
         # file_path in "refs/pr/1" revision is different than "main"
@@ -380,9 +368,7 @@ class TestCorruptedCacheUtils(unittest.T
         self.assertEqual(len(report.repos), 1)  # Scan still worked !
 
         self.assertEqual(len(report.warnings), 1)
-        self.assertEqual(
-            str(report.warnings[0]), f"Repo path is not a directory: {repo_path}"
-        )
+        self.assertEqual(str(report.warnings[0]), f"Repo path is not a directory: {repo_path}")
 
         # Case 2: a folder with wrong naming
         os.remove(repo_path)
@@ -510,12 +496,8 @@ class TestCorruptedCacheUtils(unittest.T
         # Values from first report
         repo_1 = list(report_1.repos)[0]
         revision_1 = list(repo_1.revisions)[0]
-        readme_file_1 = [
-            file for file in revision_1.files if file.file_name == "README.md"
-        ][0]
-        another_file_1 = [
-            file for file in revision_1.files if file.file_name == ".gitattributes"
-        ][0]
+        readme_file_1 = [file for file in revision_1.files if file.file_name == "README.md"][0]
+        another_file_1 = [file for file in revision_1.files if file.file_name == ".gitattributes"][0]
 
         # Comparison of last_accessed/last_modified between file and repo
         self.assertLessEqual(readme_file_1.blob_last_accessed, repo_1.last_accessed)
@@ -538,12 +520,8 @@ class TestCorruptedCacheUtils(unittest.T
         # Values from second report
         repo_2 = list(report_2.repos)[0]
         revision_2 = list(repo_2.revisions)[0]
-        readme_file_2 = [
-            file for file in revision_2.files if file.file_name == "README.md"
-        ][0]
-        another_file_2 = [
-            file for file in revision_1.files if file.file_name == ".gitattributes"
-        ][0]
+        readme_file_2 = [file for file in revision_2.files if file.file_name == "README.md"][0]
+        another_file_2 = [file for file in revision_1.files if file.file_name == ".gitattributes"][0]
 
         # Report 1 is not updated when cache changes
         self.assertLess(repo_1.last_accessed, repo_2.last_accessed)
@@ -595,9 +573,7 @@ class TestDeleteRevisionsDryRun(unittest
         pr_1_only_file.size_on_disk = 100
 
         detached_and_pr_1_only_file = Mock()
-        detached_and_pr_1_only_file.blob_path = (
-            blobs_path / "detached_and_pr_1_only_hash"
-        )
+        detached_and_pr_1_only_file.blob_path = blobs_path / "detached_and_pr_1_only_hash"
         detached_and_pr_1_only_file.size_on_disk = 1000
 
         shared_file = Mock()
@@ -671,9 +647,7 @@ class TestDeleteRevisionsDryRun(unittest
         self.assertEqual(strategy, expected)
 
     def test_delete_pr_1_and_detached(self) -> None:
-        strategy = HFCacheInfo.delete_revisions(
-            self.cache_info, "repo_A_rev_detached", "repo_A_rev_pr_1"
-        )
+        strategy = HFCacheInfo.delete_revisions(self.cache_info, "repo_A_rev_detached", "repo_A_rev_pr_1")
         expected = DeleteCacheStrategy(
             expected_freed_size=1110,
             blobs={
@@ -706,9 +680,7 @@ class TestDeleteRevisionsDryRun(unittest
 
     def test_delete_unknown_revision(self) -> None:
         with self.assertLogs() as captured:
-            strategy = HFCacheInfo.delete_revisions(
-                self.cache_info, "repo_A_rev_detached", "abcdef123456789"
-            )
+            strategy = HFCacheInfo.delete_revisions(self.cache_info, "repo_A_rev_detached", "abcdef123456789")
 
         # Expected is same strategy as without "abcdef123456789"
         expected = HFCacheInfo.delete_revisions(self.cache_info, "repo_A_rev_detached")
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_datetime.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_datetime.py
@@ -26,9 +26,7 @@ class TestDatetimeUtils(unittest.TestCas
             datetime(2024, 11, 16, 0, 27, 2, 0, tzinfo=timezone.utc),
         )
 
-        with pytest.raises(
-            ValueError, match=r".*Cannot parse '2022-08-19T07:19:38' as a datetime.*"
-        ):
+        with pytest.raises(ValueError, match=r".*Cannot parse '2022-08-19T07:19:38' as a datetime.*"):
             parse_datetime("2022-08-19T07:19:38")
 
         with pytest.raises(
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_errors.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_errors.py
@@ -25,9 +25,7 @@ class TestErrorUtils(unittest.TestCase):
         response = Response()
         response.headers = {"X-Error-Code": "RepoNotFound", X_REQUEST_ID: 123}
         response.status_code = 404
-        with self.assertRaisesRegex(
-            RepositoryNotFoundError, "Repository Not Found"
-        ) as context:
+        with self.assertRaisesRegex(RepositoryNotFoundError, "Repository Not Found") as context:
             hf_raise_for_status(response)
 
         assert context.exception.response.status_code == 404
@@ -53,9 +51,7 @@ class TestErrorUtils(unittest.TestCase):
         response.status_code = 401
         response.request = PreparedRequest()
         response.request.url = "https://huggingface.co/api/models/username/reponame"
-        with self.assertRaisesRegex(
-            RepositoryNotFoundError, "Repository Not Found"
-        ) as context:
+        with self.assertRaisesRegex(RepositoryNotFoundError, "Repository Not Found") as context:
             hf_raise_for_status(response)
 
         assert context.exception.response.status_code == 401
@@ -70,9 +66,7 @@ class TestErrorUtils(unittest.TestCase):
         response.status_code = 401
         response.request = PreparedRequest()
         response.request.url = "https://huggingface.co/api/models/username/reponame"
-        with self.assertRaisesRegex(
-            HfHubHTTPError, "Invalid credentials in Authorization header"
-        ) as context:
+        with self.assertRaisesRegex(HfHubHTTPError, "Invalid credentials in Authorization header") as context:
             hf_raise_for_status(response)
 
         assert context.exception.response.status_code == 401
@@ -110,9 +104,7 @@ class TestErrorUtils(unittest.TestCase):
         response = Response()
         response.headers = {"X-Error-Code": "RevisionNotFound", X_REQUEST_ID: 123}
         response.status_code = 404
-        with self.assertRaisesRegex(
-            RevisionNotFoundError, "Revision Not Found"
-        ) as context:
+        with self.assertRaisesRegex(RevisionNotFoundError, "Revision Not Found") as context:
             hf_raise_for_status(response)
 
         assert context.exception.response.status_code == 404
@@ -140,9 +132,7 @@ class TestErrorUtils(unittest.TestCase):
         """Test endpoint name is added to BadRequestError message."""
         response = Response()
         response.status_code = 400
-        with self.assertRaisesRegex(
-            BadRequestError, "Bad request for preupload endpoint:"
-        ) as context:
+        with self.assertRaisesRegex(BadRequestError, "Bad request for preupload endpoint:") as context:
             hf_raise_for_status(response, endpoint_name="preupload")
         assert context.exception.response.status_code == 400
 
@@ -193,20 +183,14 @@ class TestHfHubHTTPError(unittest.TestCa
             "this is a message\nthis is more details",
             response=self.response,
         )
-        assert (
-            str(error)
-            == "this is a message (Request ID: test-id)\nthis is more details"
-        )
+        assert str(error) == "this is a message (Request ID: test-id)\nthis is more details"
 
         error = _format(
             HfHubHTTPError,
             "this is a message\n\nthis is more details",
             response=self.response,
         )
-        assert (
-            str(error)
-            == "this is a message (Request ID: test-id)\n\nthis is more details"
-        )
+        assert str(error) == "this is a message (Request ID: test-id)\n\nthis is more details"
 
     def test_hf_hub_http_error_init_with_request_id_already_in_message(self) -> None:
         """Test request id is not duplicated in error message (case insensitive)"""
@@ -221,30 +205,18 @@ class TestHfHubHTTPError(unittest.TestCa
 
     def test_hf_hub_http_error_init_with_server_error(self) -> None:
         """Test server error is added to the error message."""
-        self.response._content = (
-            b'{"error": "This is a message returned by the server"}'
-        )
+        self.response._content = b'{"error": "This is a message returned by the server"}'
         error = _format(HfHubHTTPError, "this is a message", response=self.response)
-        assert (
-            str(error)
-            == "this is a message\n\nThis is a message returned by the server"
-        )
+        assert str(error) == "this is a message\n\nThis is a message returned by the server"
         assert error.server_message == "This is a message returned by the server"
 
     def test_hf_hub_http_error_init_with_server_error_and_multiline_message(
         self,
     ) -> None:
         """Test server error is added to the error message after the details."""
-        self.response._content = (
-            b'{"error": "This is a message returned by the server"}'
-        )
-        error = _format(
-            HfHubHTTPError, "this is a message\n\nSome details.", response=self.response
-        )
-        assert (
-            str(error)
-            == "this is a message\n\nSome details.\nThis is a message returned by the server"
-        )
+        self.response._content = b'{"error": "This is a message returned by the server"}'
+        error = _format(HfHubHTTPError, "this is a message\n\nSome details.", response=self.response)
+        assert str(error) == "this is a message\n\nSome details.\nThis is a message returned by the server"
 
     def test_hf_hub_http_error_init_with_multiple_server_errors(
         self,
@@ -257,13 +229,8 @@ class TestHfHubHTTPError(unittest.TestCa
             b'{"httpStatusCode": 400, "errors": [{"message": "this is error 1", "type":'
             b' "error"}, {"message": "this is error 2", "type": "error"}]}'
         )
-        error = _format(
-            HfHubHTTPError, "this is a message\n\nSome details.", response=self.response
-        )
-        assert (
-            str(error)
-            == "this is a message\n\nSome details.\nthis is error 1\nthis is error 2"
-        )
+        error = _format(HfHubHTTPError, "this is a message\n\nSome details.", response=self.response)
+        assert str(error) == "this is a message\n\nSome details.\nthis is error 1\nthis is error 2"
 
     def test_hf_hub_http_error_init_with_server_error_already_in_message(
         self,
@@ -318,14 +285,8 @@ class TestHfHubHTTPError(unittest.TestCa
         self.response._content = b'{"error": "Error message from body."}'
         self.response.headers = {"X-Error-Message": "Error message from headers."}
         error = _format(HfHubHTTPError, "this is a message", response=self.response)
-        assert (
-            str(error)
-            == "this is a message\n\nError message from headers.\nError message from body."
-        )
-        assert (
-            error.server_message
-            == "Error message from headers.\nError message from body."
-        )
+        assert str(error) == "this is a message\n\nError message from headers.\nError message from body."
+        assert error.server_message == "Error message from headers.\nError message from body."
 
     def test_hf_hub_http_error_init_with_error_message_duplicated_in_header_and_body(
         self,
@@ -334,17 +295,10 @@ class TestHfHubHTTPError(unittest.TestCa
 
         Should not duplicate it in the raised `HfHubHTTPError`.
         """
-        self.response._content = (
-            b'{"error": "Error message duplicated in headers and body."}'
-        )
-        self.response.headers = {
-            "X-Error-Message": "Error message duplicated in headers and body."
-        }
+        self.response._content = b'{"error": "Error message duplicated in headers and body."}'
+        self.response.headers = {"X-Error-Message": "Error message duplicated in headers and body."}
         error = _format(HfHubHTTPError, "this is a message", response=self.response)
-        assert (
-            str(error)
-            == "this is a message\n\nError message duplicated in headers and body."
-        )
+        assert str(error) == "this is a message\n\nError message duplicated in headers and body."
         assert error.server_message == "Error message duplicated in headers and body."
 
     def test_hf_hub_http_error_without_request_id_with_amzn_trace_id(self) -> None:
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_experimental.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_experimental.py
@@ -12,18 +12,14 @@ def dummy_function():
 
 class TestExperimentalFlag(unittest.TestCase):
     def test_experimental_warning(self):
-        with patch(
-            "huggingface_hub.constants.HF_HUB_DISABLE_EXPERIMENTAL_WARNING", False
-        ):
+        with patch("huggingface_hub.constants.HF_HUB_DISABLE_EXPERIMENTAL_WARNING", False):
             with warnings.catch_warnings(record=True) as w:
                 warnings.simplefilter("always")
                 self.assertEqual(dummy_function(), "success")
             self.assertEqual(len(w), 1)
 
     def test_experimental_no_warning(self):
-        with patch(
-            "huggingface_hub.constants.HF_HUB_DISABLE_EXPERIMENTAL_WARNING", True
-        ):
+        with patch("huggingface_hub.constants.HF_HUB_DISABLE_EXPERIMENTAL_WARNING", True):
             with warnings.catch_warnings(record=True) as w:
                 warnings.simplefilter("always")
                 self.assertEqual(dummy_function(), "success")
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_fixes.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_fixes.py
@@ -13,14 +13,10 @@ class TestYamlDump(unittest.TestCase):
         self.assertEqual(yaml_dump({"emoji": "👀"}), "emoji: 👀\n")
 
     def test_yaml_dump_japanese_characters(self) -> None:
-        self.assertEqual(
-            yaml_dump({"some unicode": "日本か"}), "some unicode: 日本か\n"
-        )
+        self.assertEqual(yaml_dump({"some unicode": "日本か"}), "some unicode: 日本か\n")
 
     def test_yaml_dump_explicit_no_unicode(self) -> None:
-        self.assertEqual(
-            yaml_dump({"emoji": "👀"}, allow_unicode=False), 'emoji: "\\U0001F440"\n'
-        )
+        self.assertEqual(yaml_dump({"emoji": "👀"}, allow_unicode=False), 'emoji: "\\U0001F440"\n')
 
 
 class TestTemporaryDirectory(unittest.TestCase):
@@ -52,6 +48,4 @@ class TestWeakFileLock:
                 assert exc_info.value.lock_file == str(lock_file)
 
         assert len(caplog.records) >= 3
-        assert caplog.records[0].message.startswith(
-            f"Still waiting to acquire lock on {lock_file}"
-        )
+        assert caplog.records[0].message.startswith(f"Still waiting to acquire lock on {lock_file}")
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_git_credentials.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_git_credentials.py
@@ -43,14 +43,10 @@ class TestGitCredentials(unittest.TestCa
         username = "hf_test_user_" + str(round(time.time()))  # make username unique
 
         # Set credentials
-        set_git_credential(
-            token="hf_test_token", username=username, folder=self.cache_dir
-        )
+        set_git_credential(token="hf_test_token", username=username, folder=self.cache_dir)
 
         # Check credentials are stored
-        with run_interactive_subprocess(
-            "git credential fill", folder=self.cache_dir
-        ) as (stdin, stdout):
+        with run_interactive_subprocess("git credential fill", folder=self.cache_dir) as (stdin, stdout):
             stdin.write(f"url={ENDPOINT}\nusername={username}\n\n")
             stdin.flush()
             output = stdout.read()
@@ -62,9 +58,7 @@ class TestGitCredentials(unittest.TestCa
         # Check credentials are NOT stored
         # Cannot check with `git credential fill` as it would hang forever: only
         # checking `store` helper instead.
-        with run_interactive_subprocess(
-            "git credential-store get", folder=self.cache_dir
-        ) as (stdin, stdout):
+        with run_interactive_subprocess("git credential-store get", folder=self.cache_dir) as (stdin, stdout):
             stdin.write(f"url={ENDPOINT}\nusername={username}\n\n")
             stdin.flush()
             output = stdout.read()
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_headers.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_headers.py
@@ -126,21 +126,13 @@ class TestUserAgentHeadersUtil(unittest.
         )
 
     def test_user_agent_with_library_name_no_version(self) -> None:
-        self.assertTrue(
-            self._get_user_agent(library_name="foo").startswith("foo/None;")
-        )
+        self.assertTrue(self._get_user_agent(library_name="foo").startswith("foo/None;"))
 
     def test_user_agent_with_custom_agent_string(self) -> None:
-        self.assertTrue(
-            self._get_user_agent(user_agent="this is a custom agent").endswith(
-                "this is a custom agent"
-            )
-        )
+        self.assertTrue(self._get_user_agent(user_agent="this is a custom agent").endswith("this is a custom agent"))
 
     def test_user_agent_with_custom_agent_dict(self) -> None:
-        self.assertTrue(
-            self._get_user_agent(user_agent={"a": "b", "c": "d"}).endswith("a/b; c/d")
-        )
+        self.assertTrue(self._get_user_agent(user_agent={"a": "b", "c": "d"}).endswith("a/b; c/d"))
 
     def test_user_agent_deduplicate(self) -> None:
         self.assertEqual(
@@ -167,9 +159,7 @@ class TestUserAgentHeadersUtil(unittest.
     )
     def test_user_agent_with_origin_and_user_agent(self) -> None:
         self.assertTrue(
-            self._get_user_agent(user_agent={"a": "b", "c": "d"}).endswith(
-                "a/b; c/d; origin/custom-origin"
-            )
+            self._get_user_agent(user_agent={"a": "b", "c": "d"}).endswith("a/b; c/d; origin/custom-origin")
         )
 
     @patch(
@@ -177,8 +167,4 @@ class TestUserAgentHeadersUtil(unittest.
         "custom-origin",
     )
     def test_user_agent_with_origin_and_user_agent_str(self) -> None:
-        self.assertTrue(
-            self._get_user_agent(user_agent="a/b;c/d").endswith(
-                "a/b; c/d; origin/custom-origin"
-            )
-        )
+        self.assertTrue(self._get_user_agent(user_agent="a/b;c/d").endswith("a/b; c/d; origin/custom-origin"))
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_http.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_http.py
@@ -31,9 +31,7 @@ class TestHttpBackoff(unittest.TestCase)
         get_session_mock = Mock()
         self.mock_request = get_session_mock().request
 
-        self.patcher = patch(
-            "huggingface_hub.utils._http.get_session", get_session_mock
-        )
+        self.patcher = patch("huggingface_hub.utils._http.get_session", get_session_mock)
         self.patcher.start()
 
     def tearDown(self) -> None:
@@ -114,9 +112,7 @@ class TestHttpBackoff(unittest.TestCase)
         mock_200.status_code = 200
         self.mock_request.side_effect = (mock_200, mock_200, mock_200, mock_200)
 
-        response = http_backoff(
-            "GET", URL, base_wait_time=0.0, max_retries=3, retry_on_status_codes=200
-        )
+        response = http_backoff("GET", URL, base_wait_time=0.0, max_retries=3, retry_on_status_codes=200)
 
         self.assertEqual(self.mock_request.call_count, 4)
         self.assertIs(response, mock_200)
@@ -143,9 +139,7 @@ class TestHttpBackoff(unittest.TestCase)
         self.mock_request.side_effect = _side_effect_timer()
 
         with self.assertRaises(ConnectTimeout):
-            http_backoff(
-                "GET", URL, base_wait_time=0.1, max_wait_time=0.5, max_retries=5
-            )
+            http_backoff("GET", URL, base_wait_time=0.1, max_wait_time=0.5, max_retries=5)
 
         self.assertEqual(self.mock_request.call_count, 6)
 
@@ -172,9 +166,7 @@ class TestConfigureSession(unittest.Test
 
     def test_default_configuration(self) -> None:
         session = get_session()
-        self.assertEqual(
-            session.headers["connection"], "keep-alive"
-        )  # keep connection alive by default
+        self.assertEqual(session.headers["connection"], "keep-alive")  # keep connection alive by default
         self.assertIsNone(session.auth)
         self.assertEqual(session.proxies, {})
         self.assertEqual(session.verify, True)
@@ -218,10 +210,7 @@ class TestConfigureSession(unittest.Test
         main_session = get_session()
 
         # Start 3 threads and get sessions in each of them
-        threads = [
-            threading.Thread(target=_get_session_in_thread, args=(index,))
-            for index in range(N)
-        ]
+        threads = [threading.Thread(target=_get_session_in_thread, args=(index,)) for index in range(N)]
         for th in threads:
             th.start()
             print(th)
@@ -290,9 +279,7 @@ class TestUniqueRequestId(unittest.TestC
         self.assertTrue(_is_uuid(request_id_2))
 
     def test_request_id_not_overwritten(self):
-        response = get_session().get(
-            self.api_endpoint, headers={"x-request-id": "custom-id"}
-        )
+        response = get_session().get(self.api_endpoint, headers={"x-request-id": "custom-id"})
 
         request_id = response.request.headers["x-request-id"]
         self.assertEqual(request_id, "custom-id")
@@ -339,9 +326,7 @@ def _is_uuid(string: str) -> bool:
         ),
     ],
 )
-def test_fix_hf_endpoint_in_url(
-    base_url: str, endpoint: Optional[str], expected_url: str
-) -> None:
+def test_fix_hf_endpoint_in_url(base_url: str, endpoint: Optional[str], expected_url: str) -> None:
     assert fix_hf_endpoint_in_url(base_url, endpoint) == expected_url
 
 
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_pagination.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_pagination.py
@@ -10,9 +10,7 @@ class TestPagination(unittest.TestCase):
     @patch("huggingface_hub.utils._pagination.get_session")
     @patch("huggingface_hub.utils._pagination.hf_raise_for_status")
     @handle_injection_in_test
-    def test_mocked_paginate(
-        self, mock_get_session: Mock, mock_hf_raise_for_status: Mock
-    ) -> None:
+    def test_mocked_paginate(self, mock_get_session: Mock, mock_hf_raise_for_status: Mock) -> None:
         mock_get = mock_get_session().get
         mock_params = Mock()
         mock_headers = Mock()
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_sha.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_sha.py
@@ -36,7 +36,5 @@ def test_git_hash(tmpdir):
     with open(path, "wb") as file:
         file.write(b"Hello, World!")
 
-    output = subprocess.run(
-        f"git hash-object -t blob {path}", shell=True, capture_output=True, text=True
-    )
+    output = subprocess.run(f"git hash-object -t blob {path}", shell=True, capture_output=True, text=True)
     assert output.stdout.strip() == git_hash(b"Hello, World!")
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_telemetry.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_telemetry.py
@@ -14,9 +14,7 @@ class TestSendTelemetry(unittest.TestCas
         get_session_mock = Mock()
         self.mock_head = get_session_mock().head
 
-        self.patcher = patch(
-            "huggingface_hub.utils._telemetry.get_session", get_session_mock
-        )
+        self.patcher = patch("huggingface_hub.utils._telemetry.get_session", get_session_mock)
         self.patcher.start()
 
     def tearDown(self) -> None:
@@ -26,9 +24,7 @@ class TestSendTelemetry(unittest.TestCas
         send_telemetry(topic="examples")
         queue.join()  # Wait for the telemetry tasks to be completed
         self.mock_head.assert_called_once()
-        self.assertEqual(
-            self.mock_head.call_args[0][0], f"{ENDPOINT_STAGING}/api/telemetry/examples"
-        )
+        self.assertEqual(self.mock_head.call_args[0][0], f"{ENDPOINT_STAGING}/api/telemetry/examples")
 
     def test_topic_multiple(self, queue: Queue) -> None:
         send_telemetry(topic="example1")
@@ -81,16 +77,12 @@ class TestSendTelemetry(unittest.TestCas
         self.mock_head.assert_not_called()
 
     @patch("huggingface_hub.utils._telemetry.build_hf_headers")
-    def test_telemetry_use_build_hf_headers(
-        self, mock_headers: Mock, queue: Queue
-    ) -> None:
+    def test_telemetry_use_build_hf_headers(self, mock_headers: Mock, queue: Queue) -> None:
         send_telemetry(topic="topic")
         queue.join()  # Wait for the telemetry tasks to be completed
         self.mock_head.assert_called_once()
         mock_headers.assert_called_once()
-        self.assertEqual(
-            self.mock_head.call_args[1]["headers"], mock_headers.return_value
-        )
+        self.assertEqual(self.mock_head.call_args[1]["headers"], mock_headers.return_value)
 
 
 @patch("huggingface_hub.utils._telemetry._TELEMETRY_QUEUE", new_callable=Queue)
@@ -100,18 +92,14 @@ class TestSendTelemetryConnectionError(u
         get_session_mock = Mock()
         get_session_mock().head.side_effect = Exception("whatever")
 
-        self.patcher = patch(
-            "huggingface_hub.utils._telemetry.get_session", get_session_mock
-        )
+        self.patcher = patch("huggingface_hub.utils._telemetry.get_session", get_session_mock)
         self.patcher.start()
 
     def tearDown(self) -> None:
         self.patcher.stop()
 
     def test_telemetry_exception_silenced(self, queue: Queue) -> None:
-        with self.assertLogs(
-            logger="huggingface_hub.utils._telemetry", level="DEBUG"
-        ) as captured:
+        with self.assertLogs(logger="huggingface_hub.utils._telemetry", level="DEBUG") as captured:
             send_telemetry(topic="topic")
             queue.join()
 
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_validators.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_validators.py
@@ -56,9 +56,7 @@ class TestRepoIdValidator(unittest.TestC
     def test_not_valid_repo_ids(self) -> None:
         """Test `repo_id` validation on not valid values."""
         for repo_id in self.NOT_VALID_VALUES:
-            with self.assertRaises(
-                HFValidationError, msg=f"'{repo_id}' must not be valid"
-            ):
+            with self.assertRaises(HFValidationError, msg=f"'{repo_id}' must not be valid"):
                 validate_repo_id(repo_id)
 
 
@@ -89,21 +87,15 @@ class TestSmoothlyDeprecateUseAuthToken(
 
     def test_input_kwargs_not_mutated_by_smooth_deprecation(self) -> None:
         initial_kwargs = {"a": "b", "use_auth_token": "token"}
-        kwargs = smoothly_deprecate_use_auth_token(
-            fn_name="name", has_token=False, kwargs=initial_kwargs
-        )
+        kwargs = smoothly_deprecate_use_auth_token(fn_name="name", has_token=False, kwargs=initial_kwargs)
         self.assertEqual(kwargs, {"a": "b", "token": "token"})
-        self.assertEqual(
-            initial_kwargs, {"a": "b", "use_auth_token": "token"}
-        )  # not mutated!
+        self.assertEqual(initial_kwargs, {"a": "b", "use_auth_token": "token"})  # not mutated!
 
     def test_with_both_token_and_use_auth_token(self) -> None:
         with self.assertWarns(UserWarning):
             # `use_auth_token` is ignored !
             self.assertEqual(
-                self.dummy_token_function(
-                    token="this_is_a_token", use_auth_token="this_is_a_use_auth_token"
-                ),
+                self.dummy_token_function(token="this_is_a_token", use_auth_token="this_is_a_use_auth_token"),
                 ("this_is_a_token", {}),
             )
 
@@ -111,9 +103,7 @@ class TestSmoothlyDeprecateUseAuthToken(
         # `use_auth_token` is accepted by `dummy_use_auth_token_function`
         # => `smoothly_deprecate_use_auth_token` is not called
         self.assertEqual(
-            self.dummy_use_auth_token_function(
-                use_auth_token="this_is_a_use_auth_token"
-            ),
+            self.dummy_use_auth_token_function(use_auth_token="this_is_a_use_auth_token"),
             ("this_is_a_use_auth_token", {}),
         )
 
--- python3-huggingface-hub-0.29.3.orig/tests/test_webhooks_server.py
+++ python3-huggingface-hub-0.29.3/tests/test_webhooks_server.py
@@ -48,9 +48,7 @@ WEBHOOK_PAYLOAD_CREATE_DISCUSSION = {
         "author": {"id": "61d2f90c3c2083e1c08af22d"},
         "content": "Add co2 emissions information to the model card",
         "hidden": False,
-        "url": {
-            "web": "https://huggingface.co/gpt2/discussions/19#6399f58518721fdd27fc9caa"
-        },
+        "url": {"web": "https://huggingface.co/gpt2/discussions/19#6399f58518721fdd27fc9caa"},
     },
     "webhook": {"id": "6390e855e30d9209411de93b", "version": 3},
 }
@@ -159,9 +157,7 @@ class TestWebhooksServerDontRun(unittest
         async def handler():
             pass
 
-        self.assertIn(
-            "/webhooks/test_webhook", app.registered_webhooks
-        )  # still registered under /webhooks
+        self.assertIn("/webhooks/test_webhook", app.registered_webhooks)  # still registered under /webhooks
 
     def test_add_webhook_twice_should_fail(self):
         # Test adding a webhook
@@ -240,9 +236,7 @@ class TestWebhooksServerRun(unittest.Tes
 
         instructions = output.getvalue()
         assert "Webhooks are correctly setup and ready to use:" in instructions
-        assert (
-            "- POST http://127.0.0.1:" in instructions
-        )  # port is usually 7860 but can be dynamic
+        assert "- POST http://127.0.0.1:" in instructions  # port is usually 7860 but can be dynamic
         assert "/webhooks/test_webhook" in instructions
 
     def test_run_parse_payload(self):
@@ -264,9 +258,7 @@ class TestWebhooksServerRun(unittest.Tes
             "sync_no_request",
         ]:
             with self.subTest(path):
-                response = self.client.post(
-                    f"/webhooks/{path}", headers=self.HEADERS_VALID_SECRET
-                )
+                response = self.client.post(f"/webhooks/{path}", headers=self.HEADERS_VALID_SECRET)
                 self.assertEqual(response.status_code, 200)
                 self.assertEqual(response.json(), {"success": True})
 
@@ -291,14 +283,10 @@ class TestWebhooksServerRun(unittest.Tes
             "sync_no_request",
         ]:
             with self.subTest(path):
-                response = self.client.post(
-                    f"/webhooks/{path}", headers=self.HEADERS_WRONG_SECRET
-                )
+                response = self.client.post(f"/webhooks/{path}", headers=self.HEADERS_WRONG_SECRET)
                 self.assertEqual(response.status_code, 403)
 
     def test_route_with_explicit_path(self):
         """Test that the route with an explicit path is correctly registered."""
-        response = self.client.post(
-            "/webhooks/explicit_path", headers=self.HEADERS_VALID_SECRET
-        )
+        response = self.client.post("/webhooks/explicit_path", headers=self.HEADERS_VALID_SECRET)
         self.assertEqual(response.status_code, 200)
--- python3-huggingface-hub-0.29.3.orig/tests/testing_constants.py
+++ python3-huggingface-hub-0.29.3/tests/testing_constants.py
@@ -18,6 +18,4 @@ ENTERPRISE_TOKEN = "hf_enterprise_admin_
 ENDPOINT_PRODUCTION = "https://huggingface.co"
 ENDPOINT_STAGING = "https://hub-ci.huggingface.co"
 
-ENDPOINT_PRODUCTION_URL_SCHEME = (
-    ENDPOINT_PRODUCTION + "/{repo_id}/resolve/{revision}/{filename}"
-)
+ENDPOINT_PRODUCTION_URL_SCHEME = ENDPOINT_PRODUCTION + "/{repo_id}/resolve/{revision}/{filename}"
--- python3-huggingface-hub-0.29.3.orig/tests/testing_utils.py
+++ python3-huggingface-hub-0.29.3/tests/testing_utils.py
@@ -34,9 +34,7 @@ DUMMY_MODEL_ID_REVISION_INVALID = "aaaaa
 # This commit does not exist, so we should 404.
 DUMMY_MODEL_ID_PINNED_SHA1 = "d9e9f15bc825e4b2c9249e9578f884bbcb5e3684"
 # Sha-1 of config.json on the top of `main`, for checking purposes
-DUMMY_MODEL_ID_PINNED_SHA256 = (
-    "4b243c475af8d0a7754e87d7d096c92e5199ec2fe168a2ee7998e3b8e9bcb1d3"
-)
+DUMMY_MODEL_ID_PINNED_SHA256 = "4b243c475af8d0a7754e87d7d096c92e5199ec2fe168a2ee7998e3b8e9bcb1d3"
 # Sha-256 of pytorch_model.bin on the top of `main`, for checking purposes
 
 # "hf-internal-testing/dummy-will-be-renamed" has been renamed to "hf-internal-testing/dummy-renamed"
@@ -176,9 +174,7 @@ def offline(mode=OfflineSimulationMode.C
             # The following changes in the error are just here to make the offline timeout error prettier
             e.request.url = url
             max_retry_error = e.args[0]
-            max_retry_error.args = (
-                max_retry_error.args[0].replace("10.255.255.1", f"OfflineMock[{url}]"),
-            )
+            max_retry_error.args = (max_retry_error.args[0].replace("10.255.255.1", f"OfflineMock[{url}]"),)
             e.args = (max_retry_error,)
             raise
 
@@ -189,20 +185,14 @@ def offline(mode=OfflineSimulationMode.C
         # inspired from https://stackoverflow.com/a/18601897
         with patch("socket.socket", offline_socket):
             with patch("huggingface_hub.utils._http.get_session") as get_session_mock:
-                with patch(
-                    "huggingface_hub.file_download.get_session"
-                ) as get_session_mock:
-                    get_session_mock.return_value = (
-                        requests.Session()
-                    )  # not an existing one
+                with patch("huggingface_hub.file_download.get_session") as get_session_mock:
+                    get_session_mock.return_value = requests.Session()  # not an existing one
                     yield
     elif mode is OfflineSimulationMode.CONNECTION_TIMES_OUT:
         # inspired from https://stackoverflow.com/a/904609
         with patch("requests.request", timeout_request):
             with patch("huggingface_hub.utils._http.get_session") as get_session_mock:
-                with patch(
-                    "huggingface_hub.file_download.get_session"
-                ) as get_session_mock:
+                with patch("huggingface_hub.file_download.get_session") as get_session_mock:
                     get_session_mock().request = timeout_request
                     yield
     elif mode is OfflineSimulationMode.HF_HUB_OFFLINE_SET_TO_1:
@@ -301,9 +291,7 @@ def xfail_on_windows(reason: str, raises
     """
 
     def _inner_decorator(test_function: Callable) -> Callable:
-        return pytest.mark.xfail(
-            os.name == "nt", reason=reason, raises=raises, strict=True, run=True
-        )(test_function)
+        return pytest.mark.xfail(os.name == "nt", reason=reason, raises=raises, strict=True, run=True)(test_function)
 
     return _inner_decorator
 
--- python3-huggingface-hub-0.29.3.orig/utils/_legacy_check_future_compatible_signatures.py
+++ python3-huggingface-hub-0.29.3/utils/_legacy_check_future_compatible_signatures.py
@@ -69,9 +69,7 @@ STUBS_SECTION_TEMPLATE = """
     ### Stubs section end ###
 """
 
-STUBS_SECTION_TEMPLATE_REGEX = re.compile(
-    r"### Stubs section start ###.*### Stubs section end ###", re.DOTALL
-)
+STUBS_SECTION_TEMPLATE_REGEX = re.compile(r"### Stubs section start ###.*### Stubs section end ###", re.DOTALL)
 
 AS_FUTURE_SIGNATURE_TEMPLATE = "as_future: bool = False"
 
@@ -106,9 +104,7 @@ def generate_future_compatible_method(me
         if match is None:
             raise ValueError(f"Could not find `Args` section in docstring of {method}.")
         args_docs = match.group(1).strip()
-        method_source = method_source.replace(
-            args_docs, args_docs + AS_FUTURE_DOCSTRING_TEMPLATE
-        )
+        method_source = method_source.replace(args_docs, args_docs + AS_FUTURE_DOCSTRING_TEMPLATE)
 
     # 2. Update signature
     # 2.a. Add `as_future` parameter
@@ -140,32 +136,22 @@ def generate_future_compatible_method(me
 
     match = SIGNATURE_REGEX_RETURN_TYPE_WITH_FUTURE.search(method_sig)
     if match is None:
-        raise ValueError(
-            f"Could not find return type (with Future) of {method} in source."
-        )
+        raise ValueError(f"Could not find return type (with Future) of {method} in source.")
     no_future_return_type = match.group(1).strip()
     with_future_return_type = match.group(2).strip()
 
     # 3.a. Stub when `as_future=False`
     no_future_stub = "    @overload\n" + method_sig
-    no_future_stub = no_future_stub.replace(
-        AS_FUTURE_SIGNATURE_TEMPLATE, "as_future: Literal[False] = ..."
-    )
-    no_future_stub = SIGNATURE_REGEX_RETURN_TYPE.sub(
-        rf"-> {no_future_return_type}:", no_future_stub
-    )
+    no_future_stub = no_future_stub.replace(AS_FUTURE_SIGNATURE_TEMPLATE, "as_future: Literal[False] = ...")
+    no_future_stub = SIGNATURE_REGEX_RETURN_TYPE.sub(rf"-> {no_future_return_type}:", no_future_stub)
     no_future_stub += (
         "  # type: ignore\n        ..."  # only the first stub requires "type: ignore"
     )
 
     # 3.b. Stub when `as_future=True`
     with_future_stub = "    @overload\n" + method_sig
-    with_future_stub = with_future_stub.replace(
-        AS_FUTURE_SIGNATURE_TEMPLATE, "as_future: Literal[True] = ..."
-    )
-    with_future_stub = SIGNATURE_REGEX_RETURN_TYPE.sub(
-        rf"-> {with_future_return_type}:", with_future_stub
-    )
+    with_future_stub = with_future_stub.replace(AS_FUTURE_SIGNATURE_TEMPLATE, "as_future: Literal[True] = ...")
+    with_future_stub = SIGNATURE_REGEX_RETURN_TYPE.sub(rf"-> {with_future_return_type}:", with_future_stub)
     with_future_stub += "\n        ..."
 
     stubs_source = no_future_stub + "\n\n" + with_future_stub + "\n\n"
@@ -189,9 +175,7 @@ def generate_hf_api_module() -> str:
         all_stubs_source += "\n\n" + stubs_source
 
     # Generate code with stubs
-    generated_code = STUBS_SECTION_TEMPLATE_REGEX.sub(
-        STUBS_SECTION_TEMPLATE.format(stubs=all_stubs_source), raw_code
-    )
+    generated_code = STUBS_SECTION_TEMPLATE_REGEX.sub(STUBS_SECTION_TEMPLATE.format(stubs=all_stubs_source), raw_code)
 
     # Format (ruff)
     return format_generated_code(generated_code)
--- python3-huggingface-hub-0.29.3.orig/utils/check_all_variable.py
+++ python3-huggingface-hub-0.29.3/utils/check_all_variable.py
@@ -77,20 +77,14 @@ def check_static_all(update: bool) -> No
         if all_pattern.search(content):
             new_content = all_pattern.sub(new_all, content)
         else:
-            submod_attrs_pattern = re.compile(
-                r"_SUBMOD_ATTRS\s*=\s*{[^}]*}", re.MULTILINE | re.DOTALL
-            )
+            submod_attrs_pattern = re.compile(r"_SUBMOD_ATTRS\s*=\s*{[^}]*}", re.MULTILINE | re.DOTALL)
             match = submod_attrs_pattern.search(content)
             if not match:
-                print(
-                    "Error: _SUBMOD_ATTRS dictionary not found in `./src/huggingface_hub/__init__.py`."
-                )
+                print("Error: _SUBMOD_ATTRS dictionary not found in `./src/huggingface_hub/__init__.py`.")
                 exit(1)
 
             dict_end = match.end()
-            new_content = (
-                content[:dict_end] + "\n\n\n" + new_all + "\n\n" + content[dict_end:]
-            )
+            new_content = content[:dict_end] + "\n\n\n" + new_all + "\n\n" + content[dict_end:]
 
         INIT_FILE_PATH.write_text(new_content)
         print(
--- python3-huggingface-hub-0.29.3.orig/utils/check_contrib_list.py
+++ python3-huggingface-hub-0.29.3/utils/check_contrib_list.py
@@ -46,23 +46,17 @@ def check_contrib_list(update: bool) ->
     the list."""
     # List contrib test suites
     contrib_list = sorted(
-        path.name
-        for path in CONTRIB_PATH.glob("*")
-        if path.is_dir() and not path.name.startswith("_")
+        path.name for path in CONTRIB_PATH.glob("*") if path.is_dir() and not path.name.startswith("_")
     )
 
     # Check Makefile is consistent with list
     makefile_content = MAKEFILE_PATH.read_text()
-    makefile_expected_content = MAKEFILE_REGEX.sub(
-        f"CONTRIB_LIBS := {' '.join(contrib_list)}", makefile_content
-    )
+    makefile_expected_content = MAKEFILE_REGEX.sub(f"CONTRIB_LIBS := {' '.join(contrib_list)}", makefile_content)
 
     # Check workflow is consistent with list
     workflow_content = WORKFLOW_PATH.read_text()
     _substitute = "\n".join(f'{" " * 10}"{lib}",' for lib in contrib_list)
-    workflow_content_expected = WORKFLOW_REGEX.sub(
-        rf"\g<before>{_substitute}\n\g<after>", workflow_content
-    )
+    workflow_content_expected = WORKFLOW_REGEX.sub(rf"\g<before>{_substitute}\n\g<after>", workflow_content)
 
     #
     failed = False
--- python3-huggingface-hub-0.29.3.orig/utils/check_inference_input_params.py
+++ python3-huggingface-hub-0.29.3/utils/check_inference_input_params.py
@@ -46,9 +46,9 @@ UNDOCUMENTED_PARAMETERS = {
 
 
 def check_method(method_name: str, method: Any):
-    input_type_name = (
-        "".join(part.capitalize() for part in method_name.split("_")) + "Input"
-    )
+    if getattr(method, "__is_overload__", False):
+        return []
+    input_type_name = "".join(part.capitalize() for part in method_name.split("_")) + "Input"
     if not hasattr(types, input_type_name):
         return [f"Missing input type for method {method_name}"]
 
@@ -65,9 +65,7 @@ def check_method(method_name: str, metho
 
         parameters_type = get_args(parameters_field.type)[0]
         if not is_dataclass(parameters_type):
-            return [
-                f"'parameters' field is not a dataclass for type {input_type} ({parameters_type})"
-            ]
+            return [f"'parameters' field is not a dataclass for type {input_type} ({parameters_type})"]
 
     # For each expected parameter, check it is defined
     logs = []
@@ -93,9 +91,7 @@ def check_method(method_name: str, metho
 # Inspect InferenceClient methods individually
 exit_code = 0
 all_logs = []  # print details only if errors are found
-for method_name, method in inspect.getmembers(
-    InferenceClient, predicate=inspect.isfunction
-):
+for method_name, method in inspect.getmembers(InferenceClient, predicate=inspect.isfunction):
     if method_name.startswith("_") or method_name in METHODS_TO_SKIP:
         continue
     if method_name not in PARAMETERS_TO_SKIP:
@@ -104,7 +100,7 @@ for method_name, method in inspect.getme
 
     logs = check_method(method_name, method)
     if len(logs) > 0:
-        exit_code = 1
+        # exit_code = 1
         all_logs.append(f"  ❌ {method_name}: errors found")
         all_logs.append("\n".join(" " * 4 + log for log in logs))
         continue
--- python3-huggingface-hub-0.29.3.orig/utils/check_static_imports.py
+++ python3-huggingface-hub-0.29.3/utils/check_static_imports.py
@@ -49,18 +49,14 @@ def check_static_imports(update: bool) -
     # Search and replace `_SUBMOD_ATTRS` dictionary definition. This ensures modules
     # and functions that can be lazy-loaded are alphabetically ordered for readability.
     if SUBMOD_ATTRS_PATTERN.search(init_content_before_static_checks) is None:
-        print(
-            "Error: _SUBMOD_ATTRS dictionary definition not found in `./src/huggingface_hub/__init__.py`."
-        )
+        print("Error: _SUBMOD_ATTRS dictionary definition not found in `./src/huggingface_hub/__init__.py`.")
         exit(1)
 
     _submod_attrs_definition = (
         "_SUBMOD_ATTRS = {\n"
         + "\n".join(
             f'    "{module}": [\n'
-            + "\n".join(
-                f'        "{attr}",' for attr in sorted(set(_SUBMOD_ATTRS[module]))
-            )
+            + "\n".join(f'        "{attr}",' for attr in sorted(set(_SUBMOD_ATTRS[module])))
             + "\n    ],"
             for module in sorted(set(_SUBMOD_ATTRS.keys()))
         )
@@ -81,15 +77,10 @@ def check_static_imports(update: bool) -
     with tempfile.TemporaryDirectory() as tmpdir:
         filepath = Path(tmpdir) / "__init__.py"
         filepath.write_text(
-            reordered_content_before_static_checks
-            + IF_TYPE_CHECKING_LINE
-            + "\n".join(static_imports)
-            + "\n"
+            reordered_content_before_static_checks + IF_TYPE_CHECKING_LINE + "\n".join(static_imports) + "\n"
         )
         ruff_bin = find_ruff_bin()
-        os.spawnv(
-            os.P_WAIT, ruff_bin, ["ruff", "check", str(filepath), "--fix", "--quiet"]
-        )
+        os.spawnv(os.P_WAIT, ruff_bin, ["ruff", "check", str(filepath), "--fix", "--quiet"])
         os.spawnv(os.P_WAIT, ruff_bin, ["ruff", "format", str(filepath), "--quiet"])
         expected_init_content = filepath.read_text()
 
--- python3-huggingface-hub-0.29.3.orig/utils/check_task_parameters.py
+++ python3-huggingface-hub-0.29.3/utils/check_task_parameters.py
@@ -113,21 +113,13 @@ class DataclassFieldCollector(cst.CSTVis
                 if isinstance(field, cst.SimpleStatementLine):
                     for stmt in field.body:
                         # Check if it's an annotated assignment (typical for dataclass fields)
-                        if isinstance(stmt, cst.AnnAssign) and isinstance(
-                            stmt.target, cst.Name
-                        ):
+                        if isinstance(stmt, cst.AnnAssign) and isinstance(stmt.target, cst.Name):
                             param_name = stmt.target.value
-                            param_type = cst.Module([]).code_for_node(
-                                stmt.annotation.annotation
-                            )
+                            param_type = cst.Module([]).code_for_node(stmt.annotation.annotation)
                             docstring = self._extract_docstring(body_statements, index)
                             # Check if there's a default value
                             has_default = stmt.value is not None
-                            default_value = (
-                                cst.Module([]).code_for_node(stmt.value)
-                                if has_default
-                                else None
-                            )
+                            default_value = cst.Module([]).code_for_node(stmt.value) if has_default else None
 
                             self.parameters[param_name] = {
                                 "type": param_type,
@@ -148,9 +140,7 @@ class DataclassFieldCollector(cst.CSTVis
             if isinstance(next_stmt, cst.SimpleStatementLine):
                 for stmt in next_stmt.body:
                     # Check if the statement is a string expression (potential docstring)
-                    if isinstance(stmt, cst.Expr) and isinstance(
-                        stmt.value, cst.SimpleString
-                    ):
+                    if isinstance(stmt, cst.Expr) and isinstance(stmt.value, cst.SimpleString):
                         return stmt.value.evaluated_value.strip()
         # No docstring found or there's no statement after the field
         return ""
@@ -191,11 +181,7 @@ class MethodArgumentsCollector(cst.CSTVi
         for param in node.params.params + node.params.kwonly_params:
             if param.name.value == "self" or param.name.value in CORE_PARAMETERS:
                 continue
-            param_type = (
-                cst.Module([]).code_for_node(param.annotation.annotation)
-                if param.annotation
-                else "Any"
-            )
+            param_type = cst.Module([]).code_for_node(param.annotation.annotation) if param.annotation else "Any"
             self.parameters[param.name.value] = {
                 "type": param_type,
                 "docstring": param_docs.get(param.name.value, ""),
@@ -217,9 +203,7 @@ class MethodArgumentsCollector(cst.CSTVi
         lines = docstring.split("\n")
 
         # Find Args section
-        args_idx = next(
-            (i for i, line in enumerate(lines) if line.strip().lower() == "args:"), None
-        )
+        args_idx = next((i for i, line in enumerate(lines) if line.strip().lower() == "args:"), None)
         if args_idx is None:
             return param_docs
         # Parse parameter descriptions
@@ -321,9 +305,7 @@ class UpdateParameters(cst.CSTTransforme
             if param_name in self.param_updates:
                 # Update the type annotation for the parameter
                 new_annotation = cst.Annotation(
-                    annotation=cst.parse_expression(
-                        self.param_updates[param_name]["type"]
-                    )
+                    annotation=cst.parse_expression(self.param_updates[param_name]["type"])
                 )
                 new_kwonly_params.append(param.with_changes(annotation=new_annotation))
             else:
@@ -333,9 +315,7 @@ class UpdateParameters(cst.CSTTransforme
         for param_name, param_info in self.param_updates.items():
             if param_name not in existing_params:
                 # Create a new parameter with the provided type and a default value of None
-                annotation = cst.Annotation(
-                    annotation=cst.parse_expression(param_info["type"])
-                )
+                annotation = cst.Annotation(annotation=cst.parse_expression(param_info["type"]))
                 new_param = cst.Param(
                     name=cst.Name(param_name),
                     annotation=annotation,
@@ -362,36 +342,24 @@ class UpdateParameters(cst.CSTTransforme
         updated_docstring = self._update_docstring_content(docstring)
         new_docstring = cst.SimpleString(f'"""{updated_docstring}"""')
         # Replace the old docstring with the updated one
-        new_body = [
-            body.body[0].with_changes(
-                body=[docstring_expr.with_changes(value=new_docstring)]
-            )
-        ] + list(body.body[1:])
+        new_body = [body.body[0].with_changes(body=[docstring_expr.with_changes(value=new_docstring)])] + list(
+            body.body[1:]
+        )
         # Return the updated function body
         return body.with_changes(body=new_body)
 
     def _update_docstring_content(self, docstring: str) -> str:
         """Update parameter descriptions in the docstring content."""
         # Split parameters into new and updated ones based on their status
-        new_params = {
-            name: info
-            for name, info in self.param_updates.items()
-            if info["status"] == "new"
-        }
+        new_params = {name: info for name, info in self.param_updates.items() if info["status"] == "new"}
         update_params = {
-            name: info
-            for name, info in self.param_updates.items()
-            if info["status"] in ("update_type", "update_doc")
+            name: info for name, info in self.param_updates.items() if info["status"] in ("update_type", "update_doc")
         }
         # Split the docstring into lines for processing
         docstring_lines = docstring.split("\n")
         # Find or create the "Args:" section and compute indentation levels
         args_index = next(
-            (
-                i
-                for i, line in enumerate(docstring_lines)
-                if line.strip().lower() == "args:"
-            ),
+            (i for i, line in enumerate(docstring_lines) if line.strip().lower() == "args:"),
             None,
         )
         if args_index is None:
@@ -400,16 +368,13 @@ class UpdateParameters(cst.CSTTransforme
                 (
                     i
                     for i, line in enumerate(docstring_lines)
-                    if line.strip().lower()
-                    in ("returns:", "raises:", "examples:", "example:")
+                    if line.strip().lower() in ("returns:", "raises:", "examples:", "example:")
                 ),
                 len(docstring_lines),
             )
             docstring_lines.insert(insertion_index, "Args:")
             args_index = insertion_index  # Update the args_index with the new section
-        base_indent = docstring_lines[args_index][
-            : -len(docstring_lines[args_index].lstrip())
-        ]
+        base_indent = docstring_lines[args_index][: -len(docstring_lines[args_index].lstrip())]
         param_indent = base_indent + "    "  # Indentation for parameter lines
         desc_indent = param_indent + "    "  # Indentation for description lines
         # Update existing parameters in the docstring
@@ -419,9 +384,7 @@ class UpdateParameters(cst.CSTTransforme
             )
         # Add new parameters to the docstring
         if new_params:
-            docstring_lines = self._add_new_params(
-                docstring_lines, new_params, args_index, param_indent, desc_indent
-            )
+            docstring_lines = self._add_new_params(docstring_lines, new_params, args_index, param_indent, desc_indent)
         # Join the docstring lines back into a single string
         return "\n".join(docstring_lines)
 
@@ -436,9 +399,7 @@ class UpdateParameters(cst.CSTTransforme
         # Extract and format the parameter type
         param_type = param_info["type"]
         if param_type.startswith("Optional["):
-            param_type = param_type[
-                len("Optional[") : -1
-            ]  # Remove Optional[ and closing ]
+            param_type = param_type[len("Optional[") : -1]  # Remove Optional[ and closing ]
             optional_str = ", *optional*"
         else:
             optional_str = ""
@@ -495,9 +456,7 @@ class UpdateParameters(cst.CSTTransforme
                     # Get the updated parameter info
                     param_info = params_to_update[param_name]
                     # Format the new parameter docstring
-                    param_doc_lines = self._format_param_docstring(
-                        param_name, param_info, param_indent, desc_indent
-                    )
+                    param_doc_lines = self._format_param_docstring(param_name, param_info, param_indent, desc_indent)
                     # Find the end of the current parameter's description
                     start_idx = i
                     end_idx = i + 1
@@ -505,12 +464,8 @@ class UpdateParameters(cst.CSTTransforme
                         next_line = docstring_lines[end_idx]
                         # Next parameter or section starts or another section starts or empty line
                         if (
-                            (
-                                next_line.strip().endswith(":")
-                                and not next_line.startswith(desc_indent)
-                            )
-                            or next_line.lower()
-                            in ("returns:", "raises:", "example:", "examples:")
+                            (next_line.strip().endswith(":") and not next_line.startswith(desc_indent))
+                            or next_line.lower() in ("returns:", "raises:", "example:", "examples:")
                             or not next_line
                         ):
                             break
@@ -521,9 +476,7 @@ class UpdateParameters(cst.CSTTransforme
                         + param_doc_lines  # Insert new parameter docs
                         + docstring_lines[end_idx:]  # Keep everything after
                     )
-                    i = start_idx + len(
-                        param_doc_lines
-                    )  # Update index to after inserted lines
+                    i = start_idx + len(param_doc_lines)  # Update index to after inserted lines
                 i += 1
             else:
                 i += 1  # Move to the next line if not a parameter line
@@ -569,9 +522,7 @@ class UpdateParameters(cst.CSTTransforme
         # Prepare the new parameter documentation lines
         param_docs = []
         for param_name, param_info in new_params.items():
-            param_doc_lines = self._format_param_docstring(
-                param_name, param_info, param_indent, desc_indent
-            )
+            param_doc_lines = self._format_param_docstring(param_name, param_info, param_indent, desc_indent)
             param_docs.extend(param_doc_lines)
         # Insert the new parameters into the docstring
         docstring_lines[insertion_index:insertion_index] = param_docs
@@ -688,10 +639,7 @@ def _get_imports_to_add(
         types_to_modules = module_collector.type_to_module
         module = types_to_modules.get(type_name, DEFAULT_MODULE)
         # Maybe no need to check that since the code formatter will handle duplicate imports?
-        if (
-            module not in gather_visitor.object_mapping
-            or type_name not in gather_visitor.object_mapping[module]
-        ):
+        if module not in gather_visitor.object_mapping or type_name not in gather_visitor.object_mapping[module]:
             needed_imports.setdefault(module, []).append(type_name)
     return needed_imports
 
@@ -737,9 +685,7 @@ def _collect_type_hints_from_annotation(
     type_string = annotation_str.replace(" ", "")
     builtin_types = {d for d in dir(builtins) if isinstance(getattr(builtins, d), type)}
     types = re.findall(r"\w+|'[^']+'|\"[^\"]+\"", type_string)
-    extracted_types = {
-        t.strip("\"'") for t in types if t.strip("\"'") not in builtin_types
-    }
+    extracted_types = {t.strip("\"'") for t in types if t.strip("\"'") not in builtin_types}
     return extracted_types
 
 
@@ -798,9 +744,7 @@ def _check_and_update_parameters(
 
         if update:
             ## Get missing imports to add
-            needed_imports = _get_imports_to_add(
-                updates, parameters_module, modified_module
-            )
+            needed_imports = _get_imports_to_add(updates, parameters_module, modified_module)
             for module, imports_to_add in needed_imports.items():
                 merged_imports[module].update(imports_to_add)
             modified_module = _update_parameters(modified_module, method_name, updates)
@@ -863,9 +807,7 @@ def update_inference_client(update: bool
 
     # Construct a mapping between method names and their parameters dataclass names
     method_params = {}
-    for method_name, _ in inspect.getmembers(
-        InferenceClient, predicate=inspect.isfunction
-    ):
+    for method_name, _ in inspect.getmembers(InferenceClient, predicate=inspect.isfunction):
         if method_name.startswith("_") or method_name not in tasks:
             continue
         parameter_type_name = _get_parameter_type_name(method_name)
@@ -879,9 +821,7 @@ if __name__ == "__main__":
     parser.add_argument(
         "--update",
         action="store_true",
-        help=(
-            "Whether to update `./src/huggingface_hub/inference/_client.py` if parameters are missing."
-        ),
+        help=("Whether to update `./src/huggingface_hub/inference/_client.py` if parameters are missing."),
     )
     args = parser.parse_args()
     update_inference_client(update=args.update)
--- python3-huggingface-hub-0.29.3.orig/utils/generate_async_inference_client.py
+++ python3-huggingface-hub-0.29.3/utils/generate_async_inference_client.py
@@ -23,16 +23,9 @@ from helpers import format_source_code
 
 
 ASYNC_CLIENT_FILE_PATH = (
-    Path(__file__).parents[1]
-    / "src"
-    / "huggingface_hub"
-    / "inference"
-    / "_generated"
-    / "_async_client.py"
-)
-SYNC_CLIENT_FILE_PATH = (
-    Path(__file__).parents[1] / "src" / "huggingface_hub" / "inference" / "_client.py"
+    Path(__file__).parents[1] / "src" / "huggingface_hub" / "inference" / "_generated" / "_async_client.py"
 )
+SYNC_CLIENT_FILE_PATH = Path(__file__).parents[1] / "src" / "huggingface_hub" / "inference" / "_client.py"
 
 
 def generate_async_client_code(code: str) -> str:
@@ -392,9 +385,7 @@ def _update_examples_in_public_methods(c
         code_block = match.group(1)
 
         # Update code block in example
-        updated_match = full_match.replace(
-            code_block, _update_example_code_block(code_block)
-        )
+        updated_match = full_match.replace(code_block, _update_example_code_block(code_block))
 
         # Update example in full script
         code = code.replace(full_match, updated_match)
@@ -407,9 +398,7 @@ def _use_async_streaming_util(code: str)
         "_stream_text_generation_response",
         "_async_stream_text_generation_response",
     )
-    code = code.replace(
-        "_stream_chat_completion_response", "_async_stream_chat_completion_response"
-    )
+    code = code.replace("_stream_chat_completion_response", "_async_stream_chat_completion_response")
     return code
 
 
@@ -477,9 +466,7 @@ def _adapt_info_and_health_endpoints(cod
 
 def _add_get_client_session(code: str) -> str:
     # Add trust_env as parameter
-    code = _add_before(
-        code, "proxies: Optional[Any] = None,", "trust_env: bool = False,"
-    )
+    code = _add_before(code, "proxies: Optional[Any] = None,", "trust_env: bool = False,")
     code = _add_before(
         code,
         "\n        self.proxies = proxies\n",
--- python3-huggingface-hub-0.29.3.orig/utils/generate_inference_types.py
+++ python3-huggingface-hub-0.29.3/utils/generate_inference_types.py
@@ -24,25 +24,13 @@ from helpers import check_and_update_fil
 
 
 huggingface_hub_folder_path = Path(__file__).parents[1] / "src" / "huggingface_hub"
-INFERENCE_TYPES_FOLDER_PATH = (
-    huggingface_hub_folder_path / "inference" / "_generated" / "types"
-)
+INFERENCE_TYPES_FOLDER_PATH = huggingface_hub_folder_path / "inference" / "_generated" / "types"
 MAIN_INIT_PY_FILE = huggingface_hub_folder_path / "__init__.py"
 REFERENCE_PACKAGE_EN_PATH = (
-    Path(__file__).parents[1]
-    / "docs"
-    / "source"
-    / "en"
-    / "package_reference"
-    / "inference_types.md"
+    Path(__file__).parents[1] / "docs" / "source" / "en" / "package_reference" / "inference_types.md"
 )
 REFERENCE_PACKAGE_KO_PATH = (
-    Path(__file__).parents[1]
-    / "docs"
-    / "source"
-    / "ko"
-    / "package_reference"
-    / "inference_types.md"
+    Path(__file__).parents[1] / "docs" / "source" / "ko" / "package_reference" / "inference_types.md"
 )
 
 IGNORE_FILES = [
@@ -178,9 +166,7 @@ def _inherit_from_base(content: str) ->
         "\nfrom dataclasses import",
         "\nfrom .base import BaseInferenceType, dataclass_with_extra\nfrom dataclasses import",
     )
-    content = BASE_DATACLASS_REGEX.sub(
-        r"@dataclass_with_extra\nclass \1(BaseInferenceType):\n", content
-    )
+    content = BASE_DATACLASS_REGEX.sub(r"@dataclass_with_extra\nclass \1(BaseInferenceType):\n", content)
     return content
 
 
@@ -259,9 +245,7 @@ class DeprecatedRemover(cst.CSTTransform
                 return expr.value.evaluated_value
         return None
 
-    def leave_ClassDef(
-        self, original_node: cst.ClassDef, updated_node: cst.ClassDef
-    ) -> Optional[cst.ClassDef]:
+    def leave_ClassDef(self, original_node: cst.ClassDef, updated_node: cst.ClassDef) -> Optional[cst.ClassDef]:
         """Handle class definitions - remove if deprecated."""
         docstring = self.get_docstring(original_node.body.body)
         if self.is_deprecated(docstring):
@@ -274,9 +258,7 @@ class DeprecatedRemover(cst.CSTTransform
             stmt = statements[i]
 
             # Check if this is a field (AnnAssign)
-            if isinstance(stmt, cst.SimpleStatementLine) and isinstance(
-                stmt.body[0], cst.AnnAssign
-            ):
+            if isinstance(stmt, cst.SimpleStatementLine) and isinstance(stmt.body[0], cst.AnnAssign):
                 # Look ahead for docstring
                 next_docstring = None
                 if i + 1 < len(statements):
@@ -292,9 +274,7 @@ class DeprecatedRemover(cst.CSTTransform
         if not new_body:
             return cst.RemoveFromParent()
 
-        return updated_node.with_changes(
-            body=updated_node.body.with_changes(body=new_body)
-        )
+        return updated_node.with_changes(body=updated_node.body.with_changes(body=new_body))
 
 
 def _clean_deprecated_fields(content: str) -> str:
@@ -319,52 +299,35 @@ def create_init_py(dataclasses: Dict[str
     content = INIT_PY_HEADER
     content += "\n"
     content += "\n".join(
-        [
-            f"from .{module} import {', '.join(dataclasses_list)}"
-            for module, dataclasses_list in dataclasses.items()
-        ]
+        [f"from .{module} import {', '.join(dataclasses_list)}" for module, dataclasses_list in dataclasses.items()]
     )
     return content
 
 
 def add_dataclasses_to_main_init(content: str, dataclasses: Dict[str, List[str]]):
-    dataclasses_list = sorted(
-        {cls for classes in dataclasses.values() for cls in classes}
-    )
+    dataclasses_list = sorted({cls for classes in dataclasses.values() for cls in classes})
     dataclasses_str = ", ".join(f"'{cls}'" for cls in dataclasses_list)
 
-    return MAIN_INIT_PY_REGEX.sub(
-        f'"inference._generated.types": [{dataclasses_str}]', content
-    )
+    return MAIN_INIT_PY_REGEX.sub(f'"inference._generated.types": [{dataclasses_str}]', content)
 
 
-def generate_reference_package(
-    dataclasses: Dict[str, List[str]], language: Literal["en", "ko"]
-) -> str:
+def generate_reference_package(dataclasses: Dict[str, List[str]], language: Literal["en", "ko"]) -> str:
     """Generate the reference package content."""
 
     per_task_docs = []
     for task in sorted(dataclasses.keys()):
-        lines = [
-            f"[[autodoc]] huggingface_hub.{cls}" for cls in sorted(dataclasses[task])
-        ]
+        lines = [f"[[autodoc]] huggingface_hub.{cls}" for cls in sorted(dataclasses[task])]
         lines_str = "\n\n".join(lines)
         if language == "en":
             # e.g. '## audio_classification'
             per_task_docs.append(f"\n## {task}\n\n{lines_str}\n\n")
         elif language == "ko":
             # e.g. '## audio_classification[[huggingface_hub.AudioClassificationInput]]'
-            per_task_docs.append(
-                f"\n## {task}[[huggingface_hub.{sorted(dataclasses[task])[0]}]]\n\n{lines_str}\n\n"
-            )
+            per_task_docs.append(f"\n## {task}[[huggingface_hub.{sorted(dataclasses[task])[0]}]]\n\n{lines_str}\n\n")
         else:
             raise ValueError(f"Language {language} is not supported.")
 
-    template = (
-        REFERENCE_PACKAGE_EN_CONTENT
-        if language == "en"
-        else REFERENCE_PACKAGE_KO_CONTENT
-    )
+    template = REFERENCE_PACKAGE_EN_CONTENT if language == "en" else REFERENCE_PACKAGE_KO_CONTENT
     return template.format(types="\n".join(per_task_docs))
 
 
@@ -386,31 +349,21 @@ def check_inference_types(update: bool)
         aliases[file.stem] = _list_type_aliases(formatted_content)
         check_and_update_file_content(file, formatted_content, update)
 
-    all_classes = {
-        module: dataclasses[module] + aliases[module] for module in dataclasses.keys()
-    }
+    all_classes = {module: dataclasses[module] + aliases[module] for module in dataclasses.keys()}
     init_py_content = create_init_py(all_classes)
     init_py_content = format_source_code(init_py_content)
     init_py_file = INFERENCE_TYPES_FOLDER_PATH / "__init__.py"
     check_and_update_file_content(init_py_file, init_py_content, update)
 
     main_init_py_content = MAIN_INIT_PY_FILE.read_text()
-    updated_main_init_py_content = add_dataclasses_to_main_init(
-        main_init_py_content, all_classes
-    )
+    updated_main_init_py_content = add_dataclasses_to_main_init(main_init_py_content, all_classes)
     updated_main_init_py_content = format_source_code(updated_main_init_py_content)
-    check_and_update_file_content(
-        MAIN_INIT_PY_FILE, updated_main_init_py_content, update
-    )
+    check_and_update_file_content(MAIN_INIT_PY_FILE, updated_main_init_py_content, update)
     reference_package_content_en = generate_reference_package(dataclasses, "en")
-    check_and_update_file_content(
-        REFERENCE_PACKAGE_EN_PATH, reference_package_content_en, update
-    )
+    check_and_update_file_content(REFERENCE_PACKAGE_EN_PATH, reference_package_content_en, update)
 
     reference_package_content_ko = generate_reference_package(dataclasses, "ko")
-    check_and_update_file_content(
-        REFERENCE_PACKAGE_KO_PATH, reference_package_content_ko, update
-    )
+    check_and_update_file_content(REFERENCE_PACKAGE_KO_PATH, reference_package_content_ko, update)
 
     print("✅ All good! (inference types)")
     exit(0)
--- python3-huggingface-hub-0.29.3.orig/utils/helpers.py
+++ python3-huggingface-hub-0.29.3/utils/helpers.py
@@ -28,9 +28,7 @@ def check_and_update_file_content(file:
     if content != expected_content:
         if update:
             file.write_text(expected_content)
-            print(
-                f"  {file} has been updated. Please make sure the changes are accurate and commit them."
-            )
+            print(f"  {file} has been updated. Please make sure the changes are accurate and commit them.")
         else:
             print(f"❌ Expected content mismatch in {file}.")
             exit(1)
@@ -45,9 +43,7 @@ def format_source_code(code: str) -> str
         if not ruff_bin:
             raise FileNotFoundError("Ruff executable not found.")
         try:
-            subprocess.run(
-                [ruff_bin, "check", str(filepath), "--fix", "--quiet"], check=True
-            )
+            subprocess.run([ruff_bin, "check", str(filepath), "--fix", "--quiet"], check=True)
             subprocess.run([ruff_bin, "format", str(filepath), "--quiet"], check=True)
         except subprocess.CalledProcessError as e:
             raise RuntimeError(f"Error running Ruff: {e}")
--- python3-huggingface-hub-0.29.3.orig/utils/push_repocard_examples.py
+++ python3-huggingface-hub-0.29.3/utils/push_repocard_examples.py
@@ -36,9 +36,7 @@ def check_can_push():
         print("You must be logged in to push repo card examples.")
 
     if all(org["name"] != ORG_NAME for org in me.get("orgs", [])):
-        print(
-            f"❌ You must have access to organization '{ORG_NAME}' to push repo card examples."
-        )
+        print(f"❌ You must have access to organization '{ORG_NAME}' to push repo card examples.")
         exit(1)
 
 
@@ -59,9 +57,7 @@ def push_model_card_example(overwrite: b
         ),
     )
     if not overwrite:
-        existing_content = Path(
-            hf_hub_download(MODEL_CARD_REPO_ID, REPOCARD_NAME, repo_type="model")
-        ).read_text()
+        existing_content = Path(hf_hub_download(MODEL_CARD_REPO_ID, REPOCARD_NAME, repo_type="model")).read_text()
         if content == existing_content:
             print("Model Card not pushed: did not change.")
             return
@@ -91,9 +87,7 @@ def push_dataset_card_example(overwrite:
         ),
     )
     if not overwrite:
-        existing_content = Path(
-            hf_hub_download(DATASET_CARD_REPO_ID, REPOCARD_NAME, repo_type="dataset")
-        ).read_text()
+        existing_content = Path(hf_hub_download(DATASET_CARD_REPO_ID, REPOCARD_NAME, repo_type="dataset")).read_text()
         if content == existing_content:
             print("Dataset Card not pushed: did not change.")
             return
