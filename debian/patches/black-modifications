Description:Formatted using black
 TODO: Put a short summary on the line above and replace this paragraph
 with a longer explanation of this change. Complete the meta-information
 with other relevant fields (see below for details). To make it easier, the
 information below has been extracted from the changelog. Adjust it or drop
 it.
 .
 python3-huggingface-hub (0.29.3-1) UNRELEASED; urgency=medium
 .
   * Initial release. (Closes: #nnnn)  <nnnn is the bug number of your ITP>
Author: anudeep <anudeep@unknown>

---
The information above should follow the Patch Tagging Guidelines, please
checkout https://dep.debian.net/deps/dep3/ to learn about the format. Here
are templates for supplementary fields that you might want to add:

Origin: (upstream|backport|vendor|other), (<patch-url>|commit:<commit-id>)
Bug: <upstream-bugtracker-url>
Bug-Debian: https://bugs.debian.org/<bugnumber>
Bug-Ubuntu: https://launchpad.net/bugs/<bugnumber>
Forwarded: (no|not-needed|<patch-forwarded-url>)
Applied-Upstream: <version>, (<commit-url>|commit:<commid-id>)
Reviewed-By: <name and email of someone who approved/reviewed the patch>
Last-Update: 2025-03-24

--- /dev/null
+++ python3-huggingface-hub-0.29.3/.ruff_cache/CACHEDIR.TAG
@@ -0,0 +1 @@
+Signature: 8a477f597d28d172789f06886806bc55
\ No newline at end of file
--- python3-huggingface-hub-0.29.3.orig/contrib/sentence_transformers/test_sentence_transformers.py
+++ python3-huggingface-hub-0.29.3/contrib/sentence_transformers/test_sentence_transformers.py
@@ -26,7 +26,9 @@ def test_from_pretrained(multi_qa_model:
     print("Similarity:", util.dot_score(query_embedding, passage_embedding))
 
 
-def test_push_to_hub(multi_qa_model: SentenceTransformer, repo_name: str, user: str, cleanup_repo: None) -> None:
+def test_push_to_hub(
+    multi_qa_model: SentenceTransformer, repo_name: str, user: str, cleanup_repo: None
+) -> None:
     multi_qa_model.save_to_hub(repo_name, organization=user)
 
     # Sleep to ensure that model_info isn't called too soon
--- python3-huggingface-hub-0.29.3.orig/contrib/utils.py
+++ python3-huggingface-hub-0.29.3/contrib/utils.py
@@ -41,7 +41,10 @@ def production_endpoint() -> Generator:
 
     patchers = (
         [patch(target + ".ENDPOINT", PROD_ENDPOINT) for target in ENDPOINT_TARGETS]
-        + [patch(target + ".HUGGINGFACE_CO_URL_TEMPLATE", PROD_URL_TEMPLATE) for target in URL_TEMPLATE_TARGETS]
+        + [
+            patch(target + ".HUGGINGFACE_CO_URL_TEMPLATE", PROD_URL_TEMPLATE)
+            for target in URL_TEMPLATE_TARGETS
+        ]
         + [patch.object(api, "endpoint", PROD_URL_TEMPLATE)]
     )
 
--- python3-huggingface-hub-0.29.3.orig/setup.py
+++ python3-huggingface-hub-0.29.3/setup.py
@@ -116,7 +116,9 @@ setup(
     packages=find_packages("src"),
     extras_require=extras,
     entry_points={
-        "console_scripts": ["huggingface-cli=huggingface_hub.commands.huggingface_cli:main"],
+        "console_scripts": [
+            "huggingface-cli=huggingface_hub.commands.huggingface_cli:main"
+        ],
         "fsspec.specs": "hf=huggingface_hub.HfFileSystem",
     },
     python_requires=">=3.8.0",
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/__init__.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/__init__.py
@@ -947,7 +947,9 @@ def _attach(package_name, submodules=Non
     else:
         submodules = set(submodules)
 
-    attr_to_modules = {attr: mod for mod, attrs in submod_attrs.items() for attr in attrs}
+    attr_to_modules = {
+        attr: mod for mod, attrs in submod_attrs.items() for attr in attrs
+    }
 
     def __getattr__(name):
         if name in submodules:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_commit_api.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_commit_api.py
@@ -11,7 +11,19 @@ from contextlib import contextmanager
 from dataclasses import dataclass, field
 from itertools import groupby
 from pathlib import Path, PurePosixPath
-from typing import TYPE_CHECKING, Any, BinaryIO, Dict, Iterable, Iterator, List, Literal, Optional, Tuple, Union
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    BinaryIO,
+    Dict,
+    Iterable,
+    Iterator,
+    List,
+    Literal,
+    Optional,
+    Tuple,
+    Union,
+)
 
 from tqdm.contrib.concurrent import thread_map
 
@@ -173,7 +185,9 @@ class CommitOperationAdd:
         if isinstance(self.path_or_fileobj, str):
             path_or_fileobj = os.path.normpath(os.path.expanduser(self.path_or_fileobj))
             if not os.path.isfile(path_or_fileobj):
-                raise ValueError(f"Provided path: '{path_or_fileobj}' is not a file on the local file system")
+                raise ValueError(
+                    f"Provided path: '{path_or_fileobj}' is not a file on the local file system"
+                )
         elif not isinstance(self.path_or_fileobj, (io.BufferedIOBase, bytes)):
             # ^^ Inspired from: https://stackoverflow.com/questions/44584829/how-to-determine-if-file-is-opened-in-binary-or-text-mode
             raise ValueError(
@@ -234,7 +248,9 @@ class CommitOperationAdd:
         config.json: 100%|█████████████████████████| 8.19k/8.19k [00:02<00:00, 3.72kB/s]
         ```
         """
-        if isinstance(self.path_or_fileobj, str) or isinstance(self.path_or_fileobj, Path):
+        if isinstance(self.path_or_fileobj, str) or isinstance(
+            self.path_or_fileobj, Path
+        ):
             if with_tqdm:
                 with tqdm_stream_file(self.path_or_fileobj) as file:
                     yield file
@@ -434,12 +450,21 @@ def _upload_lfs_files(
     def _wrapped_lfs_upload(batch_action) -> None:
         try:
             operation = oid2addop[batch_action["oid"]]
-            lfs_upload(operation=operation, lfs_batch_action=batch_action, headers=headers, endpoint=endpoint)
+            lfs_upload(
+                operation=operation,
+                lfs_batch_action=batch_action,
+                headers=headers,
+                endpoint=endpoint,
+            )
         except Exception as exc:
-            raise RuntimeError(f"Error while uploading '{operation.path_in_repo}' to the Hub.") from exc
+            raise RuntimeError(
+                f"Error while uploading '{operation.path_in_repo}' to the Hub."
+            ) from exc
 
     if constants.HF_HUB_ENABLE_HF_TRANSFER:
-        logger.debug(f"Uploading {len(filtered_actions)} LFS files to the Hub using `hf_transfer`.")
+        logger.debug(
+            f"Uploading {len(filtered_actions)} LFS files to the Hub using `hf_transfer`."
+        )
         for action in hf_tqdm(filtered_actions, name="huggingface_hub.lfs_upload"):
             _wrapped_lfs_upload(action)
     elif len(filtered_actions) == 1:
@@ -541,9 +566,15 @@ def _fetch_upload_modes(
         )
         hf_raise_for_status(resp)
         preupload_info = _validate_preupload_info(resp.json())
-        upload_modes.update(**{file["path"]: file["uploadMode"] for file in preupload_info["files"]})
-        should_ignore_info.update(**{file["path"]: file["shouldIgnore"] for file in preupload_info["files"]})
-        oid_info.update(**{file["path"]: file.get("oid") for file in preupload_info["files"]})
+        upload_modes.update(
+            **{file["path"]: file["uploadMode"] for file in preupload_info["files"]}
+        )
+        should_ignore_info.update(
+            **{file["path"]: file["shouldIgnore"] for file in preupload_info["files"]}
+        )
+        oid_info.update(
+            **{file["path"]: file.get("oid") for file in preupload_info["files"]}
+        )
 
     # Set upload mode for each addition operation
     for addition in additions:
@@ -655,7 +686,9 @@ def _fetch_files_to_copy(
                     f"Cannot copy {operation.src_path_in_repo} at revision "
                     f"{src_revision or revision}: file is missing on repo."
                 )
-            operation._src_oid = oid_info.get((operation.src_path_in_repo, operation.src_revision))
+            operation._src_oid = oid_info.get(
+                (operation.src_path_in_repo, operation.src_revision)
+            )
             operation._dest_oid = oid_info.get((operation.path_in_repo, revision))
     return files_to_copy
 
@@ -691,12 +724,17 @@ def _prepare_commit_payload(
     for operation in operations:
         # Skip ignored files
         if isinstance(operation, CommitOperationAdd) and operation._should_ignore:
-            logger.debug(f"Skipping file '{operation.path_in_repo}' in commit (ignored by gitignore file).")
+            logger.debug(
+                f"Skipping file '{operation.path_in_repo}' in commit (ignored by gitignore file)."
+            )
             nb_ignored_files += 1
             continue
 
         # 2.a. Case adding a regular file
-        if isinstance(operation, CommitOperationAdd) and operation._upload_mode == "regular":
+        if (
+            isinstance(operation, CommitOperationAdd)
+            and operation._upload_mode == "regular"
+        ):
             yield {
                 "key": "file",
                 "value": {
@@ -706,7 +744,10 @@ def _prepare_commit_payload(
                 },
             }
         # 2.b. Case adding an LFS file
-        elif isinstance(operation, CommitOperationAdd) and operation._upload_mode == "lfs":
+        elif (
+            isinstance(operation, CommitOperationAdd)
+            and operation._upload_mode == "lfs"
+        ):
             yield {
                 "key": "lfsFile",
                 "value": {
@@ -724,7 +765,9 @@ def _prepare_commit_payload(
             }
         # 2.d. Case copying a file or folder
         elif isinstance(operation, CommitOperationCopy):
-            file_to_copy = files_to_copy[(operation.src_path_in_repo, operation.src_revision)]
+            file_to_copy = files_to_copy[
+                (operation.src_path_in_repo, operation.src_revision)
+            ]
             if isinstance(file_to_copy, bytes):
                 yield {
                     "key": "file",
@@ -755,4 +798,6 @@ def _prepare_commit_payload(
             )
 
     if nb_ignored_files > 0:
-        logger.info(f"Skipped {nb_ignored_files} file(s) in commit (ignored by gitignore file).")
+        logger.info(
+            f"Skipped {nb_ignored_files} file(s) in commit (ignored by gitignore file)."
+        )
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_commit_scheduler.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_commit_scheduler.py
@@ -127,18 +127,24 @@ class CommitScheduler:
         self.ignore_patterns = ignore_patterns + DEFAULT_IGNORE_PATTERNS
 
         if self.folder_path.is_file():
-            raise ValueError(f"'folder_path' must be a directory, not a file: '{self.folder_path}'.")
+            raise ValueError(
+                f"'folder_path' must be a directory, not a file: '{self.folder_path}'."
+            )
         self.folder_path.mkdir(parents=True, exist_ok=True)
 
         # Repository
-        repo_url = self.api.create_repo(repo_id=repo_id, private=private, repo_type=repo_type, exist_ok=True)
+        repo_url = self.api.create_repo(
+            repo_id=repo_id, private=private, repo_type=repo_type, exist_ok=True
+        )
         self.repo_id = repo_url.repo_id
         self.repo_type = repo_type
         self.revision = revision
         self.token = token
 
         # Keep track of already uploaded files
-        self.last_uploaded: Dict[Path, float] = {}  # key is local path, value is timestamp
+        self.last_uploaded: Dict[Path, float] = (
+            {}
+        )  # key is local path, value is timestamp
 
         # Scheduler
         if not every > 0:
@@ -147,7 +153,9 @@ class CommitScheduler:
         self.every = every
         self.squash_history = squash_history
 
-        logger.info(f"Scheduled job to push '{self.folder_path}' to '{self.repo_id}' every {self.every} minutes.")
+        logger.info(
+            f"Scheduled job to push '{self.folder_path}' to '{self.repo_id}' every {self.every} minutes."
+        )
         self._scheduler_thread = Thread(target=self._run_scheduler, daemon=True)
         self._scheduler_thread.start()
         atexit.register(self._push_to_hub)
@@ -195,10 +203,14 @@ class CommitScheduler:
             value = self.push_to_hub()
             if self.squash_history:
                 logger.info("(Background) squashing repo history.")
-                self.api.super_squash_history(repo_id=self.repo_id, repo_type=self.repo_type, branch=self.revision)
+                self.api.super_squash_history(
+                    repo_id=self.repo_id, repo_type=self.repo_type, branch=self.revision
+                )
             return value
         except Exception as e:
-            logger.error(f"Error while pushing to Hub: {e}")  # Depending on the setup, error might be silenced
+            logger.error(
+                f"Error while pushing to Hub: {e}"
+            )  # Depending on the setup, error might be silenced
             raise
 
     def push_to_hub(self) -> Optional[CommitInfo]:
@@ -226,7 +238,9 @@ class CommitScheduler:
             # List files from folder (taken from `_prepare_upload_folder_additions`)
             relpath_to_abspath = {
                 path.relative_to(self.folder_path).as_posix(): path
-                for path in sorted(self.folder_path.glob("**/*"))  # sorted to be deterministic
+                for path in sorted(
+                    self.folder_path.glob("**/*")
+                )  # sorted to be deterministic
                 if path.is_file()
             }
             prefix = f"{self.path_in_repo.strip('/')}/" if self.path_in_repo else ""
@@ -234,11 +248,16 @@ class CommitScheduler:
             # Filter with pattern + filter out unchanged files + retrieve current file size
             files_to_upload: List[_FileToUpload] = []
             for relpath in filter_repo_objects(
-                relpath_to_abspath.keys(), allow_patterns=self.allow_patterns, ignore_patterns=self.ignore_patterns
+                relpath_to_abspath.keys(),
+                allow_patterns=self.allow_patterns,
+                ignore_patterns=self.ignore_patterns,
             ):
                 local_path = relpath_to_abspath[relpath]
                 stat = local_path.stat()
-                if self.last_uploaded.get(local_path) is None or self.last_uploaded[local_path] != stat.st_mtime:
+                if (
+                    self.last_uploaded.get(local_path) is None
+                    or self.last_uploaded[local_path] != stat.st_mtime
+                ):
                     files_to_upload.append(
                         _FileToUpload(
                             local_path=local_path,
@@ -258,7 +277,9 @@ class CommitScheduler:
         add_operations = [
             CommitOperationAdd(
                 # Cap the file to its current size, even if the user append data to it while a scheduled commit is happening
-                path_or_fileobj=PartialFileIO(file_to_upload.local_path, size_limit=file_to_upload.size_limit),
+                path_or_fileobj=PartialFileIO(
+                    file_to_upload.local_path, size_limit=file_to_upload.size_limit
+                ),
                 path_in_repo=file_to_upload.path_in_repo,
             )
             for file_to_upload in files_to_upload
@@ -309,13 +330,19 @@ class PartialFileIO(BytesIO):
         return super().__del__()
 
     def __repr__(self) -> str:
-        return f"<PartialFileIO file_path={self._file_path} size_limit={self._size_limit}>"
+        return (
+            f"<PartialFileIO file_path={self._file_path} size_limit={self._size_limit}>"
+        )
 
     def __len__(self) -> int:
         return self._size_limit
 
     def __getattribute__(self, name: str):
-        if name.startswith("_") or name in ("read", "tell", "seek"):  # only 3 public methods supported
+        if name.startswith("_") or name in (
+            "read",
+            "tell",
+            "seek",
+        ):  # only 3 public methods supported
             return super().__getattribute__(name)
         raise NotImplementedError(f"PartialFileIO does not support '{name}'.")
 
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_inference_endpoints.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_inference_endpoints.py
@@ -120,7 +120,11 @@ class InferenceEndpoint:
 
     @classmethod
     def from_raw(
-        cls, raw: Dict, namespace: str, token: Union[str, bool, None] = None, api: Optional["HfApi"] = None
+        cls,
+        raw: Dict,
+        namespace: str,
+        token: Union[str, bool, None] = None,
+        api: Optional["HfApi"] = None,
     ) -> "InferenceEndpoint":
         """Initialize object from raw dictionary."""
         if api is None:
@@ -177,7 +181,9 @@ class InferenceEndpoint:
             token=self._token,  # type: ignore[arg-type] # boolean token shouldn't be possible. In practice it's ok.
         )
 
-    def wait(self, timeout: Optional[int] = None, refresh_every: int = 5) -> "InferenceEndpoint":
+    def wait(
+        self, timeout: Optional[int] = None, refresh_every: int = 5
+    ) -> "InferenceEndpoint":
         """Wait for the Inference Endpoint to be deployed.
 
         Information from the server will be fetched every 1s. If the Inference Endpoint is not deployed after `timeout`
@@ -217,15 +223,21 @@ class InferenceEndpoint:
                 )
             if self.status == InferenceEndpointStatus.RUNNING and self.url is not None:
                 # Verify the endpoint is actually reachable
-                response = get_session().get(self.url, headers=self._api._build_hf_headers(token=self._token))
+                response = get_session().get(
+                    self.url, headers=self._api._build_hf_headers(token=self._token)
+                )
                 if response.status_code == 200:
                     logger.info("Inference Endpoint is ready to be used.")
                     return self
 
             if timeout is not None:
                 if time.time() - start > timeout:
-                    raise InferenceEndpointTimeoutError("Timeout while waiting for Inference Endpoint to be deployed.")
-            logger.info(f"Inference Endpoint is not deployed yet ({self.status}). Waiting {refresh_every}s...")
+                    raise InferenceEndpointTimeoutError(
+                        "Timeout while waiting for Inference Endpoint to be deployed."
+                    )
+            logger.info(
+                f"Inference Endpoint is not deployed yet ({self.status}). Waiting {refresh_every}s..."
+            )
             time.sleep(refresh_every)
             self.fetch()
 
@@ -353,7 +365,10 @@ class InferenceEndpoint:
             [`InferenceEndpoint`]: the same Inference Endpoint, mutated in place with the latest data.
         """
         obj = self._api.resume_inference_endpoint(
-            name=self.name, namespace=self.namespace, running_ok=running_ok, token=self._token
+            name=self.name,
+            namespace=self.namespace,
+            running_ok=running_ok,
+            token=self._token,
         )  # type: ignore [arg-type]
         self.raw = obj.raw
         self._populate_from_raw()
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_local_folder.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_local_folder.py
@@ -86,7 +86,10 @@ class LocalDownloadFilePaths:
 
     def incomplete_path(self, etag: str) -> Path:
         """Return the path where a file will be temporarily downloaded before being moved to `file_path`."""
-        return self.metadata_path.parent / f"{_short_hash(self.metadata_path.name)}.{etag}.incomplete"
+        return (
+            self.metadata_path.parent
+            / f"{_short_hash(self.metadata_path.name)}.{etag}.incomplete"
+        )
 
 
 @dataclass(frozen=True)
@@ -204,20 +207,27 @@ def get_local_download_paths(local_dir:
                 " owner to rename this file."
             )
     file_path = local_dir / sanitized_filename
-    metadata_path = _huggingface_dir(local_dir) / "download" / f"{sanitized_filename}.metadata"
+    metadata_path = (
+        _huggingface_dir(local_dir) / "download" / f"{sanitized_filename}.metadata"
+    )
     lock_path = metadata_path.with_suffix(".lock")
 
     # Some Windows versions do not allow for paths longer than 255 characters.
     # In this case, we must specify it as an extended path by using the "\\?\" prefix
     if os.name == "nt":
-        if not str(local_dir).startswith("\\\\?\\") and len(os.path.abspath(lock_path)) > 255:
+        if (
+            not str(local_dir).startswith("\\\\?\\")
+            and len(os.path.abspath(lock_path)) > 255
+        ):
             file_path = Path("\\\\?\\" + os.path.abspath(file_path))
             lock_path = Path("\\\\?\\" + os.path.abspath(lock_path))
             metadata_path = Path("\\\\?\\" + os.path.abspath(metadata_path))
 
     file_path.parent.mkdir(parents=True, exist_ok=True)
     metadata_path.parent.mkdir(parents=True, exist_ok=True)
-    return LocalDownloadFilePaths(file_path=file_path, lock_path=lock_path, metadata_path=metadata_path)
+    return LocalDownloadFilePaths(
+        file_path=file_path, lock_path=lock_path, metadata_path=metadata_path
+    )
 
 
 def get_local_upload_paths(local_dir: Path, filename: str) -> LocalUploadFilePaths:
@@ -244,13 +254,18 @@ def get_local_upload_paths(local_dir: Pa
                 " owner to rename this file."
             )
     file_path = local_dir / sanitized_filename
-    metadata_path = _huggingface_dir(local_dir) / "upload" / f"{sanitized_filename}.metadata"
+    metadata_path = (
+        _huggingface_dir(local_dir) / "upload" / f"{sanitized_filename}.metadata"
+    )
     lock_path = metadata_path.with_suffix(".lock")
 
     # Some Windows versions do not allow for paths longer than 255 characters.
     # In this case, we must specify it as an extended path by using the "\\?\" prefix
     if os.name == "nt":
-        if not str(local_dir).startswith("\\\\?\\") and len(os.path.abspath(lock_path)) > 255:
+        if (
+            not str(local_dir).startswith("\\\\?\\")
+            and len(os.path.abspath(lock_path)) > 255
+        ):
             file_path = Path("\\\\?\\" + os.path.abspath(file_path))
             lock_path = Path("\\\\?\\" + os.path.abspath(lock_path))
             metadata_path = Path("\\\\?\\" + os.path.abspath(metadata_path))
@@ -258,11 +273,16 @@ def get_local_upload_paths(local_dir: Pa
     file_path.parent.mkdir(parents=True, exist_ok=True)
     metadata_path.parent.mkdir(parents=True, exist_ok=True)
     return LocalUploadFilePaths(
-        path_in_repo=filename, file_path=file_path, lock_path=lock_path, metadata_path=metadata_path
+        path_in_repo=filename,
+        file_path=file_path,
+        lock_path=lock_path,
+        metadata_path=metadata_path,
     )
 
 
-def read_download_metadata(local_dir: Path, filename: str) -> Optional[LocalDownloadFileMetadata]:
+def read_download_metadata(
+    local_dir: Path, filename: str
+) -> Optional[LocalDownloadFileMetadata]:
     """Read metadata about a file in the local directory related to a download process.
 
     Args:
@@ -296,7 +316,9 @@ def read_download_metadata(local_dir: Pa
                 try:
                     paths.metadata_path.unlink()
                 except Exception as e:
-                    logger.warning(f"Could not remove corrupted metadata file {paths.metadata_path}: {e}")
+                    logger.warning(
+                        f"Could not remove corrupted metadata file {paths.metadata_path}: {e}"
+                    )
 
             try:
                 # check if the file exists and hasn't been modified since the metadata was saved
@@ -305,7 +327,9 @@ def read_download_metadata(local_dir: Pa
                     stat.st_mtime - 1 <= metadata.timestamp
                 ):  # allow 1s difference as stat.st_mtime might not be precise
                     return metadata
-                logger.info(f"Ignored metadata for '{filename}' (outdated). Will re-compute hash.")
+                logger.info(
+                    f"Ignored metadata for '{filename}' (outdated). Will re-compute hash."
+                )
             except FileNotFoundError:
                 # file does not exist => metadata is outdated
                 return None
@@ -336,7 +360,9 @@ def read_upload_metadata(local_dir: Path
                     size = int(f.readline().strip())  # never None
 
                     _should_ignore = f.readline().strip()
-                    should_ignore = None if _should_ignore == "" else bool(int(_should_ignore))
+                    should_ignore = (
+                        None if _should_ignore == "" else bool(int(_should_ignore))
+                    )
 
                     _sha256 = f.readline().strip()
                     sha256 = None if _sha256 == "" else _sha256
@@ -344,7 +370,9 @@ def read_upload_metadata(local_dir: Path
                     _upload_mode = f.readline().strip()
                     upload_mode = None if _upload_mode == "" else _upload_mode
                     if upload_mode not in (None, "regular", "lfs"):
-                        raise ValueError(f"Invalid upload mode in metadata {paths.path_in_repo}: {upload_mode}")
+                        raise ValueError(
+                            f"Invalid upload mode in metadata {paths.path_in_repo}: {upload_mode}"
+                        )
 
                     is_uploaded = bool(int(f.readline().strip()))
                     is_committed = bool(int(f.readline().strip()))
@@ -366,22 +394,30 @@ def read_upload_metadata(local_dir: Path
                 try:
                     paths.metadata_path.unlink()
                 except Exception as e:
-                    logger.warning(f"Could not remove corrupted metadata file {paths.metadata_path}: {e}")
+                    logger.warning(
+                        f"Could not remove corrupted metadata file {paths.metadata_path}: {e}"
+                    )
 
             # TODO: can we do better?
             if (
                 metadata.timestamp is not None
                 and metadata.is_uploaded  # file was uploaded
                 and not metadata.is_committed  # but not committed
-                and time.time() - metadata.timestamp > 20 * 3600  # and it's been more than 20 hours
+                and time.time() - metadata.timestamp
+                > 20 * 3600  # and it's been more than 20 hours
             ):  # => we consider it as garbage-collected by S3
                 metadata.is_uploaded = False
 
             # check if the file exists and hasn't been modified since the metadata was saved
             try:
-                if metadata.timestamp is not None and paths.file_path.stat().st_mtime <= metadata.timestamp:
+                if (
+                    metadata.timestamp is not None
+                    and paths.file_path.stat().st_mtime <= metadata.timestamp
+                ):
                     return metadata
-                logger.info(f"Ignored metadata for '{filename}' (outdated). Will re-compute hash.")
+                logger.info(
+                    f"Ignored metadata for '{filename}' (outdated). Will re-compute hash."
+                )
             except FileNotFoundError:
                 # file does not exist => metadata is outdated
                 pass
@@ -390,7 +426,9 @@ def read_upload_metadata(local_dir: Path
     return LocalUploadFileMetadata(size=paths.file_path.stat().st_size)
 
 
-def write_download_metadata(local_dir: Path, filename: str, commit_hash: str, etag: str) -> None:
+def write_download_metadata(
+    local_dir: Path, filename: str, commit_hash: str, etag: str
+) -> None:
     """Write metadata about a file in the local directory related to a download process.
 
     Args:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_login.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_login.py
@@ -142,7 +142,9 @@ def logout(token_name: Optional[str] = N
         [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError):
             If the access token name is not found.
     """
-    if get_token() is None and not get_stored_tokens():  # No active token and no saved access tokens
+    if (
+        get_token() is None and not get_stored_tokens()
+    ):  # No active token and no saved access tokens
         logger.warning("Not logged in!")
         return
     if not token_name:
@@ -190,7 +192,9 @@ def auth_switch(token_name: str, add_to_
     """
     token = _get_token_by_name(token_name)
     if not token:
-        raise ValueError(f"Access token {token_name} not found in {constants.HF_STORED_TOKENS_PATH}")
+        raise ValueError(
+            f"Access token {token_name} not found in {constants.HF_STORED_TOKENS_PATH}"
+        )
     # Write token to HF_TOKEN_PATH
     _set_active_token(token_name, add_to_git_credential)
     logger.info(f"The current active token is: {token_name}")
@@ -225,7 +229,11 @@ def auth_list() -> None:
         masked_token = f"{token[:3]}****{token[-4:]}" if token != "<not set>" else token
         is_current = "*" if token == current_token else " "
 
-        print(f"{is_current} {{:<{max_offset}}}| {{:<15}}".format(token_name, masked_token))
+        print(
+            f"{is_current} {{:<{max_offset}}}| {{:<15}}".format(
+                token_name, masked_token
+            )
+        )
 
     if _get_token_from_environment():
         logger.warning(
@@ -248,7 +256,9 @@ def auth_list() -> None:
     custom_message="Fine-grained tokens added complexity to the permissions, making it irrelevant to check if a token has 'write' access.",
 )
 @_deprecate_positional_args(version="1.0")
-def interpreter_login(*, new_session: bool = True, write_permission: bool = False) -> None:
+def interpreter_login(
+    *, new_session: bool = True, write_permission: bool = False
+) -> None:
     """
     Displays a prompt to log in to the HF website and store the token.
 
@@ -348,10 +358,14 @@ def notebook_login(*, new_session: bool
         logger.info("User is already logged in.")
         return
 
-    box_layout = widgets.Layout(display="flex", flex_flow="column", align_items="center", width="50%")
+    box_layout = widgets.Layout(
+        display="flex", flex_flow="column", align_items="center", width="50%"
+    )
 
     token_widget = widgets.Password(description="Token:")
-    git_checkbox_widget = widgets.Checkbox(value=True, description="Add token as git credential?")
+    git_checkbox_widget = widgets.Checkbox(
+        value=True, description="Add token as git credential?"
+    )
     token_finish_button = widgets.Button(description="Login")
 
     login_token_widget = widgets.VBox(
@@ -382,7 +396,9 @@ def notebook_login(*, new_session: bool
         except Exception as error:
             message = str(error)
         # Print result (success message or error)
-        login_token_widget.children = [widgets.Label(line) for line in message.split("\n") if line.strip()]
+        login_token_widget.children = [
+            widgets.Label(line) for line in message.split("\n") if line.strip()
+        ]
 
     token_finish_button.on_click(login_token_event)
 
@@ -399,7 +415,9 @@ def _login(
     from .hf_api import whoami  # avoid circular import
 
     if token.startswith("api_org"):
-        raise ValueError("You must use your personal account token, not an organization token.")
+        raise ValueError(
+            "You must use your personal account token, not an organization token."
+        )
 
     token_info = whoami(token)
     permission = token_info["auth"]["accessToken"]["role"]
@@ -409,7 +427,9 @@ def _login(
     # Store token locally
     _save_token(token=token, token_name=token_name)
     # Set active token
-    _set_active_token(token_name=token_name, add_to_git_credential=add_to_git_credential)
+    _set_active_token(
+        token_name=token_name, add_to_git_credential=add_to_git_credential
+    )
     logger.info("Login successful.")
     if _get_token_from_environment():
         logger.warning(
@@ -454,7 +474,9 @@ def _set_active_token(
     """
     token = _get_token_by_name(token_name)
     if not token:
-        raise ValueError(f"Token {token_name} not found in {constants.HF_STORED_TOKENS_PATH}")
+        raise ValueError(
+            f"Token {token_name} not found in {constants.HF_STORED_TOKENS_PATH}"
+        )
     if add_to_git_credential:
         if _is_git_credential_helper_configured():
             set_git_credential(token)
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_snapshot_download.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_snapshot_download.py
@@ -7,10 +7,20 @@ from tqdm.auto import tqdm as base_tqdm
 from tqdm.contrib.concurrent import thread_map
 
 from . import constants
-from .errors import GatedRepoError, LocalEntryNotFoundError, RepositoryNotFoundError, RevisionNotFoundError
+from .errors import (
+    GatedRepoError,
+    LocalEntryNotFoundError,
+    RepositoryNotFoundError,
+    RevisionNotFoundError,
+)
 from .file_download import REGEX_COMMIT_HASH, hf_hub_download, repo_folder_name
 from .hf_api import DatasetInfo, HfApi, ModelInfo, SpaceInfo
-from .utils import OfflineModeIsEnabled, filter_repo_objects, logging, validate_hf_hub_args
+from .utils import (
+    OfflineModeIsEnabled,
+    filter_repo_objects,
+    logging,
+    validate_hf_hub_args,
+)
 from .utils import tqdm as hf_tqdm
 
 
@@ -135,9 +145,13 @@ def snapshot_download(
     if repo_type is None:
         repo_type = "model"
     if repo_type not in constants.REPO_TYPES:
-        raise ValueError(f"Invalid repo type: {repo_type}. Accepted repo types are: {str(constants.REPO_TYPES)}")
+        raise ValueError(
+            f"Invalid repo type: {repo_type}. Accepted repo types are: {str(constants.REPO_TYPES)}"
+        )
 
-    storage_folder = os.path.join(cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type))
+    storage_folder = os.path.join(
+        cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type)
+    )
 
     repo_info: Union[ModelInfo, DatasetInfo, SpaceInfo, None] = None
     api_call_error: Optional[Exception] = None
@@ -152,7 +166,9 @@ def snapshot_download(
                 endpoint=endpoint,
                 headers=headers,
             )
-            repo_info = api.repo_info(repo_id=repo_id, repo_type=repo_type, revision=revision, token=token)
+            repo_info = api.repo_info(
+                repo_id=repo_id, repo_type=repo_type, revision=revision, token=token
+            )
         except (requests.exceptions.SSLError, requests.exceptions.ProxyError):
             # Actually raise for those subclasses of ConnectionError
             raise
@@ -227,7 +243,9 @@ def snapshot_download(
                 "outgoing traffic has been disabled. To enable repo look-ups and downloads online, set "
                 "'HF_HUB_OFFLINE=0' as environment variable."
             ) from api_call_error
-        elif isinstance(api_call_error, RepositoryNotFoundError) or isinstance(api_call_error, GatedRepoError):
+        elif isinstance(api_call_error, RepositoryNotFoundError) or isinstance(
+            api_call_error, GatedRepoError
+        ):
             # Repo not found => let's raise the actual error
             raise api_call_error
         else:
@@ -240,8 +258,12 @@ def snapshot_download(
 
     # At this stage, internet connection is up and running
     # => let's download the files!
-    assert repo_info.sha is not None, "Repo info returned from server must have a revision sha."
-    assert repo_info.siblings is not None, "Repo info returned from server must have a siblings list."
+    assert (
+        repo_info.sha is not None
+    ), "Repo info returned from server must have a revision sha."
+    assert (
+        repo_info.siblings is not None
+    ), "Repo info returned from server must have a siblings list."
     filtered_repo_files = list(
         filter_repo_objects(
             items=[f.rfilename for f in repo_info.siblings],
@@ -261,7 +283,9 @@ def snapshot_download(
             with open(ref_path, "w") as f:
                 f.write(commit_hash)
         except OSError as e:
-            logger.warning(f"Ignored error while writing commit hash to {ref_path}: {e}.")
+            logger.warning(
+                f"Ignored error while writing commit hash to {ref_path}: {e}."
+            )
 
     # we pass the commit_hash to hf_hub_download
     # so no network call happens if we already
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_tensorboard_logger.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_tensorboard_logger.py
@@ -148,7 +148,9 @@ class HFSummaryWriter(SummaryWriter):
 
         # Check logdir has been correctly initialized and fail early otherwise. In practice, SummaryWriter takes care of it.
         if not isinstance(self.logdir, str):
-            raise ValueError(f"`self.logdir` must be a string. Got '{self.logdir}' of type {type(self.logdir)}.")
+            raise ValueError(
+                f"`self.logdir` must be a string. Got '{self.logdir}' of type {type(self.logdir)}."
+            )
 
         # Append logdir name to `path_in_repo`
         if path_in_repo is None or path_in_repo == "":
@@ -178,7 +180,9 @@ class HFSummaryWriter(SummaryWriter):
 
         # Add `hf-summary-writer` tag to the model card metadata
         try:
-            card = ModelCard.load(repo_id_or_path=self.repo_id, repo_type=self.repo_type)
+            card = ModelCard.load(
+                repo_id_or_path=self.repo_id, repo_type=self.repo_type
+            )
         except EntryNotFoundError:
             card = ModelCard("")
         tags = card.data.get("tags", [])
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_upload_large_folder.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_upload_large_folder.py
@@ -29,7 +29,12 @@ from urllib.parse import quote
 
 from . import constants
 from ._commit_api import CommitOperationAdd, UploadInfo, _fetch_upload_modes
-from ._local_folder import LocalUploadFileMetadata, LocalUploadFilePaths, get_local_upload_paths, read_upload_metadata
+from ._local_folder import (
+    LocalUploadFileMetadata,
+    LocalUploadFilePaths,
+    get_local_upload_paths,
+    read_upload_metadata,
+)
 from .constants import DEFAULT_REVISION, REPO_TYPES
 from .utils import DEFAULT_IGNORE_PATTERNS, filter_repo_objects, tqdm
 from .utils._cache_manager import _format_size
@@ -90,17 +95,25 @@ def upload_large_folder_internal(
         num_workers = max(nb_cores - 2, 2)  # Use all but 2 cores, or at least 2 cores
 
     # 2. Create repo if missing
-    repo_url = api.create_repo(repo_id=repo_id, repo_type=repo_type, private=private, exist_ok=True)
+    repo_url = api.create_repo(
+        repo_id=repo_id, repo_type=repo_type, private=private, exist_ok=True
+    )
     logger.info(f"Repo created: {repo_url}")
     repo_id = repo_url.repo_id
 
     # 3. List files to upload
     filtered_paths_list = filter_repo_objects(
-        (path.relative_to(folder_path).as_posix() for path in folder_path.glob("**/*") if path.is_file()),
+        (
+            path.relative_to(folder_path).as_posix()
+            for path in folder_path.glob("**/*")
+            if path.is_file()
+        ),
         allow_patterns=allow_patterns,
         ignore_patterns=ignore_patterns,
     )
-    paths_list = [get_local_upload_paths(folder_path, relpath) for relpath in filtered_paths_list]
+    paths_list = [
+        get_local_upload_paths(folder_path, relpath) for relpath in filtered_paths_list
+    ]
     logger.info(f"Found {len(paths_list)} candidate files to upload")
 
     # Read metadata for each file
@@ -197,7 +210,9 @@ class LargeUploadStatus:
             elif not metadata.is_committed:
                 self.queue_commit.put(item)
             else:
-                logger.debug(f"Skipping file {paths.path_in_repo} (already uploaded and committed)")
+                logger.debug(
+                    f"Skipping file {paths.path_in_repo} (already uploaded and committed)"
+                )
 
     def current_report(self) -> str:
         """Generate a report of the current status of the large upload."""
@@ -264,7 +279,10 @@ class LargeUploadStatus:
 
     def is_done(self) -> bool:
         with self.lock:
-            return all(metadata.is_committed or metadata.should_ignore for _, metadata in self.items)
+            return all(
+                metadata.is_committed or metadata.should_ignore
+                for _, metadata in self.items
+            )
 
 
 def _worker_job(
@@ -309,7 +327,13 @@ def _worker_job(
 
         elif job == WorkerJob.GET_UPLOAD_MODE:
             try:
-                _get_upload_mode(items, api=api, repo_id=repo_id, repo_type=repo_type, revision=revision)
+                _get_upload_mode(
+                    items,
+                    api=api,
+                    repo_id=repo_id,
+                    repo_type=repo_type,
+                    revision=revision,
+                )
             except KeyboardInterrupt:
                 raise
             except Exception as e:
@@ -338,7 +362,13 @@ def _worker_job(
         elif job == WorkerJob.PREUPLOAD_LFS:
             item = items[0]  # single item
             try:
-                _preupload_lfs(item, api=api, repo_id=repo_id, repo_type=repo_type, revision=revision)
+                _preupload_lfs(
+                    item,
+                    api=api,
+                    repo_id=repo_id,
+                    repo_type=repo_type,
+                    revision=revision,
+                )
                 status.queue_commit.put(item)
             except KeyboardInterrupt:
                 raise
@@ -352,7 +382,13 @@ def _worker_job(
 
         elif job == WorkerJob.COMMIT:
             try:
-                _commit(items, api=api, repo_id=repo_id, repo_type=repo_type, revision=revision)
+                _commit(
+                    items,
+                    api=api,
+                    repo_id=repo_id,
+                    repo_type=repo_type,
+                    revision=revision,
+                )
             except KeyboardInterrupt:
                 raise
             except Exception as e:
@@ -370,7 +406,9 @@ def _worker_job(
                 status.nb_workers_waiting -= 1
 
 
-def _determine_next_job(status: LargeUploadStatus) -> Optional[Tuple[WorkerJob, List[JOB_ITEM_T]]]:
+def _determine_next_job(
+    status: LargeUploadStatus,
+) -> Optional[Tuple[WorkerJob, List[JOB_ITEM_T]]]:
     with status.lock:
         # 1. Commit if more than 5 minutes since last commit attempt (and at least 1 file)
         if (
@@ -396,7 +434,10 @@ def _determine_next_job(status: LargeUpl
             return (WorkerJob.GET_UPLOAD_MODE, _get_n(status.queue_get_upload_mode, 50))
 
         # 4. Preupload LFS file if at least 1 file and no worker is preuploading LFS
-        elif status.queue_preupload_lfs.qsize() > 0 and status.nb_workers_preupload_lfs == 0:
+        elif (
+            status.queue_preupload_lfs.qsize() > 0
+            and status.nb_workers_preupload_lfs == 0
+        ):
             status.nb_workers_preupload_lfs += 1
             logger.debug("Job: preupload LFS (no other worker preuploading LFS)")
             return (WorkerJob.PREUPLOAD_LFS, _get_one(status.queue_preupload_lfs))
@@ -408,7 +449,10 @@ def _determine_next_job(status: LargeUpl
             return (WorkerJob.SHA256, _get_one(status.queue_sha256))
 
         # 6. Get upload mode if at least 1 file and no worker is getting upload mode
-        elif status.queue_get_upload_mode.qsize() > 0 and status.nb_workers_get_upload_mode == 0:
+        elif (
+            status.queue_get_upload_mode.qsize() > 0
+            and status.nb_workers_get_upload_mode == 0
+        ):
             status.nb_workers_get_upload_mode += 1
             logger.debug("Job: get upload mode (no other worker getting upload mode)")
             return (WorkerJob.GET_UPLOAD_MODE, _get_n(status.queue_get_upload_mode, 50))
@@ -416,7 +460,8 @@ def _determine_next_job(status: LargeUpl
         # 7. Preupload LFS file if at least 1 file
         #    Skip if hf_transfer is enabled and there is already a worker preuploading LFS
         elif status.queue_preupload_lfs.qsize() > 0 and (
-            status.nb_workers_preupload_lfs == 0 or not constants.HF_HUB_ENABLE_HF_TRANSFER
+            status.nb_workers_preupload_lfs == 0
+            or not constants.HF_HUB_ENABLE_HF_TRANSFER
         ):
             status.nb_workers_preupload_lfs += 1
             logger.debug("Job: preupload LFS")
@@ -462,7 +507,10 @@ def _determine_next_job(status: LargeUpl
             return (WorkerJob.COMMIT, _get_items_to_commit(status.queue_commit))
 
         # 12. If all queues are empty, exit
-        elif all(metadata.is_committed or metadata.should_ignore for _, metadata in status.items):
+        elif all(
+            metadata.is_committed or metadata.should_ignore
+            for _, metadata in status.items
+        ):
             logger.info("All files have been processed! Exiting worker.")
             return None
 
@@ -487,7 +535,9 @@ def _compute_sha256(item: JOB_ITEM_T) ->
     metadata.save(paths)
 
 
-def _get_upload_mode(items: List[JOB_ITEM_T], api: "HfApi", repo_id: str, repo_type: str, revision: str) -> None:
+def _get_upload_mode(
+    items: List[JOB_ITEM_T], api: "HfApi", repo_id: str, repo_type: str, revision: str
+) -> None:
     """Get upload mode for each file and update metadata.
 
     Also receive info if the file should be ignored.
@@ -507,7 +557,9 @@ def _get_upload_mode(items: List[JOB_ITE
         metadata.save(paths)
 
 
-def _preupload_lfs(item: JOB_ITEM_T, api: "HfApi", repo_id: str, repo_type: str, revision: str) -> None:
+def _preupload_lfs(
+    item: JOB_ITEM_T, api: "HfApi", repo_id: str, repo_type: str, revision: str
+) -> None:
     """Preupload LFS file and update metadata."""
     paths, metadata = item
     addition = _build_hacky_operation(item)
@@ -522,7 +574,9 @@ def _preupload_lfs(item: JOB_ITEM_T, api
     metadata.save(paths)
 
 
-def _commit(items: List[JOB_ITEM_T], api: "HfApi", repo_id: str, repo_type: str, revision: str) -> None:
+def _commit(
+    items: List[JOB_ITEM_T], api: "HfApi", repo_id: str, repo_type: str, revision: str
+) -> None:
     """Commit files to the repo."""
     additions = [_build_hacky_operation(item) for item in items]
     api.create_commit(
@@ -550,12 +604,16 @@ class HackyCommitOperationAdd(CommitOper
 
 def _build_hacky_operation(item: JOB_ITEM_T) -> HackyCommitOperationAdd:
     paths, metadata = item
-    operation = HackyCommitOperationAdd(path_in_repo=paths.path_in_repo, path_or_fileobj=paths.file_path)
+    operation = HackyCommitOperationAdd(
+        path_in_repo=paths.path_in_repo, path_or_fileobj=paths.file_path
+    )
     with paths.file_path.open("rb") as file:
         sample = file.peek(512)[:512]
     if metadata.sha256 is None:
         raise ValueError("sha256 must have been computed by now!")
-    operation.upload_info = UploadInfo(sha256=bytes.fromhex(metadata.sha256), size=metadata.size, sample=sample)
+    operation.upload_info = UploadInfo(
+        sha256=bytes.fromhex(metadata.sha256), size=metadata.size, sample=sample
+    )
     return operation
 
 
@@ -583,7 +641,10 @@ def _get_items_to_commit(queue: "queue.Q
             return items
 
         # If we have enough items => commit them
-        if nb_lfs >= MAX_NB_LFS_FILES_PER_COMMIT or nb_regular >= MAX_NB_REGULAR_FILES_PER_COMMIT:
+        if (
+            nb_lfs >= MAX_NB_LFS_FILES_PER_COMMIT
+            or nb_regular >= MAX_NB_REGULAR_FILES_PER_COMMIT
+        ):
             return items
 
         # Else, get a new item and increase counter
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/_webhooks_server.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/_webhooks_server.py
@@ -178,18 +178,28 @@ class WebhooksServer:
         for path, func in self.registered_webhooks.items():
             # Add secret check if required
             if self.webhook_secret is not None:
-                func = _wrap_webhook_to_check_secret(func, webhook_secret=self.webhook_secret)
+                func = _wrap_webhook_to_check_secret(
+                    func, webhook_secret=self.webhook_secret
+                )
 
             # Add route to FastAPI app
             self.fastapi_app.post(path)(func)
 
         # Print instructions and block main thread
         space_host = os.environ.get("SPACE_HOST")
-        url = "https://" + space_host if space_host is not None else (ui.share_url or ui.local_url)
+        url = (
+            "https://" + space_host
+            if space_host is not None
+            else (ui.share_url or ui.local_url)
+        )
         url = url.strip("/")
         message = "\nWebhooks are correctly setup and ready to use:"
-        message += "\n" + "\n".join(f"  - POST {url}{webhook}" for webhook in self.registered_webhooks)
-        message += "\nGo to https://huggingface.co/settings/webhooks to setup your webhooks."
+        message += "\n" + "\n".join(
+            f"  - POST {url}{webhook}" for webhook in self.registered_webhooks
+        )
+        message += (
+            "\nGo to https://huggingface.co/settings/webhooks to setup your webhooks."
+        )
         print(message)
 
         if not prevent_thread_lock:
@@ -320,7 +330,9 @@ def _get_global_app() -> WebhooksServer:
 
 def _warn_on_empty_secret(webhook_secret: Optional[str]) -> None:
     if webhook_secret is None:
-        print("Webhook secret is not defined. This means your webhook endpoints will be open to everyone.")
+        print(
+            "Webhook secret is not defined. This means your webhook endpoints will be open to everyone."
+        )
         print(
             "To add a secret, set `WEBHOOK_SECRET` as environment variable or pass it at initialization: "
             "\n\t`app = WebhooksServer(webhook_secret='my_secret', ...)`"
@@ -359,7 +371,9 @@ def _wrap_webhook_to_check_secret(func:
     async def _protected_func(request: Request, **kwargs):
         request_secret = request.headers.get("x-webhook-secret")
         if request_secret is None:
-            return JSONResponse({"error": "x-webhook-secret header not set."}, status_code=401)
+            return JSONResponse(
+                {"error": "x-webhook-secret header not set."}, status_code=401
+            )
         if request_secret != webhook_secret:
             return JSONResponse({"error": "Invalid webhook secret."}, status_code=403)
 
@@ -377,7 +391,11 @@ def _wrap_webhook_to_check_secret(func:
     if "request" not in initial_sig.parameters:
         _protected_func.__signature__ = initial_sig.replace(  # type: ignore
             parameters=(
-                inspect.Parameter(name="request", kind=inspect.Parameter.POSITIONAL_OR_KEYWORD, annotation=Request),
+                inspect.Parameter(
+                    name="request",
+                    kind=inspect.Parameter.POSITIONAL_OR_KEYWORD,
+                    annotation=Request,
+                ),
             )
             + tuple(initial_sig.parameters.values())
         )
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/delete_cache.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/delete_cache.py
@@ -102,7 +102,9 @@ _CANCEL_DELETION_STR = "CANCEL_DELETION"
 class DeleteCacheCommand(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        delete_cache_parser = parser.add_parser("delete-cache", help="Delete revisions from the cache directory.")
+        delete_cache_parser = parser.add_parser(
+            "delete-cache", help="Delete revisions from the cache directory."
+        )
 
         delete_cache_parser.add_argument(
             "--dir",
@@ -139,7 +141,10 @@ class DeleteCacheCommand(BaseHuggingface
 
         # If deletion is not cancelled
         if len(selected_hashes) > 0 and _CANCEL_DELETION_STR not in selected_hashes:
-            confirm_message = _get_expectations_str(hf_cache_info, selected_hashes) + " Confirm deletion ?"
+            confirm_message = (
+                _get_expectations_str(hf_cache_info, selected_hashes)
+                + " Confirm deletion ?"
+            )
 
             # Confirm deletion
             if self.disable_tui:
@@ -170,7 +175,9 @@ def _manual_review_tui(hf_cache_info: HF
     Displays a multi-select menu in the terminal (TUI).
     """
     # Define multiselect list
-    choices = _get_tui_choices_from_scan(repos=hf_cache_info.repos, preselected=preselected)
+    choices = _get_tui_choices_from_scan(
+        repos=hf_cache_info.repos, preselected=preselected
+    )
     checkbox = inquirer.checkbox(
         message="Select revisions to delete:",
         choices=choices,  # List of revisions with some pre-selection
@@ -180,7 +187,9 @@ def _manual_review_tui(hf_cache_info: HF
         # deletion.
         instruction=_get_expectations_str(
             hf_cache_info,
-            selected_hashes=[c.value for c in choices if isinstance(c, Choice) and c.enabled],
+            selected_hashes=[
+                c.value for c in choices if isinstance(c, Choice) and c.enabled
+            ],
         ),
         # We use the long instruction to should keybindings instructions to the user
         long_instruction="Press <space> to select, <enter> to validate and <ctrl+c> to quit without modification.",
@@ -195,7 +204,11 @@ def _manual_review_tui(hf_cache_info: HF
         # a revision hash is selected/unselected.
         checkbox._instruction = _get_expectations_str(
             hf_cache_info,
-            selected_hashes=[choice["value"] for choice in checkbox.content_control.choices if choice["enabled"]],
+            selected_hashes=[
+                choice["value"]
+                for choice in checkbox.content_control.choices
+                if choice["enabled"]
+            ],
         )
 
     checkbox.kb_func_lookup["toggle"].append({"func": _update_expectations})
@@ -213,7 +226,9 @@ def _ask_for_confirmation_tui(message: s
     return inquirer.confirm(message, default=default).execute()
 
 
-def _get_tui_choices_from_scan(repos: Iterable[CachedRepoInfo], preselected: List[str]) -> List:
+def _get_tui_choices_from_scan(
+    repos: Iterable[CachedRepoInfo], preselected: List[str]
+) -> List:
     """Build a list of choices from the scanned repos.
 
     Args:
@@ -264,7 +279,9 @@ def _get_tui_choices_from_scan(repos: It
     return choices
 
 
-def _manual_review_no_tui(hf_cache_info: HFCacheInfo, preselected: List[str]) -> List[str]:
+def _manual_review_no_tui(
+    hf_cache_info: HFCacheInfo, preselected: List[str]
+) -> List[str]:
     """Ask the user for a manual review of the revisions to delete.
 
     Used when TUI is disabled. Manual review happens in a separate tmp file that the
@@ -337,7 +354,9 @@ def _ask_for_confirmation_no_tui(message
         print(f"Invalid input. Must be one of {ALL}")
 
 
-def _get_expectations_str(hf_cache_info: HFCacheInfo, selected_hashes: List[str]) -> str:
+def _get_expectations_str(
+    hf_cache_info: HFCacheInfo, selected_hashes: List[str]
+) -> str:
     """Format a string to display to the user how much space would be saved.
 
     Example:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/download.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/download.py
@@ -53,12 +53,19 @@ logger = logging.get_logger(__name__)
 class DownloadCommand(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        download_parser = parser.add_parser("download", help="Download files from the Hub")
+        download_parser = parser.add_parser(
+            "download", help="Download files from the Hub"
+        )
         download_parser.add_argument(
-            "repo_id", type=str, help="ID of the repo to download from (e.g. `username/repo-name`)."
+            "repo_id",
+            type=str,
+            help="ID of the repo to download from (e.g. `username/repo-name`).",
         )
         download_parser.add_argument(
-            "filenames", type=str, nargs="*", help="Files to download (e.g. `config.json`, `data/metadata.jsonl`)."
+            "filenames",
+            type=str,
+            nargs="*",
+            help="Files to download (e.g. `config.json`, `data/metadata.jsonl`).",
         )
         download_parser.add_argument(
             "--repo-type",
@@ -72,13 +79,21 @@ class DownloadCommand(BaseHuggingfaceCLI
             help="An optional Git revision id which can be a branch name, a tag, or a commit hash.",
         )
         download_parser.add_argument(
-            "--include", nargs="*", type=str, help="Glob patterns to match files to download."
+            "--include",
+            nargs="*",
+            type=str,
+            help="Glob patterns to match files to download.",
         )
         download_parser.add_argument(
-            "--exclude", nargs="*", type=str, help="Glob patterns to exclude from files to download."
+            "--exclude",
+            nargs="*",
+            type=str,
+            help="Glob patterns to exclude from files to download.",
         )
         download_parser.add_argument(
-            "--cache-dir", type=str, help="Path to the directory where to save the downloaded files."
+            "--cache-dir",
+            type=str,
+            help="Path to the directory where to save the downloaded files.",
         )
         download_parser.add_argument(
             "--local-dir",
@@ -92,7 +107,9 @@ class DownloadCommand(BaseHuggingfaceCLI
         download_parser.add_argument(
             "--local-dir-use-symlinks",
             choices=["auto", "True", "False"],
-            help=("Deprecated and ignored. Downloading to a local directory does not use symlinks anymore."),
+            help=(
+                "Deprecated and ignored. Downloading to a local directory does not use symlinks anymore."
+            ),
         )
         download_parser.add_argument(
             "--force-download",
@@ -105,7 +122,9 @@ class DownloadCommand(BaseHuggingfaceCLI
             help="Deprecated and ignored. Downloading a file to local dir always attempts to resume previously interrupted downloads (unless hf-transfer is enabled).",
         )
         download_parser.add_argument(
-            "--token", type=str, help="A User Access Token generated from https://huggingface.co/settings/tokens"
+            "--token",
+            type=str,
+            help="A User Access Token generated from https://huggingface.co/settings/tokens",
         )
         download_parser.add_argument(
             "--quiet",
@@ -157,9 +176,13 @@ class DownloadCommand(BaseHuggingfaceCLI
         # Warn user if patterns are ignored
         if len(self.filenames) > 0:
             if self.include is not None and len(self.include) > 0:
-                warnings.warn("Ignoring `--include` since filenames have being explicitly set.")
+                warnings.warn(
+                    "Ignoring `--include` since filenames have being explicitly set."
+                )
             if self.exclude is not None and len(self.exclude) > 0:
-                warnings.warn("Ignoring `--exclude` since filenames have being explicitly set.")
+                warnings.warn(
+                    "Ignoring `--exclude` since filenames have being explicitly set."
+                )
 
         # Single file to download: use `hf_hub_download`
         if len(self.filenames) == 1:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/env.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/env.py
@@ -29,7 +29,9 @@ class EnvironmentCommand(BaseHuggingface
 
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        env_parser = parser.add_parser("env", help="Print information about the environment.")
+        env_parser = parser.add_parser(
+            "env", help="Print information about the environment."
+        )
         env_parser.set_defaults(func=EnvironmentCommand)
 
     def run(self) -> None:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/huggingface_cli.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/huggingface_cli.py
@@ -28,7 +28,9 @@ from huggingface_hub.commands.version im
 
 
 def main():
-    parser = ArgumentParser("huggingface-cli", usage="huggingface-cli <command> [<args>]")
+    parser = ArgumentParser(
+        "huggingface-cli", usage="huggingface-cli <command> [<args>]"
+    )
     commands_parser = parser.add_subparsers(help="huggingface-cli command helpers")
 
     # Register commands
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/lfs.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/lfs.py
@@ -57,9 +57,12 @@ class LfsCommands(BaseHuggingfaceCLIComm
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
         enable_parser = parser.add_parser(
-            "lfs-enable-largefiles", help="Configure your repository to enable upload of files > 5GB."
+            "lfs-enable-largefiles",
+            help="Configure your repository to enable upload of files > 5GB.",
+        )
+        enable_parser.add_argument(
+            "path", type=str, help="Local path to repository you want to configure."
         )
-        enable_parser.add_argument("path", type=str, help="Local path to repository you want to configure.")
         enable_parser.set_defaults(func=lambda args: LfsEnableCommand(args))
 
         # Command will get called by git-lfs, do not call it directly.
@@ -120,7 +123,9 @@ class LfsUploadCommand:
         # sends initiation data to the process over stdin.
         # This tells the process useful information about the configuration.
         init_msg = json.loads(sys.stdin.readline().strip())
-        if not (init_msg.get("event") == "init" and init_msg.get("operation") == "upload"):
+        if not (
+            init_msg.get("event") == "init" and init_msg.get("operation") == "upload"
+        ):
             write_msg({"error": {"code": 32, "message": "Wrong lfs init operation"}})
             sys.exit(1)
 
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/repo_files.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/repo_files.py
@@ -76,9 +76,13 @@ class DeleteFilesSubCommand:
 class RepoFilesCommand(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        repo_files_parser = parser.add_parser("repo-files", help="Manage files in a repo on the Hub")
+        repo_files_parser = parser.add_parser(
+            "repo-files", help="Manage files in a repo on the Hub"
+        )
         repo_files_parser.add_argument(
-            "repo_id", type=str, help="The ID of the repo to manage (e.g. `username/repo-name`)."
+            "repo_id",
+            type=str,
+            help="The ID of the repo to manage (e.g. `username/repo-name`).",
         )
         repo_files_subparsers = repo_files_parser.add_subparsers(
             help="Action to execute against the files.",
@@ -111,13 +115,19 @@ class RepoFilesCommand(BaseHuggingfaceCL
             ),
         )
         delete_subparser.add_argument(
-            "--commit-message", type=str, help="The summary / title / first line of the generated commit."
+            "--commit-message",
+            type=str,
+            help="The summary / title / first line of the generated commit.",
         )
         delete_subparser.add_argument(
-            "--commit-description", type=str, help="The description of the generated commit."
+            "--commit-description",
+            type=str,
+            help="The description of the generated commit.",
         )
         delete_subparser.add_argument(
-            "--create-pr", action="store_true", help="Whether to create a new Pull Request for these changes."
+            "--create-pr",
+            action="store_true",
+            help="Whether to create a new Pull Request for these changes.",
         )
         repo_files_parser.add_argument(
             "--token",
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/scan_cache.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/scan_cache.py
@@ -33,7 +33,9 @@ from ._cli_utils import ANSI, tabulate
 class ScanCacheCommand(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        scan_cache_parser = parser.add_parser("scan-cache", help="Scan cache directory.")
+        scan_cache_parser = parser.add_parser(
+            "scan-cache", help="Scan cache directory."
+        )
 
         scan_cache_parser.add_argument(
             "--dir",
@@ -166,7 +168,9 @@ def get_table(hf_cache_info: HFCacheInfo
                     str(revision.snapshot_path),
                 ]
                 for repo in sorted(hf_cache_info.repos, key=lambda repo: repo.repo_path)
-                for revision in sorted(repo.revisions, key=lambda revision: revision.commit_hash)
+                for revision in sorted(
+                    repo.revisions, key=lambda revision: revision.commit_hash
+                )
             ],
             headers=[
                 "REPO ID",
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/tag.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/tag.py
@@ -47,14 +47,29 @@ from ._cli_utils import ANSI
 class TagCommands(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        tag_parser = parser.add_parser("tag", help="(create, list, delete) tags for a repo in the hub")
+        tag_parser = parser.add_parser(
+            "tag", help="(create, list, delete) tags for a repo in the hub"
+        )
 
-        tag_parser.add_argument("repo_id", type=str, help="The ID of the repo to tag (e.g. `username/repo-name`).")
-        tag_parser.add_argument("tag", nargs="?", type=str, help="The name of the tag for creation or deletion.")
-        tag_parser.add_argument("-m", "--message", type=str, help="The description of the tag to create.")
+        tag_parser.add_argument(
+            "repo_id",
+            type=str,
+            help="The ID of the repo to tag (e.g. `username/repo-name`).",
+        )
+        tag_parser.add_argument(
+            "tag",
+            nargs="?",
+            type=str,
+            help="The name of the tag for creation or deletion.",
+        )
+        tag_parser.add_argument(
+            "-m", "--message", type=str, help="The description of the tag to create."
+        )
         tag_parser.add_argument("--revision", type=str, help="The git revision to tag.")
         tag_parser.add_argument(
-            "--token", type=str, help="A User Access Token generated from https://huggingface.co/settings/tokens."
+            "--token",
+            type=str,
+            help="A User Access Token generated from https://huggingface.co/settings/tokens.",
         )
         tag_parser.add_argument(
             "--repo-type",
@@ -62,10 +77,19 @@ class TagCommands(BaseHuggingfaceCLIComm
             default="model",
             help="Set the type of repository (model, dataset, or space).",
         )
-        tag_parser.add_argument("-y", "--yes", action="store_true", help="Answer Yes to prompts automatically.")
+        tag_parser.add_argument(
+            "-y",
+            "--yes",
+            action="store_true",
+            help="Answer Yes to prompts automatically.",
+        )
 
-        tag_parser.add_argument("-l", "--list", action="store_true", help="List tags for a repository.")
-        tag_parser.add_argument("-d", "--delete", action="store_true", help="Delete a tag for a repository.")
+        tag_parser.add_argument(
+            "-l", "--list", action="store_true", help="List tags for a repository."
+        )
+        tag_parser.add_argument(
+            "-d", "--delete", action="store_true", help="Delete a tag for a repository."
+        )
 
         tag_parser.set_defaults(func=lambda args: handle_commands(args))
 
@@ -92,7 +116,9 @@ class TagCommand:
 
 class TagCreateCommand(TagCommand):
     def run(self):
-        print(f"You are about to create tag {ANSI.bold(self.args.tag)} on {self.repo_type} {ANSI.bold(self.repo_id)}")
+        print(
+            f"You are about to create tag {ANSI.bold(self.args.tag)} on {self.repo_type} {ANSI.bold(self.repo_id)}"
+        )
 
         try:
             self.api.create_tag(
@@ -110,7 +136,9 @@ class TagCreateCommand(TagCommand):
             exit(1)
         except HfHubHTTPError as e:
             if e.response.status_code == 409:
-                print(f"Tag {ANSI.bold(self.args.tag)} already exists on {ANSI.bold(self.repo_id)}")
+                print(
+                    f"Tag {ANSI.bold(self.args.tag)} already exists on {ANSI.bold(self.repo_id)}"
+                )
                 exit(1)
             raise e
 
@@ -141,7 +169,9 @@ class TagListCommand(TagCommand):
 
 class TagDeleteCommand(TagCommand):
     def run(self):
-        print(f"You are about to delete tag {ANSI.bold(self.args.tag)} on {self.repo_type} {ANSI.bold(self.repo_id)}")
+        print(
+            f"You are about to delete tag {ANSI.bold(self.args.tag)} on {self.repo_type} {ANSI.bold(self.repo_id)}"
+        )
 
         if not self.args.yes:
             choice = input("Proceed? [Y/n] ").lower()
@@ -149,11 +179,15 @@ class TagDeleteCommand(TagCommand):
                 print("Abort")
                 exit()
         try:
-            self.api.delete_tag(repo_id=self.repo_id, tag=self.args.tag, repo_type=self.repo_type)
+            self.api.delete_tag(
+                repo_id=self.repo_id, tag=self.args.tag, repo_type=self.repo_type
+            )
         except RepositoryNotFoundError:
             print(f"{self.repo_type.capitalize()} {ANSI.bold(self.repo_id)} not found.")
             exit(1)
         except RevisionNotFoundError:
-            print(f"Tag {ANSI.bold(self.args.tag)} not found on {ANSI.bold(self.repo_id)}")
+            print(
+                f"Tag {ANSI.bold(self.args.tag)} not found on {ANSI.bold(self.repo_id)}"
+            )
             exit(1)
         print(f"Tag {ANSI.bold(self.args.tag)} deleted on {ANSI.bold(self.repo_id)}")
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/upload.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/upload.py
@@ -64,12 +64,18 @@ logger = logging.get_logger(__name__)
 class UploadCommand(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        upload_parser = parser.add_parser("upload", help="Upload a file or a folder to a repo on the Hub")
+        upload_parser = parser.add_parser(
+            "upload", help="Upload a file or a folder to a repo on the Hub"
+        )
         upload_parser.add_argument(
-            "repo_id", type=str, help="The ID of the repo to upload to (e.g. `username/repo-name`)."
+            "repo_id",
+            type=str,
+            help="The ID of the repo to upload to (e.g. `username/repo-name`).",
         )
         upload_parser.add_argument(
-            "local_path", nargs="?", help="Local path to the file or folder to upload. Defaults to current directory."
+            "local_path",
+            nargs="?",
+            help="Local path to the file or folder to upload. Defaults to current directory.",
         )
         upload_parser.add_argument(
             "path_in_repo",
@@ -98,9 +104,17 @@ class UploadCommand(BaseHuggingfaceCLICo
                 " exists."
             ),
         )
-        upload_parser.add_argument("--include", nargs="*", type=str, help="Glob patterns to match files to upload.")
         upload_parser.add_argument(
-            "--exclude", nargs="*", type=str, help="Glob patterns to exclude from files to upload."
+            "--include",
+            nargs="*",
+            type=str,
+            help="Glob patterns to match files to upload.",
+        )
+        upload_parser.add_argument(
+            "--exclude",
+            nargs="*",
+            type=str,
+            help="Glob patterns to exclude from files to upload.",
         )
         upload_parser.add_argument(
             "--delete",
@@ -109,11 +123,19 @@ class UploadCommand(BaseHuggingfaceCLICo
             help="Glob patterns for file to be deleted from the repo while committing.",
         )
         upload_parser.add_argument(
-            "--commit-message", type=str, help="The summary / title / first line of the generated commit."
+            "--commit-message",
+            type=str,
+            help="The summary / title / first line of the generated commit.",
         )
-        upload_parser.add_argument("--commit-description", type=str, help="The description of the generated commit.")
         upload_parser.add_argument(
-            "--create-pr", action="store_true", help="Whether to upload content as a new Pull Request."
+            "--commit-description",
+            type=str,
+            help="The description of the generated commit.",
+        )
+        upload_parser.add_argument(
+            "--create-pr",
+            action="store_true",
+            help="Whether to upload content as a new Pull Request.",
         )
         upload_parser.add_argument(
             "--every",
@@ -121,7 +143,9 @@ class UploadCommand(BaseHuggingfaceCLICo
             help="If set, a background job is scheduled to create commits every `every` minutes.",
         )
         upload_parser.add_argument(
-            "--token", type=str, help="A User Access Token generated from https://huggingface.co/settings/tokens"
+            "--token",
+            type=str,
+            help="A User Access Token generated from https://huggingface.co/settings/tokens",
         )
         upload_parser.add_argument(
             "--quiet",
@@ -152,7 +176,9 @@ class UploadCommand(BaseHuggingfaceCLICo
         self.every: Optional[float] = args.every
 
         # Resolve `local_path` and `path_in_repo`
-        repo_name: str = args.repo_id.split("/")[-1]  # e.g. "Wauplin/my-cool-model" => "my-cool-model"
+        repo_name: str = args.repo_id.split("/")[
+            -1
+        ]  # e.g. "Wauplin/my-cool-model" => "my-cool-model"
         self.local_path: str
         self.path_in_repo: str
         if args.local_path is None and os.path.isfile(repo_name):
@@ -166,7 +192,9 @@ class UploadCommand(BaseHuggingfaceCLICo
         elif args.local_path is None:
             # Implicit case 3: user provided only a repo_id that does not match a local file or folder
             # => the user must explicitly provide a local_path => raise exception
-            raise ValueError(f"'{repo_name}' is not a local file or folder. Please set `local_path` explicitly.")
+            raise ValueError(
+                f"'{repo_name}' is not a local file or folder. Please set `local_path` explicitly."
+            )
         elif args.path_in_repo is None and os.path.isfile(args.local_path):
             # Explicit local path to file, no path in repo => upload it at root with same name
             self.local_path = args.local_path
@@ -213,7 +241,9 @@ class UploadCommand(BaseHuggingfaceCLICo
                 # If file => watch entire folder + use allow_patterns
                 folder_path = os.path.dirname(self.local_path)
                 path_in_repo = (
-                    self.path_in_repo[: -len(self.local_path)]  # remove filename from path_in_repo
+                    self.path_in_repo[
+                        : -len(self.local_path)
+                    ]  # remove filename from path_in_repo
                     if self.path_in_repo.endswith(self.local_path)
                     else self.path_in_repo
                 )
@@ -225,7 +255,9 @@ class UploadCommand(BaseHuggingfaceCLICo
                 allow_patterns = self.include or []
                 ignore_patterns = self.exclude or []
                 if self.delete is not None and len(self.delete) > 0:
-                    warnings.warn("Ignoring `--delete` when uploading with scheduled commits.")
+                    warnings.warn(
+                        "Ignoring `--delete` when uploading with scheduled commits."
+                    )
 
             scheduler = CommitScheduler(
                 folder_path=folder_path,
@@ -239,7 +271,9 @@ class UploadCommand(BaseHuggingfaceCLICo
                 every=self.every,
                 hf_api=self.api,
             )
-            print(f"Scheduling commits every {self.every} minutes to {scheduler.repo_id}.")
+            print(
+                f"Scheduling commits every {self.every} minutes to {scheduler.repo_id}."
+            )
             try:  # Block main thread until KeyboardInterrupt
                 while True:
                     time.sleep(100)
@@ -263,10 +297,17 @@ class UploadCommand(BaseHuggingfaceCLICo
         # Check if branch already exists and if not, create it
         if self.revision is not None and not self.create_pr:
             try:
-                self.api.repo_info(repo_id=repo_id, repo_type=self.repo_type, revision=self.revision)
+                self.api.repo_info(
+                    repo_id=repo_id, repo_type=self.repo_type, revision=self.revision
+                )
             except RevisionNotFoundError:
                 logger.info(f"Branch '{self.revision}' not found. Creating it...")
-                self.api.create_branch(repo_id=repo_id, repo_type=self.repo_type, branch=self.revision, exist_ok=True)
+                self.api.create_branch(
+                    repo_id=repo_id,
+                    repo_type=self.repo_type,
+                    branch=self.revision,
+                    exist_ok=True,
+                )
                 # ^ `exist_ok=True` to avoid race concurrency issues
 
         # File-based upload
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/upload_large_folder.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/upload_large_folder.py
@@ -32,11 +32,17 @@ logger = logging.get_logger(__name__)
 class UploadLargeFolderCommand(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        subparser = parser.add_parser("upload-large-folder", help="Upload a large folder to a repo on the Hub")
+        subparser = parser.add_parser(
+            "upload-large-folder", help="Upload a large folder to a repo on the Hub"
+        )
         subparser.add_argument(
-            "repo_id", type=str, help="The ID of the repo to upload to (e.g. `username/repo-name`)."
+            "repo_id",
+            type=str,
+            help="The ID of the repo to upload to (e.g. `username/repo-name`).",
+        )
+        subparser.add_argument(
+            "local_path", type=str, help="Local path to the file or folder to upload."
         )
-        subparser.add_argument("local_path", type=str, help="Local path to the file or folder to upload.")
         subparser.add_argument(
             "--repo-type",
             choices=["model", "dataset", "space"],
@@ -45,7 +51,9 @@ class UploadLargeFolderCommand(BaseHuggi
         subparser.add_argument(
             "--revision",
             type=str,
-            help=("An optional Git revision to push to. It can be a branch name or a PR reference."),
+            help=(
+                "An optional Git revision to push to. It can be a branch name or a PR reference."
+            ),
         )
         subparser.add_argument(
             "--private",
@@ -54,16 +62,36 @@ class UploadLargeFolderCommand(BaseHuggi
                 "Whether to create a private repo if repo doesn't exist on the Hub. Ignored if the repo already exists."
             ),
         )
-        subparser.add_argument("--include", nargs="*", type=str, help="Glob patterns to match files to upload.")
-        subparser.add_argument("--exclude", nargs="*", type=str, help="Glob patterns to exclude from files to upload.")
         subparser.add_argument(
-            "--token", type=str, help="A User Access Token generated from https://huggingface.co/settings/tokens"
+            "--include",
+            nargs="*",
+            type=str,
+            help="Glob patterns to match files to upload.",
+        )
+        subparser.add_argument(
+            "--exclude",
+            nargs="*",
+            type=str,
+            help="Glob patterns to exclude from files to upload.",
+        )
+        subparser.add_argument(
+            "--token",
+            type=str,
+            help="A User Access Token generated from https://huggingface.co/settings/tokens",
+        )
+        subparser.add_argument(
+            "--num-workers",
+            type=int,
+            help="Number of workers to use to hash, upload and commit files.",
+        )
+        subparser.add_argument(
+            "--no-report",
+            action="store_true",
+            help="Whether to disable regular status report.",
         )
         subparser.add_argument(
-            "--num-workers", type=int, help="Number of workers to use to hash, upload and commit files."
+            "--no-bars", action="store_true", help="Whether to disable progress bars."
         )
-        subparser.add_argument("--no-report", action="store_true", help="Whether to disable regular status report.")
-        subparser.add_argument("--no-bars", action="store_true", help="Whether to disable progress bars.")
         subparser.set_defaults(func=UploadLargeFolderCommand)
 
     def __init__(self, args: Namespace) -> None:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/user.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/user.py
@@ -41,7 +41,12 @@ from typing import List, Optional
 from requests.exceptions import HTTPError
 
 from huggingface_hub.commands import BaseHuggingfaceCLICommand
-from huggingface_hub.constants import ENDPOINT, REPO_TYPES, REPO_TYPES_URL_PREFIXES, SPACES_SDK_TYPES
+from huggingface_hub.constants import (
+    ENDPOINT,
+    REPO_TYPES,
+    REPO_TYPES_URL_PREFIXES,
+    SPACES_SDK_TYPES,
+)
 from huggingface_hub.hf_api import HfApi
 
 from .._login import (  # noqa: F401 # for backward compatibility  # noqa: F401 # for backward compatibility
@@ -72,7 +77,9 @@ except ImportError:
 class UserCommands(BaseHuggingfaceCLICommand):
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        login_parser = parser.add_parser("login", help="Log in using a token from huggingface.co/settings/tokens")
+        login_parser = parser.add_parser(
+            "login", help="Log in using a token from huggingface.co/settings/tokens"
+        )
         login_parser.add_argument(
             "--token",
             type=str,
@@ -84,7 +91,9 @@ class UserCommands(BaseHuggingfaceCLICom
             help="Optional: Save token to git credential helper.",
         )
         login_parser.set_defaults(func=lambda args: LoginCommand(args))
-        whoami_parser = parser.add_parser("whoami", help="Find out which huggingface.co account you are logged in as.")
+        whoami_parser = parser.add_parser(
+            "whoami", help="Find out which huggingface.co account you are logged in as."
+        )
         whoami_parser.set_defaults(func=lambda args: WhoamiCommand(args))
 
         logout_parser = parser.add_parser("logout", help="Log out")
@@ -95,9 +104,13 @@ class UserCommands(BaseHuggingfaceCLICom
         )
         logout_parser.set_defaults(func=lambda args: LogoutCommand(args))
 
-        auth_parser = parser.add_parser("auth", help="Other authentication related commands")
+        auth_parser = parser.add_parser(
+            "auth", help="Other authentication related commands"
+        )
         auth_subparsers = auth_parser.add_subparsers(help="Authentication subcommands")
-        auth_switch_parser = auth_subparsers.add_parser("switch", help="Switch between access tokens")
+        auth_switch_parser = auth_subparsers.add_parser(
+            "switch", help="Switch between access tokens"
+        )
         auth_switch_parser.add_argument(
             "--token-name",
             type=str,
@@ -109,12 +122,20 @@ class UserCommands(BaseHuggingfaceCLICom
             help="Optional: Save token to git credential helper.",
         )
         auth_switch_parser.set_defaults(func=lambda args: AuthSwitchCommand(args))
-        auth_list_parser = auth_subparsers.add_parser("list", help="List all stored access tokens")
+        auth_list_parser = auth_subparsers.add_parser(
+            "list", help="List all stored access tokens"
+        )
         auth_list_parser.set_defaults(func=lambda args: AuthListCommand(args))
         # new system: git-based repo system
-        repo_parser = parser.add_parser("repo", help="{create} Commands to interact with your huggingface.co repos.")
-        repo_subparsers = repo_parser.add_subparsers(help="huggingface.co repos related commands")
-        repo_create_parser = repo_subparsers.add_parser("create", help="Create a new repo on huggingface.co")
+        repo_parser = parser.add_parser(
+            "repo", help="{create} Commands to interact with your huggingface.co repos."
+        )
+        repo_subparsers = repo_parser.add_subparsers(
+            help="huggingface.co repos related commands"
+        )
+        repo_create_parser = repo_subparsers.add_parser(
+            "create", help="Create a new repo on huggingface.co"
+        )
         repo_create_parser.add_argument(
             "name",
             type=str,
@@ -125,7 +146,9 @@ class UserCommands(BaseHuggingfaceCLICom
             type=str,
             help='Optional: repo_type: set to "dataset" or "space" if creating a dataset or space, default is model.',
         )
-        repo_create_parser.add_argument("--organization", type=str, help="Optional: organization namespace.")
+        repo_create_parser.add_argument(
+            "--organization", type=str, help="Optional: organization namespace."
+        )
         repo_create_parser.add_argument(
             "--space_sdk",
             type=str,
@@ -189,7 +212,9 @@ class AuthSwitchCommand(BaseUserCommand)
             print(f"{i}. {token_name}")
         while True:
             try:
-                choice = input("Enter the number of the token to switch to (or 'q' to quit): ")
+                choice = input(
+                    "Enter the number of the token to switch to (or 'q' to quit): "
+                )
                 if choice.lower() == "q":
                     return None
                 index = int(choice) - 1
@@ -266,7 +291,9 @@ class RepoCreateCommand(BaseUserCommand)
         print("")
 
         user = self._api.whoami(token)["name"]
-        namespace = self.args.organization if self.args.organization is not None else user
+        namespace = (
+            self.args.organization if self.args.organization is not None else user
+        )
 
         repo_id = f"{namespace}/{self.args.name}"
 
@@ -299,6 +326,8 @@ class RepoCreateCommand(BaseUserCommand)
             exit(1)
         print("\nYour repo now lives at:")
         print(f"  {ANSI.bold(url)}")
-        print("\nYou can clone it locally with the command below, and commit/push as usual.")
+        print(
+            "\nYou can clone it locally with the command below, and commit/push as usual."
+        )
         print(f"\n  git clone {url}")
         print("")
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/commands/version.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/commands/version.py
@@ -30,7 +30,9 @@ class VersionCommand(BaseHuggingfaceCLIC
 
     @staticmethod
     def register_subcommand(parser: _SubParsersAction):
-        version_parser = parser.add_parser("version", help="Print information about the huggingface-cli version.")
+        version_parser = parser.add_parser(
+            "version", help="Print information about the huggingface-cli version."
+        )
         version_parser.set_defaults(func=VersionCommand)
 
     def run(self) -> None:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/community.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/community.py
@@ -81,7 +81,9 @@ class Discussion:
         """Returns the URL of the discussion on the Hub."""
         if self.repo_type is None or self.repo_type == constants.REPO_TYPE_MODEL:
             return f"{self.endpoint}/{self.repo_id}/discussions/{self.num}"
-        return f"{self.endpoint}/{self.repo_type}s/{self.repo_id}/discussions/{self.num}"
+        return (
+            f"{self.endpoint}/{self.repo_type}s/{self.repo_id}/discussions/{self.num}"
+        )
 
 
 @dataclass
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/constants.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/constants.py
@@ -39,7 +39,9 @@ HF_TRANSFER_CONCURRENCY = 100
 
 # Constants for serialization
 
-PYTORCH_WEIGHTS_FILE_PATTERN = "pytorch_model{suffix}.bin"  # Unsafe pickle: use safetensors instead
+PYTORCH_WEIGHTS_FILE_PATTERN = (
+    "pytorch_model{suffix}.bin"  # Unsafe pickle: use safetensors instead
+)
 SAFETENSORS_WEIGHTS_FILE_PATTERN = "model{suffix}.safetensors"
 TF2_WEIGHTS_FILE_PATTERN = "tf_model{suffix}.h5"
 
@@ -68,13 +70,17 @@ HUGGINGFACE_CO_URL_TEMPLATE = ENDPOINT +
 
 if _staging_mode:
     ENDPOINT = _HF_DEFAULT_STAGING_ENDPOINT
-    HUGGINGFACE_CO_URL_TEMPLATE = _HF_DEFAULT_STAGING_ENDPOINT + "/{repo_id}/resolve/{revision}/{filename}"
+    HUGGINGFACE_CO_URL_TEMPLATE = (
+        _HF_DEFAULT_STAGING_ENDPOINT + "/{repo_id}/resolve/{revision}/{filename}"
+    )
 
 HUGGINGFACE_HEADER_X_REPO_COMMIT = "X-Repo-Commit"
 HUGGINGFACE_HEADER_X_LINKED_ETAG = "X-Linked-Etag"
 HUGGINGFACE_HEADER_X_LINKED_SIZE = "X-Linked-Size"
 
-INFERENCE_ENDPOINT = os.environ.get("HF_INFERENCE_ENDPOINT", "https://api-inference.huggingface.co")
+INFERENCE_ENDPOINT = os.environ.get(
+    "HF_INFERENCE_ENDPOINT", "https://api-inference.huggingface.co"
+)
 
 # See https://huggingface.co/docs/inference-endpoints/index
 INFERENCE_ENDPOINTS_ENDPOINT = "https://api.endpoints.huggingface.cloud/v2"
@@ -104,9 +110,13 @@ REPO_TYPES_MAPPING = {
 }
 
 DiscussionTypeFilter = Literal["all", "discussion", "pull_request"]
-DISCUSSION_TYPES: Tuple[DiscussionTypeFilter, ...] = typing.get_args(DiscussionTypeFilter)
+DISCUSSION_TYPES: Tuple[DiscussionTypeFilter, ...] = typing.get_args(
+    DiscussionTypeFilter
+)
 DiscussionStatusFilter = Literal["all", "open", "closed"]
-DISCUSSION_STATUS: Tuple[DiscussionTypeFilter, ...] = typing.get_args(DiscussionStatusFilter)
+DISCUSSION_STATUS: Tuple[DiscussionTypeFilter, ...] = typing.get_args(
+    DiscussionStatusFilter
+)
 
 # Webhook subscription types
 WEBHOOK_DOMAIN_T = Literal["repo", "discussions"]
@@ -126,13 +136,17 @@ default_assets_cache_path = os.path.join
 
 # Legacy env variables
 HUGGINGFACE_HUB_CACHE = os.getenv("HUGGINGFACE_HUB_CACHE", default_cache_path)
-HUGGINGFACE_ASSETS_CACHE = os.getenv("HUGGINGFACE_ASSETS_CACHE", default_assets_cache_path)
+HUGGINGFACE_ASSETS_CACHE = os.getenv(
+    "HUGGINGFACE_ASSETS_CACHE", default_assets_cache_path
+)
 
 # New env variables
 HF_HUB_CACHE = os.getenv("HF_HUB_CACHE", HUGGINGFACE_HUB_CACHE)
 HF_ASSETS_CACHE = os.getenv("HF_ASSETS_CACHE", HUGGINGFACE_ASSETS_CACHE)
 
-HF_HUB_OFFLINE = _is_true(os.environ.get("HF_HUB_OFFLINE") or os.environ.get("TRANSFORMERS_OFFLINE"))
+HF_HUB_OFFLINE = _is_true(
+    os.environ.get("HF_HUB_OFFLINE") or os.environ.get("TRANSFORMERS_OFFLINE")
+)
 
 # If set, log level will be set to DEBUG and all requests made to the Hub will be logged
 # as curl commands for reproducibility.
@@ -152,7 +166,9 @@ if _staging_mode:
     # In staging mode, we use a different cache to ensure we don't mix up production and staging data or tokens
     # In practice in `huggingface_hub` tests, we monkeypatch these values with temporary directories. The following
     # lines are only used in third-party libraries tests (e.g. `transformers`, `diffusers`, etc.).
-    _staging_home = os.path.join(os.path.expanduser("~"), ".cache", "huggingface_staging")
+    _staging_home = os.path.join(
+        os.path.expanduser("~"), ".cache", "huggingface_staging"
+    )
     HUGGINGFACE_HUB_CACHE = os.path.join(_staging_home, "hub")
     HF_TOKEN_PATH = os.path.join(_staging_home, "token")
 
@@ -163,17 +179,25 @@ if _staging_mode:
 # TL;DR: env variable has priority over code
 __HF_HUB_DISABLE_PROGRESS_BARS = os.environ.get("HF_HUB_DISABLE_PROGRESS_BARS")
 HF_HUB_DISABLE_PROGRESS_BARS: Optional[bool] = (
-    _is_true(__HF_HUB_DISABLE_PROGRESS_BARS) if __HF_HUB_DISABLE_PROGRESS_BARS is not None else None
+    _is_true(__HF_HUB_DISABLE_PROGRESS_BARS)
+    if __HF_HUB_DISABLE_PROGRESS_BARS is not None
+    else None
 )
 
 # Disable warning on machines that do not support symlinks (e.g. Windows non-developer)
-HF_HUB_DISABLE_SYMLINKS_WARNING: bool = _is_true(os.environ.get("HF_HUB_DISABLE_SYMLINKS_WARNING"))
+HF_HUB_DISABLE_SYMLINKS_WARNING: bool = _is_true(
+    os.environ.get("HF_HUB_DISABLE_SYMLINKS_WARNING")
+)
 
 # Disable warning when using experimental features
-HF_HUB_DISABLE_EXPERIMENTAL_WARNING: bool = _is_true(os.environ.get("HF_HUB_DISABLE_EXPERIMENTAL_WARNING"))
+HF_HUB_DISABLE_EXPERIMENTAL_WARNING: bool = _is_true(
+    os.environ.get("HF_HUB_DISABLE_EXPERIMENTAL_WARNING")
+)
 
 # Disable sending the cached token by default is all HTTP requests to the Hub
-HF_HUB_DISABLE_IMPLICIT_TOKEN: bool = _is_true(os.environ.get("HF_HUB_DISABLE_IMPLICIT_TOKEN"))
+HF_HUB_DISABLE_IMPLICIT_TOKEN: bool = _is_true(
+    os.environ.get("HF_HUB_DISABLE_IMPLICIT_TOKEN")
+)
 
 # Enable fast-download using external dependency "hf_transfer"
 # See:
@@ -185,14 +209,19 @@ HF_HUB_ENABLE_HF_TRANSFER: bool = _is_tr
 # UNUSED
 # We don't use symlinks in local dir anymore.
 HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD: int = (
-    _as_int(os.environ.get("HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD")) or 5 * 1024 * 1024
+    _as_int(os.environ.get("HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD"))
+    or 5 * 1024 * 1024
 )
 
 # Used to override the etag timeout on a system level
-HF_HUB_ETAG_TIMEOUT: int = _as_int(os.environ.get("HF_HUB_ETAG_TIMEOUT")) or DEFAULT_ETAG_TIMEOUT
+HF_HUB_ETAG_TIMEOUT: int = (
+    _as_int(os.environ.get("HF_HUB_ETAG_TIMEOUT")) or DEFAULT_ETAG_TIMEOUT
+)
 
 # Used to override the get request timeout on a system level
-HF_HUB_DOWNLOAD_TIMEOUT: int = _as_int(os.environ.get("HF_HUB_DOWNLOAD_TIMEOUT")) or DEFAULT_DOWNLOAD_TIMEOUT
+HF_HUB_DOWNLOAD_TIMEOUT: int = (
+    _as_int(os.environ.get("HF_HUB_DOWNLOAD_TIMEOUT")) or DEFAULT_DOWNLOAD_TIMEOUT
+)
 
 # Allows to add information about the requester in the user-agent (eg. partner name)
 HF_HUB_USER_AGENT_ORIGIN: Optional[str] = os.environ.get("HF_HUB_USER_AGENT_ORIGIN")
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/errors.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/errors.py
@@ -67,9 +67,16 @@ class HfHubHTTPError(HTTPError):
     ```
     """
 
-    def __init__(self, message: str, response: Optional[Response] = None, *, server_message: Optional[str] = None):
+    def __init__(
+        self,
+        message: str,
+        response: Optional[Response] = None,
+        *,
+        server_message: Optional[str] = None
+    ):
         self.request_id = (
-            response.headers.get("x-request-id") or response.headers.get("X-Amzn-Trace-Id")
+            response.headers.get("x-request-id")
+            or response.headers.get("X-Amzn-Trace-Id")
             if response is not None
             else None
         )
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/fastai_utils.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/fastai_utils.py
@@ -143,11 +143,15 @@ def _check_fastai_fastcore_pyproject_ver
     # If the package is specified but not the version (e.g. "fastai" instead of "fastai=2.4"), the default versions are the highest.
     fastai_packages = [pck for pck in package_versions if pck.startswith("fastai")]
     if len(fastai_packages) == 0:
-        logger.warning("The repository does not have a fastai version specified in the `pyproject.toml`.")
+        logger.warning(
+            "The repository does not have a fastai version specified in the `pyproject.toml`."
+        )
     # fastai_version is an empty string if not specified
     else:
         fastai_version = str(fastai_packages[0]).partition("=")[2]
-        if fastai_version != "" and version.Version(fastai_version) < version.Version(fastai_min_version):
+        if fastai_version != "" and version.Version(fastai_version) < version.Version(
+            fastai_min_version
+        ):
             raise ImportError(
                 "`from_pretrained_fastai` requires"
                 f" fastai>={fastai_min_version} version but the model to load uses"
@@ -156,11 +160,15 @@ def _check_fastai_fastcore_pyproject_ver
 
     fastcore_packages = [pck for pck in package_versions if pck.startswith("fastcore")]
     if len(fastcore_packages) == 0:
-        logger.warning("The repository does not have a fastcore version specified in the `pyproject.toml`.")
+        logger.warning(
+            "The repository does not have a fastcore version specified in the `pyproject.toml`."
+        )
     # fastcore_version is an empty string if not specified
     else:
         fastcore_version = str(fastcore_packages[0]).partition("=")[2]
-        if fastcore_version != "" and version.Version(fastcore_version) < version.Version(fastcore_min_version):
+        if fastcore_version != "" and version.Version(
+            fastcore_version
+        ) < version.Version(fastcore_min_version):
             raise ImportError(
                 "`from_pretrained_fastai` requires"
                 f" fastcore>={fastcore_min_version} version, but you are using fastcore"
@@ -270,7 +278,9 @@ def _save_pretrained_fastai(
     # if the user provides config then we update it with the fastai and fastcore versions in CONFIG_TEMPLATE.
     if config is not None:
         if not isinstance(config, dict):
-            raise RuntimeError(f"Provided config should be a dict. Got: '{type(config)}'")
+            raise RuntimeError(
+                f"Provided config should be a dict. Got: '{type(config)}'"
+            )
         path = os.path.join(save_directory, constants.CONFIG_NAME)
         with open(path, "w") as f:
             json.dump(config, f)
@@ -406,7 +416,9 @@ def push_to_hub_fastai(
     """
     _check_fastai_fastcore_versions()
     api = HfApi(endpoint=api_endpoint)
-    repo_id = api.create_repo(repo_id=repo_id, token=token, private=private, exist_ok=True).repo_id
+    repo_id = api.create_repo(
+        repo_id=repo_id, token=token, private=private, exist_ok=True
+    ).repo_id
 
     # Push the files to the repo in a single commit
     with SoftTemporaryDirectory() as tmp:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/file_download.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/file_download.py
@@ -20,7 +20,11 @@ from . import (
     __version__,  # noqa: F401 # for backward compatibility
     constants,
 )
-from ._local_folder import get_local_download_paths, read_download_metadata, write_download_metadata
+from ._local_folder import (
+    get_local_download_paths,
+    read_download_metadata,
+    write_download_metadata,
+)
 from .constants import (
     HUGGINGFACE_CO_URL_TEMPLATE,  # noqa: F401 # for backward compatibility
     HUGGINGFACE_HUB_CACHE,  # noqa: F401 # for backward compatibility
@@ -258,7 +262,11 @@ def hf_hub_url(
 
 
 def _request_wrapper(
-    method: HTTP_METHOD_T, url: str, *, follow_relative_redirects: bool = False, **params
+    method: HTTP_METHOD_T,
+    url: str,
+    *,
+    follow_relative_redirects: bool = False,
+    **params,
 ) -> requests.Response:
     """Wrapper around requests methods to follow relative redirects if `follow_relative_redirects=True` even when
     `allow_redirection=False`.
@@ -296,7 +304,12 @@ def _request_wrapper(
                 # Highly inspired by `resolve_redirects` from requests library.
                 # See https://github.com/psf/requests/blob/main/requests/sessions.py#L159
                 next_url = urlparse(url)._replace(path=parsed_target.path).geturl()
-                return _request_wrapper(method=method, url=next_url, follow_relative_redirects=True, **params)
+                return _request_wrapper(
+                    method=method,
+                    url=next_url,
+                    follow_relative_redirects=True,
+                    **params,
+                )
         return response
 
     # Perform request and return if status_code is not in the retry list.
@@ -350,9 +363,13 @@ def http_get(
     hf_transfer = None
     if constants.HF_HUB_ENABLE_HF_TRANSFER:
         if resume_size != 0:
-            warnings.warn("'hf_transfer' does not support `resume_size`: falling back to regular download method")
+            warnings.warn(
+                "'hf_transfer' does not support `resume_size`: falling back to regular download method"
+            )
         elif proxies is not None:
-            warnings.warn("'hf_transfer' does not support `proxies`: falling back to regular download method")
+            warnings.warn(
+                "'hf_transfer' does not support `proxies`: falling back to regular download method"
+            )
         else:
             try:
                 import hf_transfer  # type: ignore[no-redef]
@@ -369,7 +386,12 @@ def http_get(
         headers["Range"] = _adjust_range_header(headers.get("Range"), resume_size)
 
     r = _request_wrapper(
-        method="GET", url=url, stream=True, proxies=proxies, headers=headers, timeout=constants.HF_HUB_DOWNLOAD_TIMEOUT
+        method="GET",
+        url=url,
+        stream=True,
+        proxies=proxies,
+        headers=headers,
+        timeout=constants.HF_HUB_DOWNLOAD_TIMEOUT,
     )
     hf_raise_for_status(r)
     content_length = r.headers.get("Content-Length")
@@ -416,8 +438,14 @@ def http_get(
     )
 
     with progress_cm as progress:
-        if hf_transfer and total is not None and total > 5 * constants.DOWNLOAD_CHUNK_SIZE:
-            supports_callback = "callback" in inspect.signature(hf_transfer.download).parameters
+        if (
+            hf_transfer
+            and total is not None
+            and total > 5 * constants.DOWNLOAD_CHUNK_SIZE
+        ):
+            supports_callback = (
+                "callback" in inspect.signature(hf_transfer.download).parameters
+            )
             if not supports_callback:
                 warnings.warn(
                     "You are using an outdated version of `hf_transfer`. "
@@ -442,7 +470,9 @@ def http_get(
                 ) from e
             if not supports_callback:
                 progress.update(total)
-            if expected_size is not None and expected_size != os.path.getsize(temp_file.name):
+            if expected_size is not None and expected_size != os.path.getsize(
+                temp_file.name
+            ):
                 raise EnvironmentError(
                     consistency_error_message.format(
                         actual_size=os.path.getsize(temp_file.name),
@@ -463,9 +493,17 @@ def http_get(
             # a transient error (network outage?). We log a warning message and try to resume the download a few times
             # before giving up. Tre retry mechanism is basic but should be enough in most cases.
             if _nb_retries <= 0:
-                logger.warning("Error while downloading from %s: %s\nMax retries exceeded.", url, str(e))
+                logger.warning(
+                    "Error while downloading from %s: %s\nMax retries exceeded.",
+                    url,
+                    str(e),
+                )
                 raise
-            logger.warning("Error while downloading from %s: %s\nTrying to resume download...", url, str(e))
+            logger.warning(
+                "Error while downloading from %s: %s\nTrying to resume download...",
+                url,
+                str(e),
+            )
             time.sleep(1)
             reset_sessions()  # In case of SSLError it's best to reset the shared requests.Session objects
             return http_get(
@@ -586,7 +624,9 @@ def _create_symlink(src: str, dst: str,
             os.symlink(src_rel_or_abs, abs_dst)
             return
         except FileExistsError:
-            if os.path.islink(abs_dst) and os.path.realpath(abs_dst) == os.path.realpath(abs_src):
+            if os.path.islink(abs_dst) and os.path.realpath(
+                abs_dst
+            ) == os.path.realpath(abs_src):
                 # `abs_dst` already exists and is a symlink to the `abs_src` blob. It is most likely that the file has
                 # been cached twice concurrently (exactly between `os.remove` and `os.symlink`). Do nothing.
                 return
@@ -608,7 +648,9 @@ def _create_symlink(src: str, dst: str,
         shutil.copyfile(abs_src, abs_dst)
 
 
-def _cache_commit_hash_for_specific_revision(storage_folder: str, revision: str, commit_hash: str) -> None:
+def _cache_commit_hash_for_specific_revision(
+    storage_folder: str, revision: str, commit_hash: str
+) -> None:
     """Cache reference between a revision (tag, branch or truncated commit hash) and the corresponding commit hash.
 
     Does nothing if `revision` is already a proper `commit_hash` or reference is already cached.
@@ -646,7 +688,9 @@ def _check_disk_space(expected_size: int
     """
 
     target_dir = Path(target_dir)  # format as `Path`
-    for path in [target_dir] + list(target_dir.parents):  # first check target_dir, then each parents one by one
+    for path in [target_dir] + list(
+        target_dir.parents
+    ):  # first check target_dir, then each parents one by one
         try:
             target_dir_free = shutil.disk_usage(path).free
             if target_dir_free < expected_size:
@@ -656,7 +700,9 @@ def _check_disk_space(expected_size: int
                     f"The target location {target_dir} only has {target_dir_free / 1e6:.2f} MB free disk space."
                 )
             return
-        except OSError:  # raise on anything: file does not exist or space disk cannot be checked
+        except (
+            OSError
+        ):  # raise on anything: file does not exist or space disk cannot be checked
             pass
 
 
@@ -819,7 +865,9 @@ def hf_hub_download(
     if repo_type is None:
         repo_type = "model"
     if repo_type not in constants.REPO_TYPES:
-        raise ValueError(f"Invalid repo type: {repo_type}. Accepted repo types are: {str(constants.REPO_TYPES)}")
+        raise ValueError(
+            f"Invalid repo type: {repo_type}. Accepted repo types are: {str(constants.REPO_TYPES)}"
+        )
 
     hf_headers = build_hf_headers(
         token=token,
@@ -903,7 +951,9 @@ def _hf_hub_download_to_cache_dir(
     Method should not be called directly. Please use `hf_hub_download` instead.
     """
     locks_dir = os.path.join(cache_dir, ".locks")
-    storage_folder = os.path.join(cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type))
+    storage_folder = os.path.join(
+        cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type)
+    )
 
     # cross platform transcription of filename, to be used as a local file path.
     relative_filename = os.path.join(*filename.split("/"))
@@ -922,19 +972,21 @@ def _hf_hub_download_to_cache_dir(
 
     # Try to get metadata (etag, commit_hash, url, size) from the server.
     # If we can't, a HEAD request error is returned.
-    (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(
-        repo_id=repo_id,
-        filename=filename,
-        repo_type=repo_type,
-        revision=revision,
-        endpoint=endpoint,
-        proxies=proxies,
-        etag_timeout=etag_timeout,
-        headers=headers,
-        token=token,
-        local_files_only=local_files_only,
-        storage_folder=storage_folder,
-        relative_filename=relative_filename,
+    (url_to_download, etag, commit_hash, expected_size, head_call_error) = (
+        _get_metadata_or_catch_error(
+            repo_id=repo_id,
+            filename=filename,
+            repo_type=repo_type,
+            revision=revision,
+            endpoint=endpoint,
+            proxies=proxies,
+            etag_timeout=etag_timeout,
+            headers=headers,
+            token=token,
+            local_files_only=local_files_only,
+            storage_folder=storage_folder,
+            relative_filename=relative_filename,
+        )
     )
 
     # etag can be None for several reasons:
@@ -961,7 +1013,9 @@ def _hf_hub_download_to_cache_dir(
 
             # Return pointer file if exists
             if commit_hash is not None:
-                pointer_path = _get_pointer_path(storage_folder, commit_hash, relative_filename)
+                pointer_path = _get_pointer_path(
+                    storage_folder, commit_hash, relative_filename
+                )
                 if os.path.exists(pointer_path) and not force_download:
                     return pointer_path
 
@@ -971,8 +1025,12 @@ def _hf_hub_download_to_cache_dir(
     # From now on, etag, commit_hash, url and size are not None.
     assert etag is not None, "etag must have been retrieved from server"
     assert commit_hash is not None, "commit_hash must have been retrieved from server"
-    assert url_to_download is not None, "file location must have been retrieved from server"
-    assert expected_size is not None, "expected_size must have been retrieved from server"
+    assert (
+        url_to_download is not None
+    ), "file location must have been retrieved from server"
+    assert (
+        expected_size is not None
+    ), "expected_size must have been retrieved from server"
     blob_path = os.path.join(storage_folder, "blobs", etag)
     pointer_path = _get_pointer_path(storage_folder, commit_hash, relative_filename)
 
@@ -996,7 +1054,11 @@ def _hf_hub_download_to_cache_dir(
 
     # Prevent parallel downloads of the same file with a lock.
     # etag could be duplicated across repos,
-    lock_path = os.path.join(locks_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type), f"{etag}.lock")
+    lock_path = os.path.join(
+        locks_dir,
+        repo_folder_name(repo_id=repo_id, repo_type=repo_type),
+        f"{etag}.lock",
+    )
 
     # Some Windows versions do not allow for paths longer than 255 characters.
     # In this case, we must specify it as an extended path by using the "\\?\" prefix.
@@ -1067,17 +1129,19 @@ def _hf_hub_download_to_local_dir(
         return str(paths.file_path)
 
     # Local file doesn't exist or commit_hash doesn't match => we need the etag
-    (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(
-        repo_id=repo_id,
-        filename=filename,
-        repo_type=repo_type,
-        revision=revision,
-        endpoint=endpoint,
-        proxies=proxies,
-        etag_timeout=etag_timeout,
-        headers=headers,
-        token=token,
-        local_files_only=local_files_only,
+    (url_to_download, etag, commit_hash, expected_size, head_call_error) = (
+        _get_metadata_or_catch_error(
+            repo_id=repo_id,
+            filename=filename,
+            repo_type=repo_type,
+            revision=revision,
+            endpoint=endpoint,
+            proxies=proxies,
+            etag_timeout=etag_timeout,
+            headers=headers,
+            token=token,
+            local_files_only=local_files_only,
+        )
     )
 
     if head_call_error is not None:
@@ -1093,14 +1157,23 @@ def _hf_hub_download_to_local_dir(
     # From now on, etag, commit_hash, url and size are not None.
     assert etag is not None, "etag must have been retrieved from server"
     assert commit_hash is not None, "commit_hash must have been retrieved from server"
-    assert url_to_download is not None, "file location must have been retrieved from server"
-    assert expected_size is not None, "expected_size must have been retrieved from server"
+    assert (
+        url_to_download is not None
+    ), "file location must have been retrieved from server"
+    assert (
+        expected_size is not None
+    ), "expected_size must have been retrieved from server"
 
     # Local file exists => check if it's up-to-date
     if not force_download and paths.file_path.is_file():
         # etag matches => update metadata and return file
         if local_metadata is not None and local_metadata.etag == etag:
-            write_download_metadata(local_dir=local_dir, filename=filename, commit_hash=commit_hash, etag=etag)
+            write_download_metadata(
+                local_dir=local_dir,
+                filename=filename,
+                commit_hash=commit_hash,
+                etag=etag,
+            )
             return str(paths.file_path)
 
         # metadata is outdated + etag is a sha256
@@ -1111,7 +1184,12 @@ def _hf_hub_download_to_local_dir(
             with open(paths.file_path, "rb") as f:
                 file_hash = sha_fileobj(f).hex()
             if file_hash == etag:
-                write_download_metadata(local_dir=local_dir, filename=filename, commit_hash=commit_hash, etag=etag)
+                write_download_metadata(
+                    local_dir=local_dir,
+                    filename=filename,
+                    commit_hash=commit_hash,
+                    etag=etag,
+                )
                 return str(paths.file_path)
 
     # Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)
@@ -1129,7 +1207,12 @@ def _hf_hub_download_to_local_dir(
             with WeakFileLock(paths.lock_path):
                 paths.file_path.parent.mkdir(parents=True, exist_ok=True)
                 shutil.copyfile(cached_path, paths.file_path)
-            write_download_metadata(local_dir=local_dir, filename=filename, commit_hash=commit_hash, etag=etag)
+            write_download_metadata(
+                local_dir=local_dir,
+                filename=filename,
+                commit_hash=commit_hash,
+                etag=etag,
+            )
             return str(paths.file_path)
 
     # Otherwise, let's download the file!
@@ -1146,7 +1229,9 @@ def _hf_hub_download_to_local_dir(
             force_download=force_download,
         )
 
-    write_download_metadata(local_dir=local_dir, filename=filename, commit_hash=commit_hash, etag=etag)
+    write_download_metadata(
+        local_dir=local_dir, filename=filename, commit_hash=commit_hash, etag=etag
+    )
     return str(paths.file_path)
 
 
@@ -1205,7 +1290,9 @@ def try_to_load_from_cache(
     if repo_type is None:
         repo_type = "model"
     if repo_type not in constants.REPO_TYPES:
-        raise ValueError(f"Invalid repo type: {repo_type}. Accepted repo types are: {str(constants.REPO_TYPES)}")
+        raise ValueError(
+            f"Invalid repo type: {repo_type}. Accepted repo types are: {str(constants.REPO_TYPES)}"
+        )
     if cache_dir is None:
         cache_dir = constants.HF_HUB_CACHE
 
@@ -1290,7 +1377,9 @@ def get_hf_file_metadata(
         user_agent=user_agent,
         headers=headers,
     )
-    hf_headers["Accept-Encoding"] = "identity"  # prevent any compression => we want to know the real size of the file
+    hf_headers["Accept-Encoding"] = (
+        "identity"  # prevent any compression => we want to know the real size of the file
+    )
 
     # Retrieve metadata
     r = _request_wrapper(
@@ -1309,13 +1398,17 @@ def get_hf_file_metadata(
         commit_hash=r.headers.get(constants.HUGGINGFACE_HEADER_X_REPO_COMMIT),
         # We favor a custom header indicating the etag of the linked resource, and
         # we fallback to the regular etag header.
-        etag=_normalize_etag(r.headers.get(constants.HUGGINGFACE_HEADER_X_LINKED_ETAG) or r.headers.get("ETag")),
+        etag=_normalize_etag(
+            r.headers.get(constants.HUGGINGFACE_HEADER_X_LINKED_ETAG)
+            or r.headers.get("ETag")
+        ),
         # Either from response headers (if redirected) or defaults to request url
         # Do not use directly `url`, as `_request_wrapper` might have followed relative
         # redirects.
         location=r.headers.get("Location") or r.request.url,  # type: ignore
         size=_int_or_none(
-            r.headers.get(constants.HUGGINGFACE_HEADER_X_LINKED_SIZE) or r.headers.get("Content-Length")
+            r.headers.get(constants.HUGGINGFACE_HEADER_X_LINKED_SIZE)
+            or r.headers.get("Content-Length")
         ),
     )
 
@@ -1361,7 +1454,9 @@ def _get_metadata_or_catch_error(
             ),
         )
 
-    url = hf_hub_url(repo_id, filename, repo_type=repo_type, revision=revision, endpoint=endpoint)
+    url = hf_hub_url(
+        repo_id, filename, repo_type=repo_type, revision=revision, endpoint=endpoint
+    )
     url_to_download: str = url
     etag: Optional[str] = None
     commit_hash: Optional[str] = None
@@ -1374,14 +1469,25 @@ def _get_metadata_or_catch_error(
         try:
             try:
                 metadata = get_hf_file_metadata(
-                    url=url, proxies=proxies, timeout=etag_timeout, headers=headers, token=token
+                    url=url,
+                    proxies=proxies,
+                    timeout=etag_timeout,
+                    headers=headers,
+                    token=token,
                 )
             except EntryNotFoundError as http_error:
                 if storage_folder is not None and relative_filename is not None:
                     # Cache the non-existence of the file
-                    commit_hash = http_error.response.headers.get(constants.HUGGINGFACE_HEADER_X_REPO_COMMIT)
+                    commit_hash = http_error.response.headers.get(
+                        constants.HUGGINGFACE_HEADER_X_REPO_COMMIT
+                    )
                     if commit_hash is not None:
-                        no_exist_file_path = Path(storage_folder) / ".no_exist" / commit_hash / relative_filename
+                        no_exist_file_path = (
+                            Path(storage_folder)
+                            / ".no_exist"
+                            / commit_hash
+                            / relative_filename
+                        )
                         try:
                             no_exist_file_path.parent.mkdir(parents=True, exist_ok=True)
                             no_exist_file_path.touch()
@@ -1389,7 +1495,9 @@ def _get_metadata_or_catch_error(
                             logger.error(
                                 f"Could not cache non-existence of file. Will ignore error and continue. Error: {e}"
                             )
-                        _cache_commit_hash_for_specific_revision(storage_folder, revision, commit_hash)
+                        _cache_commit_hash_for_specific_revision(
+                            storage_folder, revision, commit_hash
+                        )
                 raise
 
             # Commit hash must exist
@@ -1412,7 +1520,9 @@ def _get_metadata_or_catch_error(
             # Size must exist
             expected_size = metadata.size
             if expected_size is None:
-                raise FileMetadataError("Distant resource does not have a Content-Length.")
+                raise FileMetadataError(
+                    "Distant resource does not have a Content-Length."
+                )
 
             # In case of a redirect, save an extra redirect on the request.get call,
             # and ensure we download the exact atomic version even if it changed
@@ -1461,16 +1571,24 @@ def _get_metadata_or_catch_error(
     return (url_to_download, etag, commit_hash, expected_size, head_error_call)  # type: ignore [return-value]
 
 
-def _raise_on_head_call_error(head_call_error: Exception, force_download: bool, local_files_only: bool) -> NoReturn:
+def _raise_on_head_call_error(
+    head_call_error: Exception, force_download: bool, local_files_only: bool
+) -> NoReturn:
     """Raise an appropriate error when the HEAD call failed and we cannot locate a local file."""
     # No head call => we cannot force download.
     if force_download:
         if local_files_only:
-            raise ValueError("Cannot pass 'force_download=True' and 'local_files_only=True' at the same time.")
+            raise ValueError(
+                "Cannot pass 'force_download=True' and 'local_files_only=True' at the same time."
+            )
         elif isinstance(head_call_error, OfflineModeIsEnabled):
-            raise ValueError("Cannot pass 'force_download=True' when offline mode is enabled.") from head_call_error
+            raise ValueError(
+                "Cannot pass 'force_download=True' when offline mode is enabled."
+            ) from head_call_error
         else:
-            raise ValueError("Force download failed due to the above error.") from head_call_error
+            raise ValueError(
+                "Force download failed due to the above error."
+            ) from head_call_error
 
     # No head call + couldn't find an appropriate file on disk => raise an error.
     if local_files_only:
@@ -1479,7 +1597,8 @@ def _raise_on_head_call_error(head_call_
             " hf.co look-ups and downloads online, set 'local_files_only' to False."
         )
     elif isinstance(head_call_error, (RepositoryNotFoundError, GatedRepoError)) or (
-        isinstance(head_call_error, HfHubHTTPError) and head_call_error.response.status_code == 401
+        isinstance(head_call_error, HfHubHTTPError)
+        and head_call_error.response.status_code == 401
     ):
         # Repo not found or gated => let's raise the actual error
         # Unauthorized => likely a token issue => let's raise the actual error
@@ -1520,7 +1639,9 @@ def _download_to_tmp_and_move(
         # Do nothing if already exists (except if force_download=True)
         return
 
-    if incomplete_path.exists() and (force_download or (constants.HF_HUB_ENABLE_HF_TRANSFER and not proxies)):
+    if incomplete_path.exists() and (
+        force_download or (constants.HF_HUB_ENABLE_HF_TRANSFER and not proxies)
+    ):
         # By default, we will try to resume the download if possible.
         # However, if the user has set `force_download=True` or if `hf_transfer` is enabled, then we should
         # not resume the download => delete the incomplete file.
@@ -1612,11 +1733,16 @@ def _copy_no_matter_what(src: str, dst:
         shutil.copyfile(src, dst)
 
 
-def _get_pointer_path(storage_folder: str, revision: str, relative_filename: str) -> str:
+def _get_pointer_path(
+    storage_folder: str, revision: str, relative_filename: str
+) -> str:
     # Using `os.path.abspath` instead of `Path.resolve()` to avoid resolving symlinks
     snapshot_path = os.path.join(storage_folder, "snapshots")
     pointer_path = os.path.join(snapshot_path, revision, relative_filename)
-    if Path(os.path.abspath(snapshot_path)) not in Path(os.path.abspath(pointer_path)).parents:
+    if (
+        Path(os.path.abspath(snapshot_path))
+        not in Path(os.path.abspath(pointer_path)).parents
+    ):
         raise ValueError(
             "Invalid pointer path: cannot create pointer path in snapshot folder if"
             f" `storage_folder='{storage_folder}'`, `revision='{revision}'` and"
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/hf_api.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/hf_api.py
@@ -123,7 +123,11 @@ from .utils import (
     validate_hf_hub_args,
 )
 from .utils import tqdm as hf_tqdm
-from .utils._auth import _get_token_from_environment, _get_token_from_file, _get_token_from_google_colab
+from .utils._auth import (
+    _get_token_from_environment,
+    _get_token_from_file,
+    _get_token_from_google_colab,
+)
 from .utils._deprecation import _deprecate_method
 from .utils._typing import CallableT
 from .utils.endpoint_helpers import _is_emission_within_threshold
@@ -223,7 +227,9 @@ _AUTH_CHECK_NO_REPO_ERROR_MESSAGE = (
 logger = logging.get_logger(__name__)
 
 
-def repo_type_and_id_from_hf_id(hf_id: str, hub_url: Optional[str] = None) -> Tuple[Optional[str], Optional[str], str]:
+def repo_type_and_id_from_hf_id(
+    hf_id: str, hub_url: Optional[str] = None
+) -> Tuple[Optional[str], Optional[str], str]:
     """
     Returns the repo type and ID from a huggingface.co URL linking to a
     repository
@@ -254,7 +260,9 @@ def repo_type_and_id_from_hf_id(hf_id: s
     """
     input_hf_id = hf_id
 
-    hub_url = re.sub(r"https?://", "", hub_url if hub_url is not None else constants.ENDPOINT)
+    hub_url = re.sub(
+        r"https?://", "", hub_url if hub_url is not None else constants.ENDPOINT
+    )
     is_hf_url = hub_url in hf_id and "@" not in hf_id
 
     HFFS_PREFIX = "hf://"
@@ -296,7 +304,9 @@ def repo_type_and_id_from_hf_id(hf_id: s
             repo_id = url_segments[0]
             namespace, repo_type = None, None
     else:
-        raise ValueError(f"Unable to retrieve user and repo ID from the passed HF ID: {hf_id}")
+        raise ValueError(
+            f"Unable to retrieve user and repo ID from the passed HF ID: {hf_id}"
+        )
 
     # Check if repo type is known (mapping "spaces" => "space" + empty value => `None`)
     if repo_type in constants.REPO_TYPES_MAPPING:
@@ -561,7 +571,9 @@ class RepoUrl(str):
         super().__init__()
         # Parse URL
         self.endpoint = endpoint or constants.ENDPOINT
-        repo_type, namespace, repo_name = repo_type_and_id_from_hf_id(self, hub_url=self.endpoint)
+        repo_type, namespace, repo_name = repo_type_and_id_from_hf_id(
+            self, hub_url=self.endpoint
+        )
 
         # Populate fields
         self.namespace = namespace
@@ -644,12 +656,16 @@ class RepoFile:
         self.blob_id = kwargs.pop("oid")
         lfs = kwargs.pop("lfs", None)
         if lfs is not None:
-            lfs = BlobLfsInfo(size=lfs["size"], sha256=lfs["oid"], pointer_size=lfs["pointerSize"])
+            lfs = BlobLfsInfo(
+                size=lfs["size"], sha256=lfs["oid"], pointer_size=lfs["pointerSize"]
+            )
         self.lfs = lfs
         last_commit = kwargs.pop("lastCommit", None) or kwargs.pop("last_commit", None)
         if last_commit is not None:
             last_commit = LastCommitInfo(
-                oid=last_commit["id"], title=last_commit["title"], date=parse_datetime(last_commit["date"])
+                oid=last_commit["id"],
+                title=last_commit["title"],
+                date=parse_datetime(last_commit["date"]),
             )
         self.last_commit = last_commit
         security = kwargs.pop("securityFileStatus", None)
@@ -693,7 +709,9 @@ class RepoFolder:
         last_commit = kwargs.pop("lastCommit", None) or kwargs.pop("last_commit", None)
         if last_commit is not None:
             last_commit = LastCommitInfo(
-                oid=last_commit["id"], title=last_commit["title"], date=parse_datetime(last_commit["date"])
+                oid=last_commit["id"],
+                title=last_commit["title"],
+                date=parse_datetime(last_commit["date"]),
             )
         self.last_commit = last_commit
 
@@ -821,7 +839,9 @@ class ModelInfo:
         self.id = kwargs.pop("id")
         self.author = kwargs.pop("author", None)
         self.sha = kwargs.pop("sha", None)
-        last_modified = kwargs.pop("lastModified", None) or kwargs.pop("last_modified", None)
+        last_modified = kwargs.pop("lastModified", None) or kwargs.pop(
+            "last_modified", None
+        )
         self.last_modified = parse_datetime(last_modified) if last_modified else None
         created_at = kwargs.pop("createdAt", None) or kwargs.pop("created_at", None)
         self.created_at = parse_datetime(created_at) if created_at else None
@@ -849,14 +869,22 @@ class ModelInfo:
 
         card_data = kwargs.pop("cardData", None) or kwargs.pop("card_data", None)
         self.card_data = (
-            ModelCardData(**card_data, ignore_metadata_errors=True) if isinstance(card_data, dict) else card_data
+            ModelCardData(**card_data, ignore_metadata_errors=True)
+            if isinstance(card_data, dict)
+            else card_data
         )
 
         self.widget_data = kwargs.pop("widgetData", None)
-        self.model_index = kwargs.pop("model-index", None) or kwargs.pop("model_index", None)
+        self.model_index = kwargs.pop("model-index", None) or kwargs.pop(
+            "model_index", None
+        )
         self.config = kwargs.pop("config", None)
-        transformers_info = kwargs.pop("transformersInfo", None) or kwargs.pop("transformers_info", None)
-        self.transformers_info = TransformersInfo(**transformers_info) if transformers_info else None
+        transformers_info = kwargs.pop("transformersInfo", None) or kwargs.pop(
+            "transformers_info", None
+        )
+        self.transformers_info = (
+            TransformersInfo(**transformers_info) if transformers_info else None
+        )
         siblings = kwargs.pop("siblings", None)
         self.siblings = (
             [
@@ -970,7 +998,9 @@ class DatasetInfo:
         self.sha = kwargs.pop("sha", None)
         created_at = kwargs.pop("createdAt", None) or kwargs.pop("created_at", None)
         self.created_at = parse_datetime(created_at) if created_at else None
-        last_modified = kwargs.pop("lastModified", None) or kwargs.pop("last_modified", None)
+        last_modified = kwargs.pop("lastModified", None) or kwargs.pop(
+            "last_modified", None
+        )
         self.last_modified = parse_datetime(last_modified) if last_modified else None
         self.private = kwargs.pop("private", None)
         self.gated = kwargs.pop("gated", None)
@@ -984,7 +1014,9 @@ class DatasetInfo:
 
         card_data = kwargs.pop("cardData", None) or kwargs.pop("card_data", None)
         self.card_data = (
-            DatasetCardData(**card_data, ignore_metadata_errors=True) if isinstance(card_data, dict) else card_data
+            DatasetCardData(**card_data, ignore_metadata_errors=True)
+            if isinstance(card_data, dict)
+            else card_data
         )
         siblings = kwargs.pop("siblings", None)
         self.siblings = (
@@ -1097,7 +1129,9 @@ class SpaceInfo:
         self.sha = kwargs.pop("sha", None)
         created_at = kwargs.pop("createdAt", None) or kwargs.pop("created_at", None)
         self.created_at = parse_datetime(created_at) if created_at else None
-        last_modified = kwargs.pop("lastModified", None) or kwargs.pop("last_modified", None)
+        last_modified = kwargs.pop("lastModified", None) or kwargs.pop(
+            "last_modified", None
+        )
         self.last_modified = parse_datetime(last_modified) if last_modified else None
         self.private = kwargs.pop("private", None)
         self.gated = kwargs.pop("gated", None)
@@ -1110,7 +1144,9 @@ class SpaceInfo:
         self.trending_score = kwargs.pop("trendingScore", None)
         card_data = kwargs.pop("cardData", None) or kwargs.pop("card_data", None)
         self.card_data = (
-            SpaceCardData(**card_data, ignore_metadata_errors=True) if isinstance(card_data, dict) else card_data
+            SpaceCardData(**card_data, ignore_metadata_errors=True)
+            if isinstance(card_data, dict)
+            else card_data
         )
         siblings = kwargs.pop("siblings", None)
         self.siblings = (
@@ -1171,7 +1207,13 @@ class CollectionItem:
     note: Optional[str] = None
 
     def __init__(
-        self, _id: str, id: str, type: CollectionItemType_T, position: int, note: Optional[Dict] = None, **kwargs
+        self,
+        _id: str,
+        id: str,
+        type: CollectionItemType_T,
+        position: int,
+        note: Optional[Dict] = None,
+        **kwargs,
     ) -> None:
         self.item_object_id: str = _id  # id in database
         self.item_id: str = id  # repo_id or paper id
@@ -1503,18 +1545,26 @@ class PaperInfo:
         paper = kwargs.pop("paper", {})
         self.id = kwargs.pop("id", None) or paper.pop("id", None)
         authors = paper.pop("authors", None) or kwargs.pop("authors", None)
-        self.authors = [author.pop("name", None) for author in authors] if authors else None
+        self.authors = (
+            [author.pop("name", None) for author in authors] if authors else None
+        )
         published_at = paper.pop("publishedAt", None) or kwargs.pop("publishedAt", None)
         self.published_at = parse_datetime(published_at) if published_at else None
         self.title = kwargs.pop("title", None)
         self.source = kwargs.pop("source", None)
         self.summary = paper.pop("summary", None) or kwargs.pop("summary", None)
         self.upvotes = paper.pop("upvotes", None) or kwargs.pop("upvotes", None)
-        self.discussion_id = paper.pop("discussionId", None) or kwargs.pop("discussionId", None)
+        self.discussion_id = paper.pop("discussionId", None) or kwargs.pop(
+            "discussionId", None
+        )
         self.comments = kwargs.pop("numComments", 0)
-        submitted_at = kwargs.pop("publishedAt", None) or kwargs.pop("submittedOnDailyAt", None)
+        submitted_at = kwargs.pop("publishedAt", None) or kwargs.pop(
+            "submittedOnDailyAt", None
+        )
         self.submitted_at = parse_datetime(submitted_at) if submitted_at else None
-        submitted_by = kwargs.pop("submittedBy", None) or kwargs.pop("submittedOnDailyBy", None)
+        submitted_by = kwargs.pop("submittedBy", None) or kwargs.pop(
+            "submittedOnDailyBy", None
+        )
         self.submitted_by = User(**submitted_by) if submitted_by else None
 
         # forward compatibility
@@ -1674,7 +1724,9 @@ class HfApi:
                 )
             elif effective_token == _get_token_from_file():
                 error_message += " The token stored is invalid. Please run `huggingface-cli login` to update it."
-            raise HTTPError(error_message, request=e.request, response=e.response) from e
+            raise HTTPError(
+                error_message, request=e.request, response=e.response
+            ) from e
         return r.json()
 
     @_deprecate_method(
@@ -1872,10 +1924,14 @@ class HfApi:
         ```
         """
         if expand and (full or cardData or fetch_config):
-            raise ValueError("`expand` cannot be used if `full`, `cardData` or `fetch_config` are passed.")
+            raise ValueError(
+                "`expand` cannot be used if `full`, `cardData` or `fetch_config` are passed."
+            )
 
         if emissions_thresholds is not None and cardData is None:
-            raise ValueError("`emissions_thresholds` were passed without setting `cardData=True`.")
+            raise ValueError(
+                "`emissions_thresholds` were passed without setting `cardData=True`."
+            )
 
         path = f"{self.endpoint}/api/models"
         headers = self._build_hf_headers(token=token)
@@ -1923,11 +1979,11 @@ class HfApi:
             params["sort"] = (
                 "lastModified"
                 if sort == "last_modified"
-                else "trendingScore"
-                if sort == "trending_score"
-                else "createdAt"
-                if sort == "created_at"
-                else sort
+                else (
+                    "trendingScore"
+                    if sort == "trending_score"
+                    else "createdAt" if sort == "created_at" else sort
+                )
             )
         if direction is not None:
             params["direction"] = direction
@@ -1952,7 +2008,9 @@ class HfApi:
             if "siblings" not in item:
                 item["siblings"] = None
             model_info = ModelInfo(**item)
-            if emissions_thresholds is None or _is_emission_within_threshold(model_info, *emissions_thresholds):
+            if emissions_thresholds is None or _is_emission_within_threshold(
+                model_info, *emissions_thresholds
+            ):
                 yield model_info
 
     @validate_hf_hub_args
@@ -2141,11 +2199,11 @@ class HfApi:
             params["sort"] = (
                 "lastModified"
                 if sort == "last_modified"
-                else "trendingScore"
-                if sort == "trending_score"
-                else "createdAt"
-                if sort == "created_at"
-                else sort
+                else (
+                    "trendingScore"
+                    if sort == "trending_score"
+                    else "createdAt" if sort == "created_at" else sort
+                )
             )
         if direction is not None:
             params["direction"] = direction
@@ -2245,11 +2303,11 @@ class HfApi:
             params["sort"] = (
                 "lastModified"
                 if sort == "last_modified"
-                else "trendingScore"
-                if sort == "trending_score"
-                else "createdAt"
-                if sort == "created_at"
-                else sort
+                else (
+                    "trendingScore"
+                    if sort == "trending_score"
+                    else "createdAt" if sort == "created_at" else sort
+                )
             )
         if direction is not None:
             params["direction"] = direction
@@ -2323,7 +2381,8 @@ class HfApi:
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
         response = get_session().delete(
-            url=f"{self.endpoint}/api/{repo_type}s/{repo_id}/like", headers=self._build_hf_headers(token=token)
+            url=f"{self.endpoint}/api/{repo_type}s/{repo_id}/like",
+            headers=self._build_hf_headers(token=token),
         )
         hf_raise_for_status(response)
 
@@ -2398,9 +2457,21 @@ class HfApi:
         return UserLikes(
             user=user,
             total=len(likes),
-            models=[like["repo"]["name"] for like in likes if like["repo"]["type"] == "model"],
-            datasets=[like["repo"]["name"] for like in likes if like["repo"]["type"] == "dataset"],
-            spaces=[like["repo"]["name"] for like in likes if like["repo"]["type"] == "space"],
+            models=[
+                like["repo"]["name"]
+                for like in likes
+                if like["repo"]["type"] == "model"
+            ],
+            datasets=[
+                like["repo"]["name"]
+                for like in likes
+                if like["repo"]["type"] == "dataset"
+            ],
+            spaces=[
+                like["repo"]["name"]
+                for like in likes
+                if like["repo"]["type"] == "space"
+            ],
         )
 
     @validate_hf_hub_args
@@ -2439,8 +2510,14 @@ class HfApi:
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
         path = f"{self.endpoint}/api/{repo_type}s/{repo_id}/likers"
-        for liker in paginate(path, params={}, headers=self._build_hf_headers(token=token)):
-            yield User(username=liker["user"], fullname=liker["fullname"], avatar_url=liker["avatarUrl"])
+        for liker in paginate(
+            path, params={}, headers=self._build_hf_headers(token=token)
+        ):
+            yield User(
+                username=liker["user"],
+                fullname=liker["fullname"],
+                avatar_url=liker["avatarUrl"],
+            )
 
     @validate_hf_hub_args
     def model_info(
@@ -2500,13 +2577,17 @@ class HfApi:
         </Tip>
         """
         if expand and (securityStatus or files_metadata):
-            raise ValueError("`expand` cannot be used if `securityStatus` or `files_metadata` are set.")
+            raise ValueError(
+                "`expand` cannot be used if `securityStatus` or `files_metadata` are set."
+            )
 
         headers = self._build_hf_headers(token=token)
         path = (
             f"{self.endpoint}/api/models/{repo_id}"
             if revision is None
-            else (f"{self.endpoint}/api/models/{repo_id}/revision/{quote(revision, safe='')}")
+            else (
+                f"{self.endpoint}/api/models/{repo_id}/revision/{quote(revision, safe='')}"
+            )
         )
         params: Dict = {}
         if securityStatus:
@@ -2580,7 +2661,9 @@ class HfApi:
         path = (
             f"{self.endpoint}/api/datasets/{repo_id}"
             if revision is None
-            else (f"{self.endpoint}/api/datasets/{repo_id}/revision/{quote(revision, safe='')}")
+            else (
+                f"{self.endpoint}/api/datasets/{repo_id}/revision/{quote(revision, safe='')}"
+            )
         )
         params: Dict = {}
         if files_metadata:
@@ -2653,7 +2736,9 @@ class HfApi:
         path = (
             f"{self.endpoint}/api/spaces/{repo_id}"
             if revision is None
-            else (f"{self.endpoint}/api/spaces/{repo_id}/revision/{quote(revision, safe='')}")
+            else (
+                f"{self.endpoint}/api/spaces/{repo_id}/revision/{quote(revision, safe='')}"
+            )
         )
         params: Dict = {}
         if files_metadata:
@@ -2675,7 +2760,9 @@ class HfApi:
         repo_type: Optional[str] = None,
         timeout: Optional[float] = None,
         files_metadata: bool = False,
-        expand: Optional[Union[ExpandModelProperty_T, ExpandDatasetProperty_T, ExpandSpaceProperty_T]] = None,
+        expand: Optional[
+            Union[ExpandModelProperty_T, ExpandDatasetProperty_T, ExpandSpaceProperty_T]
+        ] = None,
         token: Union[bool, str, None] = None,
     ) -> Union[ModelInfo, DatasetInfo, SpaceInfo]:
         """
@@ -2824,7 +2911,9 @@ class HfApi:
             ```
         """
         try:
-            self.repo_info(repo_id=repo_id, revision=revision, repo_type=repo_type, token=token)
+            self.repo_info(
+                repo_id=repo_id, revision=revision, repo_type=repo_type, token=token
+            )
             return True
         except RevisionNotFoundError:
             return False
@@ -2877,7 +2966,11 @@ class HfApi:
             ```
         """
         url = hf_hub_url(
-            repo_id=repo_id, repo_type=repo_type, revision=revision, filename=filename, endpoint=self.endpoint
+            repo_id=repo_id,
+            repo_type=repo_type,
+            revision=revision,
+            filename=filename,
+            endpoint=self.endpoint,
         )
         try:
             if token is None:
@@ -2921,7 +3014,11 @@ class HfApi:
         return [
             f.rfilename
             for f in self.list_repo_tree(
-                repo_id=repo_id, recursive=True, revision=revision, repo_type=repo_type, token=token
+                repo_id=repo_id,
+                recursive=True,
+                revision=revision,
+                repo_type=repo_type,
+                token=token,
             )
             if isinstance(f, RepoFile)
         ]
@@ -3050,13 +3147,27 @@ class HfApi:
             ```
         """
         repo_type = repo_type or constants.REPO_TYPE_MODEL
-        revision = quote(revision, safe="") if revision is not None else constants.DEFAULT_REVISION
+        revision = (
+            quote(revision, safe="")
+            if revision is not None
+            else constants.DEFAULT_REVISION
+        )
         headers = self._build_hf_headers(token=token)
 
-        encoded_path_in_repo = "/" + quote(path_in_repo, safe="") if path_in_repo else ""
+        encoded_path_in_repo = (
+            "/" + quote(path_in_repo, safe="") if path_in_repo else ""
+        )
         tree_url = f"{self.endpoint}/api/{repo_type}s/{repo_id}/tree/{revision}{encoded_path_in_repo}"
-        for path_info in paginate(path=tree_url, headers=headers, params={"recursive": recursive, "expand": expand}):
-            yield (RepoFile(**path_info) if path_info["type"] == "file" else RepoFolder(**path_info))
+        for path_info in paginate(
+            path=tree_url,
+            headers=headers,
+            params={"recursive": recursive, "expand": expand},
+        ):
+            yield (
+                RepoFile(**path_info)
+                if path_info["type"] == "file"
+                else RepoFolder(**path_info)
+            )
 
     @validate_hf_hub_args
     def list_repo_refs(
@@ -3119,15 +3230,19 @@ class HfApi:
         data = response.json()
 
         def _format_as_git_ref_info(item: Dict) -> GitRefInfo:
-            return GitRefInfo(name=item["name"], ref=item["ref"], target_commit=item["targetCommit"])
+            return GitRefInfo(
+                name=item["name"], ref=item["ref"], target_commit=item["targetCommit"]
+            )
 
         return GitRefs(
             branches=[_format_as_git_ref_info(item) for item in data["branches"]],
             converts=[_format_as_git_ref_info(item) for item in data["converts"]],
             tags=[_format_as_git_ref_info(item) for item in data["tags"]],
-            pull_requests=[_format_as_git_ref_info(item) for item in data["pullRequests"]]
-            if include_pull_requests
-            else None,
+            pull_requests=(
+                [_format_as_git_ref_info(item) for item in data["pullRequests"]]
+                if include_pull_requests
+                else None
+            ),
         )
 
     @validate_hf_hub_args
@@ -3196,7 +3311,11 @@ class HfApi:
                 If revision is not found (error 404) on the repo.
         """
         repo_type = repo_type or constants.REPO_TYPE_MODEL
-        revision = quote(revision, safe="") if revision is not None else constants.DEFAULT_REVISION
+        revision = (
+            quote(revision, safe="")
+            if revision is not None
+            else constants.DEFAULT_REVISION
+        )
 
         # Paginate over results and return the list of commits.
         return [
@@ -3275,7 +3394,11 @@ class HfApi:
         ```
         """
         repo_type = repo_type or constants.REPO_TYPE_MODEL
-        revision = quote(revision, safe="") if revision is not None else constants.DEFAULT_REVISION
+        revision = (
+            quote(revision, safe="")
+            if revision is not None
+            else constants.DEFAULT_REVISION
+        )
         headers = self._build_hf_headers(token=token)
 
         response = get_session().post(
@@ -3289,7 +3412,11 @@ class HfApi:
         hf_raise_for_status(response)
         paths_info = response.json()
         return [
-            RepoFile(**path_info) if path_info["type"] == "file" else RepoFolder(**path_info)
+            (
+                RepoFile(**path_info)
+                if path_info["type"] == "file"
+                else RepoFolder(**path_info)
+            )
             for path_info in paths_info
         ]
 
@@ -3373,10 +3500,14 @@ class HfApi:
         # Prepare request
         url = f"{self.endpoint}/api/{repo_type}s/{repo_id}/super-squash/{quote(branch, safe='')}"
         headers = self._build_hf_headers(token=token)
-        commit_message = commit_message or f"Super-squash branch '{branch}' using huggingface_hub"
+        commit_message = (
+            commit_message or f"Super-squash branch '{branch}' using huggingface_hub"
+        )
 
         # Super-squash
-        response = get_session().post(url=url, headers=headers, json={"message": commit_message})
+        response = get_session().post(
+            url=url, headers=headers, json={"message": commit_message}
+        )
         hf_raise_for_status(response)
 
     @validate_hf_hub_args
@@ -3461,11 +3592,15 @@ class HfApi:
                     f" of {constants.SPACES_SDK_TYPES} when repo_type is 'space'`"
                 )
             if space_sdk not in constants.SPACES_SDK_TYPES:
-                raise ValueError(f"Invalid space_sdk. Please choose one of {constants.SPACES_SDK_TYPES}.")
+                raise ValueError(
+                    f"Invalid space_sdk. Please choose one of {constants.SPACES_SDK_TYPES}."
+                )
             json["sdk"] = space_sdk
 
         if space_sdk is not None and repo_type != "space":
-            warnings.warn("Ignoring provided space_sdk because repo_type is not 'space'.")
+            warnings.warn(
+                "Ignoring provided space_sdk because repo_type is not 'space'."
+            )
 
         function_args = [
             "space_hardware",
@@ -3474,16 +3609,32 @@ class HfApi:
             "space_secrets",
             "space_variables",
         ]
-        json_keys = ["hardware", "storageTier", "sleepTimeSeconds", "secrets", "variables"]
-        values = [space_hardware, space_storage, space_sleep_time, space_secrets, space_variables]
+        json_keys = [
+            "hardware",
+            "storageTier",
+            "sleepTimeSeconds",
+            "secrets",
+            "variables",
+        ]
+        values = [
+            space_hardware,
+            space_storage,
+            space_sleep_time,
+            space_secrets,
+            space_variables,
+        ]
 
         if repo_type == "space":
             json.update({k: v for k, v in zip(json_keys, values) if v is not None})
         else:
-            provided_space_args = [key for key, value in zip(function_args, values) if value is not None]
+            provided_space_args = [
+                key for key, value in zip(function_args, values) if value is not None
+            ]
 
             if provided_space_args:
-                warnings.warn(f"Ignoring provided {', '.join(provided_space_args)} because repo_type is not 'space'.")
+                warnings.warn(
+                    f"Ignoring provided {', '.join(provided_space_args)} because repo_type is not 'space'."
+                )
 
         if getattr(self, "_lfsmultipartthresh", None):
             # Testing purposes only.
@@ -3496,14 +3647,20 @@ class HfApi:
         headers = self._build_hf_headers(token=token)
         while True:
             r = get_session().post(path, headers=headers, json=json)
-            if r.status_code == 409 and "Cannot create repo: another conflicting operation is in progress" in r.text:
+            if (
+                r.status_code == 409
+                and "Cannot create repo: another conflicting operation is in progress"
+                in r.text
+            ):
                 # Since https://github.com/huggingface/moon-landing/pull/7272 (private repo), it is not possible to
                 # concurrently create repos on the Hub for a same user. This is rarely an issue, except when running
                 # tests. To avoid any inconvenience, we retry to create the repo for this specific error.
                 # NOTE: This could have being fixed directly in the tests but adding it here should fixed CIs for all
                 # dependent libraries.
                 # NOTE: If a fix is implemented server-side, we should be able to remove this retry mechanism.
-                logger.debug("Create repo failed due to a concurrency issue. Retrying...")
+                logger.debug(
+                    "Create repo failed due to a concurrency issue. Retrying..."
+                )
                 continue
             break
 
@@ -3578,7 +3735,9 @@ class HfApi:
             if not missing_ok:
                 raise
 
-    @_deprecate_method(version="0.32", message="Please use `update_repo_settings` instead.")
+    @_deprecate_method(
+        version="0.32", message="Please use `update_repo_settings` instead."
+    )
     @validate_hf_hub_args
     def update_repo_visibility(
         self,
@@ -3621,7 +3780,9 @@ class HfApi:
         </Tip>
         """
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
+            raise ValueError(
+                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
+            )
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL  # default repo type
 
@@ -3681,7 +3842,9 @@ class HfApi:
         """
 
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
+            raise ValueError(
+                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
+            )
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL  # default repo type
 
@@ -3697,7 +3860,9 @@ class HfApi:
 
         if gated is not None:
             if gated not in ["auto", "manual", False]:
-                raise ValueError(f"Invalid gated status, must be one of 'auto', 'manual', or False. Got '{gated}'.")
+                raise ValueError(
+                    f"Invalid gated status, must be one of 'auto', 'manual', or False. Got '{gated}'."
+                )
             payload["gated"] = gated
 
         if private is not None:
@@ -3753,10 +3918,14 @@ class HfApi:
         </Tip>
         """
         if len(from_id.split("/")) != 2:
-            raise ValueError(f"Invalid repo_id: {from_id}. It should have a namespace (:namespace:/:repo_name:)")
+            raise ValueError(
+                f"Invalid repo_id: {from_id}. It should have a namespace (:namespace:/:repo_name:)"
+            )
 
         if len(to_id.split("/")) != 2:
-            raise ValueError(f"Invalid repo_id: {to_id}. It should have a namespace (:namespace:/:repo_name:)")
+            raise ValueError(
+                f"Invalid repo_id: {to_id}. It should have a namespace (:namespace:/:repo_name:)"
+            )
 
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL  # Hub won't accept `None`.
@@ -3929,7 +4098,9 @@ class HfApi:
                 If repository is not found (error 404): wrong repo_id/repo_type, private
                 but not authenticated or repo does not exist.
         """
-        if parent_commit is not None and not constants.REGEX_COMMIT_OID.fullmatch(parent_commit):
+        if parent_commit is not None and not constants.REGEX_COMMIT_OID.fullmatch(
+            parent_commit
+        ):
             raise ValueError(
                 f"`parent_commit` is not a valid commit OID. It must match the following regex: {constants.REGEX_COMMIT_OID}"
             )
@@ -3937,10 +4108,14 @@ class HfApi:
         if commit_message is None or len(commit_message) == 0:
             raise ValueError("`commit_message` can't be empty, please pass a value.")
 
-        commit_description = commit_description if commit_description is not None else ""
+        commit_description = (
+            commit_description if commit_description is not None else ""
+        )
         repo_type = repo_type if repo_type is not None else constants.REPO_TYPE_MODEL
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
+            raise ValueError(
+                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
+            )
         unquoted_revision = revision or constants.DEFAULT_REVISION
         revision = quote(unquoted_revision, safe="")
         create_pr = create_pr if create_pr is not None else False
@@ -4016,7 +4191,9 @@ class HfApi:
                 and operation._remote_oid == operation._local_oid
             ):
                 # File already exists on the Hub and has not changed: we can skip it.
-                logger.debug(f"Skipping upload for '{operation.path_in_repo}' as the file has not changed.")
+                logger.debug(
+                    f"Skipping upload for '{operation.path_in_repo}' as the file has not changed."
+                )
                 continue
             if (
                 isinstance(operation, CommitOperationCopy)
@@ -4036,11 +4213,18 @@ class HfApi:
 
         # Return early if empty commit
         if len(operations_without_no_op) == 0:
-            logger.warning("No files have been modified since last commit. Skipping to prevent empty commit.")
+            logger.warning(
+                "No files have been modified since last commit. Skipping to prevent empty commit."
+            )
 
             # Get latest commit info
             try:
-                info = self.repo_info(repo_id=repo_id, repo_type=repo_type, revision=unquoted_revision, token=token)
+                info = self.repo_info(
+                    repo_id=repo_id,
+                    repo_type=repo_type,
+                    revision=unquoted_revision,
+                    token=token,
+                )
             except RepositoryNotFoundError as e:
                 e.append_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)
                 raise
@@ -4079,7 +4263,9 @@ class HfApi:
         params = {"create_pr": "1"} if create_pr else None
 
         try:
-            commit_resp = get_session().post(url=commit_url, headers=headers, data=data, params=params)
+            commit_resp = get_session().post(
+                url=commit_url, headers=headers, data=data, params=params
+            )
             hf_raise_for_status(commit_resp, endpoint_name="commit")
         except RepositoryNotFoundError as e:
             e.append_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)
@@ -4192,8 +4378,14 @@ class HfApi:
         """
         repo_type = repo_type if repo_type is not None else constants.REPO_TYPE_MODEL
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
-        revision = quote(revision, safe="") if revision is not None else constants.DEFAULT_REVISION
+            raise ValueError(
+                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
+            )
+        revision = (
+            quote(revision, safe="")
+            if revision is not None
+            else constants.DEFAULT_REVISION
+        )
         create_pr = create_pr if create_pr is not None else False
         headers = self._build_hf_headers(token=token)
 
@@ -4207,7 +4399,9 @@ class HfApi:
                         break
 
         # Filter out already uploaded files
-        new_additions = [addition for addition in additions if not addition._is_uploaded]
+        new_additions = [
+            addition for addition in additions if not addition._is_uploaded
+        ]
 
         # Check which new files are LFS
         try:
@@ -4226,13 +4420,17 @@ class HfApi:
             raise
 
         # Filter out regular files
-        new_lfs_additions = [addition for addition in new_additions if addition._upload_mode == "lfs"]
+        new_lfs_additions = [
+            addition for addition in new_additions if addition._upload_mode == "lfs"
+        ]
 
         # Filter out files listed in .gitignore
         new_lfs_additions_to_upload = []
         for addition in new_lfs_additions:
             if addition._should_ignore:
-                logger.debug(f"Skipping upload for LFS file '{addition.path_in_repo}' (ignored by gitignore file).")
+                logger.debug(
+                    f"Skipping upload for LFS file '{addition.path_in_repo}' (ignored by gitignore file)."
+                )
             else:
                 new_lfs_additions_to_upload.append(addition)
         if len(new_lfs_additions) != len(new_lfs_additions_to_upload):
@@ -4422,10 +4620,14 @@ class HfApi:
         ```
         """
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
+            raise ValueError(
+                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
+            )
 
         commit_message = (
-            commit_message if commit_message is not None else f"Upload {path_in_repo} with huggingface_hub"
+            commit_message
+            if commit_message is not None
+            else f"Upload {path_in_repo} with huggingface_hub"
         )
         operation = CommitOperationAdd(
             path_or_fileobj=path_or_fileobj,
@@ -4664,7 +4866,9 @@ class HfApi:
         ```
         """
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
+            raise ValueError(
+                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
+            )
 
         # By default, upload folder to the root directory in repo.
         if path_in_repo is None:
@@ -4698,7 +4902,9 @@ class HfApi:
         if len(add_operations) > 0:
             added_paths = set(op.path_in_repo for op in add_operations)
             delete_operations = [
-                delete_op for delete_op in delete_operations if delete_op.path_in_repo not in added_paths
+                delete_op
+                for delete_op in delete_operations
+                if delete_op.path_in_repo not in added_paths
             ]
         commit_operations = delete_operations + add_operations
 
@@ -4807,7 +5013,9 @@ class HfApi:
 
         """
         commit_message = (
-            commit_message if commit_message is not None else f"Delete {path_in_repo} with huggingface_hub"
+            commit_message
+            if commit_message is not None
+            else f"Delete {path_in_repo} with huggingface_hub"
         )
 
         operations = [CommitOperationDelete(path_in_repo=path_in_repo)]
@@ -4882,11 +5090,17 @@ class HfApi:
                 especially useful if the repo is updated / committed to concurrently.
         """
         operations = self._prepare_folder_deletions(
-            repo_id=repo_id, repo_type=repo_type, delete_patterns=delete_patterns, path_in_repo="", revision=revision
+            repo_id=repo_id,
+            repo_type=repo_type,
+            delete_patterns=delete_patterns,
+            path_in_repo="",
+            revision=revision,
         )
 
         if commit_message is None:
-            commit_message = f"Delete files {' '.join(delete_patterns)} with huggingface_hub"
+            commit_message = (
+                f"Delete files {' '.join(delete_patterns)} with huggingface_hub"
+            )
 
         return self.create_commit(
             repo_id=repo_id,
@@ -4958,10 +5172,14 @@ class HfApi:
             repo_id=repo_id,
             repo_type=repo_type,
             token=token,
-            operations=[CommitOperationDelete(path_in_repo=path_in_repo, is_folder=True)],
+            operations=[
+                CommitOperationDelete(path_in_repo=path_in_repo, is_folder=True)
+            ],
             revision=revision,
             commit_message=(
-                commit_message if commit_message is not None else f"Delete folder {path_in_repo} with huggingface_hub"
+                commit_message
+                if commit_message is not None
+                else f"Delete folder {path_in_repo} with huggingface_hub"
             ),
             commit_description=commit_description,
             create_pr=create_pr,
@@ -5482,7 +5700,8 @@ class HfApi:
                 metadata=None,
                 sharded=False,
                 weight_map={
-                    tensor_name: constants.SAFETENSORS_SINGLE_FILE for tensor_name in file_metadata.tensors.keys()
+                    tensor_name: constants.SAFETENSORS_SINGLE_FILE
+                    for tensor_name in file_metadata.tensors.keys()
                 },
                 files_metadata={constants.SAFETENSORS_SINGLE_FILE: file_metadata},
             )
@@ -5511,7 +5730,11 @@ class HfApi:
 
             def _parse(filename: str) -> None:
                 files_metadata[filename] = self.parse_safetensors_file_metadata(
-                    repo_id=repo_id, filename=filename, repo_type=repo_type, revision=revision, token=token
+                    repo_id=repo_id,
+                    filename=filename,
+                    repo_type=repo_type,
+                    revision=revision,
+                    token=token,
                 )
 
             thread_map(
@@ -5577,7 +5800,11 @@ class HfApi:
                 If a safetensors file header couldn't be parsed correctly.
         """
         url = hf_hub_url(
-            repo_id=repo_id, filename=filename, repo_type=repo_type, revision=revision, endpoint=self.endpoint
+            repo_id=repo_id,
+            filename=filename,
+            repo_type=repo_type,
+            revision=revision,
+            endpoint=self.endpoint,
         )
         _headers = self._build_hf_headers(token=token)
 
@@ -5586,7 +5813,9 @@ class HfApi:
         # We assume fetching 100kb is faster than making 2 GET requests. Therefore we always fetch the first 100kb to
         # avoid the 2nd GET in most cases.
         # See https://github.com/huggingface/huggingface_hub/pull/1855#discussion_r1404286419.
-        response = get_session().get(url, headers={**_headers, "range": "bytes=0-100000"})
+        response = get_session().get(
+            url, headers={**_headers, "range": "bytes=0-100000"}
+        )
         hf_raise_for_status(response)
 
         # 2. Parse metadata size
@@ -5602,7 +5831,9 @@ class HfApi:
         if metadata_size <= 100000:
             metadata_as_bytes = response.content[8 : 8 + metadata_size]
         else:  # 3.b. Request full metadata
-            response = get_session().get(url, headers={**_headers, "range": f"bytes=8-{metadata_size + 7}"})
+            response = get_session().get(
+                url, headers={**_headers, "range": f"bytes=8-{metadata_size + 7}"}
+            )
             hf_raise_for_status(response)
             metadata_as_bytes = response.content
 
@@ -5708,7 +5939,9 @@ class HfApi:
             elif exist_ok and e.response.status_code == 403:
                 # No write permission on the namespace but branch might already exist
                 try:
-                    refs = self.list_repo_refs(repo_id=repo_id, repo_type=repo_type, token=token)
+                    refs = self.list_repo_refs(
+                        repo_id=repo_id, repo_type=repo_type, token=token
+                    )
                     for branch_ref in refs.branches:
                         if branch_ref.name == branch:
                             return  # Branch already exists => do not raise
@@ -5825,7 +6058,11 @@ class HfApi:
         """
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
-        revision = quote(revision, safe="") if revision is not None else constants.DEFAULT_REVISION
+        revision = (
+            quote(revision, safe="")
+            if revision is not None
+            else constants.DEFAULT_REVISION
+        )
 
         # Prepare request
         tag_url = f"{self.endpoint}/api/{repo_type}s/{repo_id}/tag/{revision}"
@@ -5989,13 +6226,25 @@ class HfApi:
             ```
         """
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
+            raise ValueError(
+                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
+            )
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
-        if discussion_type is not None and discussion_type not in constants.DISCUSSION_TYPES:
-            raise ValueError(f"Invalid discussion_type, must be one of {constants.DISCUSSION_TYPES}")
-        if discussion_status is not None and discussion_status not in constants.DISCUSSION_STATUS:
-            raise ValueError(f"Invalid discussion_status, must be one of {constants.DISCUSSION_STATUS}")
+        if (
+            discussion_type is not None
+            and discussion_type not in constants.DISCUSSION_TYPES
+        ):
+            raise ValueError(
+                f"Invalid discussion_type, must be one of {constants.DISCUSSION_TYPES}"
+            )
+        if (
+            discussion_status is not None
+            and discussion_status not in constants.DISCUSSION_STATUS
+        ):
+            raise ValueError(
+                f"Invalid discussion_status, must be one of {constants.DISCUSSION_STATUS}"
+            )
 
         headers = self._build_hf_headers(token=token)
         path = f"{self.endpoint}/api/{repo_type}s/{repo_id}/discussions"
@@ -6083,11 +6332,15 @@ class HfApi:
         if not isinstance(discussion_num, int) or discussion_num <= 0:
             raise ValueError("Invalid discussion_num, must be a positive integer")
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
+            raise ValueError(
+                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
+            )
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
 
-        path = f"{self.endpoint}/api/{repo_type}s/{repo_id}/discussions/{discussion_num}"
+        path = (
+            f"{self.endpoint}/api/{repo_type}s/{repo_id}/discussions/{discussion_num}"
+        )
         headers = self._build_hf_headers(token=token)
         resp = get_session().get(path, params={"diff": "1"}, headers=headers)
         hf_raise_for_status(resp)
@@ -6095,9 +6348,17 @@ class HfApi:
         discussion_details = resp.json()
         is_pull_request = discussion_details["isPullRequest"]
 
-        target_branch = discussion_details["changes"]["base"] if is_pull_request else None
-        conflicting_files = discussion_details["filesWithConflicts"] if is_pull_request else None
-        merge_commit_oid = discussion_details["changes"].get("mergeCommitId", None) if is_pull_request else None
+        target_branch = (
+            discussion_details["changes"]["base"] if is_pull_request else None
+        )
+        conflicting_files = (
+            discussion_details["filesWithConflicts"] if is_pull_request else None
+        )
+        merge_commit_oid = (
+            discussion_details["changes"].get("mergeCommitId", None)
+            if is_pull_request
+            else None
+        )
 
         return DiscussionWithDetails(
             title=discussion_details["title"],
@@ -6173,7 +6434,9 @@ class HfApi:
 
         </Tip>"""
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
+            raise ValueError(
+                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
+            )
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
 
@@ -6283,7 +6546,9 @@ class HfApi:
         if not isinstance(discussion_num, int) or discussion_num <= 0:
             raise ValueError("Invalid discussion_num, must be a positive integer")
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
+            raise ValueError(
+                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
+            )
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
         repo_id = f"{repo_type}s/{repo_id}"
@@ -6743,7 +7008,9 @@ class HfApi:
         hf_raise_for_status(r)
 
     @validate_hf_hub_args
-    def delete_space_secret(self, repo_id: str, key: str, *, token: Union[bool, str, None] = None) -> None:
+    def delete_space_secret(
+        self, repo_id: str, key: str, *, token: Union[bool, str, None] = None
+    ) -> None:
         """Deletes a secret from a Space.
 
         Secrets allow to set secret keys or tokens to a Space without hardcoding them.
@@ -6768,7 +7035,9 @@ class HfApi:
         hf_raise_for_status(r)
 
     @validate_hf_hub_args
-    def get_space_variables(self, repo_id: str, *, token: Union[bool, str, None] = None) -> Dict[str, SpaceVariable]:
+    def get_space_variables(
+        self, repo_id: str, *, token: Union[bool, str, None] = None
+    ) -> Dict[str, SpaceVariable]:
         """Gets all variables from a Space.
 
         Variables allow to set environment variables to a Space without hardcoding them.
@@ -6860,7 +7129,9 @@ class HfApi:
         return {k: SpaceVariable(k, v) for k, v in r.json().items()}
 
     @validate_hf_hub_args
-    def get_space_runtime(self, repo_id: str, *, token: Union[bool, str, None] = None) -> SpaceRuntime:
+    def get_space_runtime(
+        self, repo_id: str, *, token: Union[bool, str, None] = None
+    ) -> SpaceRuntime:
         """Gets runtime information about a Space.
 
         Args:
@@ -6875,7 +7146,8 @@ class HfApi:
             [`SpaceRuntime`]: Runtime information about a Space including Space stage and hardware.
         """
         r = get_session().get(
-            f"{self.endpoint}/api/spaces/{repo_id}/runtime", headers=self._build_hf_headers(token=token)
+            f"{self.endpoint}/api/spaces/{repo_id}/runtime",
+            headers=self._build_hf_headers(token=token),
         )
         hf_raise_for_status(r)
         return SpaceRuntime(r.json())
@@ -6985,7 +7257,9 @@ class HfApi:
         return runtime
 
     @validate_hf_hub_args
-    def pause_space(self, repo_id: str, *, token: Union[bool, str, None] = None) -> SpaceRuntime:
+    def pause_space(
+        self, repo_id: str, *, token: Union[bool, str, None] = None
+    ) -> SpaceRuntime:
         """Pause your Space.
 
         A paused Space stops executing until manually restarted by its owner. This is different from the sleeping
@@ -7018,14 +7292,19 @@ class HfApi:
                 a static Space, you can set it to private.
         """
         r = get_session().post(
-            f"{self.endpoint}/api/spaces/{repo_id}/pause", headers=self._build_hf_headers(token=token)
+            f"{self.endpoint}/api/spaces/{repo_id}/pause",
+            headers=self._build_hf_headers(token=token),
         )
         hf_raise_for_status(r)
         return SpaceRuntime(r.json())
 
     @validate_hf_hub_args
     def restart_space(
-        self, repo_id: str, *, token: Union[bool, str, None] = None, factory_reboot: bool = False
+        self,
+        repo_id: str,
+        *,
+        token: Union[bool, str, None] = None,
+        factory_reboot: bool = False,
     ) -> SpaceRuntime:
         """Restart your Space.
 
@@ -7064,7 +7343,9 @@ class HfApi:
         if factory_reboot:
             params["factory"] = "true"
         r = get_session().post(
-            f"{self.endpoint}/api/spaces/{repo_id}/restart", headers=self._build_hf_headers(token=token), params=params
+            f"{self.endpoint}/api/spaces/{repo_id}/restart",
+            headers=self._build_hf_headers(token=token),
+            params=params,
         )
         hf_raise_for_status(r)
         return SpaceRuntime(r.json())
@@ -7158,7 +7439,14 @@ class HfApi:
         # repository must be a valid repo_id (namespace/repo_name).
         payload: Dict[str, Any] = {"repository": f"{to_namespace}/{to_repo_name}"}
 
-        keys = ["private", "hardware", "storageTier", "sleepTimeSeconds", "secrets", "variables"]
+        keys = [
+            "private",
+            "hardware",
+            "storageTier",
+            "sleepTimeSeconds",
+            "secrets",
+            "variables",
+        ]
         values = [private, hardware, storage, sleep_time, secrets, variables]
         payload.update({k: v for k, v in zip(keys, values) if v is not None})
 
@@ -7292,15 +7580,25 @@ class HfApi:
             user = self.whoami(token=token)
 
             # List personal endpoints first
-            endpoints: List[InferenceEndpoint] = list_inference_endpoints(namespace=self._get_namespace(token=token))
+            endpoints: List[InferenceEndpoint] = list_inference_endpoints(
+                namespace=self._get_namespace(token=token)
+            )
 
             # Then list endpoints for all orgs the user belongs to and ignore 401 errors (no billing or no access)
             for org in user.get("orgs", []):
                 try:
-                    endpoints += list_inference_endpoints(namespace=org["name"], token=token)
+                    endpoints += list_inference_endpoints(
+                        namespace=org["name"], token=token
+                    )
                 except HfHubHTTPError as error:
-                    if error.response.status_code == 401:  # Either no billing or user don't have access)
-                        logger.debug("Cannot list Inference Endpoints for org '%s': %s", org["name"], error)
+                    if (
+                        error.response.status_code == 401
+                    ):  # Either no billing or user don't have access)
+                        logger.debug(
+                            "Cannot list Inference Endpoints for org '%s': %s",
+                            org["name"],
+                            error,
+                        )
                     pass
 
             return endpoints
@@ -7447,7 +7745,11 @@ class HfApi:
         """
         namespace = namespace or self._get_namespace(token=token)
 
-        image = {"custom": custom_image} if custom_image is not None else {"huggingface": {}}
+        image = (
+            {"custom": custom_image}
+            if custom_image is not None
+            else {"huggingface": {}}
+        )
         payload: Dict = {
             "accountId": account_id,
             "compute": {
@@ -7483,10 +7785,16 @@ class HfApi:
         )
         hf_raise_for_status(response)
 
-        return InferenceEndpoint.from_raw(response.json(), namespace=namespace, token=token)
+        return InferenceEndpoint.from_raw(
+            response.json(), namespace=namespace, token=token
+        )
 
     def get_inference_endpoint(
-        self, name: str, *, namespace: Optional[str] = None, token: Union[bool, str, None] = None
+        self,
+        name: str,
+        *,
+        namespace: Optional[str] = None,
+        token: Union[bool, str, None] = None,
     ) -> InferenceEndpoint:
         """Get information about an Inference Endpoint.
 
@@ -7530,7 +7838,9 @@ class HfApi:
         )
         hf_raise_for_status(response)
 
-        return InferenceEndpoint.from_raw(response.json(), namespace=namespace, token=token)
+        return InferenceEndpoint.from_raw(
+            response.json(), namespace=namespace, token=token
+        )
 
     def update_inference_endpoint(
         self,
@@ -7638,10 +7948,16 @@ class HfApi:
         )
         hf_raise_for_status(response)
 
-        return InferenceEndpoint.from_raw(response.json(), namespace=namespace, token=token)
+        return InferenceEndpoint.from_raw(
+            response.json(), namespace=namespace, token=token
+        )
 
     def delete_inference_endpoint(
-        self, name: str, *, namespace: Optional[str] = None, token: Union[bool, str, None] = None
+        self,
+        name: str,
+        *,
+        namespace: Optional[str] = None,
+        token: Union[bool, str, None] = None,
     ) -> None:
         """Delete an Inference Endpoint.
 
@@ -7669,7 +7985,11 @@ class HfApi:
         hf_raise_for_status(response)
 
     def pause_inference_endpoint(
-        self, name: str, *, namespace: Optional[str] = None, token: Union[bool, str, None] = None
+        self,
+        name: str,
+        *,
+        namespace: Optional[str] = None,
+        token: Union[bool, str, None] = None,
     ) -> InferenceEndpoint:
         """Pause an Inference Endpoint.
 
@@ -7701,7 +8021,9 @@ class HfApi:
         )
         hf_raise_for_status(response)
 
-        return InferenceEndpoint.from_raw(response.json(), namespace=namespace, token=token)
+        return InferenceEndpoint.from_raw(
+            response.json(), namespace=namespace, token=token
+        )
 
     def resume_inference_endpoint(
         self,
@@ -7742,15 +8064,27 @@ class HfApi:
             hf_raise_for_status(response)
         except HfHubHTTPError as error:
             # If already running (and it's ok), then fetch current status and return
-            if running_ok and error.response.status_code == 400 and "already running" in error.response.text:
-                return self.get_inference_endpoint(name, namespace=namespace, token=token)
+            if (
+                running_ok
+                and error.response.status_code == 400
+                and "already running" in error.response.text
+            ):
+                return self.get_inference_endpoint(
+                    name, namespace=namespace, token=token
+                )
             # Otherwise, raise the error
             raise
 
-        return InferenceEndpoint.from_raw(response.json(), namespace=namespace, token=token)
+        return InferenceEndpoint.from_raw(
+            response.json(), namespace=namespace, token=token
+        )
 
     def scale_to_zero_inference_endpoint(
-        self, name: str, *, namespace: Optional[str] = None, token: Union[bool, str, None] = None
+        self,
+        name: str,
+        *,
+        namespace: Optional[str] = None,
+        token: Union[bool, str, None] = None,
     ) -> InferenceEndpoint:
         """Scale Inference Endpoint to zero.
 
@@ -7782,7 +8116,9 @@ class HfApi:
         )
         hf_raise_for_status(response)
 
-        return InferenceEndpoint.from_raw(response.json(), namespace=namespace, token=token)
+        return InferenceEndpoint.from_raw(
+            response.json(), namespace=namespace, token=token
+        )
 
     def _get_namespace(self, token: Union[bool, str, None] = None) -> str:
         """Get the default namespace for the current user."""
@@ -7857,7 +8193,9 @@ class HfApi:
         for position, collection_data in enumerate(items):
             yield Collection(position=position, **collection_data)
 
-    def get_collection(self, collection_slug: str, *, token: Union[bool, str, None] = None) -> Collection:
+    def get_collection(
+        self, collection_slug: str, *, token: Union[bool, str, None] = None
+    ) -> Collection:
         """Gets information about a Collection on the Hub.
 
         Args:
@@ -7891,7 +8229,8 @@ class HfApi:
         ```
         """
         r = get_session().get(
-            f"{self.endpoint}/api/collections/{collection_slug}", headers=self._build_hf_headers(token=token)
+            f"{self.endpoint}/api/collections/{collection_slug}",
+            headers=self._build_hf_headers(token=token),
         )
         hf_raise_for_status(r)
         return Collection(**{**r.json(), "endpoint": self.endpoint})
@@ -7951,7 +8290,9 @@ class HfApi:
             payload["description"] = description
 
         r = get_session().post(
-            f"{self.endpoint}/api/collections", headers=self._build_hf_headers(token=token), json=payload
+            f"{self.endpoint}/api/collections",
+            headers=self._build_hf_headers(token=token),
+            json=payload,
         )
         try:
             hf_raise_for_status(r)
@@ -8033,7 +8374,11 @@ class HfApi:
         return Collection(**{**r.json()["data"], "endpoint": self.endpoint})
 
     def delete_collection(
-        self, collection_slug: str, *, missing_ok: bool = False, token: Union[bool, str, None] = None
+        self,
+        collection_slug: str,
+        *,
+        missing_ok: bool = False,
+        token: Union[bool, str, None] = None,
     ) -> None:
         """Delete a collection on the Hub.
 
@@ -8062,7 +8407,8 @@ class HfApi:
         </Tip>
         """
         r = get_session().delete(
-            f"{self.endpoint}/api/collections/{collection_slug}", headers=self._build_hf_headers(token=token)
+            f"{self.endpoint}/api/collections/{collection_slug}",
+            headers=self._build_hf_headers(token=token),
         )
         try:
             hf_raise_for_status(r)
@@ -8266,7 +8612,11 @@ class HfApi:
 
     @validate_hf_hub_args
     def list_pending_access_requests(
-        self, repo_id: str, *, repo_type: Optional[str] = None, token: Union[bool, str, None] = None
+        self,
+        repo_id: str,
+        *,
+        repo_type: Optional[str] = None,
+        token: Union[bool, str, None] = None,
     ) -> List[AccessRequest]:
         """
         Get pending access requests for a given gated repo.
@@ -8326,11 +8676,17 @@ class HfApi:
         >>> accept_access_request("meta-llama/Llama-2-7b", "clem")
         ```
         """
-        return self._list_access_requests(repo_id, "pending", repo_type=repo_type, token=token)
+        return self._list_access_requests(
+            repo_id, "pending", repo_type=repo_type, token=token
+        )
 
     @validate_hf_hub_args
     def list_accepted_access_requests(
-        self, repo_id: str, *, repo_type: Optional[str] = None, token: Union[bool, str, None] = None
+        self,
+        repo_id: str,
+        *,
+        repo_type: Optional[str] = None,
+        token: Union[bool, str, None] = None,
     ) -> List[AccessRequest]:
         """
         Get accepted access requests for a given gated repo.
@@ -8388,11 +8744,17 @@ class HfApi:
         ]
         ```
         """
-        return self._list_access_requests(repo_id, "accepted", repo_type=repo_type, token=token)
+        return self._list_access_requests(
+            repo_id, "accepted", repo_type=repo_type, token=token
+        )
 
     @validate_hf_hub_args
     def list_rejected_access_requests(
-        self, repo_id: str, *, repo_type: Optional[str] = None, token: Union[bool, str, None] = None
+        self,
+        repo_id: str,
+        *,
+        repo_type: Optional[str] = None,
+        token: Union[bool, str, None] = None,
     ) -> List[AccessRequest]:
         """
         Get rejected access requests for a given gated repo.
@@ -8450,7 +8812,9 @@ class HfApi:
         ]
         ```
         """
-        return self._list_access_requests(repo_id, "rejected", repo_type=repo_type, token=token)
+        return self._list_access_requests(
+            repo_id, "rejected", repo_type=repo_type, token=token
+        )
 
     def _list_access_requests(
         self,
@@ -8460,7 +8824,9 @@ class HfApi:
         token: Union[bool, str, None] = None,
     ) -> List[AccessRequest]:
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
+            raise ValueError(
+                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
+            )
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
 
@@ -8483,7 +8849,12 @@ class HfApi:
 
     @validate_hf_hub_args
     def cancel_access_request(
-        self, repo_id: str, user: str, *, repo_type: Optional[str] = None, token: Union[bool, str, None] = None
+        self,
+        repo_id: str,
+        user: str,
+        *,
+        repo_type: Optional[str] = None,
+        token: Union[bool, str, None] = None,
     ) -> None:
         """
         Cancel an access request from a user for a given gated repo.
@@ -8519,11 +8890,18 @@ class HfApi:
             [`HTTPError`](https://requests.readthedocs.io/en/latest/api/#requests.HTTPError):
                 HTTP 404 if the user access request is already in the pending list.
         """
-        self._handle_access_request(repo_id, user, "pending", repo_type=repo_type, token=token)
+        self._handle_access_request(
+            repo_id, user, "pending", repo_type=repo_type, token=token
+        )
 
     @validate_hf_hub_args
     def accept_access_request(
-        self, repo_id: str, user: str, *, repo_type: Optional[str] = None, token: Union[bool, str, None] = None
+        self,
+        repo_id: str,
+        user: str,
+        *,
+        repo_type: Optional[str] = None,
+        token: Union[bool, str, None] = None,
     ) -> None:
         """
         Accept an access request from a user for a given gated repo.
@@ -8561,7 +8939,9 @@ class HfApi:
             [`HTTPError`](https://requests.readthedocs.io/en/latest/api/#requests.HTTPError):
                 HTTP 404 if the user access request is already in the accepted list.
         """
-        self._handle_access_request(repo_id, user, "accepted", repo_type=repo_type, token=token)
+        self._handle_access_request(
+            repo_id, user, "accepted", repo_type=repo_type, token=token
+        )
 
     @validate_hf_hub_args
     def reject_access_request(
@@ -8612,7 +8992,12 @@ class HfApi:
                 HTTP 404 if the user access request is already in the rejected list.
         """
         self._handle_access_request(
-            repo_id, user, "rejected", repo_type=repo_type, rejection_reason=rejection_reason, token=token
+            repo_id,
+            user,
+            "rejected",
+            repo_type=repo_type,
+            rejection_reason=rejection_reason,
+            token=token,
         )
 
     @validate_hf_hub_args
@@ -8626,7 +9011,9 @@ class HfApi:
         token: Union[bool, str, None] = None,
     ) -> None:
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
+            raise ValueError(
+                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
+            )
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
 
@@ -8634,7 +9021,9 @@ class HfApi:
 
         if rejection_reason is not None:
             if status != "rejected":
-                raise ValueError("`rejection_reason` can only be passed when rejecting an access request.")
+                raise ValueError(
+                    "`rejection_reason` can only be passed when rejecting an access request."
+                )
             payload["rejectionReason"] = rejection_reason
 
         response = get_session().post(
@@ -8646,7 +9035,12 @@ class HfApi:
 
     @validate_hf_hub_args
     def grant_access(
-        self, repo_id: str, user: str, *, repo_type: Optional[str] = None, token: Union[bool, str, None] = None
+        self,
+        repo_id: str,
+        user: str,
+        *,
+        repo_type: Optional[str] = None,
+        token: Union[bool, str, None] = None,
     ) -> None:
         """
         Grant access to a user for a given gated repo.
@@ -8683,7 +9077,9 @@ class HfApi:
                 HTTP 404 if the user does not exist on the Hub.
         """
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
+            raise ValueError(
+                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
+            )
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
 
@@ -8700,7 +9096,9 @@ class HfApi:
     ###################
 
     @validate_hf_hub_args
-    def get_webhook(self, webhook_id: str, *, token: Union[bool, str, None] = None) -> WebhookInfo:
+    def get_webhook(
+        self, webhook_id: str, *, token: Union[bool, str, None] = None
+    ) -> WebhookInfo:
         """Get a webhook by its id.
 
         Args:
@@ -8737,7 +9135,10 @@ class HfApi:
         hf_raise_for_status(response)
         webhook_data = response.json()["webhook"]
 
-        watched_items = [WebhookWatchedItem(type=item["type"], name=item["name"]) for item in webhook_data["watched"]]
+        watched_items = [
+            WebhookWatchedItem(type=item["type"], name=item["name"])
+            for item in webhook_data["watched"]
+        ]
 
         webhook = WebhookInfo(
             id=webhook_data["id"],
@@ -8751,7 +9152,9 @@ class HfApi:
         return webhook
 
     @validate_hf_hub_args
-    def list_webhooks(self, *, token: Union[bool, str, None] = None) -> List[WebhookInfo]:
+    def list_webhooks(
+        self, *, token: Union[bool, str, None] = None
+    ) -> List[WebhookInfo]:
         """List all configured webhooks.
 
         Args:
@@ -8792,7 +9195,10 @@ class HfApi:
             WebhookInfo(
                 id=webhook["id"],
                 url=webhook["url"],
-                watched=[WebhookWatchedItem(type=item["type"], name=item["name"]) for item in webhook["watched"]],
+                watched=[
+                    WebhookWatchedItem(type=item["type"], name=item["name"])
+                    for item in webhook["watched"]
+                ],
                 domains=webhook["domains"],
                 secret=webhook.get("secret"),
                 disabled=webhook["disabled"],
@@ -8851,16 +9257,27 @@ class HfApi:
             )
             ```
         """
-        watched_dicts = [asdict(item) if isinstance(item, WebhookWatchedItem) else item for item in watched]
+        watched_dicts = [
+            asdict(item) if isinstance(item, WebhookWatchedItem) else item
+            for item in watched
+        ]
 
         response = get_session().post(
             f"{constants.ENDPOINT}/api/settings/webhooks",
-            json={"watched": watched_dicts, "url": url, "domains": domains, "secret": secret},
+            json={
+                "watched": watched_dicts,
+                "url": url,
+                "domains": domains,
+                "secret": secret,
+            },
             headers=self._build_hf_headers(token=token),
         )
         hf_raise_for_status(response)
         webhook_data = response.json()["webhook"]
-        watched_items = [WebhookWatchedItem(type=item["type"], name=item["name"]) for item in webhook_data["watched"]]
+        watched_items = [
+            WebhookWatchedItem(type=item["type"], name=item["name"])
+            for item in webhook_data["watched"]
+        ]
 
         webhook = WebhookInfo(
             id=webhook_data["id"],
@@ -8929,17 +9346,28 @@ class HfApi:
         """
         if watched is None:
             watched = []
-        watched_dicts = [asdict(item) if isinstance(item, WebhookWatchedItem) else item for item in watched]
+        watched_dicts = [
+            asdict(item) if isinstance(item, WebhookWatchedItem) else item
+            for item in watched
+        ]
 
         response = get_session().post(
             f"{constants.ENDPOINT}/api/settings/webhooks/{webhook_id}",
-            json={"watched": watched_dicts, "url": url, "domains": domains, "secret": secret},
+            json={
+                "watched": watched_dicts,
+                "url": url,
+                "domains": domains,
+                "secret": secret,
+            },
             headers=self._build_hf_headers(token=token),
         )
         hf_raise_for_status(response)
         webhook_data = response.json()["webhook"]
 
-        watched_items = [WebhookWatchedItem(type=item["type"], name=item["name"]) for item in webhook_data["watched"]]
+        watched_items = [
+            WebhookWatchedItem(type=item["type"], name=item["name"])
+            for item in webhook_data["watched"]
+        ]
 
         webhook = WebhookInfo(
             id=webhook_data["id"],
@@ -8953,7 +9381,9 @@ class HfApi:
         return webhook
 
     @validate_hf_hub_args
-    def enable_webhook(self, webhook_id: str, *, token: Union[bool, str, None] = None) -> WebhookInfo:
+    def enable_webhook(
+        self, webhook_id: str, *, token: Union[bool, str, None] = None
+    ) -> WebhookInfo:
         """Enable a webhook (makes it "active").
 
         Args:
@@ -8990,7 +9420,10 @@ class HfApi:
         hf_raise_for_status(response)
         webhook_data = response.json()["webhook"]
 
-        watched_items = [WebhookWatchedItem(type=item["type"], name=item["name"]) for item in webhook_data["watched"]]
+        watched_items = [
+            WebhookWatchedItem(type=item["type"], name=item["name"])
+            for item in webhook_data["watched"]
+        ]
 
         webhook = WebhookInfo(
             id=webhook_data["id"],
@@ -9004,7 +9437,9 @@ class HfApi:
         return webhook
 
     @validate_hf_hub_args
-    def disable_webhook(self, webhook_id: str, *, token: Union[bool, str, None] = None) -> WebhookInfo:
+    def disable_webhook(
+        self, webhook_id: str, *, token: Union[bool, str, None] = None
+    ) -> WebhookInfo:
         """Disable a webhook (makes it "disabled").
 
         Args:
@@ -9041,7 +9476,10 @@ class HfApi:
         hf_raise_for_status(response)
         webhook_data = response.json()["webhook"]
 
-        watched_items = [WebhookWatchedItem(type=item["type"], name=item["name"]) for item in webhook_data["watched"]]
+        watched_items = [
+            WebhookWatchedItem(type=item["type"], name=item["name"])
+            for item in webhook_data["watched"]
+        ]
 
         webhook = WebhookInfo(
             id=webhook_data["id"],
@@ -9055,7 +9493,9 @@ class HfApi:
         return webhook
 
     @validate_hf_hub_args
-    def delete_webhook(self, webhook_id: str, *, token: Union[bool, str, None] = None) -> None:
+    def delete_webhook(
+        self, webhook_id: str, *, token: Union[bool, str, None] = None
+    ) -> None:
         """Delete a webhook.
 
         Args:
@@ -9129,21 +9569,29 @@ class HfApi:
             return []
 
         # List remote files
-        filenames = self.list_repo_files(repo_id=repo_id, revision=revision, repo_type=repo_type, token=token)
+        filenames = self.list_repo_files(
+            repo_id=repo_id, revision=revision, repo_type=repo_type, token=token
+        )
 
         # Compute relative path in repo
         if path_in_repo and path_in_repo not in (".", "./"):
             path_in_repo = path_in_repo.strip("/") + "/"  # harmonize
             relpath_to_abspath = {
-                file[len(path_in_repo) :]: file for file in filenames if file.startswith(path_in_repo)
+                file[len(path_in_repo) :]: file
+                for file in filenames
+                if file.startswith(path_in_repo)
             }
         else:
             relpath_to_abspath = {file: file for file in filenames}
 
         # Apply filter on relative paths and return
         return [
-            CommitOperationDelete(path_in_repo=relpath_to_abspath[relpath], is_folder=False)
-            for relpath in filter_repo_objects(relpath_to_abspath.keys(), allow_patterns=delete_patterns)
+            CommitOperationDelete(
+                path_in_repo=relpath_to_abspath[relpath], is_folder=False
+            )
+            for relpath in filter_repo_objects(
+                relpath_to_abspath.keys(), allow_patterns=delete_patterns
+            )
             if relpath_to_abspath[relpath] != ".gitattributes"
         ]
 
@@ -9177,7 +9625,9 @@ class HfApi:
         # Patterns are applied on the path relative to `folder_path`. `path_in_repo` is prefixed after the filtering.
         filtered_repo_objects = list(
             filter_repo_objects(
-                relpath_to_abspath.keys(), allow_patterns=allow_patterns, ignore_patterns=ignore_patterns
+                relpath_to_abspath.keys(),
+                allow_patterns=allow_patterns,
+                ignore_patterns=ignore_patterns,
             )
         )
 
@@ -9211,7 +9661,13 @@ class HfApi:
         logger.info(f"Finished hashing {len(filtered_repo_objects)} files.")
         return operations
 
-    def _validate_yaml(self, content: str, *, repo_type: Optional[str] = None, token: Union[bool, str, None] = None):
+    def _validate_yaml(
+        self,
+        content: str,
+        *,
+        repo_type: Optional[str] = None,
+        token: Union[bool, str, None] = None,
+    ):
         """
         Validate YAML from `README.md`, used before file hashing and upload.
 
@@ -9241,9 +9697,16 @@ class HfApi:
         )
         # Handle warnings (example: empty metadata)
         response_content = response.json()
-        message = "\n".join([f"- {warning.get('message')}" for warning in response_content.get("warnings", [])])
+        message = "\n".join(
+            [
+                f"- {warning.get('message')}"
+                for warning in response_content.get("warnings", [])
+            ]
+        )
         if message:
-            warnings.warn(f"Warnings while validating metadata in README.md:\n{message}")
+            warnings.warn(
+                f"Warnings while validating metadata in README.md:\n{message}"
+            )
 
         # Raise on errors
         try:
@@ -9253,7 +9716,9 @@ class HfApi:
             message = "\n".join([f"- {error.get('message')}" for error in errors])
             raise ValueError(f"Invalid metadata in README.md.\n{message}") from e
 
-    def get_user_overview(self, username: str, token: Union[bool, str, None] = None) -> User:
+    def get_user_overview(
+        self, username: str, token: Union[bool, str, None] = None
+    ) -> User:
         """
         Get an overview of a user on the Hub.
 
@@ -9274,12 +9739,15 @@ class HfApi:
                 HTTP 404 If the user does not exist on the Hub.
         """
         r = get_session().get(
-            f"{constants.ENDPOINT}/api/users/{username}/overview", headers=self._build_hf_headers(token=token)
+            f"{constants.ENDPOINT}/api/users/{username}/overview",
+            headers=self._build_hf_headers(token=token),
         )
         hf_raise_for_status(r)
         return User(**r.json())
 
-    def list_organization_members(self, organization: str, token: Union[bool, str, None] = None) -> Iterable[User]:
+    def list_organization_members(
+        self, organization: str, token: Union[bool, str, None] = None
+    ) -> Iterable[User]:
         """
         List of members of an organization on the Hub.
 
@@ -9307,7 +9775,9 @@ class HfApi:
         ):
             yield User(**member)
 
-    def list_user_followers(self, username: str, token: Union[bool, str, None] = None) -> Iterable[User]:
+    def list_user_followers(
+        self, username: str, token: Union[bool, str, None] = None
+    ) -> Iterable[User]:
         """
         Get the list of followers of a user on the Hub.
 
@@ -9335,7 +9805,9 @@ class HfApi:
         ):
             yield User(**follower)
 
-    def list_user_following(self, username: str, token: Union[bool, str, None] = None) -> Iterable[User]:
+    def list_user_following(
+        self, username: str, token: Union[bool, str, None] = None
+    ) -> Iterable[User]:
         """
         Get the list of users followed by a user on the Hub.
 
@@ -9430,7 +9902,11 @@ class HfApi:
         return PaperInfo(**r.json())
 
     def auth_check(
-        self, repo_id: str, *, repo_type: Optional[str] = None, token: Union[bool, str, None] = None
+        self,
+        repo_id: str,
+        *,
+        repo_type: Optional[str] = None,
+        token: Union[bool, str, None] = None,
     ) -> None:
         """
         Check if the provided user token has access to a specific repository on the Hugging Face Hub.
@@ -9488,7 +9964,9 @@ class HfApi:
         if repo_type is None:
             repo_type = constants.REPO_TYPE_MODEL
         if repo_type not in constants.REPO_TYPES:
-            raise ValueError(f"Invalid repo type, must be one of {constants.REPO_TYPES}")
+            raise ValueError(
+                f"Invalid repo type, must be one of {constants.REPO_TYPES}"
+            )
         path = f"{self.endpoint}/api/{repo_type}s/{repo_id}/auth-check"
         r = get_session().get(path, headers=headers)
         hf_raise_for_status(r)
@@ -9505,7 +9983,9 @@ def _parse_revision_from_pr_url(pr_url:
     """
     re_match = re.match(_REGEX_DISCUSSION_URL, pr_url)
     if re_match is None:
-        raise RuntimeError(f"Unexpected response from the hub, expected a Pull Request URL but got: '{pr_url}'")
+        raise RuntimeError(
+            f"Unexpected response from the hub, expected a Pull Request URL but got: '{pr_url}'"
+        )
     return f"refs/pr/{re_match[1]}"
 
 
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/hf_file_system.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/hf_file_system.py
@@ -46,11 +46,15 @@ class HfFileSystemResolvedPath:
     _raw_revision: Optional[str] = field(default=None, repr=False)
 
     def unresolve(self) -> str:
-        repo_path = constants.REPO_TYPES_URL_PREFIXES.get(self.repo_type, "") + self.repo_id
+        repo_path = (
+            constants.REPO_TYPES_URL_PREFIXES.get(self.repo_type, "") + self.repo_id
+        )
         if self._raw_revision:
             return f"{repo_path}@{self._raw_revision}/{self.path_in_repo}".rstrip("/")
         elif self.revision != constants.DEFAULT_REVISION:
-            return f"{repo_path}@{safe_revision(self.revision)}/{self.path_in_repo}".rstrip("/")
+            return f"{repo_path}@{safe_revision(self.revision)}/{self.path_in_repo}".rstrip(
+                "/"
+            )
         else:
             return f"{repo_path}/{self.path_in_repo}".rstrip("/")
 
@@ -123,20 +127,43 @@ class HfFileSystem(fsspec.AbstractFileSy
         if (repo_type, repo_id, revision) not in self._repo_and_revision_exists_cache:
             try:
                 self._api.repo_info(
-                    repo_id, revision=revision, repo_type=repo_type, timeout=constants.HF_HUB_ETAG_TIMEOUT
+                    repo_id,
+                    revision=revision,
+                    repo_type=repo_type,
+                    timeout=constants.HF_HUB_ETAG_TIMEOUT,
                 )
             except (RepositoryNotFoundError, HFValidationError) as e:
-                self._repo_and_revision_exists_cache[(repo_type, repo_id, revision)] = False, e
-                self._repo_and_revision_exists_cache[(repo_type, repo_id, None)] = False, e
+                self._repo_and_revision_exists_cache[(repo_type, repo_id, revision)] = (
+                    False,
+                    e,
+                )
+                self._repo_and_revision_exists_cache[(repo_type, repo_id, None)] = (
+                    False,
+                    e,
+                )
             except RevisionNotFoundError as e:
-                self._repo_and_revision_exists_cache[(repo_type, repo_id, revision)] = False, e
-                self._repo_and_revision_exists_cache[(repo_type, repo_id, None)] = True, None
+                self._repo_and_revision_exists_cache[(repo_type, repo_id, revision)] = (
+                    False,
+                    e,
+                )
+                self._repo_and_revision_exists_cache[(repo_type, repo_id, None)] = (
+                    True,
+                    None,
+                )
             else:
-                self._repo_and_revision_exists_cache[(repo_type, repo_id, revision)] = True, None
-                self._repo_and_revision_exists_cache[(repo_type, repo_id, None)] = True, None
+                self._repo_and_revision_exists_cache[(repo_type, repo_id, revision)] = (
+                    True,
+                    None,
+                )
+                self._repo_and_revision_exists_cache[(repo_type, repo_id, None)] = (
+                    True,
+                    None,
+                )
         return self._repo_and_revision_exists_cache[(repo_type, repo_id, revision)]
 
-    def resolve_path(self, path: str, revision: Optional[str] = None) -> HfFileSystemResolvedPath:
+    def resolve_path(
+        self, path: str, revision: Optional[str] = None
+    ) -> HfFileSystemResolvedPath:
         """
         Resolve a Hugging Face file system path into its components.
 
@@ -172,11 +199,15 @@ class HfFileSystem(fsspec.AbstractFileSy
         path = self._strip_protocol(path)
         if not path:
             # can't list repositories at root
-            raise NotImplementedError("Access to repositories lists is not implemented.")
+            raise NotImplementedError(
+                "Access to repositories lists is not implemented."
+            )
         elif path.split("/")[0] + "/" in constants.REPO_TYPES_URL_PREFIXES.values():
             if "/" not in path:
                 # can't list repositories at the repository type level
-                raise NotImplementedError("Access to repositories lists is not implemented.")
+                raise NotImplementedError(
+                    "Access to repositories lists is not implemented."
+                )
             repo_type, path = path.split("/", 1)
             repo_type = constants.REPO_TYPES_MAPPING[repo_type]
         else:
@@ -188,14 +219,20 @@ class HfFileSystem(fsspec.AbstractFileSy
                     match = SPECIAL_REFS_REVISION_REGEX.search(revision_in_path)
                     if match is not None and revision in (None, match.group()):
                         # Handle `refs/convert/parquet` and PR revisions separately
-                        path_in_repo = SPECIAL_REFS_REVISION_REGEX.sub("", revision_in_path).lstrip("/")
+                        path_in_repo = SPECIAL_REFS_REVISION_REGEX.sub(
+                            "", revision_in_path
+                        ).lstrip("/")
                         revision_in_path = match.group()
                     else:
                         revision_in_path, path_in_repo = revision_in_path.split("/", 1)
                 else:
                     path_in_repo = ""
-                revision = _align_revision_in_path_with_revision(unquote(revision_in_path), revision)
-                repo_and_revision_exist, err = self._repo_and_revision_exist(repo_type, repo_id, revision)
+                revision = _align_revision_in_path_with_revision(
+                    unquote(revision_in_path), revision
+                )
+                repo_and_revision_exist, err = self._repo_and_revision_exist(
+                    repo_type, repo_id, revision
+                )
                 if not repo_and_revision_exist:
                     _raise_file_not_found(path, err)
             else:
@@ -206,12 +243,16 @@ class HfFileSystem(fsspec.AbstractFileSy
                 path_in_repo_without_namespace = "/".join(path.split("/")[1:])
                 repo_id = repo_id_with_namespace
                 path_in_repo = path_in_repo_with_namespace
-                repo_and_revision_exist, err = self._repo_and_revision_exist(repo_type, repo_id, revision)
+                repo_and_revision_exist, err = self._repo_and_revision_exist(
+                    repo_type, repo_id, revision
+                )
                 if not repo_and_revision_exist:
                     if isinstance(err, (RepositoryNotFoundError, HFValidationError)):
                         repo_id = repo_id_without_namespace
                         path_in_repo = path_in_repo_without_namespace
-                        repo_and_revision_exist, _ = self._repo_and_revision_exist(repo_type, repo_id, revision)
+                        repo_and_revision_exist, _ = self._repo_and_revision_exist(
+                            repo_type, repo_id, revision
+                        )
                         if not repo_and_revision_exist:
                             _raise_file_not_found(path, err)
                     else:
@@ -221,15 +262,23 @@ class HfFileSystem(fsspec.AbstractFileSy
             path_in_repo = ""
             if "@" in path:
                 repo_id, revision_in_path = path.split("@", 1)
-                revision = _align_revision_in_path_with_revision(unquote(revision_in_path), revision)
+                revision = _align_revision_in_path_with_revision(
+                    unquote(revision_in_path), revision
+                )
             else:
                 revision_in_path = None
-            repo_and_revision_exist, _ = self._repo_and_revision_exist(repo_type, repo_id, revision)
+            repo_and_revision_exist, _ = self._repo_and_revision_exist(
+                repo_type, repo_id, revision
+            )
             if not repo_and_revision_exist:
-                raise NotImplementedError("Access to repositories lists is not implemented.")
+                raise NotImplementedError(
+                    "Access to repositories lists is not implemented."
+                )
 
         revision = revision if revision is not None else constants.DEFAULT_REVISION
-        return HfFileSystemResolvedPath(repo_type, repo_id, revision, path_in_repo, _raw_revision=revision_in_path)
+        return HfFileSystemResolvedPath(
+            repo_type, repo_id, revision, path_in_repo, _raw_revision=revision_in_path
+        )
 
     def invalidate_cache(self, path: Optional[str] = None) -> None:
         """
@@ -254,9 +303,16 @@ class HfFileSystem(fsspec.AbstractFileSy
 
             # Only clear repo cache if path is to repo root
             if not resolved_path.path_in_repo:
-                self._repo_and_revision_exists_cache.pop((resolved_path.repo_type, resolved_path.repo_id, None), None)
                 self._repo_and_revision_exists_cache.pop(
-                    (resolved_path.repo_type, resolved_path.repo_id, resolved_path.revision), None
+                    (resolved_path.repo_type, resolved_path.repo_id, None), None
+                )
+                self._repo_and_revision_exists_cache.pop(
+                    (
+                        resolved_path.repo_type,
+                        resolved_path.repo_id,
+                        resolved_path.revision,
+                    ),
+                    None,
                 )
 
     def _open(
@@ -270,9 +326,23 @@ class HfFileSystem(fsspec.AbstractFileSy
         if "a" in mode:
             raise NotImplementedError("Appending to remote files is not yet supported.")
         if block_size == 0:
-            return HfFileSystemStreamFile(self, path, mode=mode, revision=revision, block_size=block_size, **kwargs)
+            return HfFileSystemStreamFile(
+                self,
+                path,
+                mode=mode,
+                revision=revision,
+                block_size=block_size,
+                **kwargs,
+            )
         else:
-            return HfFileSystemFile(self, path, mode=mode, revision=revision, block_size=block_size, **kwargs)
+            return HfFileSystemFile(
+                self,
+                path,
+                mode=mode,
+                revision=revision,
+                block_size=block_size,
+                **kwargs,
+            )
 
     def _rm(self, path: str, revision: Optional[str] = None, **kwargs) -> None:
         resolved_path = self.resolve_path(path, revision=revision)
@@ -318,9 +388,18 @@ class HfFileSystem(fsspec.AbstractFileSy
 
         """
         resolved_path = self.resolve_path(path, revision=revision)
-        paths = self.expand_path(path, recursive=recursive, maxdepth=maxdepth, revision=revision)
-        paths_in_repo = [self.resolve_path(path).path_in_repo for path in paths if not self.isdir(path)]
-        operations = [CommitOperationDelete(path_in_repo=path_in_repo) for path_in_repo in paths_in_repo]
+        paths = self.expand_path(
+            path, recursive=recursive, maxdepth=maxdepth, revision=revision
+        )
+        paths_in_repo = [
+            self.resolve_path(path).path_in_repo
+            for path in paths
+            if not self.isdir(path)
+        ]
+        operations = [
+            CommitOperationDelete(path_in_repo=path_in_repo)
+            for path_in_repo in paths_in_repo
+        ]
         commit_message = f"Delete {path} "
         commit_message += "recursively " if recursive else ""
         commit_message += f"up to depth {maxdepth} " if maxdepth is not None else ""
@@ -337,7 +416,12 @@ class HfFileSystem(fsspec.AbstractFileSy
         self.invalidate_cache(path=resolved_path.unresolve())
 
     def ls(
-        self, path: str, detail: bool = True, refresh: bool = False, revision: Optional[str] = None, **kwargs
+        self,
+        path: str,
+        detail: bool = True,
+        refresh: bool = False,
+        revision: Optional[str] = None,
+        **kwargs,
     ) -> List[Union[str, Dict[str, Any]]]:
         """
         List the contents of a directory.
@@ -374,7 +458,9 @@ class HfFileSystem(fsspec.AbstractFileSy
             # Path could be a file
             if not resolved_path.path_in_repo:
                 _raise_file_not_found(path, None)
-            out = self._ls_tree(self._parent(path), refresh=refresh, revision=revision, **kwargs)
+            out = self._ls_tree(
+                self._parent(path), refresh=refresh, revision=revision, **kwargs
+            )
             out = [o for o in out if o["name"] == path]
             if len(out) == 0:
                 _raise_file_not_found(path, None)
@@ -407,7 +493,11 @@ class HfFileSystem(fsspec.AbstractFileSy
                 # Use BFS to traverse the cache and build the "recursive "output
                 # (The Hub uses a so-called "tree first" strategy for the tree endpoint but we sort the output to follow the spec so the result is (eventually) the same)
                 dirs_to_visit = deque(
-                    [path_info for path_info in cached_path_infos if path_info["type"] == "directory"]
+                    [
+                        path_info
+                        for path_info in cached_path_infos
+                        if path_info["type"] == "directory"
+                    ]
                 )
                 while dirs_to_visit:
                     dir_info = dirs_to_visit.popleft()
@@ -417,18 +507,28 @@ class HfFileSystem(fsspec.AbstractFileSy
                         cached_path_infos = self.dircache[dir_info["name"]]
                         out.extend(cached_path_infos)
                         dirs_to_visit.extend(
-                            [path_info for path_info in cached_path_infos if path_info["type"] == "directory"]
+                            [
+                                path_info
+                                for path_info in cached_path_infos
+                                if path_info["type"] == "directory"
+                            ]
                         )
 
             dirs_not_expanded = []
             if expand_info:
                 # Check if there are directories with non-expanded entries
-                dirs_not_expanded = [self._parent(o["name"]) for o in out if o["last_commit"] is None]
-
-            if (recursive and dirs_not_in_dircache) or (expand_info and dirs_not_expanded):
+                dirs_not_expanded = [
+                    self._parent(o["name"]) for o in out if o["last_commit"] is None
+                ]
+
+            if (recursive and dirs_not_in_dircache) or (
+                expand_info and dirs_not_expanded
+            ):
                 # If the dircache is incomplete, find the common path of the missing and non-expanded entries
                 # and extend the output with the result of `_ls_tree(common_path, recursive=True)`
-                common_prefix = os.path.commonprefix(dirs_not_in_dircache + dirs_not_expanded)
+                common_prefix = os.path.commonprefix(
+                    dirs_not_in_dircache + dirs_not_expanded
+                )
                 # Get the parent directory if the common prefix itself is not a directory
                 common_path = (
                     common_prefix.rstrip("/")
@@ -484,7 +584,9 @@ class HfFileSystem(fsspec.AbstractFileSy
                 out.append(cache_path_info)
         return out
 
-    def walk(self, path: str, *args, **kwargs) -> Iterator[Tuple[str, List[str], List[str]]]:
+    def walk(
+        self, path: str, *args, **kwargs
+    ) -> Iterator[Tuple[str, List[str], List[str]]]:
         """
         Return all files below the given path.
 
@@ -554,13 +656,25 @@ class HfFileSystem(fsspec.AbstractFileSy
         """
         if maxdepth:
             return super().find(
-                path, maxdepth=maxdepth, withdirs=withdirs, detail=detail, refresh=refresh, revision=revision, **kwargs
+                path,
+                maxdepth=maxdepth,
+                withdirs=withdirs,
+                detail=detail,
+                refresh=refresh,
+                revision=revision,
+                **kwargs,
             )
         resolved_path = self.resolve_path(path, revision=revision)
         path = resolved_path.unresolve()
         kwargs = {"expand_info": detail, **kwargs}
         try:
-            out = self._ls_tree(path, recursive=True, refresh=refresh, revision=resolved_path.revision, **kwargs)
+            out = self._ls_tree(
+                path,
+                recursive=True,
+                refresh=refresh,
+                revision=resolved_path.revision,
+                **kwargs,
+            )
         except EntryNotFoundError:
             # Path could be a file
             if self.info(path, revision=revision, **kwargs)["type"] == "file":
@@ -581,7 +695,9 @@ class HfFileSystem(fsspec.AbstractFileSy
         else:
             return {name: out[name] for name in names}
 
-    def cp_file(self, path1: str, path2: str, revision: Optional[str] = None, **kwargs) -> None:
+    def cp_file(
+        self, path1: str, path2: str, revision: Optional[str] = None, **kwargs
+    ) -> None:
         """
         Copy a file within or between repositories.
 
@@ -604,7 +720,8 @@ class HfFileSystem(fsspec.AbstractFileSy
         resolved_path2 = self.resolve_path(path2, revision=revision)
 
         same_repo = (
-            resolved_path1.repo_type == resolved_path2.repo_type and resolved_path1.repo_id == resolved_path2.repo_id
+            resolved_path1.repo_type == resolved_path2.repo_type
+            and resolved_path1.repo_id == resolved_path2.repo_id
         )
 
         if same_repo:
@@ -656,7 +773,9 @@ class HfFileSystem(fsspec.AbstractFileSy
         info = self.info(path, **kwargs)
         return info["last_commit"]["date"]
 
-    def info(self, path: str, refresh: bool = False, revision: Optional[str] = None, **kwargs) -> Dict[str, Any]:
+    def info(
+        self, path: str, refresh: bool = False, revision: Optional[str] = None, **kwargs
+    ) -> Dict[str, Any]:
         """
         Get information about a file or directory.
 
@@ -694,13 +813,17 @@ class HfFileSystem(fsspec.AbstractFileSy
             }
             if expand_info:
                 last_commit = self._api.list_repo_commits(
-                    resolved_path.repo_id, repo_type=resolved_path.repo_type, revision=resolved_path.revision
+                    resolved_path.repo_id,
+                    repo_type=resolved_path.repo_type,
+                    revision=resolved_path.revision,
                 )[-1]
                 out = {
                     **out,
                     "tree_id": None,  # TODO: tree_id of the root directory?
                     "last_commit": LastCommitInfo(
-                        oid=last_commit.commit_id, title=last_commit.title, date=last_commit.created_at
+                        oid=last_commit.commit_id,
+                        title=last_commit.title,
+                        date=last_commit.created_at,
                     ),
                 }
         else:
@@ -715,7 +838,11 @@ class HfFileSystem(fsspec.AbstractFileSy
                 if not out1:
                     _raise_file_not_found(path, None)
                 out = out1[0]
-            if refresh or out is None or (expand_info and out and out["last_commit"] is None):
+            if (
+                refresh
+                or out is None
+                or (expand_info and out and out["last_commit"] is None)
+            ):
                 paths_info = self._api.get_paths_info(
                     resolved_path.repo_id,
                     resolved_path.path_in_repo,
@@ -843,7 +970,9 @@ class HfFileSystem(fsspec.AbstractFileSy
             url = url.replace("/resolve/", "/tree/", 1)
         return url
 
-    def get_file(self, rpath, lpath, callback=_DEFAULT_CALLBACK, outfile=None, **kwargs) -> None:
+    def get_file(
+        self, rpath, lpath, callback=_DEFAULT_CALLBACK, outfile=None, **kwargs
+    ) -> None:
         """
         Copy single remote file to local.
 
@@ -866,10 +995,15 @@ class HfFileSystem(fsspec.AbstractFileSy
         """
         revision = kwargs.get("revision")
         unhandled_kwargs = set(kwargs.keys()) - {"revision"}
-        if not isinstance(callback, (NoOpCallback, TqdmCallback)) or len(unhandled_kwargs) > 0:
+        if (
+            not isinstance(callback, (NoOpCallback, TqdmCallback))
+            or len(unhandled_kwargs) > 0
+        ):
             # for now, let's not handle custom callbacks
             # and let's not handle custom kwargs
-            return super().get_file(rpath, lpath, callback=callback, outfile=outfile, **kwargs)
+            return super().get_file(
+                rpath, lpath, callback=callback, outfile=outfile, **kwargs
+            )
 
         # Taken from https://github.com/fsspec/filesystem_spec/blob/47b445ae4c284a82dd15e0287b1ffc410e8fc470/fsspec/spec.py#L883
         if isfilelike(lpath):
@@ -878,7 +1012,9 @@ class HfFileSystem(fsspec.AbstractFileSy
             os.makedirs(lpath, exist_ok=True)
             return None
 
-        if isinstance(lpath, (str, Path)):  # otherwise, let's assume it's a file-like object
+        if isinstance(
+            lpath, (str, Path)
+        ):  # otherwise, let's assume it's a file-like object
             os.makedirs(os.path.dirname(lpath), exist_ok=True)
 
         # Open file if not already open
@@ -933,7 +1069,9 @@ class HfFileSystem(fsspec.AbstractFileSy
 
 
 class HfFileSystemFile(fsspec.spec.AbstractBufferedFile):
-    def __init__(self, fs: HfFileSystem, path: str, revision: Optional[str] = None, **kwargs):
+    def __init__(
+        self, fs: HfFileSystem, path: str, revision: Optional[str] = None, **kwargs
+    ):
         try:
             self.resolved_path = fs.resolve_path(path, revision=revision)
         except FileNotFoundError as e:
@@ -1008,7 +1146,9 @@ class HfFileSystemFile(fsspec.spec.Abstr
         temporary file and read from there.
         """
         if self.mode == "rb" and (length is None or length == -1) and self.loc == 0:
-            with self.fs.open(self.path, "rb", block_size=0) as f:  # block_size=0 enables fast streaming
+            with self.fs.open(
+                self.path, "rb", block_size=0
+            ) as f:  # block_size=0 enables fast streaming
                 return f.read()
         return super().read(length)
 
@@ -1028,11 +1168,17 @@ class HfFileSystemStreamFile(fsspec.spec
         **kwargs,
     ):
         if block_size != 0:
-            raise ValueError(f"HfFileSystemStreamFile only supports block_size=0 but got {block_size}")
+            raise ValueError(
+                f"HfFileSystemStreamFile only supports block_size=0 but got {block_size}"
+            )
         if cache_type != "none":
-            raise ValueError(f"HfFileSystemStreamFile only supports cache_type='none' but got {cache_type}")
+            raise ValueError(
+                f"HfFileSystemStreamFile only supports cache_type='none' but got {cache_type}"
+            )
         if "w" in mode:
-            raise ValueError(f"HfFileSystemStreamFile only supports reading but got mode='{mode}'")
+            raise ValueError(
+                f"HfFileSystemStreamFile only supports reading but got mode='{mode}'"
+            )
         try:
             self.resolved_path = fs.resolve_path(path, revision=revision)
         except FileNotFoundError as e:
@@ -1043,7 +1189,12 @@ class HfFileSystemStreamFile(fsspec.spec
         # avoid an unnecessary .info() call to instantiate .details
         self.details = {"name": self.resolved_path.unresolve(), "size": None}
         super().__init__(
-            fs, self.resolved_path.unresolve(), mode=mode, block_size=block_size, cache_type=cache_type, **kwargs
+            fs,
+            self.resolved_path.unresolve(),
+            mode=mode,
+            block_size=block_size,
+            cache_type=cache_type,
+            **kwargs,
         )
         self.response: Optional[Response] = None
         self.fs: HfFileSystem
@@ -1090,7 +1241,10 @@ class HfFileSystemStreamFile(fsspec.spec
             self.response = http_backoff(
                 "GET",
                 url,
-                headers={"Range": "bytes=%d-" % self.loc, **self.fs._api._build_hf_headers()},
+                headers={
+                    "Range": "bytes=%d-" % self.loc,
+                    **self.fs._api._build_hf_headers(),
+                },
                 retry_on_status_codes=(500, 502, 503, 504),
                 stream=True,
                 timeout=constants.HF_HUB_DOWNLOAD_TIMEOUT,
@@ -1118,7 +1272,11 @@ class HfFileSystemStreamFile(fsspec.spec
 
 
 def safe_revision(revision: str) -> str:
-    return revision if SPECIAL_REFS_REVISION_REGEX.match(revision) else safe_quote(revision)
+    return (
+        revision
+        if SPECIAL_REFS_REVISION_REGEX.match(revision)
+        else safe_quote(revision)
+    )
 
 
 def safe_quote(s: str) -> str:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/hub_mixin.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/hub_mixin.py
@@ -3,7 +3,19 @@ import json
 import os
 from dataclasses import Field, asdict, dataclass, is_dataclass
 from pathlib import Path
-from typing import Any, Callable, ClassVar, Dict, List, Optional, Protocol, Tuple, Type, TypeVar, Union
+from typing import (
+    Any,
+    Callable,
+    ClassVar,
+    Dict,
+    List,
+    Optional,
+    Protocol,
+    Tuple,
+    Type,
+    TypeVar,
+    Union,
+)
 
 import packaging.version
 
@@ -184,8 +196,12 @@ class ModelHubMixin:
     # ^ information about the library integrating ModelHubMixin (used to generate model card)
     _hub_mixin_inject_config: bool  # whether `_from_pretrained` expects `config` or not
     _hub_mixin_init_parameters: Dict[str, inspect.Parameter]  # __init__ parameters
-    _hub_mixin_jsonable_default_values: Dict[str, Any]  # default values for __init__ parameters
-    _hub_mixin_jsonable_custom_types: Tuple[Type, ...]  # custom types that can be encoded/decoded
+    _hub_mixin_jsonable_default_values: Dict[
+        str, Any
+    ]  # default values for __init__ parameters
+    _hub_mixin_jsonable_custom_types: Tuple[
+        Type, ...
+    ]  # custom types that can be encoded/decoded
     _hub_mixin_coders: Dict[Type, CODER_T]  # encoders/decoders for custom types
     # ^ internal values to handle config
 
@@ -221,7 +237,9 @@ class ModelHubMixin:
         tags.append("model_hub_mixin")
 
         # Initialize MixinInfo if not existent
-        info = MixinInfo(model_card_template=model_card_template, model_card_data=ModelCardData())
+        info = MixinInfo(
+            model_card_template=model_card_template, model_card_data=ModelCardData()
+        )
 
         # If parent class has a MixinInfo, inherit from it as a copy
         if hasattr(cls, "_hub_mixin_info"):
@@ -230,7 +248,9 @@ class ModelHubMixin:
                 info.model_card_template = cls._hub_mixin_info.model_card_template
 
             # Inherit from parent model card data
-            info.model_card_data = ModelCardData(**cls._hub_mixin_info.model_card_data.to_dict())
+            info.model_card_data = ModelCardData(
+                **cls._hub_mixin_info.model_card_data.to_dict()
+            )
 
             # Inherit other info
             info.docs_url = cls._hub_mixin_info.docs_url
@@ -238,7 +258,10 @@ class ModelHubMixin:
         cls._hub_mixin_info = info
 
         # Update MixinInfo with metadata
-        if model_card_template is not None and model_card_template != DEFAULT_MODEL_CARD:
+        if (
+            model_card_template is not None
+            and model_card_template != DEFAULT_MODEL_CARD
+        ):
             info.model_card_template = model_card_template
         if repo_url is not None:
             info.repo_url = repo_url
@@ -269,13 +292,18 @@ class ModelHubMixin:
         cls._hub_mixin_jsonable_custom_types = tuple(cls._hub_mixin_coders.keys())
 
         # Inspect __init__ signature to handle config
-        cls._hub_mixin_init_parameters = dict(inspect.signature(cls.__init__).parameters)
+        cls._hub_mixin_init_parameters = dict(
+            inspect.signature(cls.__init__).parameters
+        )
         cls._hub_mixin_jsonable_default_values = {
             param.name: cls._encode_arg(param.default)
             for param in cls._hub_mixin_init_parameters.values()
-            if param.default is not inspect.Parameter.empty and cls._is_jsonable(param.default)
+            if param.default is not inspect.Parameter.empty
+            and cls._is_jsonable(param.default)
         }
-        cls._hub_mixin_inject_config = "config" in inspect.signature(cls._from_pretrained).parameters
+        cls._hub_mixin_inject_config = (
+            "config" in inspect.signature(cls._from_pretrained).parameters
+        )
 
     def __new__(cls: Type[T], *args, **kwargs) -> T:
         """Create a new instance of the class and handle config.
@@ -317,7 +345,9 @@ class ModelHubMixin:
             **{
                 key: cls._encode_arg(value)  # Encode custom types as jsonable value
                 for key, value in passed_values.items()
-                if instance._is_jsonable(value)  # Only if jsonable or we have a custom encoder
+                if instance._is_jsonable(
+                    value
+                )  # Only if jsonable or we have a custom encoder
             },
         }
         passed_config = init_config.pop("config", {})
@@ -421,7 +451,9 @@ class ModelHubMixin:
         model_card_path = save_directory / "README.md"
         model_card_kwargs = model_card_kwargs if model_card_kwargs is not None else {}
         if not model_card_path.exists():  # do not overwrite if already exists
-            self.generate_model_card(**model_card_kwargs).save(save_directory / "README.md")
+            self.generate_model_card(**model_card_kwargs).save(
+                save_directory / "README.md"
+            )
 
         # push to the Hub if required
         if push_to_hub:
@@ -430,7 +462,9 @@ class ModelHubMixin:
                 kwargs["config"] = config
             if repo_id is None:
                 repo_id = save_directory.name  # Defaults to `save_directory` name
-            return self.push_to_hub(repo_id=repo_id, model_card_kwargs=model_card_kwargs, **kwargs)
+            return self.push_to_hub(
+                repo_id=repo_id, model_card_kwargs=model_card_kwargs, **kwargs
+            )
         return None
 
     def _save_pretrained(self, save_directory: Path) -> None:
@@ -492,7 +526,9 @@ class ModelHubMixin:
             if constants.CONFIG_NAME in os.listdir(model_id):
                 config_file = os.path.join(model_id, constants.CONFIG_NAME)
             else:
-                logger.warning(f"{constants.CONFIG_NAME} not found in {Path(model_id).resolve()}")
+                logger.warning(
+                    f"{constants.CONFIG_NAME} not found in {Path(model_id).resolve()}"
+                )
         else:
             try:
                 config_file = hf_hub_download(
@@ -507,7 +543,9 @@ class ModelHubMixin:
                     local_files_only=local_files_only,
                 )
             except HfHubHTTPError as e:
-                logger.info(f"{constants.CONFIG_NAME} not found on the HuggingFace Hub: {str(e)}")
+                logger.info(
+                    f"{constants.CONFIG_NAME} not found on the HuggingFace Hub: {str(e)}"
+                )
 
         # Read config
         config = None
@@ -528,7 +566,10 @@ class ModelHubMixin:
                     model_kwargs[param.name] = config[param.name]
 
             # Check if `config` argument was passed at init
-            if "config" in cls._hub_mixin_init_parameters and "config" not in model_kwargs:
+            if (
+                "config" in cls._hub_mixin_init_parameters
+                and "config" not in model_kwargs
+            ):
                 # Decode `config` argument if it was passed
                 config_annotation = cls._hub_mixin_init_parameters["config"].annotation
                 config = cls._decode_arg(config_annotation, config)
@@ -541,7 +582,10 @@ class ModelHubMixin:
                 for key in cls.__dataclass_fields__:
                     if key not in model_kwargs and key in config:
                         model_kwargs[key] = config[key]
-            elif any(param.kind == inspect.Parameter.VAR_KEYWORD for param in cls._hub_mixin_init_parameters.values()):
+            elif any(
+                param.kind == inspect.Parameter.VAR_KEYWORD
+                for param in cls._hub_mixin_init_parameters.values()
+            ):
                 for key, value in config.items():
                     if key not in model_kwargs:
                         model_kwargs[key] = value
@@ -564,7 +608,9 @@ class ModelHubMixin:
 
         # Implicitly set the config as instance attribute if not already set by the class
         # This way `config` will be available when calling `save_pretrained` or `push_to_hub`.
-        if config is not None and (getattr(instance, "_hub_mixin_config", None) in (None, {})):
+        if config is not None and (
+            getattr(instance, "_hub_mixin_config", None) in (None, {})
+        ):
             instance._hub_mixin_config = config
 
         return instance
@@ -669,12 +715,16 @@ class ModelHubMixin:
             The url of the commit of your model in the given repository.
         """
         api = HfApi(token=token)
-        repo_id = api.create_repo(repo_id=repo_id, private=private, exist_ok=True).repo_id
+        repo_id = api.create_repo(
+            repo_id=repo_id, private=private, exist_ok=True
+        ).repo_id
 
         # Push the files to the repo in a single commit
         with SoftTemporaryDirectory() as tmp:
             saved_path = Path(tmp) / repo_id
-            self.save_pretrained(saved_path, config=config, model_card_kwargs=model_card_kwargs)
+            self.save_pretrained(
+                saved_path, config=config, model_card_kwargs=model_card_kwargs
+            )
             return api.upload_folder(
                 repo_id=repo_id,
                 repo_type="model",
@@ -743,7 +793,9 @@ class PyTorchModelHubMixin(ModelHubMixin
     ```
     """
 
-    def __init_subclass__(cls, *args, tags: Optional[List[str]] = None, **kwargs) -> None:
+    def __init_subclass__(
+        cls, *args, tags: Optional[List[str]] = None, **kwargs
+    ) -> None:
         tags = tags or []
         tags.append("pytorch_model_hub_mixin")
         kwargs["tags"] = tags
@@ -752,7 +804,9 @@ class PyTorchModelHubMixin(ModelHubMixin
     def _save_pretrained(self, save_directory: Path) -> None:
         """Save weights from a Pytorch model to a local directory."""
         model_to_save = self.module if hasattr(self, "module") else self  # type: ignore
-        save_model_as_safetensor(model_to_save, str(save_directory / constants.SAFETENSORS_SINGLE_FILE))
+        save_model_as_safetensor(
+            model_to_save, str(save_directory / constants.SAFETENSORS_SINGLE_FILE)
+        )
 
     @classmethod
     def _from_pretrained(
@@ -805,14 +859,20 @@ class PyTorchModelHubMixin(ModelHubMixin
                 return cls._load_as_pickle(model, model_file, map_location, strict)
 
     @classmethod
-    def _load_as_pickle(cls, model: T, model_file: str, map_location: str, strict: bool) -> T:
-        state_dict = torch.load(model_file, map_location=torch.device(map_location), weights_only=True)
+    def _load_as_pickle(
+        cls, model: T, model_file: str, map_location: str, strict: bool
+    ) -> T:
+        state_dict = torch.load(
+            model_file, map_location=torch.device(map_location), weights_only=True
+        )
         model.load_state_dict(state_dict, strict=strict)  # type: ignore
         model.eval()  # type: ignore
         return model
 
     @classmethod
-    def _load_as_safetensor(cls, model: T, model_file: str, map_location: str, strict: bool) -> T:
+    def _load_as_safetensor(
+        cls, model: T, model_file: str, map_location: str, strict: bool
+    ) -> T:
         if packaging.version.parse(safetensors.__version__) < packaging.version.parse("0.4.3"):  # type: ignore [attr-defined]
             load_model_as_safetensor(model, model_file, strict=strict)  # type: ignore [arg-type]
             if map_location != "cpu":
@@ -833,4 +893,6 @@ def _load_dataclass(datacls: Type[Datacl
 
     Fields not expected by the dataclass are ignored.
     """
-    return datacls(**{k: v for k, v in data.items() if k in datacls.__dataclass_fields__})
+    return datacls(
+        **{k: v for k, v in data.items() if k in datacls.__dataclass_fields__}
+    )
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_client.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_client.py
@@ -36,7 +36,17 @@ import base64
 import logging
 import re
 import warnings
-from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Literal, Optional, Union, overload
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Dict,
+    Iterable,
+    List,
+    Literal,
+    Optional,
+    Union,
+    overload,
+)
 
 from requests import HTTPError
 
@@ -100,7 +110,11 @@ from huggingface_hub.inference._generate
     ZeroShotClassificationOutputElement,
     ZeroShotImageClassificationOutputElement,
 )
-from huggingface_hub.inference._providers import PROVIDER_T, HFInferenceTask, get_provider_helper
+from huggingface_hub.inference._providers import (
+    PROVIDER_T,
+    HFInferenceTask,
+    get_provider_helper,
+)
 from huggingface_hub.utils import build_hf_headers, get_session, hf_raise_for_status
 from huggingface_hub.utils._deprecation import _deprecate_arguments, _deprecate_method
 
@@ -112,7 +126,9 @@ if TYPE_CHECKING:
 logger = logging.getLogger(__name__)
 
 
-MODEL_KWARGS_NOT_USED_REGEX = re.compile(r"The following `model_kwargs` are not used by the model: \[(.*?)\]")
+MODEL_KWARGS_NOT_USED_REGEX = re.compile(
+    r"The following `model_kwargs` are not used by the model: \[(.*?)\]"
+)
 
 
 class InferenceClient:
@@ -297,7 +313,10 @@ class InferenceClient:
     ) -> Union[bytes, Iterable[bytes]]:
         """Make a request to the inference server."""
         # TODO: this should be handled in provider helpers directly
-        if request_parameters.task in TASKS_EXPECTING_IMAGES and "Accept" not in request_parameters.headers:
+        if (
+            request_parameters.task in TASKS_EXPECTING_IMAGES
+            and "Accept" not in request_parameters.headers
+        ):
             request_parameters.headers["Accept"] = "image/png"
 
         while True:
@@ -321,7 +340,10 @@ class InferenceClient:
                 hf_raise_for_status(response)
                 return response.iter_lines() if stream else response.content
             except HTTPError as error:
-                if error.response.status_code == 422 and request_parameters.task != "unknown":
+                if (
+                    error.response.status_code == 422
+                    and request_parameters.task != "unknown"
+                ):
                     msg = str(error.args[0])
                     if len(error.response.text) > 0:
                         msg += f"\n{error.response.text}\n"
@@ -373,7 +395,9 @@ class InferenceClient:
         ]
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="audio-classification")
+        provider_helper = get_provider_helper(
+            self.provider, task="audio-classification"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=audio,
             parameters={"function_to_apply": function_to_apply, "top_k": top_k},
@@ -471,7 +495,9 @@ class InferenceClient:
         "hello world"
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="automatic-speech-recognition")
+        provider_helper = get_provider_helper(
+            self.provider, task="automatic-speech-recognition"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=audio,
             parameters={**(extra_body or {})},
@@ -500,7 +526,11 @@ class InferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
+        tool_choice: Optional[
+            Union[
+                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
+            ]
+        ] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
@@ -526,7 +556,11 @@ class InferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
+        tool_choice: Optional[
+            Union[
+                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
+            ]
+        ] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
@@ -552,7 +586,11 @@ class InferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
+        tool_choice: Optional[
+            Union[
+                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
+            ]
+        ] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
@@ -578,7 +616,11 @@ class InferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
+        tool_choice: Optional[
+            Union[
+                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
+            ]
+        ] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
@@ -1026,7 +1068,9 @@ class InferenceClient:
         ```
         """
         inputs: Dict[str, Any] = {"question": question, "image": _b64_encode(image)}
-        provider_helper = get_provider_helper(self.provider, task="document-question-answering")
+        provider_helper = get_provider_helper(
+            self.provider, task="document-question-answering"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=inputs,
             parameters={
@@ -1211,7 +1255,9 @@ class InferenceClient:
         [ImageClassificationOutputElement(label='Blenheim spaniel', score=0.9779096841812134), ...]
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="image-classification")
+        provider_helper = get_provider_helper(
+            self.provider, task="image-classification"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={"function_to_apply": function_to_apply, "top_k": top_k},
@@ -1272,7 +1318,9 @@ class InferenceClient:
         [ImageSegmentationOutputElement(score=0.989008, label='LABEL_184', mask=<PIL.PngImagePlugin.PngImageFile image mode=L size=400x300 at 0x7FDD2B129CC0>), ...]
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="audio-classification")
+        provider_helper = get_provider_helper(
+            self.provider, task="audio-classification"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={
@@ -1366,7 +1414,9 @@ class InferenceClient:
         response = self._inner_post(request_parameters)
         return _bytes_to_image(response)
 
-    def image_to_text(self, image: ContentT, *, model: Optional[str] = None) -> ImageToTextOutput:
+    def image_to_text(
+        self, image: ContentT, *, model: Optional[str] = None
+    ) -> ImageToTextOutput:
         """
         Takes an input image and return text.
 
@@ -1412,7 +1462,11 @@ class InferenceClient:
         return output[0] if isinstance(output, list) else output
 
     def object_detection(
-        self, image: ContentT, *, model: Optional[str] = None, threshold: Optional[float] = None
+        self,
+        image: ContentT,
+        *,
+        model: Optional[str] = None,
+        threshold: Optional[float] = None,
     ) -> List[ObjectDetectionOutputElement]:
         """
         Perform object detection on the given image using the specified model.
@@ -1704,10 +1758,17 @@ class InferenceClient:
         TableQuestionAnsweringOutputElement(answer='36542', coordinates=[[0, 1]], cells=['36542'], aggregator='AVERAGE')
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="table-question-answering")
+        provider_helper = get_provider_helper(
+            self.provider, task="table-question-answering"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=None,
-            parameters={"model": model, "padding": padding, "sequential": sequential, "truncation": truncation},
+            parameters={
+                "model": model,
+                "padding": padding,
+                "sequential": sequential,
+                "truncation": truncation,
+            },
             extra_payload={"query": query, "table": table},
             headers=self.headers,
             model=model or self.model,
@@ -1716,7 +1777,9 @@ class InferenceClient:
         response = self._inner_post(request_parameters)
         return TableQuestionAnsweringOutputElement.parse_obj_as_instance(response)
 
-    def tabular_classification(self, table: Dict[str, Any], *, model: Optional[str] = None) -> List[str]:
+    def tabular_classification(
+        self, table: Dict[str, Any], *, model: Optional[str] = None
+    ) -> List[str]:
         """
         Classifying a target category (a group) based on a set of attributes.
 
@@ -1758,7 +1821,9 @@ class InferenceClient:
         ["5", "5", "5"]
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="tabular-classification")
+        provider_helper = get_provider_helper(
+            self.provider, task="tabular-classification"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=None,
             extra_payload={"table": table},
@@ -1770,7 +1835,9 @@ class InferenceClient:
         response = self._inner_post(request_parameters)
         return _bytes_to_list(response)
 
-    def tabular_regression(self, table: Dict[str, Any], *, model: Optional[str] = None) -> List[float]:
+    def tabular_regression(
+        self, table: Dict[str, Any], *, model: Optional[str] = None
+    ) -> List[float]:
         """
         Predicting a numerical target value given a set of attributes/features in a table.
 
@@ -2053,7 +2120,9 @@ class InferenceClient:
         truncate: Optional[int] = None,
         typical_p: Optional[float] = None,
         watermark: Optional[bool] = None,
-    ) -> Union[str, TextGenerationOutput, Iterable[str], Iterable[TextGenerationStreamOutput]]:
+    ) -> Union[
+        str, TextGenerationOutput, Iterable[str], Iterable[TextGenerationStreamOutput]
+    ]:
         """
         Given a prompt, generate the following text.
 
@@ -2327,7 +2396,9 @@ class InferenceClient:
         except HTTPError as e:
             match = MODEL_KWARGS_NOT_USED_REGEX.search(str(e))
             if isinstance(e, BadRequestError) and match:
-                unused_params = [kwarg.strip("' ") for kwarg in match.group(1).split(",")]
+                unused_params = [
+                    kwarg.strip("' ") for kwarg in match.group(1).split(",")
+                ]
                 _set_unsupported_text_generation_kwargs(model, unused_params)
                 return self.text_generation(  # type: ignore
                     prompt=prompt,
@@ -2365,7 +2436,11 @@ class InferenceClient:
         if isinstance(data, list):
             data = data[0]
 
-        return TextGenerationOutput.parse_obj_as_instance(data) if details else data["generated_text"]
+        return (
+            TextGenerationOutput.parse_obj_as_instance(data)
+            if details
+            else data["generated_text"]
+        )
 
     def text_to_image(
         self,
@@ -2872,7 +2947,9 @@ class InferenceClient:
         ]
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="token-classification")
+        provider_helper = get_provider_helper(
+            self.provider, task="token-classification"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=text,
             parameters={
@@ -2953,10 +3030,14 @@ class InferenceClient:
         """
         # Throw error if only one of `src_lang` and `tgt_lang` was given
         if src_lang is not None and tgt_lang is None:
-            raise ValueError("You cannot specify `src_lang` without specifying `tgt_lang`.")
+            raise ValueError(
+                "You cannot specify `src_lang` without specifying `tgt_lang`."
+            )
 
         if src_lang is None and tgt_lang is not None:
-            raise ValueError("You cannot specify `tgt_lang` without specifying `src_lang`.")
+            raise ValueError(
+                "You cannot specify `tgt_lang` without specifying `src_lang`."
+            )
 
         provider_helper = get_provider_helper(self.provider, task="translation")
         request_parameters = provider_helper.prepare_request(
@@ -3021,7 +3102,9 @@ class InferenceClient:
         ]
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="visual-question-answering")
+        provider_helper = get_provider_helper(
+            self.provider, task="visual-question-answering"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={"top_k": top_k},
@@ -3137,7 +3220,9 @@ class InferenceClient:
         elif candidate_labels is None:
             raise ValueError("Must specify `candidate_labels`")
 
-        provider_helper = get_provider_helper(self.provider, task="zero-shot-classification")
+        provider_helper = get_provider_helper(
+            self.provider, task="zero-shot-classification"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=text,
             parameters={
@@ -3152,7 +3237,9 @@ class InferenceClient:
         response = self._inner_post(request_parameters)
         output = _bytes_to_dict(response)
         return [
-            ZeroShotClassificationOutputElement.parse_obj_as_instance({"label": label, "score": score})
+            ZeroShotClassificationOutputElement.parse_obj_as_instance(
+                {"label": label, "score": score}
+            )
             for label, score in zip(output["labels"], output["scores"])
         ]
 
@@ -3223,7 +3310,9 @@ class InferenceClient:
         if len(candidate_labels) < 2:
             raise ValueError("You must specify at least 2 classes to compare.")
 
-        provider_helper = get_provider_helper(self.provider, task="zero-shot-image-classification")
+        provider_helper = get_provider_helper(
+            self.provider, task="zero-shot-image-classification"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={
@@ -3296,7 +3385,9 @@ class InferenceClient:
         ```
         """
         if self.provider != "hf-inference":
-            raise ValueError(f"Listing deployed models is not supported on '{self.provider}'.")
+            raise ValueError(
+                f"Listing deployed models is not supported on '{self.provider}'."
+            )
 
         # Resolve which frameworks to check
         if frameworks is None:
@@ -3315,14 +3406,21 @@ class InferenceClient:
                 if framework == "sentence-transformers":
                     # Model running with the `sentence-transformers` framework can work with both tasks even if not
                     # branded as such in the API response
-                    models_by_task.setdefault("feature-extraction", []).append(model["model_id"])
-                    models_by_task.setdefault("sentence-similarity", []).append(model["model_id"])
+                    models_by_task.setdefault("feature-extraction", []).append(
+                        model["model_id"]
+                    )
+                    models_by_task.setdefault("sentence-similarity", []).append(
+                        model["model_id"]
+                    )
                 else:
-                    models_by_task.setdefault(model["task"], []).append(model["model_id"])
+                    models_by_task.setdefault(model["task"], []).append(
+                        model["model_id"]
+                    )
 
         for framework in frameworks:
             response = get_session().get(
-                f"{constants.INFERENCE_ENDPOINT}/framework/{framework}", headers=build_hf_headers(token=self.token)
+                f"{constants.INFERENCE_ENDPOINT}/framework/{framework}",
+                headers=build_hf_headers(token=self.token),
             )
             hf_raise_for_status(response)
             _unpack_response(framework, response.json())
@@ -3376,7 +3474,9 @@ class InferenceClient:
         ```
         """
         if self.provider != "hf-inference":
-            raise ValueError(f"Getting endpoint info is not supported on '{self.provider}'.")
+            raise ValueError(
+                f"Getting endpoint info is not supported on '{self.provider}'."
+            )
 
         model = model or self.model
         if model is None:
@@ -3465,13 +3565,17 @@ class InferenceClient:
         ```
         """
         if self.provider != "hf-inference":
-            raise ValueError(f"Getting model status is not supported on '{self.provider}'.")
+            raise ValueError(
+                f"Getting model status is not supported on '{self.provider}'."
+            )
 
         model = model or self.model
         if model is None:
             raise ValueError("Model id not provided.")
         if model.startswith("https://"):
-            raise NotImplementedError("Model status is only available for Inference API endpoints.")
+            raise NotImplementedError(
+                "Model status is only available for Inference API endpoints."
+            )
         url = f"{constants.INFERENCE_ENDPOINT}/status/{model}"
 
         response = get_session().get(url, headers=build_hf_headers(token=self.token))
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_common.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_common.py
@@ -49,7 +49,12 @@ from huggingface_hub.errors import (
     ValidationError,
 )
 
-from ..utils import get_session, is_aiohttp_available, is_numpy_available, is_pillow_available
+from ..utils import (
+    get_session,
+    is_aiohttp_available,
+    is_numpy_available,
+    is_pillow_available,
+)
 from ._generated.types import ChatCompletionStreamOutput, TextGenerationStreamOutput
 
 
@@ -116,7 +121,9 @@ class ModelStatus:
 def _import_aiohttp():
     # Make sure `aiohttp` is installed on the machine.
     if not is_aiohttp_available():
-        raise ImportError("Please install aiohttp to use `AsyncInferenceClient` (`pip install aiohttp`).")
+        raise ImportError(
+            "Please install aiohttp to use `AsyncInferenceClient` (`pip install aiohttp`)."
+        )
     import aiohttp
 
     return aiohttp
@@ -125,7 +132,9 @@ def _import_aiohttp():
 def _import_numpy():
     """Make sure `numpy` is installed on the machine."""
     if not is_numpy_available():
-        raise ImportError("Please install numpy to use deal with embeddings (`pip install numpy`).")
+        raise ImportError(
+            "Please install numpy to use deal with embeddings (`pip install numpy`)."
+        )
     import numpy
 
     return numpy
@@ -159,7 +168,9 @@ def _open_as_binary(
 
 
 @contextmanager  # type: ignore
-def _open_as_binary(content: Optional[ContentT]) -> Generator[Optional[BinaryT], None, None]:
+def _open_as_binary(
+    content: Optional[ContentT],
+) -> Generator[Optional[BinaryT], None, None]:
     """Open `content` as a binary file, either from a URL, a local path, or raw bytes.
 
     Do nothing if `content` is None,
@@ -171,7 +182,9 @@ def _open_as_binary(content: Optional[Co
     if isinstance(content, str):
         if content.startswith("https://") or content.startswith("http://"):
             logger.debug(f"Downloading content from {content}")
-            yield get_session().get(content).content  # TODO: retrieve as stream and pipe to post request ?
+            yield get_session().get(
+                content
+            ).content  # TODO: retrieve as stream and pipe to post request ?
             return
         content = Path(content)
         if not content.exists():
@@ -287,7 +300,9 @@ def _format_text_generation_stream_outpu
 
     # Either an error as being returned
     if json_payload.get("error") is not None:
-        raise _parse_text_generation_error(json_payload["error"], json_payload.get("error_type"))
+        raise _parse_text_generation_error(
+            json_payload["error"], json_payload.get("error_type")
+        )
 
     # Or parse token payload
     output = TextGenerationStreamOutput.parse_obj_as_instance(json_payload)
@@ -335,13 +350,17 @@ def _format_chat_completion_stream_outpu
 
     # Either an error as being returned
     if json_payload.get("error") is not None:
-        raise _parse_text_generation_error(json_payload["error"], json_payload.get("error_type"))
+        raise _parse_text_generation_error(
+            json_payload["error"], json_payload.get("error_type")
+        )
 
     # Or parse token payload
     return ChatCompletionStreamOutput.parse_obj_as_instance(json_payload)
 
 
-async def _async_yield_from(client: "ClientSession", response: "ClientResponse") -> AsyncIterable[bytes]:
+async def _async_yield_from(
+    client: "ClientSession", response: "ClientResponse"
+) -> AsyncIterable[bytes]:
     async for byte_payload in response.content:
         yield byte_payload.strip()
     await client.close()
@@ -368,7 +387,9 @@ async def _async_yield_from(client: "Cli
 _UNSUPPORTED_TEXT_GENERATION_KWARGS: Dict[Optional[str], List[str]] = {}
 
 
-def _set_unsupported_text_generation_kwargs(model: Optional[str], unsupported_kwargs: List[str]) -> None:
+def _set_unsupported_text_generation_kwargs(
+    model: Optional[str], unsupported_kwargs: List[str]
+) -> None:
     _UNSUPPORTED_TEXT_GENERATION_KWARGS.setdefault(model, []).extend(unsupported_kwargs)
 
 
@@ -395,7 +416,10 @@ def raise_text_generation_error(http_err
 
     try:
         # Hacky way to retrieve payload in case of aiohttp error
-        payload = getattr(http_error, "response_error_payload", None) or http_error.response.json()
+        payload = (
+            getattr(http_error, "response_error_payload", None)
+            or http_error.response.json()
+        )
         error = payload.get("error")
         error_type = payload.get("error_type")
     except Exception:  # no payload
@@ -410,7 +434,9 @@ def raise_text_generation_error(http_err
     raise http_error
 
 
-def _parse_text_generation_error(error: Optional[str], error_type: Optional[str]) -> TextGenerationError:
+def _parse_text_generation_error(
+    error: Optional[str], error_type: Optional[str]
+) -> TextGenerationError:
     if error_type == "generation":
         return GenerationError(error)  # type: ignore
     if error_type == "incomplete_generation":
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/_async_client.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/_async_client.py
@@ -23,7 +23,18 @@ import base64
 import logging
 import re
 import warnings
-from typing import TYPE_CHECKING, Any, AsyncIterable, Dict, List, Literal, Optional, Set, Union, overload
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    AsyncIterable,
+    Dict,
+    List,
+    Literal,
+    Optional,
+    Set,
+    Union,
+    overload,
+)
 
 from huggingface_hub import constants
 from huggingface_hub.errors import InferenceTimeoutError
@@ -85,7 +96,11 @@ from huggingface_hub.inference._generate
     ZeroShotClassificationOutputElement,
     ZeroShotImageClassificationOutputElement,
 )
-from huggingface_hub.inference._providers import PROVIDER_T, HFInferenceTask, get_provider_helper
+from huggingface_hub.inference._providers import (
+    PROVIDER_T,
+    HFInferenceTask,
+    get_provider_helper,
+)
 from huggingface_hub.utils import build_hf_headers, get_session, hf_raise_for_status
 from huggingface_hub.utils._deprecation import _deprecate_arguments, _deprecate_method
 
@@ -100,7 +115,9 @@ if TYPE_CHECKING:
 logger = logging.getLogger(__name__)
 
 
-MODEL_KWARGS_NOT_USED_REGEX = re.compile(r"The following `model_kwargs` are not used by the model: \[(.*?)\]")
+MODEL_KWARGS_NOT_USED_REGEX = re.compile(
+    r"The following `model_kwargs` are not used by the model: \[(.*?)\]"
+)
 
 
 class AsyncInferenceClient:
@@ -295,7 +312,10 @@ class AsyncInferenceClient:
         aiohttp = _import_aiohttp()
 
         # TODO: this should be handled in provider helpers directly
-        if request_parameters.task in TASKS_EXPECTING_IMAGES and "Accept" not in request_parameters.headers:
+        if (
+            request_parameters.task in TASKS_EXPECTING_IMAGES
+            and "Accept" not in request_parameters.headers
+        ):
             request_parameters.headers["Accept"] = "image/png"
 
         while True:
@@ -306,12 +326,17 @@ class AsyncInferenceClient:
 
                 try:
                     response = await session.post(
-                        request_parameters.url, json=request_parameters.json, data=data_as_binary, proxy=self.proxies
+                        request_parameters.url,
+                        json=request_parameters.json,
+                        data=data_as_binary,
+                        proxy=self.proxies,
                     )
                     response_error_payload = None
                     if response.status != 200:
                         try:
-                            response_error_payload = await response.json()  # get payload before connection closed
+                            response_error_payload = (
+                                await response.json()
+                            )  # get payload before connection closed
                         except Exception:
                             pass
                     response.raise_for_status()
@@ -405,7 +430,9 @@ class AsyncInferenceClient:
         ]
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="audio-classification")
+        provider_helper = get_provider_helper(
+            self.provider, task="audio-classification"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=audio,
             parameters={"function_to_apply": function_to_apply, "top_k": top_k},
@@ -505,7 +532,9 @@ class AsyncInferenceClient:
         "hello world"
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="automatic-speech-recognition")
+        provider_helper = get_provider_helper(
+            self.provider, task="automatic-speech-recognition"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=audio,
             parameters={**(extra_body or {})},
@@ -534,7 +563,11 @@ class AsyncInferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
+        tool_choice: Optional[
+            Union[
+                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
+            ]
+        ] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
@@ -560,7 +593,11 @@ class AsyncInferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
+        tool_choice: Optional[
+            Union[
+                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
+            ]
+        ] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
@@ -586,7 +623,11 @@ class AsyncInferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
+        tool_choice: Optional[
+            Union[
+                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
+            ]
+        ] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
@@ -612,7 +653,11 @@ class AsyncInferenceClient:
         stop: Optional[List[str]] = None,
         stream_options: Optional[ChatCompletionInputStreamOptions] = None,
         temperature: Optional[float] = None,
-        tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None,
+        tool_choice: Optional[
+            Union[
+                ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"
+            ]
+        ] = None,
         tool_prompt: Optional[str] = None,
         tools: Optional[List[ChatCompletionInputTool]] = None,
         top_logprobs: Optional[int] = None,
@@ -1067,7 +1112,9 @@ class AsyncInferenceClient:
         ```
         """
         inputs: Dict[str, Any] = {"question": question, "image": _b64_encode(image)}
-        provider_helper = get_provider_helper(self.provider, task="document-question-answering")
+        provider_helper = get_provider_helper(
+            self.provider, task="document-question-answering"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=inputs,
             parameters={
@@ -1255,7 +1302,9 @@ class AsyncInferenceClient:
         [ImageClassificationOutputElement(label='Blenheim spaniel', score=0.9779096841812134), ...]
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="image-classification")
+        provider_helper = get_provider_helper(
+            self.provider, task="image-classification"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={"function_to_apply": function_to_apply, "top_k": top_k},
@@ -1317,7 +1366,9 @@ class AsyncInferenceClient:
         [ImageSegmentationOutputElement(score=0.989008, label='LABEL_184', mask=<PIL.PngImagePlugin.PngImageFile image mode=L size=400x300 at 0x7FDD2B129CC0>), ...]
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="audio-classification")
+        provider_helper = get_provider_helper(
+            self.provider, task="audio-classification"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={
@@ -1412,7 +1463,9 @@ class AsyncInferenceClient:
         response = await self._inner_post(request_parameters)
         return _bytes_to_image(response)
 
-    async def image_to_text(self, image: ContentT, *, model: Optional[str] = None) -> ImageToTextOutput:
+    async def image_to_text(
+        self, image: ContentT, *, model: Optional[str] = None
+    ) -> ImageToTextOutput:
         """
         Takes an input image and return text.
 
@@ -1459,7 +1512,11 @@ class AsyncInferenceClient:
         return output[0] if isinstance(output, list) else output
 
     async def object_detection(
-        self, image: ContentT, *, model: Optional[str] = None, threshold: Optional[float] = None
+        self,
+        image: ContentT,
+        *,
+        model: Optional[str] = None,
+        threshold: Optional[float] = None,
     ) -> List[ObjectDetectionOutputElement]:
         """
         Perform object detection on the given image using the specified model.
@@ -1756,10 +1813,17 @@ class AsyncInferenceClient:
         TableQuestionAnsweringOutputElement(answer='36542', coordinates=[[0, 1]], cells=['36542'], aggregator='AVERAGE')
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="table-question-answering")
+        provider_helper = get_provider_helper(
+            self.provider, task="table-question-answering"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=None,
-            parameters={"model": model, "padding": padding, "sequential": sequential, "truncation": truncation},
+            parameters={
+                "model": model,
+                "padding": padding,
+                "sequential": sequential,
+                "truncation": truncation,
+            },
             extra_payload={"query": query, "table": table},
             headers=self.headers,
             model=model or self.model,
@@ -1768,7 +1832,9 @@ class AsyncInferenceClient:
         response = await self._inner_post(request_parameters)
         return TableQuestionAnsweringOutputElement.parse_obj_as_instance(response)
 
-    async def tabular_classification(self, table: Dict[str, Any], *, model: Optional[str] = None) -> List[str]:
+    async def tabular_classification(
+        self, table: Dict[str, Any], *, model: Optional[str] = None
+    ) -> List[str]:
         """
         Classifying a target category (a group) based on a set of attributes.
 
@@ -1811,7 +1877,9 @@ class AsyncInferenceClient:
         ["5", "5", "5"]
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="tabular-classification")
+        provider_helper = get_provider_helper(
+            self.provider, task="tabular-classification"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=None,
             extra_payload={"table": table},
@@ -1823,7 +1891,9 @@ class AsyncInferenceClient:
         response = await self._inner_post(request_parameters)
         return _bytes_to_list(response)
 
-    async def tabular_regression(self, table: Dict[str, Any], *, model: Optional[str] = None) -> List[float]:
+    async def tabular_regression(
+        self, table: Dict[str, Any], *, model: Optional[str] = None
+    ) -> List[float]:
         """
         Predicting a numerical target value given a set of attributes/features in a table.
 
@@ -2108,7 +2178,12 @@ class AsyncInferenceClient:
         truncate: Optional[int] = None,
         typical_p: Optional[float] = None,
         watermark: Optional[bool] = None,
-    ) -> Union[str, TextGenerationOutput, AsyncIterable[str], AsyncIterable[TextGenerationStreamOutput]]:
+    ) -> Union[
+        str,
+        TextGenerationOutput,
+        AsyncIterable[str],
+        AsyncIterable[TextGenerationStreamOutput],
+    ]:
         """
         Given a prompt, generate the following text.
 
@@ -2381,9 +2456,13 @@ class AsyncInferenceClient:
         try:
             bytes_output = await self._inner_post(request_parameters, stream=stream)
         except _import_aiohttp().ClientResponseError as e:
-            match = MODEL_KWARGS_NOT_USED_REGEX.search(e.response_error_payload["error"])
+            match = MODEL_KWARGS_NOT_USED_REGEX.search(
+                e.response_error_payload["error"]
+            )
             if e.status == 400 and match:
-                unused_params = [kwarg.strip("' ") for kwarg in match.group(1).split(",")]
+                unused_params = [
+                    kwarg.strip("' ") for kwarg in match.group(1).split(",")
+                ]
                 _set_unsupported_text_generation_kwargs(model, unused_params)
                 return await self.text_generation(  # type: ignore
                     prompt=prompt,
@@ -2421,7 +2500,11 @@ class AsyncInferenceClient:
         if isinstance(data, list):
             data = data[0]
 
-        return TextGenerationOutput.parse_obj_as_instance(data) if details else data["generated_text"]
+        return (
+            TextGenerationOutput.parse_obj_as_instance(data)
+            if details
+            else data["generated_text"]
+        )
 
     async def text_to_image(
         self,
@@ -2931,7 +3014,9 @@ class AsyncInferenceClient:
         ]
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="token-classification")
+        provider_helper = get_provider_helper(
+            self.provider, task="token-classification"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=text,
             parameters={
@@ -3013,10 +3098,14 @@ class AsyncInferenceClient:
         """
         # Throw error if only one of `src_lang` and `tgt_lang` was given
         if src_lang is not None and tgt_lang is None:
-            raise ValueError("You cannot specify `src_lang` without specifying `tgt_lang`.")
+            raise ValueError(
+                "You cannot specify `src_lang` without specifying `tgt_lang`."
+            )
 
         if src_lang is None and tgt_lang is not None:
-            raise ValueError("You cannot specify `tgt_lang` without specifying `src_lang`.")
+            raise ValueError(
+                "You cannot specify `tgt_lang` without specifying `src_lang`."
+            )
 
         provider_helper = get_provider_helper(self.provider, task="translation")
         request_parameters = provider_helper.prepare_request(
@@ -3082,7 +3171,9 @@ class AsyncInferenceClient:
         ]
         ```
         """
-        provider_helper = get_provider_helper(self.provider, task="visual-question-answering")
+        provider_helper = get_provider_helper(
+            self.provider, task="visual-question-answering"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={"top_k": top_k},
@@ -3200,7 +3291,9 @@ class AsyncInferenceClient:
         elif candidate_labels is None:
             raise ValueError("Must specify `candidate_labels`")
 
-        provider_helper = get_provider_helper(self.provider, task="zero-shot-classification")
+        provider_helper = get_provider_helper(
+            self.provider, task="zero-shot-classification"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=text,
             parameters={
@@ -3215,7 +3308,9 @@ class AsyncInferenceClient:
         response = await self._inner_post(request_parameters)
         output = _bytes_to_dict(response)
         return [
-            ZeroShotClassificationOutputElement.parse_obj_as_instance({"label": label, "score": score})
+            ZeroShotClassificationOutputElement.parse_obj_as_instance(
+                {"label": label, "score": score}
+            )
             for label, score in zip(output["labels"], output["scores"])
         ]
 
@@ -3287,7 +3382,9 @@ class AsyncInferenceClient:
         if len(candidate_labels) < 2:
             raise ValueError("You must specify at least 2 classes to compare.")
 
-        provider_helper = get_provider_helper(self.provider, task="zero-shot-image-classification")
+        provider_helper = get_provider_helper(
+            self.provider, task="zero-shot-image-classification"
+        )
         request_parameters = provider_helper.prepare_request(
             inputs=image,
             parameters={
@@ -3361,7 +3458,9 @@ class AsyncInferenceClient:
         ```
         """
         if self.provider != "hf-inference":
-            raise ValueError(f"Listing deployed models is not supported on '{self.provider}'.")
+            raise ValueError(
+                f"Listing deployed models is not supported on '{self.provider}'."
+            )
 
         # Resolve which frameworks to check
         if frameworks is None:
@@ -3380,14 +3479,21 @@ class AsyncInferenceClient:
                 if framework == "sentence-transformers":
                     # Model running with the `sentence-transformers` framework can work with both tasks even if not
                     # branded as such in the API response
-                    models_by_task.setdefault("feature-extraction", []).append(model["model_id"])
-                    models_by_task.setdefault("sentence-similarity", []).append(model["model_id"])
+                    models_by_task.setdefault("feature-extraction", []).append(
+                        model["model_id"]
+                    )
+                    models_by_task.setdefault("sentence-similarity", []).append(
+                        model["model_id"]
+                    )
                 else:
-                    models_by_task.setdefault(model["task"], []).append(model["model_id"])
+                    models_by_task.setdefault(model["task"], []).append(
+                        model["model_id"]
+                    )
 
         for framework in frameworks:
             response = get_session().get(
-                f"{constants.INFERENCE_ENDPOINT}/framework/{framework}", headers=build_hf_headers(token=self.token)
+                f"{constants.INFERENCE_ENDPOINT}/framework/{framework}",
+                headers=build_hf_headers(token=self.token),
             )
             hf_raise_for_status(response)
             _unpack_response(framework, response.json())
@@ -3483,7 +3589,9 @@ class AsyncInferenceClient:
         ```
         """
         if self.provider != "hf-inference":
-            raise ValueError(f"Getting endpoint info is not supported on '{self.provider}'.")
+            raise ValueError(
+                f"Getting endpoint info is not supported on '{self.provider}'."
+            )
 
         model = model or self.model
         if model is None:
@@ -3493,7 +3601,9 @@ class AsyncInferenceClient:
         else:
             url = f"{constants.INFERENCE_ENDPOINT}/models/{model}/info"
 
-        async with self._get_client_session(headers=build_hf_headers(token=self.token)) as client:
+        async with self._get_client_session(
+            headers=build_hf_headers(token=self.token)
+        ) as client:
             response = await client.get(url, proxy=self.proxies)
             response.raise_for_status()
             return await response.json()
@@ -3533,7 +3643,9 @@ class AsyncInferenceClient:
             )
         url = model.rstrip("/") + "/health"
 
-        async with self._get_client_session(headers=build_hf_headers(token=self.token)) as client:
+        async with self._get_client_session(
+            headers=build_hf_headers(token=self.token)
+        ) as client:
             response = await client.get(url, proxy=self.proxies)
             return response.status == 200
 
@@ -3576,16 +3688,22 @@ class AsyncInferenceClient:
         ```
         """
         if self.provider != "hf-inference":
-            raise ValueError(f"Getting model status is not supported on '{self.provider}'.")
+            raise ValueError(
+                f"Getting model status is not supported on '{self.provider}'."
+            )
 
         model = model or self.model
         if model is None:
             raise ValueError("Model id not provided.")
         if model.startswith("https://"):
-            raise NotImplementedError("Model status is only available for Inference API endpoints.")
+            raise NotImplementedError(
+                "Model status is only available for Inference API endpoints."
+            )
         url = f"{constants.INFERENCE_ENDPOINT}/status/{model}"
 
-        async with self._get_client_session(headers=build_hf_headers(token=self.token)) as client:
+        async with self._get_client_session(
+            headers=build_hf_headers(token=self.token)
+        ) as client:
             response = await client.get(url, proxy=self.proxies)
             response.raise_for_status()
             response_data = await response.json()
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/types/__init__.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/types/__init__.py
@@ -59,7 +59,10 @@ from .document_question_answering import
     DocumentQuestionAnsweringOutputElement,
     DocumentQuestionAnsweringParameters,
 )
-from .feature_extraction import FeatureExtractionInput, FeatureExtractionInputTruncationDirection
+from .feature_extraction import (
+    FeatureExtractionInput,
+    FeatureExtractionInputTruncationDirection,
+)
 from .fill_mask import FillMaskInput, FillMaskOutputElement, FillMaskParameters
 from .image_classification import (
     ImageClassificationInput,
@@ -73,7 +76,12 @@ from .image_segmentation import (
     ImageSegmentationParameters,
     ImageSegmentationSubtask,
 )
-from .image_to_image import ImageToImageInput, ImageToImageOutput, ImageToImageParameters, ImageToImageTargetSize
+from .image_to_image import (
+    ImageToImageInput,
+    ImageToImageOutput,
+    ImageToImageParameters,
+    ImageToImageTargetSize,
+)
 from .image_to_text import (
     ImageToTextEarlyStoppingEnum,
     ImageToTextGenerationParameters,
@@ -156,7 +164,12 @@ from .token_classification import (
     TokenClassificationOutputElement,
     TokenClassificationParameters,
 )
-from .translation import TranslationInput, TranslationOutput, TranslationParameters, TranslationTruncationStrategy
+from .translation import (
+    TranslationInput,
+    TranslationOutput,
+    TranslationParameters,
+    TranslationTruncationStrategy,
+)
 from .video_classification import (
     VideoClassificationInput,
     VideoClassificationOutputElement,
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/types/automatic_speech_recognition.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/types/automatic_speech_recognition.py
@@ -17,7 +17,9 @@ class AutomaticSpeechRecognitionGenerati
 
     do_sample: Optional[bool] = None
     """Whether to use sampling instead of greedy decoding when generating new tokens."""
-    early_stopping: Optional[Union[bool, "AutomaticSpeechRecognitionEarlyStoppingEnum"]] = None
+    early_stopping: Optional[
+        Union[bool, "AutomaticSpeechRecognitionEarlyStoppingEnum"]
+    ] = None
     """Controls the stopping condition for beam-based methods."""
     epsilon_cutoff: Optional[float] = None
     """If set to float strictly between 0 and 1, only tokens with a conditional probability
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/types/base.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/types/base.py
@@ -56,7 +56,9 @@ class BaseInferenceType(dict):
         """
         output = cls.parse_obj(data)
         if not isinstance(output, list):
-            raise ValueError(f"Invalid input data for {cls}. Expected a list, but got {type(output)}.")
+            raise ValueError(
+                f"Invalid input data for {cls}. Expected a list, but got {type(output)}."
+            )
         return output
 
     @classmethod
@@ -67,11 +69,15 @@ class BaseInferenceType(dict):
         """
         output = cls.parse_obj(data)
         if isinstance(output, list):
-            raise ValueError(f"Invalid input data for {cls}. Expected a single instance, but got a list.")
+            raise ValueError(
+                f"Invalid input data for {cls}. Expected a single instance, but got a list."
+            )
         return output
 
     @classmethod
-    def parse_obj(cls: Type[T], data: Union[bytes, str, List, Dict]) -> Union[List[T], T]:
+    def parse_obj(
+        cls: Type[T], data: Union[bytes, str, List, Dict]
+    ) -> Union[List[T], T]:
         """Parse server response as a dataclass or list of dataclasses.
 
         To enable future-compatibility, we want to handle cases where the server return more fields than expected.
@@ -101,7 +107,9 @@ class BaseInferenceType(dict):
                     field_type = cls.__dataclass_fields__[key].type
 
                     # if `field_type` is a `BaseInferenceType`, parse it
-                    if inspect.isclass(field_type) and issubclass(field_type, BaseInferenceType):
+                    if inspect.isclass(field_type) and issubclass(
+                        field_type, BaseInferenceType
+                    ):
                         value = field_type.parse_obj(value)
 
                     # otherwise, recursively parse nested dataclasses (if possible)
@@ -113,7 +121,9 @@ class BaseInferenceType(dict):
                                 expected_type = get_args(expected_type)[
                                     0
                                 ]  # assume same type for all items in the list
-                            if inspect.isclass(expected_type) and issubclass(expected_type, BaseInferenceType):
+                            if inspect.isclass(expected_type) and issubclass(
+                                expected_type, BaseInferenceType
+                            ):
                                 value = expected_type.parse_obj(value)
                                 break
                 init_values[key] = value
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/types/chat_completion.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/types/chat_completion.py
@@ -140,7 +140,9 @@ class ChatCompletionInput(BaseInferenceT
     lower values like 0.2 will make it more focused and deterministic.
     We generally recommend altering this or `top_p` but not both.
     """
-    tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]] = None
+    tool_choice: Optional[
+        Union[ChatCompletionInputToolChoiceClass, "ChatCompletionInputToolChoiceEnum"]
+    ] = None
     tool_prompt: Optional[str] = None
     """A prompt to be appended before the tools"""
     tools: Optional[List[ChatCompletionInputTool]] = None
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/types/summarization.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/types/summarization.py
@@ -8,7 +8,9 @@ from typing import Any, Dict, Literal, O
 from .base import BaseInferenceType, dataclass_with_extra
 
 
-SummarizationTruncationStrategy = Literal["do_not_truncate", "longest_first", "only_first", "only_second"]
+SummarizationTruncationStrategy = Literal[
+    "do_not_truncate", "longest_first", "only_first", "only_second"
+]
 
 
 @dataclass_with_extra
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/types/text2text_generation.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/types/text2text_generation.py
@@ -8,7 +8,9 @@ from typing import Any, Dict, Literal, O
 from .base import BaseInferenceType, dataclass_with_extra
 
 
-Text2TextGenerationTruncationStrategy = Literal["do_not_truncate", "longest_first", "only_first", "only_second"]
+Text2TextGenerationTruncationStrategy = Literal[
+    "do_not_truncate", "longest_first", "only_first", "only_second"
+]
 
 
 @dataclass_with_extra
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/types/token_classification.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/types/token_classification.py
@@ -8,7 +8,9 @@ from typing import List, Literal, Option
 from .base import BaseInferenceType, dataclass_with_extra
 
 
-TokenClassificationAggregationStrategy = Literal["none", "simple", "first", "average", "max"]
+TokenClassificationAggregationStrategy = Literal[
+    "none", "simple", "first", "average", "max"
+]
 
 
 @dataclass_with_extra
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_generated/types/translation.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_generated/types/translation.py
@@ -8,7 +8,9 @@ from typing import Any, Dict, Literal, O
 from .base import BaseInferenceType, dataclass_with_extra
 
 
-TranslationTruncationStrategy = Literal["do_not_truncate", "longest_first", "only_first", "only_second"]
+TranslationTruncationStrategy = Literal[
+    "do_not_truncate", "longest_first", "only_first", "only_second"
+]
 
 
 @dataclass_with_extra
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/__init__.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/__init__.py
@@ -11,13 +11,25 @@ from .fal_ai import (
     FalAITextToVideoTask,
 )
 from .fireworks_ai import FireworksAIConversationalTask
-from .hf_inference import HFInferenceBinaryInputTask, HFInferenceConversational, HFInferenceTask
+from .hf_inference import (
+    HFInferenceBinaryInputTask,
+    HFInferenceConversational,
+    HFInferenceTask,
+)
 from .hyperbolic import HyperbolicTextGenerationTask, HyperbolicTextToImageTask
-from .nebius import NebiusConversationalTask, NebiusTextGenerationTask, NebiusTextToImageTask
+from .nebius import (
+    NebiusConversationalTask,
+    NebiusTextGenerationTask,
+    NebiusTextToImageTask,
+)
 from .novita import NovitaConversationalTask, NovitaTextGenerationTask
 from .replicate import ReplicateTask, ReplicateTextToSpeechTask
 from .sambanova import SambanovaConversationalTask
-from .together import TogetherConversationalTask, TogetherTextGenerationTask, TogetherTextToImageTask
+from .together import (
+    TogetherConversationalTask,
+    TogetherTextGenerationTask,
+    TogetherTextToImageTask,
+)
 
 
 PROVIDER_T = Literal[
@@ -61,7 +73,9 @@ PROVIDERS: Dict[PROVIDER_T, Dict[str, Ta
         "text-classification": HFInferenceTask("text-classification"),
         "question-answering": HFInferenceTask("question-answering"),
         "audio-classification": HFInferenceBinaryInputTask("audio-classification"),
-        "automatic-speech-recognition": HFInferenceBinaryInputTask("automatic-speech-recognition"),
+        "automatic-speech-recognition": HFInferenceBinaryInputTask(
+            "automatic-speech-recognition"
+        ),
         "fill-mask": HFInferenceTask("fill-mask"),
         "feature-extraction": HFInferenceTask("feature-extraction"),
         "image-classification": HFInferenceBinaryInputTask("image-classification"),
@@ -70,7 +84,9 @@ PROVIDERS: Dict[PROVIDER_T, Dict[str, Ta
         "image-to-text": HFInferenceBinaryInputTask("image-to-text"),
         "object-detection": HFInferenceBinaryInputTask("object-detection"),
         "audio-to-audio": HFInferenceBinaryInputTask("audio-to-audio"),
-        "zero-shot-image-classification": HFInferenceBinaryInputTask("zero-shot-image-classification"),
+        "zero-shot-image-classification": HFInferenceBinaryInputTask(
+            "zero-shot-image-classification"
+        ),
         "zero-shot-classification": HFInferenceTask("zero-shot-classification"),
         "image-to-image": HFInferenceBinaryInputTask("image-to-image"),
         "sentence-similarity": HFInferenceTask("sentence-similarity"),
@@ -80,7 +96,9 @@ PROVIDERS: Dict[PROVIDER_T, Dict[str, Ta
         "token-classification": HFInferenceTask("token-classification"),
         "translation": HFInferenceTask("translation"),
         "summarization": HFInferenceTask("summarization"),
-        "visual-question-answering": HFInferenceBinaryInputTask("visual-question-answering"),
+        "visual-question-answering": HFInferenceBinaryInputTask(
+            "visual-question-answering"
+        ),
     },
     "hyperbolic": {
         "text-to-image": HyperbolicTextToImageTask(),
@@ -126,7 +144,9 @@ def get_provider_helper(provider: PROVID
         ValueError: If provider or task is not supported
     """
     if provider not in PROVIDERS:
-        raise ValueError(f"Provider '{provider}' not supported. Available providers: {list(PROVIDERS.keys())}")
+        raise ValueError(
+            f"Provider '{provider}' not supported. Available providers: {list(PROVIDERS.keys())}"
+        )
     if task not in PROVIDERS[provider]:
         raise ValueError(
             f"Task '{task}' not supported for provider '{provider}'. "
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/_common.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/_common.py
@@ -70,19 +70,30 @@ class TaskProviderHelper:
         url = self._prepare_url(api_key, mapped_model)
 
         # prepare payload (to customize in subclasses)
-        payload = self._prepare_payload_as_dict(inputs, parameters, mapped_model=mapped_model)
+        payload = self._prepare_payload_as_dict(
+            inputs, parameters, mapped_model=mapped_model
+        )
         if payload is not None:
             payload = recursive_merge(payload, extra_payload or {})
 
         # body data (to customize in subclasses)
-        data = self._prepare_payload_as_bytes(inputs, parameters, mapped_model, extra_payload)
+        data = self._prepare_payload_as_bytes(
+            inputs, parameters, mapped_model, extra_payload
+        )
 
         # check if both payload and data are set and return
         if payload is not None and data is not None:
             raise ValueError("Both payload and data cannot be set in the same request.")
         if payload is None and data is None:
             raise ValueError("Either payload or data must be set in the request.")
-        return RequestParameters(url=url, task=self.task, model=mapped_model, json=payload, data=data, headers=headers)
+        return RequestParameters(
+            url=url,
+            task=self.task,
+            model=mapped_model,
+            json=payload,
+            data=data,
+            headers=headers,
+        )
 
     def get_response(self, response: Union[bytes, Dict]) -> Any:
         """
@@ -108,7 +119,9 @@ class TaskProviderHelper:
 
         Usually not overwritten in subclasses."""
         if model is None:
-            raise ValueError(f"Please provide an HF model ID supported by {self.provider}.")
+            raise ValueError(
+                f"Please provide an HF model ID supported by {self.provider}."
+            )
 
         # hardcoded mapping for local testing
         if HARDCODED_MODEL_ID_MAPPING.get(self.provider, {}).get(model):
@@ -116,7 +129,9 @@ class TaskProviderHelper:
 
         provider_mapping = _fetch_inference_provider_mapping(model).get(self.provider)
         if provider_mapping is None:
-            raise ValueError(f"Model {model} is not supported by provider {self.provider}.")
+            raise ValueError(
+                f"Model {model} is not supported by provider {self.provider}."
+            )
 
         if provider_mapping.task != self.task:
             raise ValueError(
@@ -151,7 +166,9 @@ class TaskProviderHelper:
         Usually not overwritten in subclasses."""
         # Route to the proxy if the api_key is a HF TOKEN
         if api_key.startswith("hf_"):
-            logger.info(f"Calling '{self.provider}' provider through Hugging Face router.")
+            logger.info(
+                f"Calling '{self.provider}' provider through Hugging Face router."
+            )
             return constants.INFERENCE_PROXY_TEMPLATE.format(provider=self.provider)
         else:
             logger.info(f"Calling '{self.provider}' provider directly.")
@@ -164,7 +181,9 @@ class TaskProviderHelper:
         """
         return ""
 
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
         """Return the payload to use for the request, as a dict.
 
         Override this method in subclasses for customized payloads.
@@ -173,7 +192,11 @@ class TaskProviderHelper:
         return None
 
     def _prepare_payload_as_bytes(
-        self, inputs: Any, parameters: Dict, mapped_model: str, extra_payload: Optional[Dict]
+        self,
+        inputs: Any,
+        parameters: Dict,
+        mapped_model: str,
+        extra_payload: Optional[Dict],
     ) -> Optional[bytes]:
         """Return the body to use for the request, as bytes.
 
@@ -195,7 +218,9 @@ class BaseConversationalTask(TaskProvide
     def _prepare_route(self, mapped_model: str) -> str:
         return "/v1/chat/completions"
 
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
         return {"messages": inputs, **filter_none(parameters), "model": mapped_model}
 
 
@@ -211,7 +236,9 @@ class BaseTextGenerationTask(TaskProvide
     def _prepare_route(self, mapped_model: str) -> str:
         return "/v1/completions"
 
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
         return {"prompt": inputs, **filter_none(parameters), "model": mapped_model}
 
 
@@ -233,9 +260,15 @@ def recursive_merge(dict1: Dict, dict2:
     return {
         **dict1,
         **{
-            key: recursive_merge(dict1[key], value)
-            if (key in dict1 and isinstance(dict1[key], dict) and isinstance(value, dict))
-            else value
+            key: (
+                recursive_merge(dict1[key], value)
+                if (
+                    key in dict1
+                    and isinstance(dict1[key], dict)
+                    and isinstance(value, dict)
+                )
+                else value
+            )
             for key, value in dict2.items()
         },
     }
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/black_forest_labs.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/black_forest_labs.py
@@ -15,7 +15,11 @@ POLLING_INTERVAL = 1.0
 
 class BlackForestLabsTextToImageTask(TaskProviderHelper):
     def __init__(self):
-        super().__init__(provider="black-forest-labs", base_url="https://api.us1.bfl.ai/v1", task="text-to-image")
+        super().__init__(
+            provider="black-forest-labs",
+            base_url="https://api.us1.bfl.ai/v1",
+            task="text-to-image",
+        )
 
     def _prepare_headers(self, headers: Dict, api_key: str) -> Dict:
         headers = super()._prepare_headers(headers, api_key)
@@ -27,7 +31,9 @@ class BlackForestLabsTextToImageTask(Tas
     def _prepare_route(self, mapped_model: str) -> str:
         return mapped_model
 
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
         parameters = filter_none(parameters)
         if "num_inference_steps" in parameters:
             parameters["steps"] = parameters.pop("num_inference_steps")
@@ -63,4 +69,6 @@ class BlackForestLabsTextToImageTask(Tas
                 image_resp.raise_for_status()
                 return image_resp.content
 
-        raise TimeoutError(f"Failed to get the image URL after {MAX_POLLING_ATTEMPTS} attempts.")
+        raise TimeoutError(
+            f"Failed to get the image URL after {MAX_POLLING_ATTEMPTS} attempts."
+        )
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/fal_ai.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/fal_ai.py
@@ -25,7 +25,9 @@ class FalAIAutomaticSpeechRecognitionTas
     def __init__(self):
         super().__init__("automatic-speech-recognition")
 
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
         if isinstance(inputs, str) and inputs.startswith(("http://", "https://")):
             # If input is a URL, pass it directly
             audio_url = inputs
@@ -44,7 +46,9 @@ class FalAIAutomaticSpeechRecognitionTas
     def get_response(self, response: Union[bytes, Dict]) -> Any:
         text = _as_dict(response)["text"]
         if not isinstance(text, str):
-            raise ValueError(f"Unexpected output format from FalAI API. Expected string, got {type(text)}.")
+            raise ValueError(
+                f"Unexpected output format from FalAI API. Expected string, got {type(text)}."
+            )
         return text
 
 
@@ -52,7 +56,9 @@ class FalAITextToImageTask(FalAITask):
     def __init__(self):
         super().__init__("text-to-image")
 
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
         parameters = filter_none(parameters)
         if "width" in parameters and "height" in parameters:
             parameters["image_size"] = {
@@ -70,7 +76,9 @@ class FalAITextToSpeechTask(FalAITask):
     def __init__(self):
         super().__init__("text-to-speech")
 
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
         return {"lyrics": inputs, **filter_none(parameters)}
 
     def get_response(self, response: Union[bytes, Dict]) -> Any:
@@ -82,7 +90,9 @@ class FalAITextToVideoTask(FalAITask):
     def __init__(self):
         super().__init__("text-to-video")
 
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
         return {"prompt": inputs, **filter_none(parameters)}
 
     def get_response(self, response: Union[bytes, Dict]) -> Any:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/fireworks_ai.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/fireworks_ai.py
@@ -3,4 +3,6 @@ from ._common import BaseConversationalT
 
 class FireworksAIConversationalTask(BaseConversationalTask):
     def __init__(self):
-        super().__init__(provider="fireworks-ai", base_url="https://api.fireworks.ai/inference")
+        super().__init__(
+            provider="fireworks-ai", base_url="https://api.fireworks.ai/inference"
+        )
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/hf_inference.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/hf_inference.py
@@ -6,7 +6,12 @@ from typing import Any, Dict, Optional
 from huggingface_hub import constants
 from huggingface_hub.inference._common import _b64_encode, _open_as_binary
 from huggingface_hub.inference._providers._common import TaskProviderHelper, filter_none
-from huggingface_hub.utils import build_hf_headers, get_session, get_token, hf_raise_for_status
+from huggingface_hub.utils import (
+    build_hf_headers,
+    get_session,
+    get_token,
+    hf_raise_for_status,
+)
 
 
 class HFInferenceTask(TaskProviderHelper):
@@ -46,20 +51,30 @@ class HFInferenceTask(TaskProviderHelper
             else f"{self.base_url}/models/{mapped_model}"
         )
 
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
         if isinstance(inputs, bytes):
             raise ValueError(f"Unexpected binary input for task {self.task}.")
         if isinstance(inputs, Path):
-            raise ValueError(f"Unexpected path input for task {self.task} (got {inputs})")
+            raise ValueError(
+                f"Unexpected path input for task {self.task} (got {inputs})"
+            )
         return {"inputs": inputs, "parameters": filter_none(parameters)}
 
 
 class HFInferenceBinaryInputTask(HFInferenceTask):
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
         return None
 
     def _prepare_payload_as_bytes(
-        self, inputs: Any, parameters: Dict, mapped_model: str, extra_payload: Optional[Dict]
+        self,
+        inputs: Any,
+        parameters: Dict,
+        mapped_model: str,
+        extra_payload: Optional[Dict],
     ) -> Optional[bytes]:
         parameters = filter_none({k: v for k, v in parameters.items() if v is not None})
         extra_payload = extra_payload or {}
@@ -67,7 +82,9 @@ class HFInferenceBinaryInputTask(HFInfer
 
         # Raise if not a binary object or a local path or a URL.
         if not isinstance(inputs, (bytes, Path)) and not isinstance(inputs, str):
-            raise ValueError(f"Expected binary inputs or a local path or a URL. Got {inputs}")
+            raise ValueError(
+                f"Expected binary inputs or a local path or a URL. Got {inputs}"
+            )
 
         # Send inputs as raw content when no parameters are provided
         if not has_parameters:
@@ -76,14 +93,18 @@ class HFInferenceBinaryInputTask(HFInfer
                 return data_as_bytes
 
         # Otherwise encode as b64
-        return json.dumps({"inputs": _b64_encode(inputs), "parameters": parameters, **extra_payload}).encode("utf-8")
+        return json.dumps(
+            {"inputs": _b64_encode(inputs), "parameters": parameters, **extra_payload}
+        ).encode("utf-8")
 
 
 class HFInferenceConversational(HFInferenceTask):
     def __init__(self):
         super().__init__("text-generation")
 
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
         payload_model = parameters.get("model") or mapped_model
 
         if payload_model is None or payload_model.startswith(("http://", "https://")):
@@ -117,6 +138,11 @@ def _build_chat_completion_url(model_url
 
 @lru_cache(maxsize=1)
 def _fetch_recommended_models() -> Dict[str, Optional[str]]:
-    response = get_session().get(f"{constants.ENDPOINT}/api/tasks", headers=build_hf_headers())
+    response = get_session().get(
+        f"{constants.ENDPOINT}/api/tasks", headers=build_hf_headers()
+    )
     hf_raise_for_status(response)
-    return {task: next(iter(details["widgetModels"]), None) for task, details in response.json().items()}
+    return {
+        task: next(iter(details["widgetModels"]), None)
+        for task, details in response.json().items()
+    }
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/hyperbolic.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/hyperbolic.py
@@ -2,17 +2,27 @@ import base64
 from typing import Any, Dict, Optional, Union
 
 from huggingface_hub.inference._common import _as_dict
-from huggingface_hub.inference._providers._common import BaseConversationalTask, TaskProviderHelper, filter_none
+from huggingface_hub.inference._providers._common import (
+    BaseConversationalTask,
+    TaskProviderHelper,
+    filter_none,
+)
 
 
 class HyperbolicTextToImageTask(TaskProviderHelper):
     def __init__(self):
-        super().__init__(provider="hyperbolic", base_url="https://api.hyperbolic.xyz", task="text-to-image")
+        super().__init__(
+            provider="hyperbolic",
+            base_url="https://api.hyperbolic.xyz",
+            task="text-to-image",
+        )
 
     def _prepare_route(self, mapped_model: str) -> str:
         return "/v1/images/generations"
 
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
         parameters = filter_none(parameters)
         if "num_inference_steps" in parameters:
             parameters["steps"] = parameters.pop("num_inference_steps")
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/nebius.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/nebius.py
@@ -22,12 +22,18 @@ class NebiusConversationalTask(BaseConve
 
 class NebiusTextToImageTask(TaskProviderHelper):
     def __init__(self):
-        super().__init__(task="text-to-image", provider="nebius", base_url="https://api.studio.nebius.ai")
+        super().__init__(
+            task="text-to-image",
+            provider="nebius",
+            base_url="https://api.studio.nebius.ai",
+        )
 
     def _prepare_route(self, mapped_model: str) -> str:
         return "/v1/images/generations"
 
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
         parameters = filter_none(parameters)
         if "guidance_scale" in parameters:
             parameters.pop("guidance_scale")
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/replicate.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/replicate.py
@@ -23,8 +23,12 @@ class ReplicateTask(TaskProviderHelper):
             return "/v1/predictions"
         return f"/v1/models/{mapped_model}/predictions"
 
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
-        payload: Dict[str, Any] = {"input": {"prompt": inputs, **filter_none(parameters)}}
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
+        payload: Dict[str, Any] = {
+            "input": {"prompt": inputs, **filter_none(parameters)}
+        }
         if ":" in mapped_model:
             version = mapped_model.split(":", 1)[1]
             payload["version"] = version
@@ -38,7 +42,9 @@ class ReplicateTask(TaskProviderHelper):
                 "The model might be in cold state or starting up. Please try again later."
             )
         output_url = (
-            response_dict["output"] if isinstance(response_dict["output"], str) else response_dict["output"][0]
+            response_dict["output"]
+            if isinstance(response_dict["output"], str)
+            else response_dict["output"][0]
         )
         return get_session().get(output_url).content
 
@@ -47,7 +53,11 @@ class ReplicateTextToSpeechTask(Replicat
     def __init__(self):
         super().__init__("text-to-speech")
 
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
         payload: Dict = super()._prepare_payload_as_dict(inputs, parameters, mapped_model)  # type: ignore[assignment]
-        payload["input"]["text"] = payload["input"].pop("prompt")  # rename "prompt" to "text" for TTS
+        payload["input"]["text"] = payload["input"].pop(
+            "prompt"
+        )  # rename "prompt" to "text" for TTS
         return payload
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference/_providers/together.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference/_providers/together.py
@@ -45,14 +45,21 @@ class TogetherTextToImageTask(TogetherTa
     def __init__(self):
         super().__init__("text-to-image")
 
-    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:
+    def _prepare_payload_as_dict(
+        self, inputs: Any, parameters: Dict, mapped_model: str
+    ) -> Optional[Dict]:
         parameters = filter_none(parameters)
         if "num_inference_steps" in parameters:
             parameters["steps"] = parameters.pop("num_inference_steps")
         if "guidance_scale" in parameters:
             parameters["guidance"] = parameters.pop("guidance_scale")
 
-        return {"prompt": inputs, "response_format": "base64", **parameters, "model": mapped_model}
+        return {
+            "prompt": inputs,
+            "response_format": "base64",
+            **parameters,
+            "model": mapped_model,
+        }
 
     def get_response(self, response: Union[bytes, Dict]) -> Any:
         response_dict = _as_dict(response)
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/inference_api.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/inference_api.py
@@ -3,7 +3,13 @@ from typing import Any, Dict, List, Opti
 
 from . import constants
 from .hf_api import HfApi
-from .utils import build_hf_headers, get_session, is_pillow_available, logging, validate_hf_hub_args
+from .utils import (
+    build_hf_headers,
+    get_session,
+    is_pillow_available,
+    logging,
+    validate_hf_hub_args,
+)
 from .utils._deprecation import _deprecate_method
 
 
@@ -187,7 +193,9 @@ class InferenceApi:
             payload["parameters"] = params
 
         # Make API call
-        response = get_session().post(self.api_url, headers=self.headers, json=payload, data=data)
+        response = get_session().post(
+            self.api_url, headers=self.headers, json=payload, data=data
+        )
 
         # Let the user handle the response
         if raw_response:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/keras_mixin.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/keras_mixin.py
@@ -88,7 +88,9 @@ def _create_hyperparameter_table(model):
         optimizer_params = model.optimizer.get_config()
         # flatten the configuration
         optimizer_params = _flatten_dict(optimizer_params)
-        optimizer_params["training_precision"] = keras.mixed_precision.global_policy().name
+        optimizer_params["training_precision"] = (
+            keras.mixed_precision.global_policy().name
+        )
         table = "| Hyperparameters | Value |\n| :-- | :-- |\n"
         for key, value in optimizer_params.items():
             table += f"| {key} | {value} |\n"
@@ -189,7 +191,9 @@ def save_pretrained_keras(
             [`tf.keras.models.save_model()`](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model).
     """
     if keras is None:
-        raise ImportError("Called a Tensorflow-specific function but could not import it.")
+        raise ImportError(
+            "Called a Tensorflow-specific function but could not import it."
+        )
 
     if not model.built:
         raise ValueError("Model should be built before trying to save")
@@ -200,7 +204,9 @@ def save_pretrained_keras(
     # saving config
     if config:
         if not isinstance(config, dict):
-            raise RuntimeError(f"Provided config to save_pretrained_keras should be a dict. Got: '{type(config)}'")
+            raise RuntimeError(
+                f"Provided config to save_pretrained_keras should be a dict. Got: '{type(config)}'"
+            )
 
         with (save_directory / constants.CONFIG_NAME).open("w") as f:
             json.dump(config, f)
@@ -234,7 +240,9 @@ def save_pretrained_keras(
                 json.dump(model.history.history, f, indent=2, sort_keys=True)
 
     _create_model_card(model, save_directory, plot_model, metadata)
-    keras.models.save_model(model, save_directory, include_optimizer=include_optimizer, **model_save_kwargs)
+    keras.models.save_model(
+        model, save_directory, include_optimizer=include_optimizer, **model_save_kwargs
+    )
 
 
 def from_pretrained_keras(*args, **kwargs) -> "KerasModelHubMixin":
@@ -374,7 +382,9 @@ def push_to_hub_keras(
         The url of the commit of your model in the given repository.
     """
     api = HfApi(endpoint=api_endpoint)
-    repo_id = api.create_repo(repo_id=repo_id, token=token, private=private, exist_ok=True).repo_id
+    repo_id = api.create_repo(
+        repo_id=repo_id, token=token, private=private, exist_ok=True
+    ).repo_id
 
     # Push the files to the repo in a single commit
     with SoftTemporaryDirectory() as tmp:
@@ -477,7 +487,9 @@ class KerasModelHubMixin(ModelHubMixin):
                 snapshot_download instead of hf_hub_download.
         """
         if keras is None:
-            raise ImportError("Called a TensorFlow-specific function but could not import it.")
+            raise ImportError(
+                "Called a TensorFlow-specific function but could not import it."
+            )
 
         # Root is either a local filepath matching model_id or a cached snapshot
         if not os.path.isdir(model_id):
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/lfs.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/lfs.py
@@ -22,7 +22,16 @@ from dataclasses import dataclass
 from math import ceil
 from os.path import getsize
 from pathlib import Path
-from typing import TYPE_CHECKING, BinaryIO, Dict, Iterable, List, Optional, Tuple, TypedDict
+from typing import (
+    TYPE_CHECKING,
+    BinaryIO,
+    Dict,
+    Iterable,
+    List,
+    Optional,
+    Tuple,
+    TypedDict,
+)
 from urllib.parse import unquote
 
 from huggingface_hub import constants
@@ -157,7 +166,9 @@ def post_lfs_batch_info(
         "hash_algo": "sha256",
     }
     if revision is not None:
-        payload["ref"] = {"name": unquote(revision)}  # revision has been previously 'quoted'
+        payload["ref"] = {
+            "name": unquote(revision)
+        }  # revision has been previously 'quoted'
 
     headers = {
         **LFS_HEADERS,
@@ -222,7 +233,9 @@ def lfs_upload(
     actions = lfs_batch_action.get("actions")
     if actions is None:
         # The file was already uploaded
-        logger.debug(f"Content of file {operation.path_in_repo} is already present upstream - skipping upload")
+        logger.debug(
+            f"Content of file {operation.path_in_repo} is already present upstream - skipping upload"
+        )
         return
 
     # 1. Validate server response (check required keys in dict)
@@ -243,7 +256,12 @@ def lfs_upload(
             raise ValueError(
                 f"Malformed response from LFS batch endpoint: `chunk_size` should be an integer. Got '{chunk_size}'."
             )
-        _upload_multi_part(operation=operation, header=header, chunk_size=chunk_size, upload_url=upload_url)
+        _upload_multi_part(
+            operation=operation,
+            header=header,
+            chunk_size=chunk_size,
+            upload_url=upload_url,
+        )
     else:
         _upload_single_part(operation=operation, upload_url=upload_url)
 
@@ -254,7 +272,10 @@ def lfs_upload(
         verify_resp = get_session().post(
             verify_url,
             headers=build_hf_headers(token=token, headers=headers),
-            json={"oid": operation.upload_info.sha256.hex(), "size": operation.upload_info.size},
+            json={
+                "oid": operation.upload_info.sha256.hex(),
+                "size": operation.upload_info.size,
+            },
         )
         hf_raise_for_status(verify_resp)
     logger.debug(f"{operation.path_in_repo}: Upload successful")
@@ -264,7 +285,10 @@ def _validate_lfs_action(lfs_action: dic
     """validates response from the LFS batch endpoint"""
     if not (
         isinstance(lfs_action.get("href"), str)
-        and (lfs_action.get("header") is None or isinstance(lfs_action.get("header"), dict))
+        and (
+            lfs_action.get("header") is None
+            or isinstance(lfs_action.get("header"), dict)
+        )
     ):
         raise ValueError("lfs_action is improperly formatted")
     return lfs_action
@@ -272,7 +296,10 @@ def _validate_lfs_action(lfs_action: dic
 
 def _validate_batch_actions(lfs_batch_actions: dict):
     """validates response from the LFS batch endpoint"""
-    if not (isinstance(lfs_batch_actions.get("oid"), str) and isinstance(lfs_batch_actions.get("size"), int)):
+    if not (
+        isinstance(lfs_batch_actions.get("oid"), str)
+        and isinstance(lfs_batch_actions.get("size"), int)
+    ):
         raise ValueError("lfs_batch_actions is improperly formatted")
 
     upload_action = lfs_batch_actions.get("actions", {}).get("upload")
@@ -286,7 +313,10 @@ def _validate_batch_actions(lfs_batch_ac
 
 def _validate_batch_error(lfs_batch_error: dict):
     """validates response from the LFS batch endpoint"""
-    if not (isinstance(lfs_batch_error.get("oid"), str) and isinstance(lfs_batch_error.get("size"), int)):
+    if not (
+        isinstance(lfs_batch_error.get("oid"), str)
+        and isinstance(lfs_batch_error.get("size"), int)
+    ):
         raise ValueError("lfs_batch_error is improperly formatted")
     error_info = lfs_batch_error.get("error")
     if not (
@@ -316,16 +346,22 @@ def _upload_single_part(operation: "Comm
     """
     with operation.as_file(with_tqdm=True) as fileobj:
         # S3 might raise a transient 500 error -> let's retry if that happens
-        response = http_backoff("PUT", upload_url, data=fileobj, retry_on_status_codes=(500, 502, 503, 504))
+        response = http_backoff(
+            "PUT", upload_url, data=fileobj, retry_on_status_codes=(500, 502, 503, 504)
+        )
         hf_raise_for_status(response)
 
 
-def _upload_multi_part(operation: "CommitOperationAdd", header: Dict, chunk_size: int, upload_url: str) -> None:
+def _upload_multi_part(
+    operation: "CommitOperationAdd", header: Dict, chunk_size: int, upload_url: str
+) -> None:
     """
     Uploads file using HF multipart LFS transfer protocol.
     """
     # 1. Get upload URLs for each part
-    sorted_parts_urls = _get_sorted_parts_urls(header=header, upload_info=operation.upload_info, chunk_size=chunk_size)
+    sorted_parts_urls = _get_sorted_parts_urls(
+        header=header, upload_info=operation.upload_info, chunk_size=chunk_size
+    )
 
     # 2. Upload parts (either with hf_transfer or in pure Python)
     use_hf_transfer = constants.HF_HUB_ENABLE_HF_TRANSFER
@@ -341,21 +377,33 @@ def _upload_multi_part(operation: "Commi
         use_hf_transfer = False
 
     response_headers = (
-        _upload_parts_hf_transfer(operation=operation, sorted_parts_urls=sorted_parts_urls, chunk_size=chunk_size)
+        _upload_parts_hf_transfer(
+            operation=operation,
+            sorted_parts_urls=sorted_parts_urls,
+            chunk_size=chunk_size,
+        )
         if use_hf_transfer
-        else _upload_parts_iteratively(operation=operation, sorted_parts_urls=sorted_parts_urls, chunk_size=chunk_size)
+        else _upload_parts_iteratively(
+            operation=operation,
+            sorted_parts_urls=sorted_parts_urls,
+            chunk_size=chunk_size,
+        )
     )
 
     # 3. Send completion request
     completion_res = get_session().post(
         upload_url,
-        json=_get_completion_payload(response_headers, operation.upload_info.sha256.hex()),
+        json=_get_completion_payload(
+            response_headers, operation.upload_info.sha256.hex()
+        ),
         headers=LFS_HEADERS,
     )
     hf_raise_for_status(completion_res)
 
 
-def _get_sorted_parts_urls(header: Dict, upload_info: UploadInfo, chunk_size: int) -> List[str]:
+def _get_sorted_parts_urls(
+    header: Dict, upload_info: UploadInfo, chunk_size: int
+) -> List[str]:
     sorted_part_upload_urls = [
         upload_url
         for _, upload_url in sorted(
@@ -373,12 +421,16 @@ def _get_sorted_parts_urls(header: Dict,
     return sorted_part_upload_urls
 
 
-def _get_completion_payload(response_headers: List[Dict], oid: str) -> CompletionPayloadT:
+def _get_completion_payload(
+    response_headers: List[Dict], oid: str
+) -> CompletionPayloadT:
     parts: List[PayloadPartT] = []
     for part_number, header in enumerate(response_headers):
         etag = header.get("etag")
         if etag is None or etag == "":
-            raise ValueError(f"Invalid etag (`{etag}`) returned for part {part_number + 1}")
+            raise ValueError(
+                f"Invalid etag (`{etag}`) returned for part {part_number + 1}"
+            )
         parts.append(
             {
                 "partNumber": part_number + 1,
@@ -401,7 +453,10 @@ def _upload_parts_iteratively(
             ) as fileobj_slice:
                 # S3 might raise a transient 500 error -> let's retry if that happens
                 part_upload_res = http_backoff(
-                    "PUT", part_upload_url, data=fileobj_slice, retry_on_status_codes=(500, 502, 503, 504)
+                    "PUT",
+                    part_upload_url,
+                    data=fileobj_slice,
+                    retry_on_status_codes=(500, 502, 503, 504),
                 )
                 hf_raise_for_status(part_upload_res)
                 headers.append(part_upload_res.headers)
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/repocard.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/repocard.py
@@ -28,7 +28,9 @@ logger = logging.get_logger(__name__)
 
 
 TEMPLATE_MODELCARD_PATH = Path(__file__).parent / "templates" / "modelcard_template.md"
-TEMPLATE_DATASETCARD_PATH = Path(__file__).parent / "templates" / "datasetcard_template.md"
+TEMPLATE_DATASETCARD_PATH = (
+    Path(__file__).parent / "templates" / "datasetcard_template.md"
+)
 
 # exact same regex as in the Hub server. Please keep in sync.
 # See https://github.com/huggingface/moon-landing/blob/main/server/lib/ViewMarkdown.ts#L18
@@ -105,11 +107,15 @@ class RepoCard:
                 raise ValueError("repo card metadata block should be a dict")
         else:
             # Model card without metadata... create empty metadata
-            logger.warning("Repo card metadata block was not found. Setting CardData to empty.")
+            logger.warning(
+                "Repo card metadata block was not found. Setting CardData to empty."
+            )
             data_dict = {}
             self.text = content
 
-        self.data = self.card_data_class(**data_dict, ignore_metadata_errors=self.ignore_metadata_errors)
+        self.data = self.card_data_class(
+            **data_dict, ignore_metadata_errors=self.ignore_metadata_errors
+        )
         self._original_order = list(data_dict.keys())
 
     def __str__(self):
@@ -183,7 +189,9 @@ class RepoCard:
                 )
             )
         else:
-            raise ValueError(f"Cannot load RepoCard: path not found on disk ({repo_id_or_path}).")
+            raise ValueError(
+                f"Cannot load RepoCard: path not found on disk ({repo_id_or_path})."
+            )
 
         # Preserve newlines in the existing file.
         with card_path.open(mode="r", newline="", encoding="utf-8") as f:
@@ -220,7 +228,9 @@ class RepoCard:
         headers = {"Accept": "text/plain"}
 
         try:
-            r = get_session().post("https://huggingface.co/api/validate-yaml", body, headers=headers)
+            r = get_session().post(
+                "https://huggingface.co/api/validate-yaml", body, headers=headers
+            )
             r.raise_for_status()
         except requests.exceptions.HTTPError as exc:
             if r.status_code == 400:
@@ -413,7 +423,9 @@ class ModelCard(RepoCard):
 
             ```
         """
-        return super().from_template(card_data, template_path, template_str, **template_kwargs)
+        return super().from_template(
+            card_data, template_path, template_str, **template_kwargs
+        )
 
 
 class DatasetCard(RepoCard):
@@ -478,7 +490,9 @@ class DatasetCard(RepoCard):
 
             ```
         """
-        return super().from_template(card_data, template_path, template_str, **template_kwargs)
+        return super().from_template(
+            card_data, template_path, template_str, **template_kwargs
+        )
 
 
 class SpaceCard(RepoCard):
@@ -487,7 +501,9 @@ class SpaceCard(RepoCard):
     repo_type = "space"
 
 
-def _detect_line_ending(content: str) -> Literal["\r", "\n", "\r\n", None]:  # noqa: F722
+def _detect_line_ending(
+    content: str,
+) -> Literal["\r", "\n", "\r\n", None]:  # noqa: F722
     """Detect the line ending of a string. Used by RepoCard to avoid making huge diff on newlines.
 
     Uses same implementation as in Hub server, keep it in sync.
@@ -545,7 +561,11 @@ def metadata_save(local_path: Union[str,
         # sort_keys: keep dict order
         match = REGEX_YAML_BLOCK.search(content)
         if match:
-            output = content[: match.start()] + f"---{line_break}{data_yaml}---{line_break}" + content[match.end() :]
+            output = (
+                content[: match.start()]
+                + f"---{line_break}{data_yaml}---{line_break}"
+                + content[match.end() :]
+            )
         else:
             output = f"---{line_break}{data_yaml}---{line_break}{content}"
 
@@ -749,7 +769,11 @@ def metadata_update(
 
         ```
     """
-    commit_message = commit_message if commit_message is not None else "Update metadata with huggingface_hub"
+    commit_message = (
+        commit_message
+        if commit_message is not None
+        else "Update metadata with huggingface_hub"
+    )
 
     # Card class given repo_type
     card_class: Type[RepoCard]
@@ -768,7 +792,9 @@ def metadata_update(
         card = card_class.load(repo_id, token=token, repo_type=repo_type)
     except EntryNotFoundError:
         if repo_type == "space":
-            raise ValueError("Cannot update metadata on a Space that doesn't contain a `README.md` file.")
+            raise ValueError(
+                "Cannot update metadata on a Space that doesn't contain a `README.md` file."
+            )
 
         # Initialize a ModelCard or DatasetCard from default template and no data.
         card = card_class.from_template(CardData())
@@ -810,7 +836,11 @@ def metadata_update(
                         card.data.eval_results.append(new_result)
         else:
             # Any metadata that is not a result metric
-            if card.data.get(key) is not None and not overwrite and card.data.get(key) != value:
+            if (
+                card.data.get(key) is not None
+                and not overwrite
+                and card.data.get(key) != value
+            ):
                 raise ValueError(
                     f"You passed a new value for the existing meta data field '{key}'."
                     " Set `overwrite=True` to overwrite existing metadata."
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/repocard_data.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/repocard_data.py
@@ -158,7 +158,9 @@ class EvalResult:
 
     def __post_init__(self) -> None:
         if self.source_name is not None and self.source_url is None:
-            raise ValueError("If `source_name` is provided, `source_url` must also be provided.")
+            raise ValueError(
+                "If `source_name` is provided, `source_url` must also be provided."
+            )
 
 
 @dataclass
@@ -195,7 +197,9 @@ class CardData:
         """
         pass
 
-    def to_yaml(self, line_break=None, original_order: Optional[List[str]] = None) -> str:
+    def to_yaml(
+        self, line_break=None, original_order: Optional[List[str]] = None
+    ) -> str:
         """Dumps CardData to a YAML block for inclusion in a README.md file.
 
         Args:
@@ -208,7 +212,8 @@ class CardData:
         if original_order:
             self.__dict__ = {
                 k: self.__dict__[k]
-                for k in original_order + list(set(self.__dict__.keys()) - set(original_order))
+                for k in original_order
+                + list(set(self.__dict__.keys()) - set(original_order))
                 if k in self.__dict__
             }
         return yaml_dump(self.to_dict(), sort_keys=False, line_break=line_break).strip()
@@ -348,7 +353,9 @@ class ModelCardData(CardData):
                 self.eval_results = eval_results
             except (KeyError, TypeError) as error:
                 if ignore_metadata_errors:
-                    logger.warning("Invalid model-index. Not loading eval results into CardData.")
+                    logger.warning(
+                        "Invalid model-index. Not loading eval results into CardData."
+                    )
                 else:
                     raise ValueError(
                         f"Invalid `model_index` in metadata cannot be parsed: {error.__class__} {error}. Pass"
@@ -362,12 +369,16 @@ class ModelCardData(CardData):
             if isinstance(self.eval_results, EvalResult):
                 self.eval_results = [self.eval_results]
             if self.model_name is None:
-                raise ValueError("Passing `eval_results` requires `model_name` to be set.")
+                raise ValueError(
+                    "Passing `eval_results` requires `model_name` to be set."
+                )
 
     def _to_dict(self, data_dict):
         """Format the internal data dict. In this case, we convert eval results to a valid model index"""
         if self.eval_results is not None:
-            data_dict["model-index"] = eval_results_to_model_index(self.model_name, self.eval_results)
+            data_dict["model-index"] = eval_results_to_model_index(
+                self.model_name, self.eval_results
+            )
             del data_dict["eval_results"], data_dict["model_name"]
 
 
@@ -532,7 +543,9 @@ class SpaceCardData(CardData):
         super().__init__(**kwargs)
 
 
-def model_index_to_eval_results(model_index: List[Dict[str, Any]]) -> Tuple[str, List[EvalResult]]:
+def model_index_to_eval_results(
+    model_index: List[Dict[str, Any]],
+) -> Tuple[str, List[EvalResult]]:
     """Takes in a model index and returns the model name and a list of `huggingface_hub.EvalResult` objects.
 
     A detailed spec of the model index can be found here:
@@ -643,12 +656,18 @@ def _remove_none(obj):
     if isinstance(obj, (list, tuple, set)):
         return type(obj)(_remove_none(x) for x in obj if x is not None)
     elif isinstance(obj, dict):
-        return type(obj)((_remove_none(k), _remove_none(v)) for k, v in obj.items() if k is not None and v is not None)
+        return type(obj)(
+            (_remove_none(k), _remove_none(v))
+            for k, v in obj.items()
+            if k is not None and v is not None
+        )
     else:
         return obj
 
 
-def eval_results_to_model_index(model_name: str, eval_results: List[EvalResult]) -> List[Dict[str, Any]]:
+def eval_results_to_model_index(
+    model_name: str, eval_results: List[EvalResult]
+) -> List[Dict[str, Any]]:
     """Takes in given model name and list of `huggingface_hub.EvalResult` and returns a
     valid model-index that will be compatible with the format expected by the
     Hugging Face Hub.
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/repository.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/repository.py
@@ -124,7 +124,9 @@ def is_git_repo(folder: Union[str, Path]
         otherwise.
     """
     folder_exists = os.path.exists(os.path.join(folder, ".git"))
-    git_branch = subprocess.run("git branch".split(), cwd=folder, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
+    git_branch = subprocess.run(
+        "git branch".split(), cwd=folder, stdout=subprocess.PIPE, stderr=subprocess.PIPE
+    )
     return folder_exists and git_branch.returncode == 0
 
 
@@ -232,13 +234,17 @@ def is_binary_file(filename: Union[str,
 
         # Code sample taken from the following stack overflow thread
         # https://stackoverflow.com/questions/898669/how-can-i-detect-if-a-file-is-binary-non-text-in-python/7392391#7392391
-        text_chars = bytearray({7, 8, 9, 10, 12, 13, 27} | set(range(0x20, 0x100)) - {0x7F})
+        text_chars = bytearray(
+            {7, 8, 9, 10, 12, 13, 27} | set(range(0x20, 0x100)) - {0x7F}
+        )
         return bool(content.translate(None, text_chars))
     except UnicodeDecodeError:
         return True
 
 
-def files_to_be_staged(pattern: str = ".", folder: Union[str, Path, None] = None) -> List[str]:
+def files_to_be_staged(
+    pattern: str = ".", folder: Union[str, Path, None] = None
+) -> List[str]:
     """
     Returns a list of filenames that are to be staged.
 
@@ -252,7 +258,9 @@ def files_to_be_staged(pattern: str = ".
         `List[str]`: List of files that are to be staged.
     """
     try:
-        p = run_subprocess("git ls-files --exclude-standard -mo".split() + [pattern], folder)
+        p = run_subprocess(
+            "git ls-files --exclude-standard -mo".split() + [pattern], folder
+        )
         if len(p.stdout.strip()):
             files = p.stdout.strip().split("\n")
         else:
@@ -536,9 +544,13 @@ class Repository:
             if is_git_repo(self.local_dir):
                 logger.debug("[Repository] is a valid git repo")
             else:
-                raise ValueError("If not specifying `clone_from`, you need to pass Repository a valid git clone.")
+                raise ValueError(
+                    "If not specifying `clone_from`, you need to pass Repository a valid git clone."
+                )
 
-        if self.huggingface_token is not None and (git_email is None or git_user is None):
+        if self.huggingface_token is not None and (
+            git_email is None or git_user is None
+        ):
             user = self.client.whoami(self.huggingface_token)
 
             if git_email is None:
@@ -569,7 +581,9 @@ class Repository:
             `str`: Current checked out branch.
         """
         try:
-            result = run_subprocess("git rev-parse --abbrev-ref HEAD", self.local_dir).stdout.strip()
+            result = run_subprocess(
+                "git rev-parse --abbrev-ref HEAD", self.local_dir
+            ).stdout.strip()
         except subprocess.CalledProcessError as exc:
             raise EnvironmentError(exc.stderr)
 
@@ -586,10 +600,14 @@ class Repository:
         try:
             git_version = run_subprocess("git --version", self.local_dir).stdout.strip()
         except FileNotFoundError:
-            raise EnvironmentError("Looks like you do not have git installed, please install.")
+            raise EnvironmentError(
+                "Looks like you do not have git installed, please install."
+            )
 
         try:
-            lfs_version = run_subprocess("git-lfs --version", self.local_dir).stdout.strip()
+            lfs_version = run_subprocess(
+                "git-lfs --version", self.local_dir
+            ).stdout.strip()
         except FileNotFoundError:
             raise EnvironmentError(
                 "Looks like you do not have git-lfs installed, please install."
@@ -650,8 +668,12 @@ class Repository:
             )
 
         hub_url = self.client.endpoint
-        if hub_url in repo_url or ("http" not in repo_url and len(repo_url.split("/")) <= 2):
-            repo_type, namespace, repo_name = repo_type_and_id_from_hf_id(repo_url, hub_url=hub_url)
+        if hub_url in repo_url or (
+            "http" not in repo_url and len(repo_url.split("/")) <= 2
+        ):
+            repo_type, namespace, repo_name = repo_type_and_id_from_hf_id(
+                repo_url, hub_url=hub_url
+            )
             repo_id = f"{namespace}/{repo_name}" if namespace is not None else repo_name
 
             if repo_type is not None:
@@ -711,7 +733,9 @@ class Repository:
                         " `repo.git_pull()`."
                     )
                 else:
-                    output = run_subprocess("git remote get-url origin", self.local_dir, check=False)
+                    output = run_subprocess(
+                        "git remote get-url origin", self.local_dir, check=False
+                    )
 
                     error_msg = (
                         f"Tried to clone {clean_repo_url} in an unrelated git"
@@ -719,14 +743,18 @@ class Repository:
                         f" a remote with the following URL: {clean_repo_url}."
                     )
                     if output.returncode == 0:
-                        clean_local_remote_url = re.sub(r"https://.*@", "https://", output.stdout)
+                        clean_local_remote_url = re.sub(
+                            r"https://.*@", "https://", output.stdout
+                        )
                         error_msg += f"\nLocal path has its origin defined as: {clean_local_remote_url}"
                     raise EnvironmentError(error_msg)
 
         except subprocess.CalledProcessError as exc:
             raise EnvironmentError(exc.stderr)
 
-    def git_config_username_and_email(self, git_user: Optional[str] = None, git_email: Optional[str] = None):
+    def git_config_username_and_email(
+        self, git_user: Optional[str] = None, git_email: Optional[str] = None
+    ):
         """
         Sets git username and email (only in the current repo).
 
@@ -738,10 +766,14 @@ class Repository:
         """
         try:
             if git_user is not None:
-                run_subprocess("git config user.name".split() + [git_user], self.local_dir)
+                run_subprocess(
+                    "git config user.name".split() + [git_user], self.local_dir
+                )
 
             if git_email is not None:
-                run_subprocess(f"git config user.email {git_email}".split(), self.local_dir)
+                run_subprocess(
+                    f"git config user.email {git_email}".split(), self.local_dir
+                )
         except subprocess.CalledProcessError as exc:
             raise EnvironmentError(exc.stderr)
 
@@ -824,10 +856,14 @@ class Repository:
         modified_files_statuses = [status.strip() for status in git_status.split("\n")]
 
         # Only keep files that are deleted using the D prefix
-        deleted_files_statuses = [status for status in modified_files_statuses if "D" in status.split()[0]]
+        deleted_files_statuses = [
+            status for status in modified_files_statuses if "D" in status.split()[0]
+        ]
 
         # Remove the D prefix and strip to keep only the relevant filename
-        deleted_files = [status.split()[-1].strip() for status in deleted_files_statuses]
+        deleted_files = [
+            status.split()[-1].strip() for status in deleted_files_statuses
+        ]
 
         return deleted_files
 
@@ -953,7 +989,11 @@ class Repository:
             path_to_file = os.path.join(os.getcwd(), self.local_dir, filename)
             size_in_mb = os.path.getsize(path_to_file) / (1024 * 1024)
 
-            if size_in_mb >= 10 and not is_tracked_with_lfs(path_to_file) and not is_git_ignored(path_to_file):
+            if (
+                size_in_mb >= 10
+                and not is_tracked_with_lfs(path_to_file)
+                and not is_git_ignored(path_to_file)
+            ):
                 self.lfs_track(filename)
                 files_to_be_tracked_with_lfs.append(filename)
 
@@ -975,7 +1015,9 @@ class Repository:
         """
         try:
             with _lfs_log_progress():
-                result = run_subprocess(f"git lfs prune {'--recent' if recent else ''}", self.local_dir)
+                result = run_subprocess(
+                    f"git lfs prune {'--recent' if recent else ''}", self.local_dir
+                )
                 logger.info(result.stdout)
         except subprocess.CalledProcessError as exc:
             raise EnvironmentError(exc.stderr)
@@ -1047,7 +1089,9 @@ class Repository:
                 The message attributed to the commit.
         """
         try:
-            result = run_subprocess("git commit -v -m".split() + [commit_message], self.local_dir)
+            result = run_subprocess(
+                "git commit -v -m".split() + [commit_message], self.local_dir
+            )
             logger.info(f"Committed:\n{result.stdout}\n")
         except subprocess.CalledProcessError as exc:
             if len(exc.stderr) > 0:
@@ -1091,7 +1135,9 @@ class Repository:
         number_of_commits = commits_to_push(self.local_dir, upstream)
 
         if number_of_commits > 1:
-            logger.warning(f"Several commits ({number_of_commits}) will be pushed upstream.")
+            logger.warning(
+                f"Several commits ({number_of_commits}) will be pushed upstream."
+            )
             if blocking:
                 logger.warning("The progress bars may be unreliable.")
 
@@ -1114,7 +1160,9 @@ class Repository:
                         logger.warning(stderr)
 
                     if return_code:
-                        raise subprocess.CalledProcessError(return_code, process.args, output=stdout, stderr=stderr)
+                        raise subprocess.CalledProcessError(
+                            return_code, process.args, output=stdout, stderr=stderr
+                        )
 
         except subprocess.CalledProcessError as exc:
             raise EnvironmentError(exc.stderr)
@@ -1169,7 +1217,9 @@ class Repository:
                 raise EnvironmentError(exc.stderr)
             else:
                 try:
-                    result = run_subprocess(f"git checkout -b {revision}", self.local_dir)
+                    result = run_subprocess(
+                        f"git checkout -b {revision}", self.local_dir
+                    )
                     logger.warning(
                         f"Revision `{revision}` does not exist. Created and checked out branch `{revision}`."
                     )
@@ -1193,7 +1243,9 @@ class Repository:
         """
         if remote:
             try:
-                result = run_subprocess(f"git ls-remote origin refs/tags/{tag_name}", self.local_dir).stdout.strip()
+                result = run_subprocess(
+                    f"git ls-remote origin refs/tags/{tag_name}", self.local_dir
+                ).stdout.strip()
             except subprocess.CalledProcessError as exc:
                 raise EnvironmentError(exc.stderr)
 
@@ -1232,19 +1284,25 @@ class Repository:
 
         if delete_locally:
             try:
-                run_subprocess(["git", "tag", "-d", tag_name], self.local_dir).stdout.strip()
+                run_subprocess(
+                    ["git", "tag", "-d", tag_name], self.local_dir
+                ).stdout.strip()
             except subprocess.CalledProcessError as exc:
                 raise EnvironmentError(exc.stderr)
 
         if remote and delete_remotely:
             try:
-                run_subprocess(f"git push {remote} --delete {tag_name}", self.local_dir).stdout.strip()
+                run_subprocess(
+                    f"git push {remote} --delete {tag_name}", self.local_dir
+                ).stdout.strip()
             except subprocess.CalledProcessError as exc:
                 raise EnvironmentError(exc.stderr)
 
         return True
 
-    def add_tag(self, tag_name: str, message: Optional[str] = None, remote: Optional[str] = None):
+    def add_tag(
+        self, tag_name: str, message: Optional[str] = None, remote: Optional[str] = None
+    ):
         """
         Add a tag at the current head and push it
 
@@ -1274,7 +1332,9 @@ class Repository:
 
         if remote:
             try:
-                run_subprocess(f"git push {remote} {tag_name}", self.local_dir).stdout.strip()
+                run_subprocess(
+                    f"git push {remote} {tag_name}", self.local_dir
+                ).stdout.strip()
             except subprocess.CalledProcessError as exc:
                 raise EnvironmentError(exc.stderr)
 
@@ -1286,7 +1346,9 @@ class Repository:
             `bool`: `True` if the git status is clean, `False` otherwise.
         """
         try:
-            git_status = run_subprocess("git status --porcelain", self.local_dir).stdout.strip()
+            git_status = run_subprocess(
+                "git status --porcelain", self.local_dir
+            ).stdout.strip()
         except subprocess.CalledProcessError as exc:
             raise EnvironmentError(exc.stderr)
 
@@ -1386,7 +1448,11 @@ class Repository:
         files_to_stage = files_to_be_staged(".", folder=self.local_dir)
 
         if len(files_to_stage):
-            files_in_msg = str(files_to_stage[:5])[:-1] + ", ...]" if len(files_to_stage) > 5 else str(files_to_stage)
+            files_in_msg = (
+                str(files_to_stage[:5])[:-1] + ", ...]"
+                if len(files_to_stage) > 5
+                else str(files_to_stage)
+            )
             logger.error(
                 "There exists some updated files in the local repository that are not"
                 f" committed: {files_in_msg}. This may lead to errors if checking out"
@@ -1401,7 +1467,9 @@ class Repository:
             logger.warning("Pulling changes ...")
             self.git_pull(rebase=True)
         else:
-            logger.warning(f"The current branch has no upstream branch. Will push to 'origin {self.current_branch}'")
+            logger.warning(
+                f"The current branch has no upstream branch. Will push to 'origin {self.current_branch}'"
+            )
 
         current_working_directory = os.getcwd()
         os.chdir(os.path.join(current_working_directory, self.local_dir))
@@ -1427,7 +1495,9 @@ class Repository:
             except OSError as e:
                 # If no changes are detected, there is nothing to commit.
                 if "could not read Username" in str(e):
-                    raise OSError("Couldn't authenticate user for push. Did you set `token` to `True`?") from e
+                    raise OSError(
+                        "Couldn't authenticate user for push. Did you set `token` to `True`?"
+                    ) from e
                 else:
                     raise e
 
@@ -1440,7 +1510,9 @@ class Repository:
         return None
 
     def repocard_metadata_save(self, data: Dict) -> None:
-        return metadata_save(os.path.join(self.local_dir, constants.REPOCARD_NAME), data)
+        return metadata_save(
+            os.path.join(self.local_dir, constants.REPOCARD_NAME), data
+        )
 
     @property
     def commands_failed(self):
@@ -1463,7 +1535,9 @@ class Repository:
         """
         index = 0
         for command_failed in self.commands_failed:
-            logger.error(f"The {command_failed.title} command with PID {command_failed._process.pid} failed.")
+            logger.error(
+                f"The {command_failed.title} command with PID {command_failed._process.pid} failed."
+            )
             logger.error(command_failed.stderr)
 
         while self.commands_in_progress:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/serialization/_base.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/serialization/_base.py
@@ -198,13 +198,17 @@ def parse_size_to_int(size_as_str: str)
     # Parse unit
     unit = size_as_str[-2:].upper()
     if unit not in SIZE_UNITS:
-        raise ValueError(f"Unit '{unit}' not supported. Supported units are TB, GB, MB, KB. Got '{size_as_str}'.")
+        raise ValueError(
+            f"Unit '{unit}' not supported. Supported units are TB, GB, MB, KB. Got '{size_as_str}'."
+        )
     multiplier = SIZE_UNITS[unit]
 
     # Parse value
     try:
         value = float(size_as_str[:-2].strip())
     except ValueError as e:
-        raise ValueError(f"Could not parse the size value from '{size_as_str}': {e}") from e
+        raise ValueError(
+            f"Could not parse the size value from '{size_as_str}': {e}"
+        ) from e
 
     return int(value * multiplier)
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/serialization/_dduf.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/serialization/_dduf.py
@@ -133,22 +133,31 @@ def read_dduf_file(dduf_path: Union[os.P
         for info in zf.infolist():
             logger.debug(f"Reading entry {info.filename}")
             if info.compress_type != zipfile.ZIP_STORED:
-                raise DDUFCorruptedFileError("Data must not be compressed in DDUF file.")
+                raise DDUFCorruptedFileError(
+                    "Data must not be compressed in DDUF file."
+                )
 
             try:
                 _validate_dduf_entry_name(info.filename)
             except DDUFInvalidEntryNameError as e:
-                raise DDUFCorruptedFileError(f"Invalid entry name in DDUF file: {info.filename}") from e
+                raise DDUFCorruptedFileError(
+                    f"Invalid entry name in DDUF file: {info.filename}"
+                ) from e
 
             offset = _get_data_offset(zf, info)
 
             entries[info.filename] = DDUFEntry(
-                filename=info.filename, offset=offset, length=info.file_size, dduf_path=dduf_path
+                filename=info.filename,
+                offset=offset,
+                length=info.file_size,
+                dduf_path=dduf_path,
             )
 
     # Consistency checks on the DDUF file
     if "model_index.json" not in entries:
-        raise DDUFCorruptedFileError("Missing required 'model_index.json' entry in DDUF file.")
+        raise DDUFCorruptedFileError(
+            "Missing required 'model_index.json' entry in DDUF file."
+        )
     index = json.loads(entries["model_index.json"].read_text())
     _validate_dduf_structure(index, entries.keys())
 
@@ -157,7 +166,8 @@ def read_dduf_file(dduf_path: Union[os.P
 
 
 def export_entries_as_dduf(
-    dduf_path: Union[str, os.PathLike], entries: Iterable[Tuple[str, Union[str, Path, bytes]]]
+    dduf_path: Union[str, os.PathLike],
+    entries: Iterable[Tuple[str, Union[str, Path, bytes]]],
 ) -> None:
     """Write a DDUF file from an iterable of entries.
 
@@ -247,7 +257,9 @@ def export_entries_as_dduf(
     logger.info(f"Done writing DDUF file {dduf_path}")
 
 
-def export_folder_as_dduf(dduf_path: Union[str, os.PathLike], folder_path: Union[str, os.PathLike]) -> None:
+def export_folder_as_dduf(
+    dduf_path: Union[str, os.PathLike], folder_path: Union[str, os.PathLike]
+) -> None:
     """
     Export a folder as a DDUF file.
 
@@ -283,7 +295,9 @@ def export_folder_as_dduf(dduf_path: Uni
     export_entries_as_dduf(dduf_path, _iterate_over_folder())
 
 
-def _dump_content_in_archive(archive: zipfile.ZipFile, filename: str, content: Union[str, os.PathLike, bytes]) -> None:
+def _dump_content_in_archive(
+    archive: zipfile.ZipFile, filename: str, content: Union[str, os.PathLike, bytes]
+) -> None:
     with archive.open(filename, "w", force_zip64=True) as archive_fh:
         if isinstance(content, (str, Path)):
             content_path = Path(content)
@@ -292,7 +306,9 @@ def _dump_content_in_archive(archive: zi
         elif isinstance(content, bytes):
             archive_fh.write(content)
         else:
-            raise DDUFExportError(f"Invalid content type for {filename}. Must be str, Path or bytes.")
+            raise DDUFExportError(
+                f"Invalid content type for {filename}. Must be str, Path or bytes."
+            )
 
 
 def _load_content(content: Union[str, Path, bytes]) -> bytes:
@@ -305,17 +321,23 @@ def _load_content(content: Union[str, Pa
     elif isinstance(content, bytes):
         return content
     else:
-        raise DDUFExportError(f"Invalid content type. Must be str, Path or bytes. Got {type(content)}.")
+        raise DDUFExportError(
+            f"Invalid content type. Must be str, Path or bytes. Got {type(content)}."
+        )
 
 
 def _validate_dduf_entry_name(entry_name: str) -> str:
     if "." + entry_name.split(".")[-1] not in DDUF_ALLOWED_ENTRIES:
         raise DDUFInvalidEntryNameError(f"File type not allowed: {entry_name}")
     if "\\" in entry_name:
-        raise DDUFInvalidEntryNameError(f"Entry names must use UNIX separators ('/'). Got {entry_name}.")
+        raise DDUFInvalidEntryNameError(
+            f"Entry names must use UNIX separators ('/'). Got {entry_name}."
+        )
     entry_name = entry_name.strip("/")
     if entry_name.count("/") > 1:
-        raise DDUFInvalidEntryNameError(f"DDUF only supports 1 level of directory. Got {entry_name}.")
+        raise DDUFInvalidEntryNameError(
+            f"DDUF only supports 1 level of directory. Got {entry_name}."
+        )
     return entry_name
 
 
@@ -338,13 +360,20 @@ def _validate_dduf_structure(index: Any,
         - [`DDUFCorruptedFileError`]: If the DDUF file is corrupted (i.e. doesn't follow the DDUF format).
     """
     if not isinstance(index, dict):
-        raise DDUFCorruptedFileError(f"Invalid 'model_index.json' content. Must be a dictionary. Got {type(index)}.")
+        raise DDUFCorruptedFileError(
+            f"Invalid 'model_index.json' content. Must be a dictionary. Got {type(index)}."
+        )
 
     dduf_folders = {entry.split("/")[0] for entry in entry_names if "/" in entry}
     for folder in dduf_folders:
         if folder not in index:
-            raise DDUFCorruptedFileError(f"Missing required entry '{folder}' in 'model_index.json'.")
-        if not any(f"{folder}/{required_entry}" in entry_names for required_entry in DDUF_FOLDER_REQUIRED_ENTRIES):
+            raise DDUFCorruptedFileError(
+                f"Missing required entry '{folder}' in 'model_index.json'."
+            )
+        if not any(
+            f"{folder}/{required_entry}" in entry_names
+            for required_entry in DDUF_FOLDER_REQUIRED_ENTRIES
+        ):
             raise DDUFCorruptedFileError(
                 f"Missing required file in folder '{folder}'. Must contains at least one of {DDUF_FOLDER_REQUIRED_ENTRIES}."
             )
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/serialization/_torch.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/serialization/_torch.py
@@ -20,7 +20,18 @@ import re
 from collections import defaultdict, namedtuple
 from functools import lru_cache
 from pathlib import Path
-from typing import TYPE_CHECKING, Any, Dict, Iterable, List, NamedTuple, Optional, Set, Tuple, Union
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Dict,
+    Iterable,
+    List,
+    NamedTuple,
+    Optional,
+    Set,
+    Tuple,
+    Union,
+)
 
 from packaging import version
 
@@ -260,7 +271,9 @@ def save_torch_state_dict(
 
     # Only main process should clean up existing files to avoid race conditions in distributed environment
     if is_main_process:
-        existing_files_regex = re.compile(filename_pattern.format(suffix=r"(-\d{5}-of-\d{5})?") + r"(\.index\.json)?")
+        existing_files_regex = re.compile(
+            filename_pattern.format(suffix=r"(-\d{5}-of-\d{5})?") + r"(\.index\.json)?"
+        )
         for filename in os.listdir(save_directory):
             if existing_files_regex.match(filename):
                 try:
@@ -452,7 +465,9 @@ def load_torch_model(
     # 2. If not, checkpoint_path is a directory
     if filename_pattern is None:
         filename_pattern = constants.SAFETENSORS_WEIGHTS_FILE_PATTERN
-        index_path = checkpoint_path / (filename_pattern.format(suffix="") + ".index.json")
+        index_path = checkpoint_path / (
+            filename_pattern.format(suffix="") + ".index.json"
+        )
         # Only fallback to pickle format if safetensors index is not found and safe is False.
         if not index_path.is_file() and not safe:
             filename_pattern = constants.PYTORCH_WEIGHTS_FILE_PATTERN
@@ -551,7 +566,8 @@ def _load_sharded_checkpoint(
     loaded_keys = set(index["weight_map"].keys())
     model_keys = set(model.state_dict().keys())
     return _IncompatibleKeys(
-        missing_keys=list(model_keys - loaded_keys), unexpected_keys=list(loaded_keys - model_keys)
+        missing_keys=list(model_keys - loaded_keys),
+        unexpected_keys=list(loaded_keys - model_keys),
     )
 
 
@@ -637,10 +653,16 @@ def load_state_dict_from_file(
                 f"The safetensors archive passed at {checkpoint_file} does not contain the valid metadata. Make sure "
                 "you save your model with the `save_torch_model` method."
             )
-        device = str(map_location.type) if map_location is not None and hasattr(map_location, "type") else map_location
+        device = (
+            str(map_location.type)
+            if map_location is not None and hasattr(map_location, "type")
+            else map_location
+        )
         # meta device is not supported with safetensors, falling back to CPU
         if device == "meta":
-            logger.warning("Meta device is not supported with safetensors. Falling back to CPU device.")
+            logger.warning(
+                "Meta device is not supported with safetensors. Falling back to CPU device."
+            )
             device = "cpu"
         return load_file(checkpoint_file, device=device)  # type: ignore[arg-type]
     # Otherwise, load from pickle
@@ -687,7 +709,9 @@ def _validate_keys_for_strict_loading(
     loaded_keys_set = set(loaded_keys)
     model_keys = set(model.state_dict().keys())
     missing_keys = model_keys - loaded_keys_set  # Keys in model but not in checkpoint
-    unexpected_keys = loaded_keys_set - model_keys  # Keys in checkpoint but not in model
+    unexpected_keys = (
+        loaded_keys_set - model_keys
+    )  # Keys in checkpoint but not in model
 
     if missing_keys or unexpected_keys:
         error_message = f"Error(s) in loading state_dict for {model.__class__.__name__}"
@@ -732,7 +756,9 @@ def _get_unique_id(tensor: "torch.Tensor
     return unique_id
 
 
-def get_torch_storage_id(tensor: "torch.Tensor") -> Optional[Tuple["torch.device", Union[int, Tuple[Any, ...]], int]]:
+def get_torch_storage_id(
+    tensor: "torch.Tensor",
+) -> Optional[Tuple["torch.device", Union[int, Tuple[Any, ...]], int]]:
     """
     Return unique identifier to a tensor storage.
 
@@ -834,7 +860,9 @@ def _clean_state_dict_for_safetensors(
 
     Taken from https://github.com/huggingface/safetensors/blob/079781fd0dc455ba0fe851e2b4507c33d0c0d407/bindings/python/py_src/safetensors/torch.py#L155.
     """
-    to_removes = _remove_duplicate_names(state_dict, discard_names=shared_tensors_to_discard)
+    to_removes = _remove_duplicate_names(
+        state_dict, discard_names=shared_tensors_to_discard
+    )
     for kept_name, to_remove_group in to_removes.items():
         for to_remove in to_remove_group:
             if metadata is None:
@@ -860,7 +888,9 @@ def _end_ptr(tensor: "torch.Tensor") ->
     return stop
 
 
-def _filter_shared_not_shared(tensors: List[Set[str]], state_dict: Dict[str, "torch.Tensor"]) -> List[Set[str]]:
+def _filter_shared_not_shared(
+    tensors: List[Set[str]], state_dict: Dict[str, "torch.Tensor"]
+) -> List[Set[str]]:
     """
     Taken from https://github.com/huggingface/safetensors/blob/079781fd0dc455ba0fe851e2b4507c33d0c0d407/bindings/python/py_src/safetensors/torch.py#L44
     """
@@ -896,7 +926,11 @@ def _find_shared_tensors(state_dict: Dic
 
     tensors_dict = defaultdict(set)
     for k, v in state_dict.items():
-        if v.device != torch.device("meta") and storage_ptr(v) != 0 and get_torch_storage_size(v) != 0:
+        if (
+            v.device != torch.device("meta")
+            and storage_ptr(v) != 0
+            and get_torch_storage_size(v) != 0
+        ):
             # Need to add device as key because of multiple GPU.
             tensors_dict[(v.device, storage_ptr(v), get_torch_storage_size(v))].add(k)
     tensors = list(sorted(tensors_dict.values()))
@@ -919,9 +953,11 @@ def _is_complete(tensor: "torch.Tensor")
         # for torch version less than 2.1, we can fallback to original implementation
         pass
 
-    return tensor.data_ptr() == storage_ptr(tensor) and tensor.nelement() * _get_dtype_size(
-        tensor.dtype
-    ) == get_torch_storage_size(tensor)
+    return tensor.data_ptr() == storage_ptr(
+        tensor
+    ) and tensor.nelement() * _get_dtype_size(tensor.dtype) == get_torch_storage_size(
+        tensor
+    )
 
 
 def _remove_duplicate_names(
@@ -943,7 +979,9 @@ def _remove_duplicate_names(
     shareds = _find_shared_tensors(state_dict)
     to_remove = defaultdict(list)
     for shared in shareds:
-        complete_names = set([name for name in shared if _is_complete(state_dict[name])])
+        complete_names = set(
+            [name for name in shared if _is_complete(state_dict[name])]
+        )
         if not complete_names:
             raise RuntimeError(
                 "Error while trying to find names to remove to save state dict, but found no suitable name to keep"
@@ -1000,7 +1038,9 @@ def _get_dtype_size(dtype: "torch.dtype"
     return _SIZE[dtype]
 
 
-class _IncompatibleKeys(namedtuple("IncompatibleKeys", ["missing_keys", "unexpected_keys"])):
+class _IncompatibleKeys(
+    namedtuple("IncompatibleKeys", ["missing_keys", "unexpected_keys"])
+):
     """
     This is used to report missing and unexpected keys in the state dict.
     Taken from https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/module.py#L52.
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/__init__.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/__init__.py
@@ -49,7 +49,11 @@ from ._chunk_utils import chunk_iterable
 from ._datetime import parse_datetime
 from ._experimental import experimental
 from ._fixes import SoftTemporaryDirectory, WeakFileLock, yaml_dump
-from ._git_credential import list_credential_helpers, set_git_credential, unset_git_credential
+from ._git_credential import (
+    list_credential_helpers,
+    set_git_credential,
+    unset_git_credential,
+)
 from ._headers import build_hf_headers, get_token_to_send
 from ._hf_folder import HfFolder
 from ._http import (
@@ -106,5 +110,15 @@ from ._safetensors import SafetensorsFil
 from ._subprocess import capture_output, run_interactive_subprocess, run_subprocess
 from ._telemetry import send_telemetry
 from ._typing import is_jsonable, is_simple_optional_type, unwrap_simple_optional_type
-from ._validators import smoothly_deprecate_use_auth_token, validate_hf_hub_args, validate_repo_id
-from .tqdm import are_progress_bars_disabled, disable_progress_bars, enable_progress_bars, tqdm, tqdm_stream_file
+from ._validators import (
+    smoothly_deprecate_use_auth_token,
+    validate_hf_hub_args,
+    validate_repo_id,
+)
+from .tqdm import (
+    are_progress_bars_disabled,
+    disable_progress_bars,
+    enable_progress_bars,
+    tqdm,
+    tqdm_stream_file,
+)
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_auth.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_auth.py
@@ -46,7 +46,11 @@ def get_token() -> Optional[str]:
     Returns:
         `str` or `None`: The token, `None` if it doesn't exist.
     """
-    return _get_token_from_google_colab() or _get_token_from_environment() or _get_token_from_file()
+    return (
+        _get_token_from_google_colab()
+        or _get_token_from_environment()
+        or _get_token_from_file()
+    )
 
 
 def _get_token_from_google_colab() -> Optional[str]:
@@ -115,7 +119,9 @@ def _get_token_from_google_colab() -> Op
 
 def _get_token_from_environment() -> Optional[str]:
     # `HF_TOKEN` has priority (keep `HUGGING_FACE_HUB_TOKEN` for backward compatibility)
-    return _clean_token(os.environ.get("HF_TOKEN") or os.environ.get("HUGGING_FACE_HUB_TOKEN"))
+    return _clean_token(
+        os.environ.get("HF_TOKEN") or os.environ.get("HUGGING_FACE_HUB_TOKEN")
+    )
 
 
 def _get_token_from_file() -> Optional[str]:
@@ -140,7 +146,10 @@ def get_stored_tokens() -> Dict[str, str
     config = configparser.ConfigParser()
     try:
         config.read(tokens_path)
-        stored_tokens = {token_name: config.get(token_name, "hf_token") for token_name in config.sections()}
+        stored_tokens = {
+            token_name: config.get(token_name, "hf_token")
+            for token_name in config.sections()
+        }
     except configparser.Error as e:
         logger.error(f"Error parsing stored tokens file: {e}")
         stored_tokens = {}
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_cache_assets.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_cache_assets.py
@@ -129,7 +129,9 @@ def cached_assets_path(
     try:
         path.mkdir(exist_ok=True, parents=True)
     except (FileExistsError, NotADirectoryError):
-        raise ValueError(f"Corrupted assets folder: cannot create directory because of an existing file ({path}).")
+        raise ValueError(
+            f"Corrupted assets folder: cannot create directory because of an existing file ({path})."
+        )
 
     # Return
     return path
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_cache_manager.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_cache_manager.py
@@ -422,7 +422,9 @@ class HFCacheInfo:
         """
         hashes_to_delete: Set[str] = set(revisions)
 
-        repos_with_revisions: Dict[CachedRepoInfo, Set[CachedRevisionInfo]] = defaultdict(set)
+        repos_with_revisions: Dict[CachedRepoInfo, Set[CachedRevisionInfo]] = (
+            defaultdict(set)
+        )
 
         for repo in self.repos:
             for revision in repo.revisions:
@@ -431,7 +433,9 @@ class HFCacheInfo:
                     hashes_to_delete.remove(revision.commit_hash)
 
         if len(hashes_to_delete) > 0:
-            logger.warning(f"Revision(s) not found - cannot delete them: {', '.join(hashes_to_delete)}")
+            logger.warning(
+                f"Revision(s) not found - cannot delete them: {', '.join(hashes_to_delete)}"
+            )
 
         delete_strategy_blobs: Set[Path] = set()
         delete_strategy_refs: Set[Path] = set()
@@ -565,7 +569,9 @@ class HFCacheInfo:
                         str(revision.snapshot_path),
                     ]
                     for repo in sorted(self.repos, key=lambda repo: repo.repo_path)
-                    for revision in sorted(repo.revisions, key=lambda revision: revision.commit_hash)
+                    for revision in sorted(
+                        repo.revisions, key=lambda revision: revision.commit_hash
+                    )
                 ],
                 headers=[
                     "REPO ID",
@@ -707,7 +713,9 @@ def _scan_cached_repo(repo_path: Path) -
         raise CorruptedCacheException(f"Repo path is not a directory: {repo_path}")
 
     if "--" not in repo_path.name:
-        raise CorruptedCacheException(f"Repo path is not a valid HuggingFace cache directory: {repo_path}")
+        raise CorruptedCacheException(
+            f"Repo path is not a valid HuggingFace cache directory: {repo_path}"
+        )
 
     repo_type, repo_id = repo_path.name.split("--", maxsplit=1)
     repo_type = repo_type[:-1]  # "models" -> "model"
@@ -724,7 +732,9 @@ def _scan_cached_repo(repo_path: Path) -
     refs_path = repo_path / "refs"
 
     if not snapshots_path.exists() or not snapshots_path.is_dir():
-        raise CorruptedCacheException(f"Snapshots dir doesn't exist in cached repo: {snapshots_path}")
+        raise CorruptedCacheException(
+            f"Snapshots dir doesn't exist in cached repo: {snapshots_path}"
+        )
 
     # Scan over `refs` directory
 
@@ -738,7 +748,9 @@ def _scan_cached_repo(repo_path: Path) -
         #         └── pr
         #             └── 1
         if refs_path.is_file():
-            raise CorruptedCacheException(f"Refs directory cannot be a file: {refs_path}")
+            raise CorruptedCacheException(
+                f"Refs directory cannot be a file: {refs_path}"
+            )
 
         for ref_path in refs_path.glob("**/*"):
             # glob("**/*") iterates over all files and directories -> skip directories
@@ -758,7 +770,9 @@ def _scan_cached_repo(repo_path: Path) -
         if revision_path.name in FILES_TO_IGNORE:
             continue
         if revision_path.is_file():
-            raise CorruptedCacheException(f"Snapshots folder corrupted. Found a file: {revision_path}")
+            raise CorruptedCacheException(
+                f"Snapshots folder corrupted. Found a file: {revision_path}"
+            )
 
         cached_files = set()
         for file_path in revision_path.glob("**/*"):
@@ -768,7 +782,9 @@ def _scan_cached_repo(repo_path: Path) -
 
             blob_path = Path(file_path).resolve()
             if not blob_path.exists():
-                raise CorruptedCacheException(f"Blob missing (broken symlink): {blob_path}")
+                raise CorruptedCacheException(
+                    f"Blob missing (broken symlink): {blob_path}"
+                )
 
             if blob_path not in blob_stats:
                 blob_stats[blob_path] = blob_path.stat()
@@ -787,7 +803,9 @@ def _scan_cached_repo(repo_path: Path) -
         # Last modified is either the last modified blob file or the revision folder
         # itself if it is empty
         if len(cached_files) > 0:
-            revision_last_modified = max(blob_stats[file.blob_path].st_mtime for file in cached_files)
+            revision_last_modified = max(
+                blob_stats[file.blob_path].st_mtime for file in cached_files
+            )
         else:
             revision_last_modified = revision_path.stat().st_mtime
 
@@ -797,7 +815,8 @@ def _scan_cached_repo(repo_path: Path) -
                 files=frozenset(cached_files),
                 refs=frozenset(refs_by_hash.pop(revision_path.name, set())),
                 size_on_disk=sum(
-                    blob_stats[blob_path].st_size for blob_path in set(file.blob_path for file in cached_files)
+                    blob_stats[blob_path].st_size
+                    for blob_path in set(file.blob_path for file in cached_files)
                 ),
                 snapshot_path=revision_path,
                 last_modified=revision_last_modified,
@@ -891,6 +910,10 @@ def _try_delete_path(path: Path, path_ty
         else:
             shutil.rmtree(path)
     except FileNotFoundError:
-        logger.warning(f"Couldn't delete {path_type}: file not found ({path})", exc_info=True)
+        logger.warning(
+            f"Couldn't delete {path_type}: file not found ({path})", exc_info=True
+        )
     except PermissionError:
-        logger.warning(f"Couldn't delete {path_type}: permission denied ({path})", exc_info=True)
+        logger.warning(
+            f"Couldn't delete {path_type}: permission denied ({path})", exc_info=True
+        )
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_datetime.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_datetime.py
@@ -59,7 +59,9 @@ def parse_datetime(date_string: str) ->
                 # fraction[:6] takes first 6 digits and :0<6 pads with zeros if less than 6 digits
                 date_string = f"{base}.{fraction[:6]:0<6}Z"
 
-        return datetime.strptime(date_string, "%Y-%m-%dT%H:%M:%S.%fZ").replace(tzinfo=timezone.utc)
+        return datetime.strptime(date_string, "%Y-%m-%dT%H:%M:%S.%fZ").replace(
+            tzinfo=timezone.utc
+        )
     except ValueError as e:
         raise ValueError(
             f"Cannot parse '{date_string}' as a datetime. Date string is expected to"
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_deprecation.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_deprecation.py
@@ -123,9 +123,7 @@ def _deprecate_method(*, version: str, m
 
         @wraps(f)
         def inner_f(*args, **kwargs):
-            warning_message = (
-                f"'{name}' (from '{f.__module__}') is deprecated and will be removed from version '{version}'."
-            )
+            warning_message = f"'{name}' (from '{f.__module__}') is deprecated and will be removed from version '{version}'."
             if message is not None:
                 warning_message += " " + message
             warnings.warn(warning_message, FutureWarning)
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_experimental.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_experimental.py
@@ -50,7 +50,11 @@ def experimental(fn: Callable) -> Callab
     ```
     """
     # For classes, put the "experimental" around the "__new__" method => __new__ will be removed in warning message
-    name = fn.__qualname__[: -len(".__new__")] if fn.__qualname__.endswith(".__new__") else fn.__qualname__
+    name = (
+        fn.__qualname__[: -len(".__new__")]
+        if fn.__qualname__.endswith(".__new__")
+        else fn.__qualname__
+    )
 
     @wraps(fn)
     def _inner_fn(*args, **kwargs):
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_fixes.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_fixes.py
@@ -57,7 +57,9 @@ def SoftTemporaryDirectory(
 
     See https://www.scivision.dev/python-tempfile-permission-error-windows/.
     """
-    tmpdir = tempfile.TemporaryDirectory(prefix=prefix, suffix=suffix, dir=dir, **kwargs)
+    tmpdir = tempfile.TemporaryDirectory(
+        prefix=prefix, suffix=suffix, dir=dir, **kwargs
+    )
     yield Path(tmpdir.name).resolve()
 
     try:
@@ -106,7 +108,13 @@ def WeakFileLock(
             raise Timeout(str(lock_file))
 
         try:
-            lock.acquire(timeout=min(log_interval, timeout - elapsed_time) if timeout else log_interval)
+            lock.acquire(
+                timeout=(
+                    min(log_interval, timeout - elapsed_time)
+                    if timeout
+                    else log_interval
+                )
+            )
         except Timeout:
             logger.info(
                 f"Still waiting to acquire lock on {lock_file} (elapsed: {time.time() - start_time:.1f} seconds)"
@@ -114,7 +122,8 @@ def WeakFileLock(
         except NotImplementedError as e:
             if "use SoftFileLock instead" in str(e):
                 logger.warning(
-                    "FileSystem does not appear to support flock. Falling back to SoftFileLock for %s", lock_file
+                    "FileSystem does not appear to support flock. Falling back to SoftFileLock for %s",
+                    lock_file,
                 )
                 lock = SoftFileLock(lock_file, timeout=log_interval)
                 continue
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_git_credential.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_git_credential.py
@@ -54,7 +54,9 @@ def list_credential_helpers(folder: Opti
         raise EnvironmentError(exc.stderr)
 
 
-def set_git_credential(token: str, username: str = "hf_user", folder: Optional[str] = None) -> None:
+def set_git_credential(
+    token: str, username: str = "hf_user", folder: Optional[str] = None
+) -> None:
     """Save a username/token pair in git credential for HF Hub registry.
 
     Credentials are saved in all configured helpers (store, cache, macOS keychain,...).
@@ -73,11 +75,15 @@ def set_git_credential(token: str, usern
         stdin,
         _,
     ):
-        stdin.write(f"url={ENDPOINT}\nusername={username.lower()}\npassword={token}\n\n")
+        stdin.write(
+            f"url={ENDPOINT}\nusername={username.lower()}\npassword={token}\n\n"
+        )
         stdin.flush()
 
 
-def unset_git_credential(username: str = "hf_user", folder: Optional[str] = None) -> None:
+def unset_git_credential(
+    username: str = "hf_user", folder: Optional[str] = None
+) -> None:
     """Erase credentials from git credential for HF Hub registry.
 
     Credentials are erased from the configured helpers (store, cache, macOS
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_http.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_http.py
@@ -80,7 +80,9 @@ class UniqueRequestIdAdapter(HTTPAdapter
 
         # Add random request ID => easier for server-side debug
         if X_AMZN_TRACE_ID not in request.headers:
-            request.headers[X_AMZN_TRACE_ID] = request.headers.get(X_REQUEST_ID) or str(uuid.uuid4())
+            request.headers[X_AMZN_TRACE_ID] = request.headers.get(X_REQUEST_ID) or str(
+                uuid.uuid4()
+            )
 
         # Add debug log
         has_token = len(str(request.headers.get("authorization", ""))) > 0
@@ -124,7 +126,9 @@ BACKEND_FACTORY_T = Callable[[], request
 _GLOBAL_BACKEND_FACTORY: BACKEND_FACTORY_T = _default_backend_factory
 
 
-def configure_http_backend(backend_factory: BACKEND_FACTORY_T = _default_backend_factory) -> None:
+def configure_http_backend(
+    backend_factory: BACKEND_FACTORY_T = _default_backend_factory,
+) -> None:
     """
     Configure the HTTP backend by providing a `backend_factory`. Any HTTP calls made by `huggingface_hub` will use a
     Session object instantiated by this factory. This can be useful if you are running your scripts in a specific
@@ -189,7 +193,9 @@ def get_session() -> requests.Session:
     session = get_session()
     ```
     """
-    return _get_session_from_cache(process_id=os.getpid(), thread_id=threading.get_ident())
+    return _get_session_from_cache(
+        process_id=os.getpid(), thread_id=threading.get_ident()
+    )
 
 
 def reset_sessions() -> None:
@@ -312,7 +318,9 @@ def http_backoff(
                 return response
 
             # Wrong status code returned (HTTP 503 for instance)
-            logger.warning(f"HTTP Error {response.status_code} thrown while requesting {method} {url}")
+            logger.warning(
+                f"HTTP Error {response.status_code} thrown while requesting {method} {url}"
+            )
             if nb_tries > max_retries:
                 response.raise_for_status()  # Will raise uncaught exception
                 # We return response to avoid infinite loop in the corner case where the
@@ -343,13 +351,18 @@ def fix_hf_endpoint_in_url(url: str, end
     """
     endpoint = endpoint.rstrip("/") if endpoint else constants.ENDPOINT
     # check if a proxy has been set => if yes, update the returned URL to use the proxy
-    if endpoint not in (constants._HF_DEFAULT_ENDPOINT, constants._HF_DEFAULT_STAGING_ENDPOINT):
+    if endpoint not in (
+        constants._HF_DEFAULT_ENDPOINT,
+        constants._HF_DEFAULT_STAGING_ENDPOINT,
+    ):
         url = url.replace(constants._HF_DEFAULT_ENDPOINT, endpoint)
         url = url.replace(constants._HF_DEFAULT_STAGING_ENDPOINT, endpoint)
     return url
 
 
-def hf_raise_for_status(response: Response, endpoint_name: Optional[str] = None) -> None:
+def hf_raise_for_status(
+    response: Response, endpoint_name: Optional[str] = None
+) -> None:
     """
     Internal version of `response.raise_for_status()` that will refine a
     potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.
@@ -412,16 +425,26 @@ def hf_raise_for_status(response: Respon
         error_message = response.headers.get("X-Error-Message")
 
         if error_code == "RevisionNotFound":
-            message = f"{response.status_code} Client Error." + "\n\n" + f"Revision Not Found for url: {response.url}."
+            message = (
+                f"{response.status_code} Client Error."
+                + "\n\n"
+                + f"Revision Not Found for url: {response.url}."
+            )
             raise _format(RevisionNotFoundError, message, response) from e
 
         elif error_code == "EntryNotFound":
-            message = f"{response.status_code} Client Error." + "\n\n" + f"Entry Not Found for url: {response.url}."
+            message = (
+                f"{response.status_code} Client Error."
+                + "\n\n"
+                + f"Entry Not Found for url: {response.url}."
+            )
             raise _format(EntryNotFoundError, message, response) from e
 
         elif error_code == "GatedRepo":
             message = (
-                f"{response.status_code} Client Error." + "\n\n" + f"Cannot access gated repo for url {response.url}."
+                f"{response.status_code} Client Error."
+                + "\n\n"
+                + f"Cannot access gated repo for url {response.url}."
             )
             raise _format(GatedRepoError, message, response) from e
 
@@ -459,7 +482,9 @@ def hf_raise_for_status(response: Respon
 
         elif response.status_code == 400:
             message = (
-                f"\n\nBad request for {endpoint_name} endpoint:" if endpoint_name is not None else "\n\nBad request:"
+                f"\n\nBad request for {endpoint_name} endpoint:"
+                if endpoint_name is not None
+                else "\n\nBad request:"
             )
             raise _format(BadRequestError, message, response) from e
 
@@ -481,7 +506,9 @@ def hf_raise_for_status(response: Respon
         raise _format(HfHubHTTPError, str(e), response) from e
 
 
-def _format(error_type: Type[HfHubHTTPError], custom_message: str, response: Response) -> HfHubHTTPError:
+def _format(
+    error_type: Type[HfHubHTTPError], custom_message: str, response: Response
+) -> HfHubHTTPError:
     server_errors = []
 
     # Retrieve server error from header
@@ -546,13 +573,19 @@ def _format(error_type: Type[HfHubHTTPEr
         if "\n" in final_error_message:
             newline_index = final_error_message.index("\n")
             final_error_message = (
-                final_error_message[:newline_index] + request_id_message + final_error_message[newline_index:]
+                final_error_message[:newline_index]
+                + request_id_message
+                + final_error_message[newline_index:]
             )
         else:
             final_error_message += request_id_message
 
     # Return
-    return error_type(final_error_message.strip(), response=response, server_message=server_message or None)
+    return error_type(
+        final_error_message.strip(),
+        response=response,
+        server_message=server_message or None,
+    )
 
 
 def _curlify(request: requests.PreparedRequest) -> str:
@@ -597,7 +630,9 @@ def _curlify(request: requests.PreparedR
 RANGE_REGEX = re.compile(r"^\s*bytes\s*=\s*(\d*)\s*-\s*(\d*)\s*$", re.IGNORECASE)
 
 
-def _adjust_range_header(original_range: Optional[str], resume_size: int) -> Optional[str]:
+def _adjust_range_header(
+    original_range: Optional[str], resume_size: int
+) -> Optional[str]:
     """
     Adjust HTTP Range header to account for resume position.
     """
@@ -605,7 +640,9 @@ def _adjust_range_header(original_range:
         return f"bytes={resume_size}-"
 
     if "," in original_range:
-        raise ValueError(f"Multiple ranges detected - {original_range!r}, not supported yet.")
+        raise ValueError(
+            f"Multiple ranges detected - {original_range!r}, not supported yet."
+        )
 
     match = RANGE_REGEX.match(original_range)
     if not match:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_lfs.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_lfs.py
@@ -86,7 +86,9 @@ class SliceFileObj(AbstractContextManage
         if pos >= self._len:
             return b""
         remaining_amount = self._len - pos
-        data = self.fileobj.read(remaining_amount if n < 0 else min(n, remaining_amount))
+        data = self.fileobj.read(
+            remaining_amount if n < 0 else min(n, remaining_amount)
+        )
         return data
 
     def tell(self) -> int:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_paths.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_paths.py
@@ -117,7 +117,9 @@ def filter_repo_objects(
                 return item
             if isinstance(item, Path):
                 return str(item)
-            raise ValueError(f"Please provide `key` argument in `filter_repo_objects`: `{item}` is not a string.")
+            raise ValueError(
+                f"Please provide `key` argument in `filter_repo_objects`: `{item}` is not a string."
+            )
 
         key = _identity  # Items must be `str` or `Path`, otherwise raise ValueError
 
@@ -125,11 +127,15 @@ def filter_repo_objects(
         path = key(item)
 
         # Skip if there's an allowlist and path doesn't match any
-        if allow_patterns is not None and not any(fnmatch(path, r) for r in allow_patterns):
+        if allow_patterns is not None and not any(
+            fnmatch(path, r) for r in allow_patterns
+        ):
             continue
 
         # Skip if there's a denylist and path matches any
-        if ignore_patterns is not None and any(fnmatch(path, r) for r in ignore_patterns):
+        if ignore_patterns is not None and any(
+            fnmatch(path, r) for r in ignore_patterns
+        ):
             continue
 
         yield item
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_runtime.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_runtime.py
@@ -327,7 +327,9 @@ def dump_environment_info() -> Dict[str,
         info["Running in iPython ?"] = "No"
     info["Running in notebook ?"] = "Yes" if is_notebook() else "No"
     info["Running in Google Colab ?"] = "Yes" if is_google_colab() else "No"
-    info["Running in Google Colab Enterprise ?"] = "Yes" if is_colab_enterprise() else "No"
+    info["Running in Google Colab Enterprise ?"] = (
+        "Yes" if is_colab_enterprise() else "No"
+    )
     # Login info
     info["Token path ?"] = constants.HF_TOKEN_PATH
     info["Has saved token ?"] = token is not None
@@ -368,7 +370,9 @@ def dump_environment_info() -> Dict[str,
     info["HF_HUB_DISABLE_TELEMETRY"] = constants.HF_HUB_DISABLE_TELEMETRY
     info["HF_HUB_DISABLE_PROGRESS_BARS"] = constants.HF_HUB_DISABLE_PROGRESS_BARS
     info["HF_HUB_DISABLE_SYMLINKS_WARNING"] = constants.HF_HUB_DISABLE_SYMLINKS_WARNING
-    info["HF_HUB_DISABLE_EXPERIMENTAL_WARNING"] = constants.HF_HUB_DISABLE_EXPERIMENTAL_WARNING
+    info["HF_HUB_DISABLE_EXPERIMENTAL_WARNING"] = (
+        constants.HF_HUB_DISABLE_EXPERIMENTAL_WARNING
+    )
     info["HF_HUB_DISABLE_IMPLICIT_TOKEN"] = constants.HF_HUB_DISABLE_IMPLICIT_TOKEN
     info["HF_HUB_ENABLE_HF_TRANSFER"] = constants.HF_HUB_ENABLE_HF_TRANSFER
     info["HF_HUB_ETAG_TIMEOUT"] = constants.HF_HUB_ETAG_TIMEOUT
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_telemetry.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_telemetry.py
@@ -69,7 +69,12 @@ def send_telemetry(
 
     _start_telemetry_thread()  # starts thread only if doesn't exist yet
     _TELEMETRY_QUEUE.put(
-        {"topic": topic, "library_name": library_name, "library_version": library_version, "user_agent": user_agent}
+        {
+            "topic": topic,
+            "library_name": library_name,
+            "library_version": library_version,
+            "user_agent": user_agent,
+        }
     )
 
 
@@ -78,7 +83,9 @@ def _start_telemetry_thread():
 
     If the thread is interrupted, start a new one.
     """
-    with _TELEMETRY_THREAD_LOCK:  # avoid to start multiple threads if called concurrently
+    with (
+        _TELEMETRY_THREAD_LOCK
+    ):  # avoid to start multiple threads if called concurrently
         global _TELEMETRY_THREAD
         if _TELEMETRY_THREAD is None or not _TELEMETRY_THREAD.is_alive():
             _TELEMETRY_THREAD = Thread(target=_telemetry_worker, daemon=True)
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_typing.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_typing.py
@@ -15,7 +15,17 @@
 """Handle typing imports based on system compatibility."""
 
 import sys
-from typing import Any, Callable, List, Literal, Type, TypeVar, Union, get_args, get_origin
+from typing import (
+    Any,
+    Callable,
+    List,
+    Literal,
+    Type,
+    TypeVar,
+    Union,
+    get_args,
+    get_origin,
+)
 
 
 UNION_TYPES: List[Any] = [Union]
@@ -50,7 +60,10 @@ def is_jsonable(obj: Any) -> bool:
         if isinstance(obj, (list, tuple)):
             return all(is_jsonable(item) for item in obj)
         if isinstance(obj, dict):
-            return all(isinstance(key, _JSON_SERIALIZABLE_TYPES) and is_jsonable(value) for key, value in obj.items())
+            return all(
+                isinstance(key, _JSON_SERIALIZABLE_TYPES) and is_jsonable(value)
+                for key, value in obj.items()
+            )
         if hasattr(obj, "__json__"):
             return True
         return False
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/_validators.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/_validators.py
@@ -93,7 +93,9 @@ def validate_hf_hub_args(fn: CallableT)
 
     # Should the validator switch `use_auth_token` values to `token`? In practice, always
     # True in `huggingface_hub`. Might not be the case in a downstream library.
-    check_use_auth_token = "use_auth_token" not in signature.parameters and "token" in signature.parameters
+    check_use_auth_token = (
+        "use_auth_token" not in signature.parameters and "token" in signature.parameters
+    )
 
     @wraps(fn)
     def _inner_fn(*args, **kwargs):
@@ -109,7 +111,9 @@ def validate_hf_hub_args(fn: CallableT)
                 has_token = True
 
         if check_use_auth_token:
-            kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)
+            kwargs = smoothly_deprecate_use_auth_token(
+                fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
+            )
 
         return fn(*args, **kwargs)
 
@@ -148,7 +152,9 @@ def validate_repo_id(repo_id: str) -> No
     """
     if not isinstance(repo_id, str):
         # Typically, a Path is not a repo_id
-        raise HFValidationError(f"Repo id must be a string, not {type(repo_id)}: '{repo_id}'.")
+        raise HFValidationError(
+            f"Repo id must be a string, not {type(repo_id)}: '{repo_id}'."
+        )
 
     if repo_id.count("/") > 1:
         raise HFValidationError(
@@ -170,7 +176,9 @@ def validate_repo_id(repo_id: str) -> No
         raise HFValidationError(f"Repo_id cannot end by '.git': '{repo_id}'.")
 
 
-def smoothly_deprecate_use_auth_token(fn_name: str, has_token: bool, kwargs: Dict[str, Any]) -> Dict[str, Any]:
+def smoothly_deprecate_use_auth_token(
+    fn_name: str, has_token: bool, kwargs: Dict[str, Any]
+) -> Dict[str, Any]:
     """Smoothly deprecate `use_auth_token` in the `huggingface_hub` codebase.
 
     The long-term goal is to remove any mention of `use_auth_token` in the codebase in
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/endpoint_helpers.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/endpoint_helpers.py
@@ -25,7 +25,9 @@ if TYPE_CHECKING:
     from ..hf_api import ModelInfo
 
 
-def _is_emission_within_threshold(model_info: "ModelInfo", minimum_threshold: float, maximum_threshold: float) -> bool:
+def _is_emission_within_threshold(
+    model_info: "ModelInfo", minimum_threshold: float, maximum_threshold: float
+) -> bool:
     """Checks if a model's emission is within a given threshold.
 
     Args:
@@ -40,7 +42,9 @@ def _is_emission_within_threshold(model_
         `bool`: Whether the model's emission is within the given threshold.
     """
     if minimum_threshold is None and maximum_threshold is None:
-        raise ValueError("Both `minimum_threshold` and `maximum_threshold` cannot both be `None`")
+        raise ValueError(
+            "Both `minimum_threshold` and `maximum_threshold` cannot both be `None`"
+        )
     if minimum_threshold is None:
         minimum_threshold = -1
     if maximum_threshold is None:
--- python3-huggingface-hub-0.29.3.orig/src/huggingface_hub/utils/tqdm.py
+++ python3-huggingface-hub-0.29.3/src/huggingface_hub/utils/tqdm.py
@@ -131,7 +131,9 @@ def disable_progress_bars(name: Optional
         progress_bar_states.clear()
         progress_bar_states["_global"] = False
     else:
-        keys_to_remove = [key for key in progress_bar_states if key.startswith(f"{name}.")]
+        keys_to_remove = [
+            key for key in progress_bar_states if key.startswith(f"{name}.")
+        ]
         for key in keys_to_remove:
             del progress_bar_states[key]
         progress_bar_states[name] = False
@@ -163,7 +165,9 @@ def enable_progress_bars(name: Optional[
         progress_bar_states.clear()
         progress_bar_states["_global"] = True
     else:
-        keys_to_remove = [key for key in progress_bar_states if key.startswith(f"{name}.")]
+        keys_to_remove = [
+            key for key in progress_bar_states if key.startswith(f"{name}.")
+        ]
         for key in keys_to_remove:
             del progress_bar_states[key]
         progress_bar_states[name] = True
--- python3-huggingface-hub-0.29.3.orig/tests/conftest.py
+++ python3-huggingface-hub-0.29.3/tests/conftest.py
@@ -17,10 +17,18 @@ def patch_constants(mocker):
     with SoftTemporaryDirectory() as cache_dir:
         mocker.patch.object(constants, "HF_HOME", cache_dir)
         mocker.patch.object(constants, "HF_HUB_CACHE", os.path.join(cache_dir, "hub"))
-        mocker.patch.object(constants, "HUGGINGFACE_HUB_CACHE", os.path.join(cache_dir, "hub"))
-        mocker.patch.object(constants, "HF_ASSETS_CACHE", os.path.join(cache_dir, "assets"))
-        mocker.patch.object(constants, "HF_TOKEN_PATH", os.path.join(cache_dir, "token"))
-        mocker.patch.object(constants, "HF_STORED_TOKENS_PATH", os.path.join(cache_dir, "stored_tokens"))
+        mocker.patch.object(
+            constants, "HUGGINGFACE_HUB_CACHE", os.path.join(cache_dir, "hub")
+        )
+        mocker.patch.object(
+            constants, "HF_ASSETS_CACHE", os.path.join(cache_dir, "assets")
+        )
+        mocker.patch.object(
+            constants, "HF_TOKEN_PATH", os.path.join(cache_dir, "token")
+        )
+        mocker.patch.object(
+            constants, "HF_STORED_TOKENS_PATH", os.path.join(cache_dir, "stored_tokens")
+        )
         yield
 
 
@@ -68,7 +76,9 @@ def disable_symlinks_on_windows_ci(monke
 
 @pytest.fixture(autouse=True)
 def disable_experimental_warnings(monkeypatch: pytest.MonkeyPatch) -> None:
-    monkeypatch.setattr(huggingface_hub.constants, "HF_HUB_DISABLE_EXPERIMENTAL_WARNING", True)
+    monkeypatch.setattr(
+        huggingface_hub.constants, "HF_HUB_DISABLE_EXPERIMENTAL_WARNING", True
+    )
 
 
 @pytest.fixture(scope="module")
--- python3-huggingface-hub-0.29.3.orig/tests/test_auth.py
+++ python3-huggingface-hub-0.29.3/tests/test_auth.py
@@ -6,7 +6,12 @@ import pytest
 
 from huggingface_hub import constants
 from huggingface_hub._login import _login, _set_active_token, auth_switch, logout
-from huggingface_hub.utils._auth import _get_token_by_name, _get_token_from_file, _save_token, get_stored_tokens
+from huggingface_hub.utils._auth import (
+    _get_token_by_name,
+    _get_token_from_file,
+    _save_token,
+    get_stored_tokens,
+)
 
 from .testing_constants import ENDPOINT_STAGING, OTHER_TOKEN, TOKEN
 
--- python3-huggingface-hub-0.29.3.orig/tests/test_auth_cli.py
+++ python3-huggingface-hub-0.29.3/tests/test_auth_cli.py
@@ -7,7 +7,12 @@ import pytest
 from pytest import CaptureFixture, LogCaptureFixture
 
 from huggingface_hub import constants
-from huggingface_hub.commands.user import AuthListCommand, AuthSwitchCommand, LoginCommand, LogoutCommand
+from huggingface_hub.commands.user import (
+    AuthListCommand,
+    AuthSwitchCommand,
+    LoginCommand,
+    LogoutCommand,
+)
 
 from .testing_constants import ENDPOINT_STAGING
 
@@ -58,14 +63,18 @@ def mock_stored_tokens():
         "active_token": "hf_9012",
     }
     with patch("huggingface_hub._login.get_stored_tokens", return_value=stored_tokens):
-        with patch("huggingface_hub.utils._auth.get_stored_tokens", return_value=stored_tokens):
+        with patch(
+            "huggingface_hub.utils._auth.get_stored_tokens", return_value=stored_tokens
+        ):
             yield stored_tokens
 
 
 def assert_in_logs(caplog: LogCaptureFixture, expected_output):
     """Helper to check if a message appears in logs."""
     log_text = "\n".join(record.message for record in caplog.records)
-    assert expected_output in log_text, f"Expected '{expected_output}' not found in logs"
+    assert (
+        expected_output in log_text
+    ), f"Expected '{expected_output}' not found in logs"
 
 
 def test_login_command_basic(mock_whoami_api_call, caplog: LogCaptureFixture):
@@ -88,12 +97,16 @@ def test_login_command_with_git(mock_who
     args = type("Args", (), {"token": MOCK_TOKEN, "add_to_git_credential": True})()
     cmd = LoginCommand(args)
 
-    with patch("huggingface_hub._login._is_git_credential_helper_configured", return_value=True):
+    with patch(
+        "huggingface_hub._login._is_git_credential_helper_configured", return_value=True
+    ):
         with patch("huggingface_hub.utils.set_git_credential"):
             cmd.run()
 
     assert_in_logs(caplog, "Login successful")
-    assert_in_logs(caplog, "Your token has been saved in your configured git credential helpers")
+    assert_in_logs(
+        caplog, "Your token has been saved in your configured git credential helpers"
+    )
 
 
 def test_logout_specific_token(mock_stored_tokens, caplog: LogCaptureFixture):
@@ -116,7 +129,9 @@ def test_logout_active_token(mock_stored
         cmd = LogoutCommand(args)
         cmd.run()
 
-        assert_in_logs(caplog, "Successfully logged out from access token: active_token")
+        assert_in_logs(
+            caplog, "Successfully logged out from access token: active_token"
+        )
         assert_in_logs(caplog, "Active token 'active_token' has been deleted")
 
 
@@ -144,7 +159,9 @@ def test_switch_token(mock_stored_tokens
 
 def test_switch_nonexistent_token(mock_stored_tokens):
     """Test switching to a non-existent token."""
-    args = type("Args", (), {"token_name": "nonexistent", "add_to_git_credential": False})()
+    args = type(
+        "Args", (), {"token_name": "nonexistent", "add_to_git_credential": False}
+    )()
     cmd = AuthSwitchCommand(args)
 
     with pytest.raises(ValueError, match="Access token nonexistent not found"):
--- python3-huggingface-hub-0.29.3.orig/tests/test_cache_layout.py
+++ python3-huggingface-hub-0.29.3/tests/test_cache_layout.py
@@ -42,7 +42,9 @@ class CacheFileLayoutHfHubDownload(unitt
                     revision=revision,
                 )
 
-                expected_directory_name = f"models--{MODEL_IDENTIFIER.replace('/', '--')}"
+                expected_directory_name = (
+                    f"models--{MODEL_IDENTIFIER.replace('/', '--')}"
+                )
                 expected_path = os.path.join(cache, expected_directory_name)
 
                 refs = os.listdir(os.path.join(expected_path, "refs"))
@@ -69,7 +71,9 @@ class CacheFileLayoutHfHubDownload(unitt
                 self.assertTrue(os.path.islink(snapshot_content_path))
 
                 resolved_blob_relative = os.readlink(snapshot_content_path)
-                resolved_blob_absolute = os.path.normpath(os.path.join(snapshot_path, resolved_blob_relative))
+                resolved_blob_absolute = os.path.normpath(
+                    os.path.join(snapshot_path, resolved_blob_relative)
+                )
 
                 with open(resolved_blob_absolute) as f:
                     blob_contents = f.readline().strip()
@@ -85,13 +89,19 @@ class CacheFileLayoutHfHubDownload(unitt
                 filename = "this_does_not_exist.txt"
                 with self.assertRaises(EntryNotFoundError):
                     # The file does not exist, so we get an exception.
-                    hf_hub_download(MODEL_IDENTIFIER, filename, cache_dir=cache, revision=revision)
+                    hf_hub_download(
+                        MODEL_IDENTIFIER, filename, cache_dir=cache, revision=revision
+                    )
 
-                expected_directory_name = f"models--{MODEL_IDENTIFIER.replace('/', '--')}"
+                expected_directory_name = (
+                    f"models--{MODEL_IDENTIFIER.replace('/', '--')}"
+                )
                 expected_path = os.path.join(cache, expected_directory_name)
 
                 refs = os.listdir(os.path.join(expected_path, "refs"))
-                no_exist_snapshots = os.listdir(os.path.join(expected_path, ".no_exist"))
+                no_exist_snapshots = os.listdir(
+                    os.path.join(expected_path, ".no_exist")
+                )
 
                 # Only reference should be `main`.
                 self.assertListEqual(refs, [expected_reference])
@@ -140,7 +150,9 @@ class CacheFileLayoutHfHubDownload(unitt
 
             time.sleep(2)
 
-            path = hf_hub_download(MODEL_IDENTIFIER, "file_0.txt", cache_dir=cache, revision="file-2")
+            path = hf_hub_download(
+                MODEL_IDENTIFIER, "file_0.txt", cache_dir=cache, revision="file-2"
+            )
             creation_time_1 = os.path.getmtime(path)
 
             self.assertEqual(creation_time_0, creation_time_1)
@@ -149,7 +161,9 @@ class CacheFileLayoutHfHubDownload(unitt
     def test_multiple_refs_for_same_file(self):
         with SoftTemporaryDirectory() as cache:
             hf_hub_download(MODEL_IDENTIFIER, "file_0.txt", cache_dir=cache)
-            hf_hub_download(MODEL_IDENTIFIER, "file_0.txt", cache_dir=cache, revision="file-2")
+            hf_hub_download(
+                MODEL_IDENTIFIER, "file_0.txt", cache_dir=cache, revision="file-2"
+            )
 
             expected_directory_name = f"models--{MODEL_IDENTIFIER.replace('/', '--')}"
             expected_path = os.path.join(cache, expected_directory_name)
@@ -163,14 +177,19 @@ class CacheFileLayoutHfHubDownload(unitt
             # Directory should contain two revisions
             self.assertListEqual(refs, ["file-2", "main"])
 
-            refs_contents = [get_file_contents(os.path.join(expected_path, "refs", f)) for f in refs]
+            refs_contents = [
+                get_file_contents(os.path.join(expected_path, "refs", f)) for f in refs
+            ]
             refs_contents.sort()
 
             # snapshots directory should contain two snapshots
             self.assertListEqual(refs_contents, snapshots)
 
             snapshot_links = [
-                os.readlink(os.path.join(expected_path, "snapshots", filename, "file_0.txt")) for filename in snapshots
+                os.readlink(
+                    os.path.join(expected_path, "snapshots", filename, "file_0.txt")
+                )
+                for filename in snapshots
             ]
 
             # All snapshot links should point to the same file.
@@ -195,7 +214,9 @@ class CacheFileLayoutSnapshotDownload(un
             # Directory should contain two revisions
             self.assertListEqual(refs, ["main"])
 
-            ref_content = get_file_contents(os.path.join(expected_path, "refs", refs[0]))
+            ref_content = get_file_contents(
+                os.path.join(expected_path, "refs", refs[0])
+            )
 
             # snapshots directory should contain two snapshots
             self.assertListEqual([ref_content], snapshots)
@@ -204,11 +225,19 @@ class CacheFileLayoutSnapshotDownload(un
 
             files_in_snapshot = os.listdir(snapshot_path)
 
-            snapshot_links = [os.readlink(os.path.join(snapshot_path, filename)) for filename in files_in_snapshot]
+            snapshot_links = [
+                os.readlink(os.path.join(snapshot_path, filename))
+                for filename in files_in_snapshot
+            ]
 
-            resolved_snapshot_links = [os.path.normpath(os.path.join(snapshot_path, link)) for link in snapshot_links]
+            resolved_snapshot_links = [
+                os.path.normpath(os.path.join(snapshot_path, link))
+                for link in snapshot_links
+            ]
 
-            self.assertTrue(all([os.path.isfile(link) for link in resolved_snapshot_links]))
+            self.assertTrue(
+                all([os.path.isfile(link) for link in resolved_snapshot_links])
+            )
 
     @xfail_on_windows(reason="Symlinks are deactivated in Windows tests.")
     def test_file_downloaded_in_cache_several_revisions(self):
@@ -228,21 +257,28 @@ class CacheFileLayoutSnapshotDownload(un
             # Directory should contain two revisions
             self.assertListEqual(refs, ["file-2", "file-3"])
 
-            refs_content = [get_file_contents(os.path.join(expected_path, "refs", ref)) for ref in refs]
+            refs_content = [
+                get_file_contents(os.path.join(expected_path, "refs", ref))
+                for ref in refs
+            ]
             refs_content.sort()
 
             # snapshots directory should contain two snapshots
             self.assertListEqual(refs_content, snapshots)
 
-            snapshots_paths = [os.path.join(expected_path, "snapshots", s) for s in snapshots]
+            snapshots_paths = [
+                os.path.join(expected_path, "snapshots", s) for s in snapshots
+            ]
 
             files_in_snapshots = {s: os.listdir(s) for s in snapshots_paths}
             links_in_snapshots = {
-                k: [os.readlink(os.path.join(k, _v)) for _v in v] for k, v in files_in_snapshots.items()
+                k: [os.readlink(os.path.join(k, _v)) for _v in v]
+                for k, v in files_in_snapshots.items()
             }
 
             resolved_snapshots_links = {
-                k: [os.path.normpath(os.path.join(k, link)) for link in v] for k, v in links_in_snapshots.items()
+                k: [os.path.normpath(os.path.join(k, link)) for link in v]
+                for k, v in links_in_snapshots.items()
             }
 
             all_links = [b for a in resolved_snapshots_links.values() for b in a]
@@ -284,7 +320,11 @@ class ReferenceUpdates(unittest.TestCase
         repo_id = self._api.create_repo(repo_name(), exist_ok=True).repo_id
 
         try:
-            self._api.upload_file(path_or_fileobj=BytesIO(b"Some string"), path_in_repo="file.txt", repo_id=repo_id)
+            self._api.upload_file(
+                path_or_fileobj=BytesIO(b"Some string"),
+                path_in_repo="file.txt",
+                repo_id=repo_id,
+            )
 
             with SoftTemporaryDirectory() as cache:
                 hf_hub_download(repo_id, "file.txt", cache_dir=cache)
@@ -297,7 +337,9 @@ class ReferenceUpdates(unittest.TestCase
                 # Directory should contain two revisions
                 self.assertListEqual(refs, ["main"])
 
-                initial_ref_content = get_file_contents(os.path.join(expected_path, "refs", refs[0]))
+                initial_ref_content = get_file_contents(
+                    os.path.join(expected_path, "refs", refs[0])
+                )
 
                 # Upload a new file on the same branch
                 self._api.upload_file(
@@ -308,18 +350,36 @@ class ReferenceUpdates(unittest.TestCase
 
                 hf_hub_download(repo_id, "file.txt", cache_dir=cache)
 
-                final_ref_content = get_file_contents(os.path.join(expected_path, "refs", refs[0]))
+                final_ref_content = get_file_contents(
+                    os.path.join(expected_path, "refs", refs[0])
+                )
 
                 # The `main` reference should point to two different, but existing snapshots which contain
                 # a 'file.txt'
                 self.assertNotEqual(initial_ref_content, final_ref_content)
-                self.assertTrue(os.path.isdir(os.path.join(expected_path, "snapshots", initial_ref_content)))
                 self.assertTrue(
-                    os.path.isfile(os.path.join(expected_path, "snapshots", initial_ref_content, "file.txt"))
+                    os.path.isdir(
+                        os.path.join(expected_path, "snapshots", initial_ref_content)
+                    )
+                )
+                self.assertTrue(
+                    os.path.isfile(
+                        os.path.join(
+                            expected_path, "snapshots", initial_ref_content, "file.txt"
+                        )
+                    )
+                )
+                self.assertTrue(
+                    os.path.isdir(
+                        os.path.join(expected_path, "snapshots", final_ref_content)
+                    )
                 )
-                self.assertTrue(os.path.isdir(os.path.join(expected_path, "snapshots", final_ref_content)))
                 self.assertTrue(
-                    os.path.isfile(os.path.join(expected_path, "snapshots", final_ref_content, "file.txt"))
+                    os.path.isfile(
+                        os.path.join(
+                            expected_path, "snapshots", final_ref_content, "file.txt"
+                        )
+                    )
                 )
         except Exception:
             raise
--- python3-huggingface-hub-0.29.3.orig/tests/test_cache_no_symlinks.py
+++ python3-huggingface-hub-0.29.3/tests/test_cache_no_symlinks.py
@@ -26,7 +26,9 @@ class TestCacheLayoutIfSymlinksNotSuppor
 
     @patch("huggingface_hub.file_download.os.symlink")
     @patch("huggingface_hub.file_download._are_symlinks_supported_in_dir", {})
-    def test_are_symlinks_supported_windows_specific_dir(self, mock_symlink: Mock) -> None:
+    def test_are_symlinks_supported_windows_specific_dir(
+        self, mock_symlink: Mock
+    ) -> None:
         mock_symlink.side_effect = [OSError(), None]  # First dir not supported then yes
         this_dir = Path(__file__).parent
 
@@ -46,7 +48,9 @@ class TestCacheLayoutIfSymlinksNotSuppor
             self.assertTrue(are_symlinks_supported())  # True
 
     @patch("huggingface_hub.file_download.are_symlinks_supported")
-    def test_download_no_symlink_new_file(self, mock_are_symlinks_supported: Mock) -> None:
+    def test_download_no_symlink_new_file(
+        self, mock_are_symlinks_supported: Mock
+    ) -> None:
         mock_are_symlinks_supported.return_value = False
         filepath = Path(
             hf_hub_download(
@@ -64,7 +68,9 @@ class TestCacheLayoutIfSymlinksNotSuppor
         self.assertEqual(len(list((Path(filepath).parents[2] / "blobs").glob("*"))), 0)
 
     @patch("huggingface_hub.file_download.are_symlinks_supported")
-    def test_download_no_symlink_existing_file(self, mock_are_symlinks_supported: Mock) -> None:
+    def test_download_no_symlink_existing_file(
+        self, mock_are_symlinks_supported: Mock
+    ) -> None:
         mock_are_symlinks_supported.return_value = True
         filepath = Path(
             hf_hub_download(
@@ -100,7 +106,9 @@ class TestCacheLayoutIfSymlinksNotSuppor
         self.assertTrue(blob_path.is_file())
 
     @patch("huggingface_hub.file_download.are_symlinks_supported")
-    def test_scan_and_delete_cache_no_symlinks(self, mock_are_symlinks_supported: Mock) -> None:
+    def test_scan_and_delete_cache_no_symlinks(
+        self, mock_are_symlinks_supported: Mock
+    ) -> None:
         """Test scan_cache_dir works as well when cache-system doesn't use symlinks."""
         OLDER_REVISION = "44c70f043cfe8162efc274ff531575e224a0e6f0"
 
@@ -175,7 +183,9 @@ class TestCacheLayoutIfSymlinksNotSuppor
         # Since files are not shared (README.md is duplicated in cache), the total size
         # of the repo is the sum of each revision size. If symlinks were used, the total
         # size of the repo would be lower.
-        self.assertEqual(repo.size_on_disk, main_revision.size_on_disk + older_revision.size_on_disk)
+        self.assertEqual(
+            repo.size_on_disk, main_revision.size_on_disk + older_revision.size_on_disk
+        )
 
         # Test delete repo strategy
         strategy_delete_repo = report.delete_revisions(main_ref, OLDER_REVISION)
@@ -191,7 +201,9 @@ class TestCacheLayoutIfSymlinksNotSuppor
             strategy_delete_revision.blobs,
             {file.blob_path for file in older_revision.files},
         )
-        self.assertEqual(strategy_delete_revision.snapshots, {older_revision.snapshot_path})
+        self.assertEqual(
+            strategy_delete_revision.snapshots, {older_revision.snapshot_path}
+        )
         self.assertEqual(len(strategy_delete_revision.refs), 0)
         self.assertEqual(len(strategy_delete_revision.repos), 0)
         strategy_delete_revision.execute()  # Execute without error
--- python3-huggingface-hub-0.29.3.orig/tests/test_cli.py
+++ python3-huggingface-hub-0.29.3/tests/test_cli.py
@@ -24,7 +24,9 @@ class TestCacheCommand(unittest.TestCase
         """
         Set up scan-cache/delete-cache commands as in `src/huggingface_hub/commands/huggingface_cli.py`.
         """
-        self.parser = ArgumentParser("huggingface-cli", usage="huggingface-cli <command> [<args>]")
+        self.parser = ArgumentParser(
+            "huggingface-cli", usage="huggingface-cli <command> [<args>]"
+        )
         commands_parser = self.parser.add_subparsers()
         ScanCacheCommand.register_subcommand(commands_parser)
         DeleteCacheCommand.register_subcommand(commands_parser)
@@ -69,13 +71,17 @@ class TestUploadCommand(unittest.TestCas
         """
         Set up CLI as in `src/huggingface_hub/commands/huggingface_cli.py`.
         """
-        self.parser = ArgumentParser("huggingface-cli", usage="huggingface-cli <command> [<args>]")
+        self.parser = ArgumentParser(
+            "huggingface-cli", usage="huggingface-cli <command> [<args>]"
+        )
         commands_parser = self.parser.add_subparsers()
         UploadCommand.register_subcommand(commands_parser)
 
     def test_upload_basic(self) -> None:
         """Test `huggingface-cli upload my-folder to dummy-repo`."""
-        cmd = UploadCommand(self.parser.parse_args(["upload", DUMMY_MODEL_ID, "my-folder"]))
+        cmd = UploadCommand(
+            self.parser.parse_args(["upload", DUMMY_MODEL_ID, "my-folder"])
+        )
         self.assertEqual(cmd.repo_id, DUMMY_MODEL_ID)
         self.assertEqual(cmd.local_path, "my-folder")
         self.assertEqual(cmd.path_in_repo, ".")  # implicit
@@ -165,7 +171,9 @@ class TestUploadCommand(unittest.TestCas
         with tmp_current_directory() as cache_dir:
             folder_path = Path(cache_dir) / "my-cool-model"
             folder_path.mkdir()
-            cmd = UploadCommand(self.parser.parse_args(["upload", "my-cool-org/my-cool-model"]))
+            cmd = UploadCommand(
+                self.parser.parse_args(["upload", "my-cool-org/my-cool-model"])
+            )
 
         # A folder with the same name as the repo exists => upload it at the root of the repo
         self.assertEqual(cmd.local_path, "my-cool-model")
@@ -181,54 +189,90 @@ class TestUploadCommand(unittest.TestCas
         with tmp_current_directory() as cache_dir:
             folder_path = Path(cache_dir) / "path" / "to" / "folder"
             folder_path.mkdir(parents=True, exist_ok=True)
-            cmd = UploadCommand(self.parser.parse_args(["upload", "my-repo", "./path/to/folder"]))
+            cmd = UploadCommand(
+                self.parser.parse_args(["upload", "my-repo", "./path/to/folder"])
+            )
         self.assertEqual(cmd.local_path, "./path/to/folder")
-        self.assertEqual(cmd.path_in_repo, ".")  # Always upload the folder at the root of the repo
+        self.assertEqual(
+            cmd.path_in_repo, "."
+        )  # Always upload the folder at the root of the repo
 
     def test_upload_explicit_local_path_to_file_implicit_path_in_repo(self) -> None:
         with tmp_current_directory() as cache_dir:
             file_path = Path(cache_dir) / "path" / "to" / "file.txt"
             file_path.parent.mkdir(parents=True, exist_ok=True)
             file_path.touch()
-            cmd = UploadCommand(self.parser.parse_args(["upload", "my-repo", "./path/to/file.txt"]))
+            cmd = UploadCommand(
+                self.parser.parse_args(["upload", "my-repo", "./path/to/file.txt"])
+            )
         self.assertEqual(cmd.local_path, "./path/to/file.txt")
-        self.assertEqual(cmd.path_in_repo, "file.txt")  # If a file, upload it at the root of the repo and keep name
+        self.assertEqual(
+            cmd.path_in_repo, "file.txt"
+        )  # If a file, upload it at the root of the repo and keep name
 
     def test_upload_explicit_paths(self) -> None:
-        cmd = UploadCommand(self.parser.parse_args(["upload", "my-repo", "./path/to/folder", "data/"]))
+        cmd = UploadCommand(
+            self.parser.parse_args(["upload", "my-repo", "./path/to/folder", "data/"])
+        )
         self.assertEqual(cmd.local_path, "./path/to/folder")
         self.assertEqual(cmd.path_in_repo, "data/")
 
     def test_every_must_be_positive(self) -> None:
         with self.assertRaises(ValueError):
-            UploadCommand(self.parser.parse_args(["upload", DUMMY_MODEL_ID, ".", "--every", "0"]))
+            UploadCommand(
+                self.parser.parse_args(["upload", DUMMY_MODEL_ID, ".", "--every", "0"])
+            )
 
         with self.assertRaises(ValueError):
-            UploadCommand(self.parser.parse_args(["upload", DUMMY_MODEL_ID, ".", "--every", "-10"]))
+            UploadCommand(
+                self.parser.parse_args(
+                    ["upload", DUMMY_MODEL_ID, ".", "--every", "-10"]
+                )
+            )
 
     def test_every_as_int(self) -> None:
-        cmd = UploadCommand(self.parser.parse_args(["upload", DUMMY_MODEL_ID, ".", "--every", "10"]))
+        cmd = UploadCommand(
+            self.parser.parse_args(["upload", DUMMY_MODEL_ID, ".", "--every", "10"])
+        )
         self.assertEqual(cmd.every, 10)
 
     def test_every_as_float(self) -> None:
-        cmd = UploadCommand(self.parser.parse_args(["upload", DUMMY_MODEL_ID, ".", "--every", "0.5"]))
+        cmd = UploadCommand(
+            self.parser.parse_args(["upload", DUMMY_MODEL_ID, ".", "--every", "0.5"])
+        )
         self.assertEqual(cmd.every, 0.5)
 
     @patch("huggingface_hub.commands.upload.HfApi.repo_info")
     @patch("huggingface_hub.commands.upload.HfApi.upload_folder")
     @patch("huggingface_hub.commands.upload.HfApi.create_repo")
-    def test_upload_folder_mock(self, create_mock: Mock, upload_mock: Mock, repo_info_mock: Mock) -> None:
+    def test_upload_folder_mock(
+        self, create_mock: Mock, upload_mock: Mock, repo_info_mock: Mock
+    ) -> None:
         with SoftTemporaryDirectory() as cache_dir:
             cache_path = cache_dir.absolute().as_posix()
             cmd = UploadCommand(
                 self.parser.parse_args(
-                    ["upload", "my-model", cache_path, ".", "--private", "--include", "*.json", "--delete", "*.json"]
+                    [
+                        "upload",
+                        "my-model",
+                        cache_path,
+                        ".",
+                        "--private",
+                        "--include",
+                        "*.json",
+                        "--delete",
+                        "*.json",
+                    ]
                 )
             )
             cmd.run()
 
             create_mock.assert_called_once_with(
-                repo_id="my-model", repo_type="model", exist_ok=True, private=True, space_sdk=None
+                repo_id="my-model",
+                repo_type="model",
+                exist_ok=True,
+                private=True,
+                space_sdk=None,
             )
             upload_mock.assert_called_once_with(
                 folder_path=cache_path,
@@ -247,19 +291,33 @@ class TestUploadCommand(unittest.TestCas
     @patch("huggingface_hub.commands.upload.HfApi.repo_info")
     @patch("huggingface_hub.commands.upload.HfApi.upload_file")
     @patch("huggingface_hub.commands.upload.HfApi.create_repo")
-    def test_upload_file_mock(self, create_mock: Mock, upload_mock: Mock, repo_info_mock: Mock) -> None:
+    def test_upload_file_mock(
+        self, create_mock: Mock, upload_mock: Mock, repo_info_mock: Mock
+    ) -> None:
         with SoftTemporaryDirectory() as cache_dir:
             file_path = Path(cache_dir) / "file.txt"
             file_path.write_text("content")
             cmd = UploadCommand(
                 self.parser.parse_args(
-                    ["upload", "my-dataset", str(file_path), "logs/file.txt", "--repo-type", "dataset", "--create-pr"]
+                    [
+                        "upload",
+                        "my-dataset",
+                        str(file_path),
+                        "logs/file.txt",
+                        "--repo-type",
+                        "dataset",
+                        "--create-pr",
+                    ]
                 )
             )
             cmd.run()
 
             create_mock.assert_called_once_with(
-                repo_id="my-dataset", repo_type="dataset", exist_ok=True, private=False, space_sdk=None
+                repo_id="my-dataset",
+                repo_type="dataset",
+                exist_ok=True,
+                private=False,
+                space_sdk=None,
             )
             upload_mock.assert_called_once_with(
                 path_or_fileobj=str(file_path),
@@ -275,11 +333,17 @@ class TestUploadCommand(unittest.TestCas
     @patch("huggingface_hub.commands.upload.HfApi.repo_info")
     @patch("huggingface_hub.commands.upload.HfApi.upload_file")
     @patch("huggingface_hub.commands.upload.HfApi.create_repo")
-    def test_upload_file_no_revision_mock(self, create_mock: Mock, upload_mock: Mock, repo_info_mock: Mock) -> None:
+    def test_upload_file_no_revision_mock(
+        self, create_mock: Mock, upload_mock: Mock, repo_info_mock: Mock
+    ) -> None:
         with SoftTemporaryDirectory() as cache_dir:
             file_path = Path(cache_dir) / "file.txt"
             file_path.write_text("content")
-            cmd = UploadCommand(self.parser.parse_args(["upload", "my-model", str(file_path), "logs/file.txt"]))
+            cmd = UploadCommand(
+                self.parser.parse_args(
+                    ["upload", "my-model", str(file_path), "logs/file.txt"]
+                )
+            )
             cmd.run()
             # Revision not specified => no need to check
             repo_info_mock.assert_not_called()
@@ -289,7 +353,11 @@ class TestUploadCommand(unittest.TestCas
     @patch("huggingface_hub.commands.upload.HfApi.upload_file")
     @patch("huggingface_hub.commands.upload.HfApi.create_repo")
     def test_upload_file_with_revision_mock(
-        self, create_mock: Mock, upload_mock: Mock, repo_info_mock: Mock, create_branch_mock: Mock
+        self,
+        create_mock: Mock,
+        upload_mock: Mock,
+        repo_info_mock: Mock,
+        create_branch_mock: Mock,
     ) -> None:
         repo_info_mock.side_effect = RevisionNotFoundError("revision not found")
 
@@ -298,19 +366,31 @@ class TestUploadCommand(unittest.TestCas
             file_path.write_text("content")
             cmd = UploadCommand(
                 self.parser.parse_args(
-                    ["upload", "my-model", str(file_path), "logs/file.txt", "--revision", "my-branch"]
+                    [
+                        "upload",
+                        "my-model",
+                        str(file_path),
+                        "logs/file.txt",
+                        "--revision",
+                        "my-branch",
+                    ]
                 )
             )
             cmd.run()
 
             # Revision specified => check that it exists
             repo_info_mock.assert_called_once_with(
-                repo_id=create_mock.return_value.repo_id, repo_type="model", revision="my-branch"
+                repo_id=create_mock.return_value.repo_id,
+                repo_type="model",
+                revision="my-branch",
             )
 
             # Revision does not exist => create it
             create_branch_mock.assert_called_once_with(
-                repo_id=create_mock.return_value.repo_id, repo_type="model", branch="my-branch", exist_ok=True
+                repo_id=create_mock.return_value.repo_id,
+                repo_type="model",
+                branch="my-branch",
+                exist_ok=True,
             )
 
     @patch("huggingface_hub.commands.upload.HfApi.repo_info")
@@ -324,7 +404,15 @@ class TestUploadCommand(unittest.TestCas
             file_path.write_text("content")
             cmd = UploadCommand(
                 self.parser.parse_args(
-                    ["upload", "my-model", str(file_path), "logs/file.txt", "--revision", "my-branch", "--create-pr"]
+                    [
+                        "upload",
+                        "my-model",
+                        str(file_path),
+                        "logs/file.txt",
+                        "--revision",
+                        "my-branch",
+                        "--create-pr",
+                    ]
                 )
             )
             cmd.run()
@@ -333,7 +421,11 @@ class TestUploadCommand(unittest.TestCas
 
     @patch("huggingface_hub.commands.upload.HfApi.create_repo")
     def test_upload_missing_path(self, create_mock: Mock) -> None:
-        cmd = UploadCommand(self.parser.parse_args(["upload", "my-model", "/path/to/missing_file", "logs/file.txt"]))
+        cmd = UploadCommand(
+            self.parser.parse_args(
+                ["upload", "my-model", "/path/to/missing_file", "logs/file.txt"]
+            )
+        )
         with self.assertRaises(FileNotFoundError):
             cmd.run()  # File/folder does not exist locally
 
@@ -346,7 +438,9 @@ class TestDownloadCommand(unittest.TestC
         """
         Set up CLI as in `src/huggingface_hub/commands/huggingface_cli.py`.
         """
-        self.parser = ArgumentParser("huggingface-cli", usage="huggingface-cli <command> [<args>]")
+        self.parser = ArgumentParser(
+            "huggingface-cli", usage="huggingface-cli <command> [<args>]"
+        )
         commands_parser = self.parser.add_subparsers()
         DownloadCommand.register_subcommand(commands_parser)
 
@@ -432,7 +526,9 @@ class TestDownloadCommand(unittest.TestC
         # Output path is printed to terminal once run is completed
         with capture_output() as output:
             DownloadCommand(args).run()
-        self.assertRegex(output.getvalue(), r"<MagicMock name='hf_hub_download\(\)' id='\d+'>")
+        self.assertRegex(
+            output.getvalue(), r"<MagicMock name='hf_hub_download\(\)' id='\d+'>"
+        )
 
         mock.assert_called_once_with(
             repo_id="author/dataset",
@@ -546,7 +642,10 @@ class TestDownloadCommand(unittest.TestC
             repo_id="author/model",
             repo_type="model",
             revision=None,
-            allow_patterns=["README.md", "config.json"],  # `filenames` has priority over the patterns
+            allow_patterns=[
+                "README.md",
+                "config.json",
+            ],  # `filenames` has priority over the patterns
             ignore_patterns=None,  # cleaned up
             resume_download=True,
             force_download=True,
@@ -570,12 +669,16 @@ class TestTagCommands(unittest.TestCase)
         """
         Set up CLI as in `src/huggingface_hub/commands/huggingface_cli.py`.
         """
-        self.parser = ArgumentParser("huggingface-cli", usage="huggingface-cli <command> [<args>]")
+        self.parser = ArgumentParser(
+            "huggingface-cli", usage="huggingface-cli <command> [<args>]"
+        )
         commands_parser = self.parser.add_subparsers()
         TagCommands.register_subcommand(commands_parser)
 
     def test_tag_create_basic(self) -> None:
-        args = self.parser.parse_args(["tag", DUMMY_MODEL_ID, "1.0", "-m", "My tag message"])
+        args = self.parser.parse_args(
+            ["tag", DUMMY_MODEL_ID, "1.0", "-m", "My tag message"]
+        )
         self.assertEqual(args.repo_id, DUMMY_MODEL_ID)
         self.assertEqual(args.tag, "1.0")
         self.assertIsNotNone(args.message)
@@ -643,7 +746,9 @@ class TestRepoFilesCommand(unittest.Test
         """
         Set up CLI as in `src/huggingface_hub/commands/huggingface_cli.py`.
         """
-        self.parser = ArgumentParser("huggingface-cli", usage="huggingface-cli <command> [<args>]")
+        self.parser = ArgumentParser(
+            "huggingface-cli", usage="huggingface-cli <command> [<args>]"
+        )
         commands_parser = self.parser.add_subparsers()
         RepoFilesCommand.register_subcommand(commands_parser)
 
@@ -789,7 +894,9 @@ class TestRepoFilesCommand(unittest.Test
             with self.subTest(expected):
                 delete_files_args = expected["delete_files_args"]
 
-                cmd = DeleteFilesSubCommand(self.parser.parse_args(expected["input_args"]))
+                cmd = DeleteFilesSubCommand(
+                    self.parser.parse_args(expected["input_args"])
+                )
                 cmd.run()
 
                 if delete_files_args is None:
--- python3-huggingface-hub-0.29.3.orig/tests/test_command_delete_cache.py
+++ python3-huggingface-hub-0.29.3/tests/test_command_delete_cache.py
@@ -49,7 +49,9 @@ class TestDeleteCacheHelpers(unittest.Te
 
         # Dataset repo separator
         self.assertIsInstance(choices[1], Separator)
-        self.assertEqual(choices[1]._line, "\nDataset dummy_dataset (8M, used 2 weeks ago)")
+        self.assertEqual(
+            choices[1]._line, "\nDataset dummy_dataset (8M, used 2 weeks ago)"
+        )
 
         # Only revision of `dummy_dataset`
         self.assertIsInstance(choices[2], Choice)
@@ -63,7 +65,9 @@ class TestDeleteCacheHelpers(unittest.Te
 
         # Model `dummy_model` separator
         self.assertIsInstance(choices[3], Separator)
-        self.assertEqual(choices[3]._line, "\nModel dummy_model (1.4K, used 2 years ago)")
+        self.assertEqual(
+            choices[3]._line, "\nModel dummy_model (1.4K, used 2 years ago)"
+        )
 
         # Oldest revision of `dummy_model`
         self.assertIsInstance(choices[4], Choice)
@@ -84,7 +88,9 @@ class TestDeleteCacheHelpers(unittest.Te
         # Only revision of `gpt2`
         self.assertIsInstance(choices[7], Choice)
         self.assertEqual(choices[7].value, "abcdef123456789")
-        self.assertEqual(choices[7].name, "abcdef12: main, refs/pr/1 # modified 2 years ago")
+        self.assertEqual(
+            choices[7].name, "abcdef12: main, refs/pr/1 # modified 2 years ago"
+        )
         self.assertFalse(choices[7].enabled)
 
     def test_get_expectations_str_on_no_deletion_item(self) -> None:
@@ -177,7 +183,9 @@ class TestDeleteCacheHelpers(unittest.Te
             self.assertIn("#    recent_hash_id", content)
 
             # Select dataset revision
-            content = content.replace("#    dataset_revision_hash_id", "dataset_revision_hash_id")
+            content = content.replace(
+                "#    dataset_revision_hash_id", "dataset_revision_hash_id"
+            )
             # Deselect abcdef123456789
             content = content.replace("abcdef123456789", "# abcdef123456789")
             with open(tmp_path, "w") as f:
@@ -200,7 +208,9 @@ class TestDeleteCacheHelpers(unittest.Te
         self.assertFalse(os.path.isfile(tmp_path))  # now deleted
 
         # User changed the selection
-        self.assertListEqual(selected_hashes, ["dataset_revision_hash_id", "older_hash_id"])
+        self.assertListEqual(
+            selected_hashes, ["dataset_revision_hash_id", "older_hash_id"]
+        )
 
         # Check printed instructions
         printed = output.getvalue()
@@ -283,8 +293,12 @@ class TestMockedDeleteCacheCommand(unitt
         mock__manual_review_tui.assert_called_once_with(cache_mock, preselected=[])
 
         # Step 3: ask confirmation
-        mock__get_expectations_str.assert_called_once_with(cache_mock, ["hash_1", "hash_2"])
-        mock_confirm.assert_called_once_with("Will delete A and B. Confirm deletion ?", default=True)
+        mock__get_expectations_str.assert_called_once_with(
+            cache_mock, ["hash_1", "hash_2"]
+        )
+        mock_confirm.assert_called_once_with(
+            "Will delete A and B. Confirm deletion ?", default=True
+        )
         mock_confirm().execute.assert_called_once_with()
 
         # Step 4: delete
@@ -311,7 +325,9 @@ class TestMockedDeleteCacheCommand(unitt
         # Check output
         self.assertEqual(output.getvalue(), "Deletion is cancelled. Do nothing.\n")
 
-    def test_run_stuff_selected_but_cancel_item_as_well_with_tui(self, mock__manual_review_tui: Mock) -> None:
+    def test_run_stuff_selected_but_cancel_item_as_well_with_tui(
+        self, mock__manual_review_tui: Mock
+    ) -> None:
         """Test command run when some are selected but "cancel item" as well."""
         # Mock return value
         mock__manual_review_tui.return_value = [
@@ -355,8 +371,12 @@ class TestMockedDeleteCacheCommand(unitt
         mock__manual_review_no_tui.assert_called_once_with(cache_mock, preselected=[])
 
         # Step 3: ask confirmation
-        mock__get_expectations_str.assert_called_once_with(cache_mock, ["hash_1", "hash_2"])
-        mock__ask_for_confirmation_no_tui.assert_called_once_with("Will delete A and B. Confirm deletion ?")
+        mock__get_expectations_str.assert_called_once_with(
+            cache_mock, ["hash_1", "hash_2"]
+        )
+        mock__ask_for_confirmation_no_tui.assert_called_once_with(
+            "Will delete A and B. Confirm deletion ?"
+        )
 
         # Step 4: delete
         cache_mock.delete_revisions.assert_called_once_with("hash_1", "hash_2")
--- python3-huggingface-hub-0.29.3.orig/tests/test_commit_api.py
+++ python3-huggingface-hub-0.29.3/tests/test_commit_api.py
@@ -10,22 +10,42 @@ from huggingface_hub._commit_api import
 class TestCommitOperationDelete(unittest.TestCase):
     def test_implicit_file(self):
         self.assertFalse(CommitOperationDelete(path_in_repo="path/to/file").is_folder)
-        self.assertFalse(CommitOperationDelete(path_in_repo="path/to/file.md").is_folder)
+        self.assertFalse(
+            CommitOperationDelete(path_in_repo="path/to/file.md").is_folder
+        )
 
     def test_implicit_folder(self):
         self.assertTrue(CommitOperationDelete(path_in_repo="path/to/folder/").is_folder)
-        self.assertTrue(CommitOperationDelete(path_in_repo="path/to/folder.md/").is_folder)
+        self.assertTrue(
+            CommitOperationDelete(path_in_repo="path/to/folder.md/").is_folder
+        )
 
     def test_explicit_file(self):
         # Weird case: if user explicitly set as file (`is_folder`=False) but path has a
         # trailing "/" => user input has priority
-        self.assertFalse(CommitOperationDelete(path_in_repo="path/to/folder/", is_folder=False).is_folder)
-        self.assertFalse(CommitOperationDelete(path_in_repo="path/to/folder.md/", is_folder=False).is_folder)
+        self.assertFalse(
+            CommitOperationDelete(
+                path_in_repo="path/to/folder/", is_folder=False
+            ).is_folder
+        )
+        self.assertFalse(
+            CommitOperationDelete(
+                path_in_repo="path/to/folder.md/", is_folder=False
+            ).is_folder
+        )
 
     def test_explicit_folder(self):
         # No need for the trailing "/" is `is_folder` explicitly passed
-        self.assertTrue(CommitOperationDelete(path_in_repo="path/to/folder", is_folder=True).is_folder)
-        self.assertTrue(CommitOperationDelete(path_in_repo="path/to/folder.md", is_folder=True).is_folder)
+        self.assertTrue(
+            CommitOperationDelete(
+                path_in_repo="path/to/folder", is_folder=True
+            ).is_folder
+        )
+        self.assertTrue(
+            CommitOperationDelete(
+                path_in_repo="path/to/folder.md", is_folder=True
+            ).is_folder
+        )
 
     def test_is_folder_wrong_value(self):
         with self.assertRaises(ValueError):
@@ -44,8 +64,15 @@ class TestCommitOperationPathInRepo(unit
     def test_path_in_repo_valid(self) -> None:
         for input, expected in self.valid_values.items():
             with self.subTest(f"Testing with valid input: '{input}'"):
-                self.assertEqual(CommitOperationAdd(path_in_repo=input, path_or_fileobj=b"").path_in_repo, expected)
-                self.assertEqual(CommitOperationDelete(path_in_repo=input).path_in_repo, expected)
+                self.assertEqual(
+                    CommitOperationAdd(
+                        path_in_repo=input, path_or_fileobj=b""
+                    ).path_in_repo,
+                    expected,
+                )
+                self.assertEqual(
+                    CommitOperationDelete(path_in_repo=input).path_in_repo, expected
+                )
 
     def test_path_in_repo_invalid(self) -> None:
         for input in self.invalid_values:
@@ -107,7 +134,9 @@ class TestWarnOnOverwritingOperations(un
     add_file_ab = CommitOperationAdd(path_in_repo="a/b.txt", path_or_fileobj=b"data")
     add_file_abc = CommitOperationAdd(path_in_repo="a/b/c.md", path_or_fileobj=b"data")
     add_file_abd = CommitOperationAdd(path_in_repo="a/b/d.md", path_or_fileobj=b"data")
-    update_file_abc = CommitOperationAdd(path_in_repo="a/b/c.md", path_or_fileobj=b"updated data")
+    update_file_abc = CommitOperationAdd(
+        path_in_repo="a/b/c.md", path_or_fileobj=b"updated data"
+    )
     delete_file_abc = CommitOperationDelete(path_in_repo="a/b/c.md")
     delete_folder_a = CommitOperationDelete(path_in_repo="a/")
     delete_folder_e = CommitOperationDelete(path_in_repo="e/")
@@ -141,4 +170,6 @@ class TestWarnOnOverwritingOperations(un
         _warn_on_overwriting_operations([self.delete_file_abc, self.add_file_abc])
 
     def test_delete_folder_then_add(self) -> None:
-        _warn_on_overwriting_operations([self.delete_folder_a, self.add_file_ab, self.add_file_abc])
+        _warn_on_overwriting_operations(
+            [self.delete_folder_a, self.add_file_ab, self.add_file_abc]
+        )
--- python3-huggingface-hub-0.29.3.orig/tests/test_commit_scheduler.py
+++ python3-huggingface-hub-0.29.3/tests/test_commit_scheduler.py
@@ -46,7 +46,9 @@ class TestCommitScheduler(unittest.TestC
         self.assertGreater(len(push_to_hub_mock.call_args_list), 2)
 
         # Can get the last upload result
-        self.assertEqual(self.scheduler.last_future.result(), push_to_hub_mock.return_value)
+        self.assertEqual(
+            self.scheduler.last_future.result(), push_to_hub_mock.return_value
+        )
 
     def test_invalid_folder_path_is_a_file(self) -> None:
         """Test cannot scheduler upload of a single file."""
@@ -54,11 +56,15 @@ class TestCommitScheduler(unittest.TestC
         file_path.write_text("something")
 
         with self.assertRaises(ValueError):
-            CommitScheduler(folder_path=file_path, repo_id=self.repo_name, hf_api=self.api)
+            CommitScheduler(
+                folder_path=file_path, repo_id=self.repo_name, hf_api=self.api
+            )
 
     def test_missing_folder_is_created(self) -> None:
         folder_path = self.cache_dir / "folder" / "subfolder"
-        self.scheduler = CommitScheduler(folder_path=folder_path, repo_id=self.repo_name, hf_api=self.api)
+        self.scheduler = CommitScheduler(
+            folder_path=folder_path, repo_id=self.repo_name, hf_api=self.api
+        )
         self.assertTrue(folder_path.is_dir())
 
     def test_sync_local_folder(self) -> None:
@@ -114,7 +120,14 @@ class TestCommitScheduler(unittest.TestC
         push_3 = commits[0].commit_id
 
         def _download(filename: str, revision: str) -> Path:
-            return Path(hf_hub_download(repo_id=repo_id, filename=filename, cache_dir=hub_cache, revision=revision))
+            return Path(
+                hf_hub_download(
+                    repo_id=repo_id,
+                    filename=filename,
+                    cache_dir=hub_cache,
+                    revision=revision,
+                )
+            )
 
         # Check file.txt consistency
         file_push1 = _download(filename="file.txt", revision=push_1)
@@ -158,7 +171,9 @@ class TestCommitScheduler(unittest.TestC
         # Branch history has been squashed
         commits = self.api.list_repo_commits(repo_id=self.scheduler.repo_id)
         self.assertEqual(len(commits), 1)
-        self.assertEqual(commits[0].title, "Super-squash branch 'main' using huggingface_hub")
+        self.assertEqual(
+            commits[0].title, "Super-squash branch 'main' using huggingface_hub"
+        )
 
     def test_context_manager(self) -> None:
         watched_folder = self.cache_dir / "watched_folder"
@@ -175,7 +190,9 @@ class TestCommitScheduler(unittest.TestC
                 f.write("first line\n")
 
         assert "file.txt" in self.api.list_repo_files(scheduler.repo_id)
-        assert scheduler._CommitScheduler__stopped  # means the scheduler has been stopped when exiting the context
+        assert (
+            scheduler._CommitScheduler__stopped
+        )  # means the scheduler has been stopped when exiting the context
 
 
 @pytest.mark.usefixtures("fx_cache_dir")
@@ -264,7 +281,8 @@ class TestPartialFileIO(unittest.TestCas
     def test_with_commit_operation_add(self) -> None:
         # Truncated file
         op_truncated = CommitOperationAdd(
-            path_or_fileobj=PartialFileIO(self.file_path, size_limit=5), path_in_repo="file.txt"
+            path_or_fileobj=PartialFileIO(self.file_path, size_limit=5),
+            path_in_repo="file.txt",
         )
         self.assertEqual(op_truncated.upload_info.size, 5)
         self.assertEqual(op_truncated.upload_info.sample, b"12345")
@@ -274,7 +292,8 @@ class TestPartialFileIO(unittest.TestCas
 
         # Full file
         op_full = CommitOperationAdd(
-            path_or_fileobj=PartialFileIO(self.file_path, size_limit=9), path_in_repo="file.txt"
+            path_or_fileobj=PartialFileIO(self.file_path, size_limit=9),
+            path_in_repo="file.txt",
         )
         self.assertEqual(op_full.upload_info.size, 9)
         self.assertEqual(op_full.upload_info.sample, b"123456789")
--- python3-huggingface-hub-0.29.3.orig/tests/test_dduf.py
+++ python3-huggingface-hub-0.29.3/tests/test_dduf.py
@@ -6,7 +6,11 @@ from typing import Iterable, Tuple, Unio
 import pytest
 from pytest_mock import MockerFixture
 
-from huggingface_hub.errors import DDUFCorruptedFileError, DDUFExportError, DDUFInvalidEntryNameError
+from huggingface_hub.errors import (
+    DDUFCorruptedFileError,
+    DDUFExportError,
+    DDUFInvalidEntryNameError,
+)
 from huggingface_hub.serialization._dduf import (
     DDUFEntry,
     _load_content,
@@ -23,7 +27,9 @@ class TestDDUFEntry:
     def dummy_entry(self, tmp_path: Path) -> DDUFEntry:
         dummy_dduf = tmp_path / "dummy_dduf.dduf"
         dummy_dduf.write_bytes(b"somethingCONTENTsomething")
-        return DDUFEntry(filename="dummy.json", length=7, offset=9, dduf_path=dummy_dduf)
+        return DDUFEntry(
+            filename="dummy.json", length=7, offset=9, dduf_path=dummy_dduf
+        )
 
     def test_dataclass(self, dummy_entry: DDUFEntry):
         assert dummy_entry.filename == "dummy.json"
@@ -40,11 +46,15 @@ class TestDDUFEntry:
 
 
 class TestUtils:
-    @pytest.mark.parametrize("filename", ["dummy.txt", "dummy.json", "dummy.safetensors"])
+    @pytest.mark.parametrize(
+        "filename", ["dummy.txt", "dummy.json", "dummy.safetensors"]
+    )
     def test_entry_name_valid_extension(self, filename: str):
         assert _validate_dduf_entry_name(filename) == filename
 
-    @pytest.mark.parametrize("filename", ["dummy", "dummy.bin", "dummy.dduf", "dummy.gguf"])
+    @pytest.mark.parametrize(
+        "filename", ["dummy", "dummy.bin", "dummy.dduf", "dummy.gguf"]
+    )
     def test_entry_name_invalid_extension(self, filename: str):
         with pytest.raises(DDUFInvalidEntryNameError):
             _validate_dduf_entry_name(filename)
@@ -92,14 +102,23 @@ class TestUtils:
 
     def test_validate_dduf_structure_not_a_dict(self):
         with pytest.raises(DDUFCorruptedFileError, match="Must be a dictionary."):
-            _validate_dduf_structure(["not a dict"], {})  # content from 'model_index.json'
+            _validate_dduf_structure(
+                ["not a dict"], {}
+            )  # content from 'model_index.json'
 
     def test_validate_dduf_structure_missing_folder(self):
-        with pytest.raises(DDUFCorruptedFileError, match="Missing required entry 'encoder' in 'model_index.json'."):
-            _validate_dduf_structure({}, {"encoder/config.json", "encoder/model.safetensors"})
+        with pytest.raises(
+            DDUFCorruptedFileError,
+            match="Missing required entry 'encoder' in 'model_index.json'.",
+        ):
+            _validate_dduf_structure(
+                {}, {"encoder/config.json", "encoder/model.safetensors"}
+            )
 
     def test_validate_dduf_structure_missing_config_file(self):
-        with pytest.raises(DDUFCorruptedFileError, match="Missing required file in folder 'encoder'."):
+        with pytest.raises(
+            DDUFCorruptedFileError, match="Missing required file in folder 'encoder'."
+        ):
             _validate_dduf_structure(
                 {"encoder": {}},
                 {
@@ -129,7 +148,9 @@ class TestExportFolder:
         return folder_path
 
     def test_export_folder(self, dummy_folder: Path, mocker: MockerFixture):
-        mock = mocker.patch("huggingface_hub.serialization._dduf.export_entries_as_dduf")
+        mock = mocker.patch(
+            "huggingface_hub.serialization._dduf.export_entries_as_dduf"
+        )
         export_folder_as_dduf("dummy.dduf", dummy_folder)
         mock.assert_called_once()
         args = mock.call_args_list[0].args
@@ -146,33 +167,53 @@ class TestExportFolder:
 
 class TestExportEntries:
     @pytest.fixture
-    def dummy_entries(self, tmp_path: Path) -> Iterable[Tuple[str, Union[str, Path, bytes]]]:
+    def dummy_entries(
+        self, tmp_path: Path
+    ) -> Iterable[Tuple[str, Union[str, Path, bytes]]]:
         (tmp_path / "model_index.json").write_text(json.dumps({"foo": "bar"}))
-        (tmp_path / "doesnt_have_to_be_same_name.safetensors").write_bytes(b"this is safetensors content")
+        (tmp_path / "doesnt_have_to_be_same_name.safetensors").write_bytes(
+            b"this is safetensors content"
+        )
 
         return [
             ("model_index.json", str(tmp_path / "model_index.json")),  # string path
-            ("model.safetensors", tmp_path / "doesnt_have_to_be_same_name.safetensors"),  # pathlib path
+            (
+                "model.safetensors",
+                tmp_path / "doesnt_have_to_be_same_name.safetensors",
+            ),  # pathlib path
             ("hello.txt", b"hello world"),  # raw bytes
         ]
 
     def test_export_entries(
-        self, tmp_path: Path, dummy_entries: Iterable[Tuple[str, Union[str, Path, bytes]]], mocker: MockerFixture
+        self,
+        tmp_path: Path,
+        dummy_entries: Iterable[Tuple[str, Union[str, Path, bytes]]],
+        mocker: MockerFixture,
     ):
-        mock = mocker.patch("huggingface_hub.serialization._dduf._validate_dduf_structure")
+        mock = mocker.patch(
+            "huggingface_hub.serialization._dduf._validate_dduf_structure"
+        )
         export_entries_as_dduf(tmp_path / "dummy.dduf", dummy_entries)
-        mock.assert_called_once_with({"foo": "bar"}, {"model_index.json", "model.safetensors", "hello.txt"})
+        mock.assert_called_once_with(
+            {"foo": "bar"}, {"model_index.json", "model.safetensors", "hello.txt"}
+        )
 
         with zipfile.ZipFile(tmp_path / "dummy.dduf", "r") as archive:
             assert archive.compression == zipfile.ZIP_STORED  # uncompressed!
-            assert archive.namelist() == ["model_index.json", "model.safetensors", "hello.txt"]
+            assert archive.namelist() == [
+                "model_index.json",
+                "model.safetensors",
+                "hello.txt",
+            ]
             assert archive.read("model_index.json") == b'{"foo": "bar"}'
             assert archive.read("model.safetensors") == b"this is safetensors content"
             assert archive.read("hello.txt") == b"hello world"
 
     def test_export_entries_invalid_name(self, tmp_path: Path):
         with pytest.raises(DDUFExportError, match="Invalid entry name") as e:
-            export_entries_as_dduf(tmp_path / "dummy.dduf", [("config", "model_index.json")])
+            export_entries_as_dduf(
+                tmp_path / "dummy.dduf", [("config", "model_index.json")]
+            )
         assert isinstance(e.value.__cause__, DDUFInvalidEntryNameError)
 
     def test_export_entries_no_duplicate(self, tmp_path: Path):
@@ -186,8 +227,12 @@ class TestExportEntries:
             )
 
     def test_export_entries_model_index_required(self, tmp_path: Path):
-        with pytest.raises(DDUFExportError, match="Missing required 'model_index.json' entry"):
-            export_entries_as_dduf(tmp_path / "dummy.dduf", [("model.safetensors", b"content")])
+        with pytest.raises(
+            DDUFExportError, match="Missing required 'model_index.json' entry"
+        ):
+            export_entries_as_dduf(
+                tmp_path / "dummy.dduf", [("model.safetensors", b"content")]
+            )
 
 
 class TestReadDDUFFile:
@@ -200,7 +245,9 @@ class TestReadDDUFFile:
         return tmp_path / "dummy.dduf"
 
     def test_read_dduf_file(self, dummy_dduf_file: Path, mocker: MockerFixture):
-        mock = mocker.patch("huggingface_hub.serialization._dduf._validate_dduf_structure")
+        mock = mocker.patch(
+            "huggingface_hub.serialization._dduf._validate_dduf_structure"
+        )
 
         entries = read_dduf_file(dummy_dduf_file)
         assert len(entries) == 3
@@ -208,7 +255,9 @@ class TestReadDDUFFile:
         model_entry = entries["model.safetensors"]
         hello_entry = entries["hello.txt"]
 
-        mock.assert_called_once_with({"foo": "bar"}, {"model_index.json", "model.safetensors", "hello.txt"})
+        mock.assert_called_once_with(
+            {"foo": "bar"}, {"model_index.json", "model.safetensors", "hello.txt"}
+        )
 
         assert index_entry.filename == "model_index.json"
         assert index_entry.dduf_path == dummy_dduf_file
@@ -234,5 +283,7 @@ class TestReadDDUFFile:
     def test_model_index_required(self, tmp_path: Path):
         with zipfile.ZipFile(tmp_path / "dummy.dduf", "w") as archive:
             archive.writestr("model.safetensors", b"this is safetensors content")
-        with pytest.raises(DDUFCorruptedFileError, match="Missing required 'model_index.json' entry"):
+        with pytest.raises(
+            DDUFCorruptedFileError, match="Missing required 'model_index.json' entry"
+        ):
             read_dduf_file(tmp_path / "dummy.dduf")
--- python3-huggingface-hub-0.29.3.orig/tests/test_fastai_integration.py
+++ python3-huggingface-hub-0.29.3/tests/test_fastai_integration.py
@@ -19,7 +19,9 @@ from .testing_utils import repo_name
 
 
 WORKING_REPO_SUBDIR = f"fixtures/working_repo_{__name__.split('.')[-1]}"
-WORKING_REPO_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), WORKING_REPO_SUBDIR)
+WORKING_REPO_DIR = os.path.join(
+    os.path.dirname(os.path.abspath(__file__)), WORKING_REPO_SUBDIR
+)
 
 if is_fastai_available():
     from fastai.data.block import DataBlock
@@ -84,7 +86,9 @@ class TestFastaiUtils(TestCase):
         api = HfApi(endpoint=ENDPOINT_STAGING, token=TOKEN)
         repo_id = f"{USER}/{repo_name()}"
 
-        push_to_hub_fastai(learner=dummy_model, repo_id=repo_id, token=TOKEN, config=dummy_config)
+        push_to_hub_fastai(
+            learner=dummy_model, repo_id=repo_id, token=TOKEN, config=dummy_config
+        )
         model_info = api.model_info(repo_id)
         assert model_info.id == repo_id
         loaded_model = from_pretrained_fastai(repo_id)
--- python3-huggingface-hub-0.29.3.orig/tests/test_file_download.py
+++ python3-huggingface-hub-0.29.3/tests/test_file_download.py
@@ -29,7 +29,11 @@ from requests import Response
 import huggingface_hub.file_download
 from huggingface_hub import HfApi, RepoUrl, constants
 from huggingface_hub._local_folder import write_download_metadata
-from huggingface_hub.errors import EntryNotFoundError, GatedRepoError, LocalEntryNotFoundError
+from huggingface_hub.errors import (
+    EntryNotFoundError,
+    GatedRepoError,
+    LocalEntryNotFoundError,
+)
 from huggingface_hub.file_download import (
     _CACHED_NO_EXIST,
     HfFileMetadata,
@@ -44,7 +48,11 @@ from huggingface_hub.file_download impor
     http_get,
     try_to_load_from_cache,
 )
-from huggingface_hub.utils import SoftTemporaryDirectory, get_session, hf_raise_for_status
+from huggingface_hub.utils import (
+    SoftTemporaryDirectory,
+    get_session,
+    hf_raise_for_status,
+)
 
 from .testing_constants import ENDPOINT_STAGING, OTHER_TOKEN, TOKEN
 from .testing_utils import (
@@ -85,7 +93,9 @@ class TestDiskUsageWarning(unittest.Test
         with warnings.catch_warnings(record=True) as w:
             # Cause all warnings to always be triggered.
             warnings.simplefilter("always")
-            _check_disk_space(expected_size=self.expected_size, target_dir=disk_usage_mock)
+            _check_disk_space(
+                expected_size=self.expected_size, target_dir=disk_usage_mock
+            )
             assert len(w) == 1
             assert issubclass(w[-1].category, UserWarning)
 
@@ -94,7 +104,9 @@ class TestDiskUsageWarning(unittest.Test
         with warnings.catch_warnings(record=True) as w:
             # Cause all warnings to always be triggered.
             warnings.simplefilter("always")
-            _check_disk_space(expected_size=self.expected_size, target_dir=disk_usage_mock)
+            _check_disk_space(
+                expected_size=self.expected_size, target_dir=disk_usage_mock
+            )
             assert len(w) == 0
 
     def test_disk_usage_warning_with_non_existent_path(self) -> None:
@@ -102,14 +114,19 @@ class TestDiskUsageWarning(unittest.Test
         with warnings.catch_warnings(record=True) as w:
             # Cause all warnings to always be triggered.
             warnings.simplefilter("always")
-            _check_disk_space(expected_size=self.expected_size, target_dir="path/to/not_existent_path")
+            _check_disk_space(
+                expected_size=self.expected_size, target_dir="path/to/not_existent_path"
+            )
             assert len(w) == 0
 
         # Test for not existent (relative) path
         with warnings.catch_warnings(record=True) as w:
             # Cause all warnings to always be triggered.
             warnings.simplefilter("always")
-            _check_disk_space(expected_size=self.expected_size, target_dir="/path/to/not_existent_path")
+            _check_disk_space(
+                expected_size=self.expected_size,
+                target_dir="/path/to/not_existent_path",
+            )
             assert len(w) == 0
 
 
@@ -117,7 +134,9 @@ class StagingDownloadTests(unittest.Test
     _api = HfApi(endpoint=ENDPOINT_STAGING, token=TOKEN)
 
     @use_tmp_repo()
-    def test_download_from_a_gated_repo_with_hf_hub_download(self, repo_url: RepoUrl) -> None:
+    def test_download_from_a_gated_repo_with_hf_hub_download(
+        self, repo_url: RepoUrl
+    ) -> None:
         """Checks `hf_hub_download` outputs error on gated repo.
 
         Regression test for #1121.
@@ -136,14 +155,20 @@ class StagingDownloadTests(unittest.Test
         # Cannot download file as repo is gated
         with SoftTemporaryDirectory() as tmpdir:
             with self.assertRaisesRegex(
-                GatedRepoError, "Access to model .* is restricted and you are not in the authorized list"
+                GatedRepoError,
+                "Access to model .* is restricted and you are not in the authorized list",
             ):
                 hf_hub_download(
-                    repo_id=repo_url.repo_id, filename=".gitattributes", token=OTHER_TOKEN, cache_dir=tmpdir
+                    repo_id=repo_url.repo_id,
+                    filename=".gitattributes",
+                    token=OTHER_TOKEN,
+                    cache_dir=tmpdir,
                 )
 
     @use_tmp_repo()
-    def test_download_regular_file_from_private_renamed_repo(self, repo_url: RepoUrl) -> None:
+    def test_download_regular_file_from_private_renamed_repo(
+        self, repo_url: RepoUrl
+    ) -> None:
         """Regression test for #1999.
 
         See https://github.com/huggingface/huggingface_hub/pull/1999.
@@ -153,7 +178,9 @@ class StagingDownloadTests(unittest.Test
 
         # Make private + rename + upload regular file
         self._api.update_repo_settings(repo_id_before, private=True)
-        self._api.upload_file(repo_id=repo_id_before, path_in_repo="file.txt", path_or_fileobj=b"content")
+        self._api.upload_file(
+            repo_id=repo_id_before, path_in_repo="file.txt", path_or_fileobj=b"content"
+        )
         self._api.move_repo(repo_id_before, repo_id_after)
 
         # Download from private renamed repo
@@ -192,15 +219,26 @@ class CachedDownloadTests(unittest.TestC
 
     def test_private_repo_and_file_cached_locally(self):
         api = HfApi(endpoint=ENDPOINT_STAGING)
-        repo_id = api.create_repo(repo_id=repo_name(), private=True, token=TOKEN).repo_id
-        api.upload_file(path_or_fileobj=b"content", path_in_repo="config.json", repo_id=repo_id, token=TOKEN)
+        repo_id = api.create_repo(
+            repo_id=repo_name(), private=True, token=TOKEN
+        ).repo_id
+        api.upload_file(
+            path_or_fileobj=b"content",
+            path_in_repo="config.json",
+            repo_id=repo_id,
+            token=TOKEN,
+        )
 
         with SoftTemporaryDirectory() as tmpdir:
             # Download a first time with token => file is cached
-            filepath_1 = api.hf_hub_download(repo_id, filename="config.json", cache_dir=tmpdir, token=TOKEN)
+            filepath_1 = api.hf_hub_download(
+                repo_id, filename="config.json", cache_dir=tmpdir, token=TOKEN
+            )
 
             # Download without token => return cached file
-            filepath_2 = api.hf_hub_download(repo_id, filename="config.json", cache_dir=tmpdir, token=False)
+            filepath_2 = api.hf_hub_download(
+                repo_id, filename="config.json", cache_dir=tmpdir, token=False
+            )
 
             assert filepath_1 == filepath_2
 
@@ -212,13 +250,17 @@ class CachedDownloadTests(unittest.TestC
         # Valid file but missing locally and network is disabled.
         with SoftTemporaryDirectory() as tmpdir:
             # Download a first time to get the refs ok
-            hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=tmpdir)
+            hf_hub_download(
+                DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=tmpdir
+            )
 
             # Set read-only permission recursively
             _recursive_chmod(tmpdir, 0o555)
 
             # Get without write-access must succeed
-            hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=tmpdir)
+            hf_hub_download(
+                DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=tmpdir
+            )
 
             # Set permission back for cleanup
             _recursive_chmod(tmpdir, 0o777)
@@ -235,7 +277,9 @@ class CachedDownloadTests(unittest.TestC
             # Equivalent to umask u=rwx,g=r,o=
             previous_umask = os.umask(0o037)
             try:
-                filepath = hf_hub_download(DUMMY_RENAMED_OLD_MODEL_ID, "config.json", cache_dir=tmpdir)
+                filepath = hf_hub_download(
+                    DUMMY_RENAMED_OLD_MODEL_ID, "config.json", cache_dir=tmpdir
+                )
                 # Permissions are honored (640: u=rw,g=r,o=)
                 self.assertEqual(stat.S_IMODE(os.stat(filepath).st_mode), 0o640)
             finally:
@@ -248,7 +292,9 @@ class CachedDownloadTests(unittest.TestC
         https://github.com/huggingface/huggingface_hub/issues/981
         """
         with SoftTemporaryDirectory() as tmpdir:
-            filepath = hf_hub_download(DUMMY_RENAMED_OLD_MODEL_ID, "config.json", cache_dir=tmpdir)
+            filepath = hf_hub_download(
+                DUMMY_RENAMED_OLD_MODEL_ID, "config.json", cache_dir=tmpdir
+            )
             self.assertTrue(os.path.exists(filepath))
 
     def test_hf_hub_download_with_empty_subfolder(self):
@@ -304,7 +350,9 @@ class CachedDownloadTests(unittest.TestC
             assert "foo/bar" in headers["user-agent"]
 
         with SoftTemporaryDirectory() as cache_dir:
-            with patch("huggingface_hub.file_download._request_wrapper", wraps=_request_wrapper) as mock_request:
+            with patch(
+                "huggingface_hub.file_download._request_wrapper", wraps=_request_wrapper
+            ) as mock_request:
                 # First download
                 hf_hub_download(
                     DUMMY_MODEL_ID,
@@ -319,7 +367,9 @@ class CachedDownloadTests(unittest.TestC
                 for call in calls:
                     _check_user_agent(call.kwargs["headers"])
 
-            with patch("huggingface_hub.file_download._request_wrapper", wraps=_request_wrapper) as mock_request:
+            with patch(
+                "huggingface_hub.file_download._request_wrapper", wraps=_request_wrapper
+            ) as mock_request:
                 # Second download: no GET call
                 hf_hub_download(
                     DUMMY_MODEL_ID,
@@ -372,10 +422,14 @@ class CachedDownloadTests(unittest.TestC
         # Make sure the file is cached
         filepath = hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME)
 
-        new_file_path = try_to_load_from_cache(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME)
+        new_file_path = try_to_load_from_cache(
+            DUMMY_MODEL_ID, filename=constants.CONFIG_NAME
+        )
         self.assertEqual(filepath, new_file_path)
 
-        new_file_path = try_to_load_from_cache(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, revision="main")
+        new_file_path = try_to_load_from_cache(
+            DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, revision="main"
+        )
         self.assertEqual(filepath, new_file_path)
 
         # If file is not cached, returns None
@@ -389,21 +443,35 @@ class CachedDownloadTests(unittest.TestC
             )
         )
         # Same for uncached models
-        self.assertIsNone(try_to_load_from_cache("bert-base", filename=constants.CONFIG_NAME))
+        self.assertIsNone(
+            try_to_load_from_cache("bert-base", filename=constants.CONFIG_NAME)
+        )
 
     def test_try_to_load_from_cache_specific_pr_revision_exists(self):
         # Make sure the file is cached
-        file_path = hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, revision="refs/pr/1")
+        file_path = hf_hub_download(
+            DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, revision="refs/pr/1"
+        )
 
-        new_file_path = try_to_load_from_cache(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, revision="refs/pr/1")
+        new_file_path = try_to_load_from_cache(
+            DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, revision="refs/pr/1"
+        )
         self.assertEqual(file_path, new_file_path)
 
         # If file is not cached, returns None
-        self.assertIsNone(try_to_load_from_cache(DUMMY_MODEL_ID, filename="conf.json", revision="refs/pr/1"))
+        self.assertIsNone(
+            try_to_load_from_cache(
+                DUMMY_MODEL_ID, filename="conf.json", revision="refs/pr/1"
+            )
+        )
 
         # If revision does not exist, returns None
         self.assertIsNone(
-            try_to_load_from_cache(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, revision="does-not-exist")
+            try_to_load_from_cache(
+                DUMMY_MODEL_ID,
+                filename=constants.CONFIG_NAME,
+                revision="does-not-exist",
+            )
         )
 
     def test_try_to_load_from_cache_no_exist(self):
@@ -414,7 +482,9 @@ class CachedDownloadTests(unittest.TestC
         new_file_path = try_to_load_from_cache(DUMMY_MODEL_ID, filename="dummy")
         self.assertEqual(new_file_path, _CACHED_NO_EXIST)
 
-        new_file_path = try_to_load_from_cache(DUMMY_MODEL_ID, filename="dummy", revision="main")
+        new_file_path = try_to_load_from_cache(
+            DUMMY_MODEL_ID, filename="dummy", revision="main"
+        )
         self.assertEqual(new_file_path, _CACHED_NO_EXIST)
 
         # If file non-existence is not cached, returns None
@@ -477,7 +547,9 @@ class CachedDownloadTests(unittest.TestC
         metadata = get_hf_file_metadata(url)
 
         # Metadata
-        self.assertEqual(metadata.commit_hash, DUMMY_MODEL_ID_REVISION_ONE_SPECIFIC_COMMIT)
+        self.assertEqual(
+            metadata.commit_hash, DUMMY_MODEL_ID_REVISION_ONE_SPECIFIC_COMMIT
+        )
         self.assertIsNotNone(metadata.etag)  # example: "85c2fc2dcdd86563aaa85ef4911..."
         self.assertEqual(metadata.location, url)  # no redirect
         self.assertEqual(metadata.size, 851)
@@ -525,9 +597,16 @@ class CachedDownloadTests(unittest.TestC
                     size=450,  # will expect 450 bytes but will download 496 bytes
                 )
 
-            with patch("huggingface_hub.file_download.get_hf_file_metadata", _mocked_hf_file_metadata):
+            with patch(
+                "huggingface_hub.file_download.get_hf_file_metadata",
+                _mocked_hf_file_metadata,
+            ):
                 with self.assertRaises(EnvironmentError):
-                    hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=cache_dir)
+                    hf_hub_download(
+                        DUMMY_MODEL_ID,
+                        filename=constants.CONFIG_NAME,
+                        cache_dir=cache_dir,
+                    )
 
     def test_file_consistency_check_fails_LFS_file(self):
         """Regression test for #1396 (LFS file).
@@ -546,9 +625,16 @@ class CachedDownloadTests(unittest.TestC
                     size=65000,  # will expect 65000 bytes but will download 65074 bytes
                 )
 
-            with patch("huggingface_hub.file_download.get_hf_file_metadata", _mocked_hf_file_metadata):
+            with patch(
+                "huggingface_hub.file_download.get_hf_file_metadata",
+                _mocked_hf_file_metadata,
+            ):
                 with self.assertRaises(EnvironmentError):
-                    hf_hub_download(DUMMY_MODEL_ID, filename="pytorch_model.bin", cache_dir=cache_dir)
+                    hf_hub_download(
+                        DUMMY_MODEL_ID,
+                        filename="pytorch_model.bin",
+                        cache_dir=cache_dir,
+                    )
 
     def test_hf_hub_download_when_tmp_file_is_complete(self):
         """Regression test for #2511.
@@ -562,24 +648,34 @@ class CachedDownloadTests(unittest.TestC
         """
         with SoftTemporaryDirectory() as tmpdir:
             # Download the file once
-            filepath = Path(hf_hub_download(DUMMY_MODEL_ID, filename="pytorch_model.bin", cache_dir=tmpdir))
+            filepath = Path(
+                hf_hub_download(
+                    DUMMY_MODEL_ID, filename="pytorch_model.bin", cache_dir=tmpdir
+                )
+            )
 
             # Fake tmp file
             incomplete_filepath = Path(str(filepath.resolve()) + ".incomplete")
-            incomplete_filepath.write_bytes(filepath.read_bytes())  # fake a partial download
+            incomplete_filepath.write_bytes(
+                filepath.read_bytes()
+            )  # fake a partial download
             filepath.resolve().unlink()
 
             # delete snapshot folder to re-trigger a download
             shutil.rmtree(filepath.parents[2] / "snapshots")
 
             # Download must not fail
-            hf_hub_download(DUMMY_MODEL_ID, filename="pytorch_model.bin", cache_dir=tmpdir)
+            hf_hub_download(
+                DUMMY_MODEL_ID, filename="pytorch_model.bin", cache_dir=tmpdir
+            )
 
     @unittest.skipIf(os.name == "nt", "Lock files are always deleted on Windows.")
     def test_keep_lock_file(self):
         """Lock files should not be deleted on Linux."""
         with SoftTemporaryDirectory() as tmpdir:
-            hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=tmpdir)
+            hf_hub_download(
+                DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=tmpdir
+            )
             lock_file_exist = False
             locks_dir = os.path.join(tmpdir, ".locks")
             for subdir, dirs, files in os.walk(locks_dir):
@@ -623,10 +719,16 @@ class HfHubDownloadToLocalDir(unittest.T
     def setUpClass(cls):
         cls.api = HfApi(endpoint=ENDPOINT_STAGING, token=TOKEN)
         cls.repo_id = cls.api.create_repo(repo_id=repo_name()).repo_id
-        commit_1 = cls.api.upload_file(path_or_fileobj=b"content", path_in_repo=cls.file_name, repo_id=cls.repo_id)
-        commit_2 = cls.api.upload_file(path_or_fileobj=b"content", path_in_repo=cls.lfs_name, repo_id=cls.repo_id)
+        commit_1 = cls.api.upload_file(
+            path_or_fileobj=b"content", path_in_repo=cls.file_name, repo_id=cls.repo_id
+        )
+        commit_2 = cls.api.upload_file(
+            path_or_fileobj=b"content", path_in_repo=cls.lfs_name, repo_id=cls.repo_id
+        )
 
-        info = cls.api.get_paths_info(repo_id=cls.repo_id, paths=[cls.file_name, cls.lfs_name])
+        info = cls.api.get_paths_info(
+            repo_id=cls.repo_id, paths=[cls.file_name, cls.lfs_name]
+        )
         info = {item.path: item for item in info}
         cls.commit_hash_1 = commit_1.oid
         cls.commit_hash_2 = commit_2.oid
@@ -639,7 +741,9 @@ class HfHubDownloadToLocalDir(unittest.T
 
     @contextmanager
     def with_patch_head(self):
-        with patch("huggingface_hub.file_download._get_metadata_or_catch_error") as mock:
+        with patch(
+            "huggingface_hub.file_download._get_metadata_or_catch_error"
+        ) as mock:
             yield mock
 
     @contextmanager
@@ -650,7 +754,10 @@ class HfHubDownloadToLocalDir(unittest.T
     def test_empty_local_dir(self):
         # Download to local dir
         returned_path = self.api.hf_hub_download(
-            self.repo_id, filename=self.file_name, cache_dir=self.hub_cache_dir, local_dir=self.local_dir
+            self.repo_id,
+            filename=self.file_name,
+            cache_dir=self.hub_cache_dir,
+            local_dir=self.local_dir,
         )
         assert self.local_dir in Path(returned_path).parents
 
@@ -663,12 +770,17 @@ class HfHubDownloadToLocalDir(unittest.T
     def test_metadata_ok_and_revision_is_a_commit_hash_and_match(self):
         # File already exists + commit_hash matches (and etag not even required)
         self.file_path.write_text("content")
-        write_download_metadata(self.local_dir, self.file_name, self.commit_hash_1, etag="...")
+        write_download_metadata(
+            self.local_dir, self.file_name, self.commit_hash_1, etag="..."
+        )
 
         # Download to local dir => no HEAD call needed
         with self.with_patch_head() as mock:
             self.api.hf_hub_download(
-                self.repo_id, filename=self.file_name, revision=self.commit_hash_1, local_dir=self.local_dir
+                self.repo_id,
+                filename=self.file_name,
+                revision=self.commit_hash_1,
+                local_dir=self.local_dir,
             )
         mock.assert_not_called()
 
@@ -676,12 +788,17 @@ class HfHubDownloadToLocalDir(unittest.T
         # 1 HEAD call + 1 download
         # File already exists + commit_hash mismatch
         self.file_path.write_text("content")
-        write_download_metadata(self.local_dir, self.file_name, self.commit_hash_1, etag="...")
+        write_download_metadata(
+            self.local_dir, self.file_name, self.commit_hash_1, etag="..."
+        )
 
         # Mismatch => download
         with self.with_patch_download() as mock:
             self.api.hf_hub_download(
-                self.repo_id, filename=self.file_name, revision=self.commit_hash_2, local_dir=self.local_dir
+                self.repo_id,
+                filename=self.file_name,
+                revision=self.commit_hash_2,
+                local_dir=self.local_dir,
             )
         mock.assert_called_once()
 
@@ -693,7 +810,10 @@ class HfHubDownloadToLocalDir(unittest.T
         # Mismatch => download
         with self.with_patch_download() as mock:
             self.api.hf_hub_download(
-                self.repo_id, filename=self.file_name, revision=self.commit_hash_1, local_dir=self.local_dir
+                self.repo_id,
+                filename=self.file_name,
+                revision=self.commit_hash_1,
+                local_dir=self.local_dir,
             )
         mock.assert_called_once()
 
@@ -702,46 +822,67 @@ class HfHubDownloadToLocalDir(unittest.T
         self.file_path.write_text("content2")
 
         path = self.api.hf_hub_download(
-            self.repo_id, filename=self.file_name, local_dir=self.local_dir, local_files_only=True
+            self.repo_id,
+            filename=self.file_name,
+            local_dir=self.local_dir,
+            local_files_only=True,
         )
         assert Path(path) == self.file_path
-        assert self.file_path.read_text() == "content2"  # not overwritten even if wrong content
+        assert (
+            self.file_path.read_text() == "content2"
+        )  # not overwritten even if wrong content
 
     def test_local_files_only_and_file_missing(self):
         # must raise
         with self.assertRaises(LocalEntryNotFoundError):
             self.api.hf_hub_download(
-                self.repo_id, filename=self.file_name, local_dir=self.local_dir, local_files_only=True
+                self.repo_id,
+                filename=self.file_name,
+                local_dir=self.local_dir,
+                local_files_only=True,
             )
 
     def test_metadata_ok_and_etag_match(self):
         # 1 HEAD call + return early
         self.file_path.write_text("something")
-        write_download_metadata(self.local_dir, self.file_name, self.commit_hash_1, etag=self.file_etag)
+        write_download_metadata(
+            self.local_dir, self.file_name, self.commit_hash_1, etag=self.file_etag
+        )
 
         with self.with_patch_download() as mock:
             # Download from main => commit_hash mismatch but etag match => return early
-            self.api.hf_hub_download(self.repo_id, filename=self.file_name, local_dir=self.local_dir)
+            self.api.hf_hub_download(
+                self.repo_id, filename=self.file_name, local_dir=self.local_dir
+            )
         mock.assert_not_called()
 
     def test_metadata_ok_and_etag_mismatch(self):
         # 1 HEAD call + 1 download
         self.file_path.write_text("something")
-        write_download_metadata(self.local_dir, self.file_name, self.commit_hash_1, etag="some_other_etag")
+        write_download_metadata(
+            self.local_dir, self.file_name, self.commit_hash_1, etag="some_other_etag"
+        )
 
         with self.with_patch_download() as mock:
             # Download from main => commit_hash mismatch but etag match => return early
-            self.api.hf_hub_download(self.repo_id, filename=self.file_name, local_dir=self.local_dir)
+            self.api.hf_hub_download(
+                self.repo_id, filename=self.file_name, local_dir=self.local_dir
+            )
         mock.assert_called_once()
 
     def test_metadata_ok_and_etag_match_and_force_download(self):
         # force_download=True takes precedence on any other rule
         self.file_path.write_text("something")
-        write_download_metadata(self.local_dir, self.file_name, self.commit_hash_1, etag=self.file_etag)
+        write_download_metadata(
+            self.local_dir, self.file_name, self.commit_hash_1, etag=self.file_etag
+        )
 
         with self.with_patch_download() as mock:
             self.api.hf_hub_download(
-                self.repo_id, filename=self.file_name, local_dir=self.local_dir, force_download=True
+                self.repo_id,
+                filename=self.file_name,
+                local_dir=self.local_dir,
+                force_download=True,
             )
         mock.assert_called_once()
 
@@ -753,7 +894,9 @@ class HfHubDownloadToLocalDir(unittest.T
             # Download from main
             # => no metadata but it's an LFS file
             # => compute local hash => matches => return early
-            self.api.hf_hub_download(self.repo_id, filename=self.lfs_name, local_dir=self.local_dir)
+            self.api.hf_hub_download(
+                self.repo_id, filename=self.lfs_name, local_dir=self.local_dir
+            )
         mock.assert_not_called()
 
     def test_metadata_not_ok_and_lfs_file_and_sha256_mismatch(self):
@@ -763,14 +906,18 @@ class HfHubDownloadToLocalDir(unittest.T
         # Download from main
         # => no metadata but it's an LFS file
         # => compute local hash => mismatches => download
-        path = self.api.hf_hub_download(self.repo_id, filename=self.lfs_name, local_dir=self.local_dir)
+        path = self.api.hf_hub_download(
+            self.repo_id, filename=self.lfs_name, local_dir=self.local_dir
+        )
 
         # existing file overwritten
         assert Path(path).read_text() == "content"
 
     def test_file_exists_in_cache(self):
         # 1 HEAD call + return early
-        self.api.hf_hub_download(self.repo_id, filename=self.file_name, cache_dir=self.hub_cache_dir)
+        self.api.hf_hub_download(
+            self.repo_id, filename=self.file_name, cache_dir=self.hub_cache_dir
+        )
 
         with self.with_patch_download() as mock:
             # Download to local dir
@@ -778,7 +925,10 @@ class HfHubDownloadToLocalDir(unittest.T
             # => we assume it's faster to make a local copy rather than re-downloading
             # => duplicate file locally
             path = self.api.hf_hub_download(
-                self.repo_id, filename=self.file_name, cache_dir=self.hub_cache_dir, local_dir=self.local_dir
+                self.repo_id,
+                filename=self.file_name,
+                cache_dir=self.hub_cache_dir,
+                local_dir=self.local_dir,
             )
         mock.assert_not_called()
 
@@ -787,23 +937,44 @@ class HfHubDownloadToLocalDir(unittest.T
     def test_file_exists_and_overwrites(self):
         # 1 HEAD call + 1 download
         self.file_path.write_text("another content")
-        self.api.hf_hub_download(self.repo_id, filename=self.file_name, local_dir=self.local_dir)
+        self.api.hf_hub_download(
+            self.repo_id, filename=self.file_name, local_dir=self.local_dir
+        )
         assert self.file_path.read_text() == "content"
 
     def test_resume_from_incomplete(self):
         # An incomplete file already exists => use it
-        incomplete_path = self.local_dir / ".cache" / "huggingface" / "download" / (self.file_name + ".incomplete")
+        incomplete_path = (
+            self.local_dir
+            / ".cache"
+            / "huggingface"
+            / "download"
+            / (self.file_name + ".incomplete")
+        )
         incomplete_path.parent.mkdir(parents=True, exist_ok=True)
         incomplete_path.write_text("XXXX")  # Here we put fake data to test the resume
-        self.api.hf_hub_download(self.repo_id, filename=self.file_name, local_dir=self.local_dir)
+        self.api.hf_hub_download(
+            self.repo_id, filename=self.file_name, local_dir=self.local_dir
+        )
         self.file_path.read_text() == "XXXXent"
 
     def test_do_not_resume_on_force_download(self):
         # An incomplete file already exists but force_download=True
-        incomplete_path = self.local_dir / ".cache" / "huggingface" / "download" / (self.file_name + ".incomplete")
+        incomplete_path = (
+            self.local_dir
+            / ".cache"
+            / "huggingface"
+            / "download"
+            / (self.file_name + ".incomplete")
+        )
         incomplete_path.parent.mkdir(parents=True, exist_ok=True)
         incomplete_path.write_text("XXXX")
-        self.api.hf_hub_download(self.repo_id, filename=self.file_name, local_dir=self.local_dir, force_download=True)
+        self.api.hf_hub_download(
+            self.repo_id,
+            filename=self.file_name,
+            local_dir=self.local_dir,
+            force_download=True,
+        )
         self.file_path.read_text() == "content"
 
     @patch("huggingface_hub.file_download.build_hf_headers")
@@ -816,14 +987,18 @@ class HfHubDownloadToLocalDir(unittest.T
         """
         # Download to local dir
         mock.reset_mock(return_value={})
-        self.api.hf_hub_download(self.repo_id, filename=self.file_name, local_dir=self.local_dir, token=False)
+        self.api.hf_hub_download(
+            self.repo_id, filename=self.file_name, local_dir=self.local_dir, token=False
+        )
         mock.assert_called()
         for call in mock.call_args_list:
             assert call.kwargs["token"] is False
 
         # Download to cache dir
         mock.reset_mock(return_value={})
-        self.api.hf_hub_download(self.repo_id, filename=self.file_name, cache_dir=self.local_dir, token=False)
+        self.api.hf_hub_download(
+            self.repo_id, filename=self.file_name, cache_dir=self.local_dir, token=False
+        )
         mock.assert_called()
         for call in mock.call_args_list:
             assert call.kwargs["token"] is False
@@ -847,9 +1022,7 @@ class StagingCachedDownloadOnAwfulFilena
     def setUpClass(cls):
         cls.api = HfApi(endpoint=ENDPOINT_STAGING, token=TOKEN)
         cls.repo_url = cls.api.create_repo(repo_id=repo_name("awful_filename"))
-        cls.expected_resolve_url = (
-            f"{cls.repo_url}/resolve/main/subfolder/to%3F/awful%3Ffilename%25you%3Ashould%2Cnever.give"
-        )
+        cls.expected_resolve_url = f"{cls.repo_url}/resolve/main/subfolder/to%3F/awful%3Ffilename%25you%3Ashould%2Cnever.give"
         cls.api.upload_file(
             path_or_fileobj=b"content",
             path_in_repo=cls.filepath,
@@ -861,7 +1034,9 @@ class StagingCachedDownloadOnAwfulFilena
         cls.api.delete_repo(repo_id=cls.repo_url.repo_id)
 
     def test_hf_hub_url_on_awful_filepath(self):
-        self.assertEqual(hf_hub_url(self.repo_url.repo_id, self.filepath), self.expected_resolve_url)
+        self.assertEqual(
+            hf_hub_url(self.repo_url.repo_id, self.filepath), self.expected_resolve_url
+        )
 
     def test_hf_hub_url_on_awful_subfolder_and_filename(self):
         self.assertEqual(
@@ -871,7 +1046,9 @@ class StagingCachedDownloadOnAwfulFilena
 
     @xfail_on_windows(reason="Windows paths cannot contain a '?'.")
     def test_hf_hub_download_on_awful_filepath(self):
-        local_path = hf_hub_download(self.repo_url.repo_id, self.filepath, cache_dir=self.cache_dir)
+        local_path = hf_hub_download(
+            self.repo_url.repo_id, self.filepath, cache_dir=self.cache_dir
+        )
         # Local path is not url-encoded
         self.assertTrue(local_path.endswith(self.filepath))
 
@@ -903,20 +1080,35 @@ class TestHfHubDownloadRelativePaths(uni
     def setUpClass(cls):
         cls.api = HfApi(endpoint=ENDPOINT_STAGING, token=TOKEN)
         cls.repo_id = cls.api.create_repo(repo_id=repo_name()).repo_id
-        cls.api.upload_file(path_or_fileobj=b"content", path_in_repo="folder/..\\..\\..\\file", repo_id=cls.repo_id)
+        cls.api.upload_file(
+            path_or_fileobj=b"content",
+            path_in_repo="folder/..\\..\\..\\file",
+            repo_id=cls.repo_id,
+        )
 
     @classmethod
     def tearDownClass(cls) -> None:
         cls.api.delete_repo(repo_id=cls.repo_id)
 
-    @xfail_on_windows(reason="Windows paths cannot contain '\\..\\'.", raises=ValueError)
+    @xfail_on_windows(
+        reason="Windows paths cannot contain '\\..\\'.", raises=ValueError
+    )
     def test_download_folder_file_in_cache_dir(self) -> None:
-        hf_hub_download(self.repo_id, "folder/..\\..\\..\\file", cache_dir=self.cache_dir)
+        hf_hub_download(
+            self.repo_id, "folder/..\\..\\..\\file", cache_dir=self.cache_dir
+        )
 
-    @xfail_on_windows(reason="Windows paths cannot contain '\\..\\'.", raises=ValueError)
+    @xfail_on_windows(
+        reason="Windows paths cannot contain '\\..\\'.", raises=ValueError
+    )
     def test_download_folder_file_to_local_dir(self) -> None:
         with SoftTemporaryDirectory() as local_dir:
-            hf_hub_download(self.repo_id, "folder/..\\..\\..\\file", cache_dir=self.cache_dir, local_dir=local_dir)
+            hf_hub_download(
+                self.repo_id,
+                "folder/..\\..\\..\\file",
+                cache_dir=self.cache_dir,
+                local_dir=local_dir,
+            )
 
     def test_get_pointer_path_and_valid_relative_filename(self) -> None:
         # Cannot happen because of other protections, but just in case.
@@ -927,7 +1119,11 @@ class TestHfHubDownloadRelativePaths(uni
 
     def test_get_pointer_path_but_invalid_relative_filename(self) -> None:
         # Cannot happen because of other protections, but just in case.
-        relative_filename = "folder\\..\\..\\..\\file.txt" if os.name == "nt" else "folder/../../../file.txt"
+        relative_filename = (
+            "folder\\..\\..\\..\\file.txt"
+            if os.name == "nt"
+            else "folder/../../../file.txt"
+        )
         with self.assertRaises(ValueError):
             _get_pointer_path("path/to/storage", "abcdef", relative_filename)
 
@@ -1016,7 +1212,9 @@ class TestHttpGet:
             ),
         ],
     )
-    def test_http_get_with_range_headers(self, caplog, initial_range: str, expected_ranges: List[str]):
+    def test_http_get_with_range_headers(
+        self, caplog, initial_range: str, expected_ranges: List[str]
+    ):
         def _iter_content_1() -> Iterable[bytes]:
             yield b"0" * 10
             yield b"0" * 10
@@ -1064,7 +1262,9 @@ class TestHttpGet:
 class CreateSymlinkTest(unittest.TestCase):
     @unittest.skipIf(os.name == "nt", "No symlinks on Windows")
     @patch("huggingface_hub.file_download.are_symlinks_supported")
-    def test_create_symlink_concurrent_access(self, mock_are_symlinks_supported: Mock) -> None:
+    def test_create_symlink_concurrent_access(
+        self, mock_are_symlinks_supported: Mock
+    ) -> None:
         with SoftTemporaryDirectory() as tmpdir:
             src = os.path.join(tmpdir, "source")
             other = os.path.join(tmpdir, "other")
@@ -1125,33 +1325,40 @@ class TestNormalizeEtag(unittest.TestCas
 
     def test_strong_reference(self):
         self.assertEqual(
-            _normalize_etag('"a16a55fda99d2f2e7b69cce5cf93ff4ad3049930"'), "a16a55fda99d2f2e7b69cce5cf93ff4ad3049930"
+            _normalize_etag('"a16a55fda99d2f2e7b69cce5cf93ff4ad3049930"'),
+            "a16a55fda99d2f2e7b69cce5cf93ff4ad3049930",
         )
 
     def test_weak_reference(self):
         self.assertEqual(
-            _normalize_etag('W/"a16a55fda99d2f2e7b69cce5cf93ff4ad3049930"'), "a16a55fda99d2f2e7b69cce5cf93ff4ad3049930"
+            _normalize_etag('W/"a16a55fda99d2f2e7b69cce5cf93ff4ad3049930"'),
+            "a16a55fda99d2f2e7b69cce5cf93ff4ad3049930",
         )
 
     @with_production_testing
     def test_resolve_endpoint_on_regular_file(self):
         url = "https://huggingface.co/gpt2/resolve/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/README.md"
         response = requests.head(url)
-        self.assertEqual(self._get_etag_and_normalize(response), "a16a55fda99d2f2e7b69cce5cf93ff4ad3049930")
+        self.assertEqual(
+            self._get_etag_and_normalize(response),
+            "a16a55fda99d2f2e7b69cce5cf93ff4ad3049930",
+        )
 
     @with_production_testing
     def test_resolve_endpoint_on_lfs_file(self):
         url = "https://huggingface.co/gpt2/resolve/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin"
         response = requests.head(url)
         self.assertEqual(
-            self._get_etag_and_normalize(response), "7c5d3f4b8b76583b422fcb9189ad6c89d5d97a094541ce8932dce3ecabde1421"
+            self._get_etag_and_normalize(response),
+            "7c5d3f4b8b76583b422fcb9189ad6c89d5d97a094541ce8932dce3ecabde1421",
         )
 
     @staticmethod
     def _get_etag_and_normalize(response: Response) -> str:
         response.raise_for_status()
         return _normalize_etag(
-            response.headers.get(constants.HUGGINGFACE_HEADER_X_LINKED_ETAG) or response.headers.get("ETag")
+            response.headers.get(constants.HUGGINGFACE_HEADER_X_LINKED_ETAG)
+            or response.headers.get("ETag")
         )
 
 
@@ -1166,7 +1373,9 @@ class TestEtagTimeoutConfig(unittest.Tes
                 "get_hf_file_metadata",
                 wraps=huggingface_hub.file_download.get_hf_file_metadata,
             ) as mock_etag_call:
-                hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=cache_dir)
+                hf_hub_download(
+                    DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=cache_dir
+                )
                 kwargs = mock_etag_call.call_args.kwargs
                 self.assertIn("timeout", kwargs)
                 self.assertEqual(kwargs["timeout"], 10)
@@ -1180,13 +1389,22 @@ class TestEtagTimeoutConfig(unittest.Tes
                 "get_hf_file_metadata",
                 wraps=huggingface_hub.file_download.get_hf_file_metadata,
             ) as mock_etag_call:
-                hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=cache_dir, etag_timeout=12)
+                hf_hub_download(
+                    DUMMY_MODEL_ID,
+                    filename=constants.CONFIG_NAME,
+                    cache_dir=cache_dir,
+                    etag_timeout=12,
+                )
                 kwargs = mock_etag_call.call_args.kwargs
                 self.assertIn("timeout", kwargs)
-                self.assertEqual(kwargs["timeout"], 12)  # passed as parameter, takes priority
+                self.assertEqual(
+                    kwargs["timeout"], 12
+                )  # passed as parameter, takes priority
 
     @patch("huggingface_hub.file_download.constants.DEFAULT_ETAG_TIMEOUT", 10)
-    @patch("huggingface_hub.file_download.constants.HF_HUB_ETAG_TIMEOUT", 15)  # takes priority
+    @patch(
+        "huggingface_hub.file_download.constants.HF_HUB_ETAG_TIMEOUT", 15
+    )  # takes priority
     def test_etag_timeout_set_as_env_variable(self):
         with SoftTemporaryDirectory() as cache_dir:
             with patch.object(
@@ -1194,13 +1412,17 @@ class TestEtagTimeoutConfig(unittest.Tes
                 "get_hf_file_metadata",
                 wraps=huggingface_hub.file_download.get_hf_file_metadata,
             ) as mock_etag_call:
-                hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=cache_dir)
+                hf_hub_download(
+                    DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=cache_dir
+                )
                 kwargs = mock_etag_call.call_args.kwargs
                 self.assertIn("timeout", kwargs)
                 self.assertEqual(kwargs["timeout"], 15)
 
     @patch("huggingface_hub.file_download.constants.DEFAULT_ETAG_TIMEOUT", 10)
-    @patch("huggingface_hub.file_download.constants.HF_HUB_ETAG_TIMEOUT", 12)  # takes priority
+    @patch(
+        "huggingface_hub.file_download.constants.HF_HUB_ETAG_TIMEOUT", 12
+    )  # takes priority
     def test_etag_timeout_set_as_env_variable_parameter_ignored(self):
         with SoftTemporaryDirectory() as cache_dir:
             with patch.object(
@@ -1208,10 +1430,17 @@ class TestEtagTimeoutConfig(unittest.Tes
                 "get_hf_file_metadata",
                 wraps=huggingface_hub.file_download.get_hf_file_metadata,
             ) as mock_etag_call:
-                hf_hub_download(DUMMY_MODEL_ID, filename=constants.CONFIG_NAME, cache_dir=cache_dir, etag_timeout=12)
+                hf_hub_download(
+                    DUMMY_MODEL_ID,
+                    filename=constants.CONFIG_NAME,
+                    cache_dir=cache_dir,
+                    etag_timeout=12,
+                )
                 kwargs = mock_etag_call.call_args.kwargs
                 self.assertIn("timeout", kwargs)
-                self.assertEqual(kwargs["timeout"], 12)  # passed value ignored, HF_HUB_ETAG_TIMEOUT takes priority
+                self.assertEqual(
+                    kwargs["timeout"], 12
+                )  # passed value ignored, HF_HUB_ETAG_TIMEOUT takes priority
 
 
 def _recursive_chmod(path: str, mode: int) -> None:
--- python3-huggingface-hub-0.29.3.orig/tests/test_hf_api.py
+++ python3-huggingface-hub-0.29.3/tests/test_hf_api.py
@@ -111,7 +111,9 @@ from .testing_utils import (
 
 logger = logging.get_logger(__name__)
 
-WORKING_REPO_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "fixtures/working_repo")
+WORKING_REPO_DIR = os.path.join(
+    os.path.dirname(os.path.abspath(__file__)), "fixtures/working_repo"
+)
 LARGE_FILE_14MB = "https://cdn-media.huggingface.co/lfs-largefiles/progit.epub"
 LARGE_FILE_18MB = "https://cdn-media.huggingface.co/lfs-largefiles/progit.pdf"
 
@@ -136,7 +138,9 @@ class HfApiRepoFileExistsTest(HfApiCommo
     def setUp(self) -> None:
         super().setUp()
         self.repo_id = self._api.create_repo(repo_name(), private=True).repo_id
-        self.upload = self._api.upload_file(repo_id=self.repo_id, path_in_repo="file.txt", path_or_fileobj=b"content")
+        self.upload = self._api.upload_file(
+            repo_id=self.repo_id, path_in_repo="file.txt", path_or_fileobj=b"content"
+        )
 
     def tearDown(self) -> None:
         self._api.delete_repo(self.repo_id)
@@ -144,14 +148,24 @@ class HfApiRepoFileExistsTest(HfApiCommo
 
     def test_repo_exists(self):
         assert self._api.repo_exists(self.repo_id)
-        self.assertFalse(self._api.repo_exists(self.repo_id, token=False))  # private repo
-        self.assertFalse(self._api.repo_exists("repo-that-does-not-exist"))  # missing repo
+        self.assertFalse(
+            self._api.repo_exists(self.repo_id, token=False)
+        )  # private repo
+        self.assertFalse(
+            self._api.repo_exists("repo-that-does-not-exist")
+        )  # missing repo
 
     def test_revision_exists(self):
         assert self._api.revision_exists(self.repo_id, "main")
-        assert not self._api.revision_exists(self.repo_id, "revision-that-does-not-exist")  # missing revision
-        assert not self._api.revision_exists(self.repo_id, "main", token=False)  # private repo
-        assert not self._api.revision_exists("repo-that-does-not-exist", "main")  # missing repo
+        assert not self._api.revision_exists(
+            self.repo_id, "revision-that-does-not-exist"
+        )  # missing revision
+        assert not self._api.revision_exists(
+            self.repo_id, "main", token=False
+        )  # private repo
+        assert not self._api.revision_exists(
+            "repo-that-does-not-exist", "main"
+        )  # missing repo
 
     @patch("huggingface_hub.constants.ENDPOINT", "https://hub-ci.huggingface.co")
     @patch(
@@ -160,12 +174,20 @@ class HfApiRepoFileExistsTest(HfApiCommo
     )
     def test_file_exists(self):
         assert self._api.file_exists(self.repo_id, "file.txt")
-        self.assertFalse(self._api.file_exists("repo-that-does-not-exist", "file.txt"))  # missing repo
-        self.assertFalse(self._api.file_exists(self.repo_id, "file-does-not-exist"))  # missing file
         self.assertFalse(
-            self._api.file_exists(self.repo_id, "file.txt", revision="revision-that-does-not-exist")
+            self._api.file_exists("repo-that-does-not-exist", "file.txt")
+        )  # missing repo
+        self.assertFalse(
+            self._api.file_exists(self.repo_id, "file-does-not-exist")
+        )  # missing file
+        self.assertFalse(
+            self._api.file_exists(
+                self.repo_id, "file.txt", revision="revision-that-does-not-exist"
+            )
         )  # missing revision
-        self.assertFalse(self._api.file_exists(self.repo_id, "file.txt", token=False))  # private repo
+        self.assertFalse(
+            self._api.file_exists(self.repo_id, "file.txt", token=False)
+        )  # private repo
 
 
 class HfApiEndpointsTest(HfApiCommonTest):
@@ -234,8 +256,12 @@ class HfApiEndpointsTest(HfApiCommonTest
         self._api.create_repo(repo_id=repo_id_1)
         self._api.create_repo(repo_id=repo_id_2)
 
-        with pytest.raises(HfHubHTTPError, match=r"A model repository called .* already exists"):
-            self._api.move_repo(from_id=repo_id_1, to_id=repo_id_2, repo_type=constants.REPO_TYPE_MODEL)
+        with pytest.raises(
+            HfHubHTTPError, match=r"A model repository called .* already exists"
+        ):
+            self._api.move_repo(
+                from_id=repo_id_1, to_id=repo_id_2, repo_type=constants.REPO_TYPE_MODEL
+            )
 
         self._api.delete_repo(repo_id=repo_id_1)
         self._api.delete_repo(repo_id=repo_id_2)
@@ -254,7 +280,9 @@ class HfApiEndpointsTest(HfApiCommonTest
 
         for gated_value in ["auto", "manual", False]:
             for private_value in [True, False]:  # Test both private and public settings
-                self._api.update_repo_settings(repo_id=repo_id, gated=gated_value, private=private_value)
+                self._api.update_repo_settings(
+                    repo_id=repo_id, gated=gated_value, private=private_value
+                )
                 info = self._api.model_info(repo_id)
                 assert info.gated == gated_value
                 assert info.private == private_value  # Verify the private setting
@@ -267,7 +295,10 @@ class HfApiEndpointsTest(HfApiCommonTest
         for gated_value in ["auto", "manual", False]:
             for private_value in [True, False]:
                 self._api.update_repo_settings(
-                    repo_id=repo_id, repo_type=repo_type, gated=gated_value, private=private_value
+                    repo_id=repo_id,
+                    repo_type=repo_type,
+                    gated=gated_value,
+                    private=private_value,
                 )
                 info = self._api.dataset_info(repo_id)
                 assert info.gated == gated_value
@@ -317,7 +348,9 @@ class CommitApiTest(HfApiCommonTest):
             ):
                 CommitOperationAdd(path_or_fileobj=ftext, path_in_repo="README.md")  # type: ignore
 
-        with self.assertRaises(ValueError, msg="path_or_fileobj is str but does not point to a file"):
+        with self.assertRaises(
+            ValueError, msg="path_or_fileobj is str but does not point to a file"
+        ):
             CommitOperationAdd(
                 path_or_fileobj=os.path.join(self.tmp_dir, "nofile.pth"),
                 path_in_repo="README.md",
@@ -335,13 +368,21 @@ class CommitApiTest(HfApiCommonTest):
         self.assertIsInstance(return_val, CommitInfo)
 
         with SoftTemporaryDirectory() as cache_dir:
-            with open(hf_hub_download(repo_id=repo_id, filename="temp/new_file.md", cache_dir=cache_dir)) as f:
+            with open(
+                hf_hub_download(
+                    repo_id=repo_id, filename="temp/new_file.md", cache_dir=cache_dir
+                )
+            ) as f:
                 self.assertEqual(f.read(), self.tmp_file_content)
 
     @use_tmp_repo()
     def test_upload_file_pathlib_path(self, repo_url: RepoUrl) -> None:
         """Regression test for https://github.com/huggingface/huggingface_hub/issues/1246."""
-        self._api.upload_file(path_or_fileobj=Path(self.tmp_file), path_in_repo="file.txt", repo_id=repo_url.repo_id)
+        self._api.upload_file(
+            path_or_fileobj=Path(self.tmp_file),
+            path_in_repo="file.txt",
+            repo_id=repo_url.repo_id,
+        )
         self.assertIn("file.txt", self._api.list_repo_files(repo_id=repo_url.repo_id))
 
     @use_tmp_repo()
@@ -356,7 +397,11 @@ class CommitApiTest(HfApiCommonTest):
         self.assertEqual(return_val, f"{repo_url}/blob/main/temp/new_file.md")
 
         with SoftTemporaryDirectory() as cache_dir:
-            with open(hf_hub_download(repo_id=repo_id, filename="temp/new_file.md", cache_dir=cache_dir)) as f:
+            with open(
+                hf_hub_download(
+                    repo_id=repo_id, filename="temp/new_file.md", cache_dir=cache_dir
+                )
+            ) as f:
                 self.assertEqual(f.read(), self.tmp_file_content)
 
     @use_tmp_repo()
@@ -371,7 +416,11 @@ class CommitApiTest(HfApiCommonTest):
         self.assertEqual(return_val, f"{repo_url}/blob/main/temp/new_file.md")
 
         with SoftTemporaryDirectory() as cache_dir:
-            with open(hf_hub_download(repo_id=repo_id, filename="temp/new_file.md", cache_dir=cache_dir)) as f:
+            with open(
+                hf_hub_download(
+                    repo_id=repo_id, filename="temp/new_file.md", cache_dir=cache_dir
+                )
+            ) as f:
                 self.assertEqual(f.read(), content.getvalue().decode())
 
     @use_tmp_repo()
@@ -437,13 +486,19 @@ class CommitApiTest(HfApiCommonTest):
             repo_id=repo_id,
             create_pr=True,
         )
-        self.assertEqual(return_val, f"{repo_url}/blob/{quote('refs/pr/1', safe='')}/temp/new_file.md")
+        self.assertEqual(
+            return_val,
+            f"{repo_url}/blob/{quote('refs/pr/1', safe='')}/temp/new_file.md",
+        )
         self.assertIsInstance(return_val, CommitInfo)
 
         with SoftTemporaryDirectory() as cache_dir:
             with open(
                 hf_hub_download(
-                    repo_id=repo_id, filename="temp/new_file.md", revision="refs/pr/1", cache_dir=cache_dir
+                    repo_id=repo_id,
+                    filename="temp/new_file.md",
+                    revision="refs/pr/1",
+                    cache_dir=cache_dir,
                 )
             ) as f:
                 self.assertEqual(f.read(), self.tmp_file_content)
@@ -455,7 +510,9 @@ class CommitApiTest(HfApiCommonTest):
             path_in_repo="temp/new_file.md",
             repo_id=repo_url.repo_id,
         )
-        return_val = self._api.delete_file(path_in_repo="temp/new_file.md", repo_id=repo_url.repo_id)
+        return_val = self._api.delete_file(
+            path_in_repo="temp/new_file.md", repo_id=repo_url.repo_id
+        )
         self.assertIsInstance(return_val, CommitInfo)
 
         with self.assertRaises(EntryNotFoundError):
@@ -466,7 +523,9 @@ class CommitApiTest(HfApiCommonTest):
         repo_name_with_no_org = self._api.get_full_repo_name("model")
         self.assertEqual(repo_name_with_no_org, f"{USER}/model")
 
-        repo_name_with_no_org = self._api.get_full_repo_name("model", organization="org")
+        repo_name_with_no_org = self._api.get_full_repo_name(
+            "model", organization="org"
+        )
         self.assertEqual(repo_name_with_no_org, "org/model")
 
     @use_tmp_repo()
@@ -474,7 +533,9 @@ class CommitApiTest(HfApiCommonTest):
         repo_id = repo_url.repo_id
 
         # Upload folder
-        url = self._api.upload_folder(folder_path=self.tmp_dir, path_in_repo="temp/dir", repo_id=repo_id)
+        url = self._api.upload_folder(
+            folder_path=self.tmp_dir, path_in_repo="temp/dir", repo_id=repo_id
+        )
         self.assertEqual(
             url,
             f"{self._api.endpoint}/{repo_id}/tree/main/temp/dir",
@@ -486,7 +547,10 @@ class CommitApiTest(HfApiCommonTest):
             local_path = os.path.join(self.tmp_dir, rpath)
             remote_path = f"temp/dir/{rpath}"
             filepath = hf_hub_download(
-                repo_id=repo_id, filename=remote_path, revision="main", use_auth_token=self._token
+                repo_id=repo_id,
+                filename=remote_path,
+                revision="main",
+                use_auth_token=self._token,
             )
             assert filepath is not None
             with open(filepath, "rb") as downloaded_file:
@@ -496,7 +560,9 @@ class CommitApiTest(HfApiCommonTest):
             self.assertEqual(content, expected_content)
 
         # Re-uploading the same folder twice should be fine
-        return_val = self._api.upload_folder(folder_path=self.tmp_dir, path_in_repo="temp/dir", repo_id=repo_id)
+        return_val = self._api.upload_folder(
+            folder_path=self.tmp_dir, path_in_repo="temp/dir", repo_id=repo_id
+        )
         self.assertIsInstance(return_val, CommitInfo)
 
     @use_tmp_repo()
@@ -505,20 +571,29 @@ class CommitApiTest(HfApiCommonTest):
 
         # Upload folder as a new PR
         return_val = self._api.upload_folder(
-            folder_path=self.tmp_dir, path_in_repo="temp/dir", repo_id=repo_id, create_pr=True
+            folder_path=self.tmp_dir,
+            path_in_repo="temp/dir",
+            repo_id=repo_id,
+            create_pr=True,
+        )
+        self.assertEqual(
+            return_val, f"{self._api.endpoint}/{repo_id}/tree/refs%2Fpr%2F1/temp/dir"
         )
-        self.assertEqual(return_val, f"{self._api.endpoint}/{repo_id}/tree/refs%2Fpr%2F1/temp/dir")
 
         # Check files are uploaded
         for rpath in ["temp", "nested/file.bin"]:
             local_path = os.path.join(self.tmp_dir, rpath)
-            filepath = hf_hub_download(repo_id=repo_id, filename=f"temp/dir/{rpath}", revision="refs/pr/1")
+            filepath = hf_hub_download(
+                repo_id=repo_id, filename=f"temp/dir/{rpath}", revision="refs/pr/1"
+            )
             assert Path(local_path).read_bytes() == Path(filepath).read_bytes()
 
     def test_upload_folder_default_path_in_repo(self):
         REPO_NAME = repo_name("upload_folder_to_root")
         self._api.create_repo(repo_id=REPO_NAME, exist_ok=False)
-        url = self._api.upload_folder(folder_path=self.tmp_dir, repo_id=f"{USER}/{REPO_NAME}")
+        url = self._api.upload_folder(
+            folder_path=self.tmp_dir, repo_id=f"{USER}/{REPO_NAME}"
+        )
         # URL to root of repository
         self.assertEqual(url, f"{self._api.endpoint}/{USER}/{REPO_NAME}/tree/main/")
 
@@ -543,13 +618,23 @@ class CommitApiTest(HfApiCommonTest):
         self._api.upload_folder(folder_path=self.tmp_dir, repo_id=repo_url.repo_id)
         self.assertEqual(
             set(self._api.list_repo_files(repo_id=repo_url.repo_id)),
-            {".gitattributes", ".git_something/file.txt", "file.git", "temp", "nested/file.bin"},
+            {
+                ".gitattributes",
+                ".git_something/file.txt",
+                "file.git",
+                "temp",
+                "nested/file.bin",
+            },
         )
 
     @use_tmp_repo()
     def test_upload_folder_gitignore_already_exists(self, repo_url: RepoUrl) -> None:
         # Ignore nested folder
-        self._api.upload_file(path_or_fileobj=b"nested/*\n", path_in_repo=".gitignore", repo_id=repo_url.repo_id)
+        self._api.upload_file(
+            path_or_fileobj=b"nested/*\n",
+            path_in_repo=".gitignore",
+            repo_id=repo_url.repo_id,
+        )
 
         # Upload folder
         self._api.upload_folder(folder_path=self.tmp_dir, repo_id=repo_url.repo_id)
@@ -573,7 +658,11 @@ class CommitApiTest(HfApiCommonTest):
         repo_id = repo_url.repo_id
 
         # Upload a first file
-        self._api.upload_file(path_or_fileobj=self.tmp_file, path_in_repo="temp/new_file.md", repo_id=repo_id)
+        self._api.upload_file(
+            path_or_fileobj=self.tmp_file,
+            path_in_repo="temp/new_file.md",
+            repo_id=repo_id,
+        )
 
         # Create a commit with a PR
         operations = [
@@ -581,7 +670,10 @@ class CommitApiTest(HfApiCommonTest):
             CommitOperationAdd(path_in_repo="buffer", path_or_fileobj=b"Buffer data"),
         ]
         resp = self._api.create_commit(
-            operations=operations, commit_message="Test create_commit", repo_id=repo_id, create_pr=True
+            operations=operations,
+            commit_message="Test create_commit",
+            repo_id=repo_id,
+            create_pr=True,
         )
 
         # Check commit info
@@ -590,7 +682,9 @@ class CommitApiTest(HfApiCommonTest):
         self.assertIn("pr_revision='refs/pr/1'", repr(resp))
         self.assertIsInstance(commit_id, str)
         self.assertGreater(len(commit_id), 0)
-        self.assertEqual(resp.commit_url, f"{self._api.endpoint}/{repo_id}/commit/{commit_id}")
+        self.assertEqual(
+            resp.commit_url, f"{self._api.endpoint}/{repo_id}/commit/{commit_id}"
+        )
         self.assertEqual(resp.commit_message, "Test create_commit")
         self.assertEqual(resp.commit_description, "")
         self.assertEqual(resp.pr_url, f"{self._api.endpoint}/{repo_id}/discussions/1")
@@ -604,7 +698,9 @@ class CommitApiTest(HfApiCommonTest):
             self.assertEqual(ctx.exception.response.status_code, 404)
 
         # ...but exists on PR
-        filepath = self._api.hf_hub_download(filename="buffer", repo_id=repo_id, revision="refs/pr/1")
+        filepath = self._api.hf_hub_download(
+            filename="buffer", repo_id=repo_id, revision="refs/pr/1"
+        )
         with open(filepath, "rb") as downloaded_file:
             content = downloaded_file.read()
         self.assertEqual(content, b"Buffer data")
@@ -633,7 +729,9 @@ class CommitApiTest(HfApiCommonTest):
         with self.assertRaises(RevisionNotFoundError):
             self._api.create_commit(
                 operations=[
-                    CommitOperationAdd(path_in_repo="file.txt", path_or_fileobj=b"content"),
+                    CommitOperationAdd(
+                        path_in_repo="file.txt", path_or_fileobj=b"content"
+                    ),
                 ],
                 commit_message="PR against a oid",
                 repo_id=repo_id,
@@ -645,7 +743,9 @@ class CommitApiTest(HfApiCommonTest):
         with self.assertRaises(RevisionNotFoundError):
             self._api.create_commit(
                 operations=[
-                    CommitOperationAdd(path_in_repo="file.txt", path_or_fileobj=b"content"),
+                    CommitOperationAdd(
+                        path_in_repo="file.txt", path_or_fileobj=b"content"
+                    ),
                 ],
                 commit_message="PR against missing branch",
                 repo_id=repo_id,
@@ -664,8 +764,12 @@ class CommitApiTest(HfApiCommonTest):
 
         self._api.create_commit(
             operations=[
-                CommitOperationAdd(path_in_repo="regular.txt", path_or_fileobj=b"File content"),
-                CommitOperationAdd(path_in_repo="lfs.pkl", path_or_fileobj=b"File content"),
+                CommitOperationAdd(
+                    path_in_repo="regular.txt", path_or_fileobj=b"File content"
+                ),
+                CommitOperationAdd(
+                    path_in_repo="lfs.pkl", path_or_fileobj=b"File content"
+                ),
             ],
             commit_message="PR on foreign repo",
             repo_id=foreign_repo_url.repo_id,
@@ -677,11 +781,17 @@ class CommitApiTest(HfApiCommonTest):
     @use_tmp_repo()
     def test_create_commit(self, repo_url: RepoUrl) -> None:
         repo_id = repo_url.repo_id
-        self._api.upload_file(path_or_fileobj=self.tmp_file, path_in_repo="temp/new_file.md", repo_id=repo_id)
+        self._api.upload_file(
+            path_or_fileobj=self.tmp_file,
+            path_in_repo="temp/new_file.md",
+            repo_id=repo_id,
+        )
         with open(self.tmp_file, "rb") as fileobj:
             operations = [
                 CommitOperationDelete(path_in_repo="temp/new_file.md"),
-                CommitOperationAdd(path_in_repo="buffer", path_or_fileobj=b"Buffer data"),
+                CommitOperationAdd(
+                    path_in_repo="buffer", path_or_fileobj=b"Buffer data"
+                ),
                 CommitOperationAdd(
                     path_in_repo="bytesio",
                     path_or_fileobj=BytesIO(b"BytesIO data"),
@@ -692,7 +802,11 @@ class CommitApiTest(HfApiCommonTest):
                     path_or_fileobj=self.tmp_file,
                 ),
             ]
-            resp = self._api.create_commit(operations=operations, commit_message="Test create_commit", repo_id=repo_id)
+            resp = self._api.create_commit(
+                operations=operations,
+                commit_message="Test create_commit",
+                repo_id=repo_id,
+            )
             # Check commit info
             self.assertIsInstance(resp, CommitInfo)
             self.assertIsNone(resp.pr_url)  # No pr created
@@ -722,7 +836,11 @@ class CommitApiTest(HfApiCommonTest):
         parent_commit = self._api.model_info(repo_id).sha
 
         # Upload new file
-        self._api.upload_file(path_or_fileobj=self.tmp_file, path_in_repo="temp/new_file.md", repo_id=repo_id)
+        self._api.upload_file(
+            path_or_fileobj=self.tmp_file,
+            path_in_repo="temp/new_file.md",
+            repo_id=repo_id,
+        )
 
         # Creating a commit with a parent commit that is not the current main should fail
         operations = [
@@ -793,7 +911,9 @@ class CommitApiTest(HfApiCommonTest):
             # Upload a PNG file
             self._api.create_commit(
                 operations=[
-                    CommitOperationAdd(path_in_repo="image.png", path_or_fileobj=b"image data"),
+                    CommitOperationAdd(
+                        path_in_repo="image.png", path_or_fileobj=b"image data"
+                    ),
                 ],
                 commit_message="Test upload lfs file",
                 repo_id=repo_id,
@@ -882,7 +1002,9 @@ class CommitApiTest(HfApiCommonTest):
             commit_message="Add 1 regular and 1 LFs files.",
             operations=[
                 CommitOperationAdd(path_in_repo="file.txt", path_or_fileobj=b"content"),
-                CommitOperationAdd(path_in_repo="lfs.bin", path_or_fileobj=b"LFS content"),
+                CommitOperationAdd(
+                    path_in_repo="lfs.bin", path_or_fileobj=b"LFS content"
+                ),
             ],
         )
         repo_files = self._api.list_repo_files(repo_id=repo_id)
@@ -897,28 +1019,43 @@ class CommitApiTest(HfApiCommonTest):
         """
         repo_id = repo_url.repo_id
 
-        self._api.upload_file(path_or_fileobj=b"content", repo_id=repo_id, path_in_repo="file.txt")
-        self._api.upload_file(path_or_fileobj=b"LFS content", repo_id=repo_id, path_in_repo="lfs.bin")
+        self._api.upload_file(
+            path_or_fileobj=b"content", repo_id=repo_id, path_in_repo="file.txt"
+        )
+        self._api.upload_file(
+            path_or_fileobj=b"LFS content", repo_id=repo_id, path_in_repo="lfs.bin"
+        )
 
         self._api.create_commit(
             repo_id=repo_id,
             commit_message="Copy LFS file.",
             operations=[
-                CommitOperationCopy(src_path_in_repo="lfs.bin", path_in_repo="lfs Copy.bin"),
-                CommitOperationCopy(src_path_in_repo="lfs.bin", path_in_repo="lfs Copy (1).bin"),
+                CommitOperationCopy(
+                    src_path_in_repo="lfs.bin", path_in_repo="lfs Copy.bin"
+                ),
+                CommitOperationCopy(
+                    src_path_in_repo="lfs.bin", path_in_repo="lfs Copy (1).bin"
+                ),
             ],
         )
         self._api.create_commit(
             repo_id=repo_id,
             commit_message="Copy regular file.",
-            operations=[CommitOperationCopy(src_path_in_repo="file.txt", path_in_repo="file Copy.txt")],
+            operations=[
+                CommitOperationCopy(
+                    src_path_in_repo="file.txt", path_in_repo="file Copy.txt"
+                )
+            ],
         )
         with self.assertRaises(EntryNotFoundError):
             self._api.create_commit(
                 repo_id=repo_id,
                 commit_message="Copy a file that doesn't exist.",
                 operations=[
-                    CommitOperationCopy(src_path_in_repo="doesnt-exist.txt", path_in_repo="doesnt-exist Copy.txt")
+                    CommitOperationCopy(
+                        src_path_in_repo="doesnt-exist.txt",
+                        path_in_repo="doesnt-exist Copy.txt",
+                    )
                 ],
             )
 
@@ -931,7 +1068,9 @@ class CommitApiTest(HfApiCommonTest):
         self.assertIn("lfs Copy (1).bin", repo_files)
 
         # Check same LFS file
-        repo_file1, repo_file2 = self._api.get_paths_info(repo_id=repo_id, paths=["lfs.bin", "lfs Copy.bin"])
+        repo_file1, repo_file2 = self._api.get_paths_info(
+            repo_id=repo_id, paths=["lfs.bin", "lfs Copy.bin"]
+        )
         self.assertEqual(repo_file1.lfs["sha256"], repo_file2.lfs["sha256"])
 
     @use_tmp_repo()
@@ -950,7 +1089,9 @@ class CommitApiTest(HfApiCommonTest):
 
         assert operations[0]._is_committed
         assert operations[0]._is_uploaded  # LFS file
-        self.assertEqual(operations[0].path_or_fileobj, b"content")  # not removed by default
+        self.assertEqual(
+            operations[0].path_or_fileobj, b"content"
+        )  # not removed by default
         assert operations[1]._is_committed
         self.assertEqual(operations[1].path_or_fileobj, b"content")
 
@@ -991,9 +1132,16 @@ class CommitApiTest(HfApiCommonTest):
                 self._api.create_commit(
                     repo_id=repo_url.repo_id,
                     operations=[
-                        CommitOperationAdd(path_in_repo="README.md", path_or_fileobj=INVALID_MODELCARD.encode()),
-                        CommitOperationAdd(path_in_repo="file.txt", path_or_fileobj=b"content"),
-                        CommitOperationAdd(path_in_repo="lfs.bin", path_or_fileobj=b"content"),
+                        CommitOperationAdd(
+                            path_in_repo="README.md",
+                            path_or_fileobj=INVALID_MODELCARD.encode(),
+                        ),
+                        CommitOperationAdd(
+                            path_in_repo="file.txt", path_or_fileobj=b"content"
+                        ),
+                        CommitOperationAdd(
+                            path_in_repo="lfs.bin", path_or_fileobj=b"content"
+                        ),
                     ],
                     commit_message="Test commit",
                 )
@@ -1004,13 +1152,21 @@ class CommitApiTest(HfApiCommonTest):
     @use_tmp_repo()
     def test_commit_modelcard_empty_metadata(self, repo_url: RepoUrl) -> None:
         modelcard = "This is a modelcard without metadata"
-        with self.assertWarnsRegex(UserWarning, "Warnings while validating metadata in README.md"):
+        with self.assertWarnsRegex(
+            UserWarning, "Warnings while validating metadata in README.md"
+        ):
             commit = self._api.create_commit(
                 repo_id=repo_url.repo_id,
                 operations=[
-                    CommitOperationAdd(path_in_repo="README.md", path_or_fileobj=modelcard.encode()),
-                    CommitOperationAdd(path_in_repo="file.txt", path_or_fileobj=b"content"),
-                    CommitOperationAdd(path_in_repo="lfs.bin", path_or_fileobj=b"content"),
+                    CommitOperationAdd(
+                        path_in_repo="README.md", path_or_fileobj=modelcard.encode()
+                    ),
+                    CommitOperationAdd(
+                        path_in_repo="file.txt", path_or_fileobj=b"content"
+                    ),
+                    CommitOperationAdd(
+                        path_in_repo="lfs.bin", path_or_fileobj=b"content"
+                    ),
                 ],
                 commit_message="Test commit",
             )
@@ -1028,13 +1184,17 @@ class CommitApiTest(HfApiCommonTest):
         """
         repo_id = self._api.create_repo(repo_id=repo_name()).repo_id
         with self.assertRaises(HfHubHTTPError) as cm:
-            self._api.upload_file(path_or_fileobj=b"content", path_in_repo="..\\ddd", repo_id=repo_id)
+            self._api.upload_file(
+                path_or_fileobj=b"content", path_in_repo="..\\ddd", repo_id=repo_id
+            )
         assert cm.exception.response.status_code == 422
 
     @use_tmp_repo()
     def test_prevent_empty_commit_if_no_op(self, repo_url: RepoUrl) -> None:
         with self.assertLogs("huggingface_hub", level="INFO") as logs:
-            self._api.create_commit(repo_id=repo_url.repo_id, commit_message="Empty commit", operations=[])
+            self._api.create_commit(
+                repo_id=repo_url.repo_id, commit_message="Empty commit", operations=[]
+            )
         assert (
             logs.records[0].message
             == "No files have been modified since last commit. Skipping to prevent empty commit."
@@ -1047,8 +1207,12 @@ class CommitApiTest(HfApiCommonTest):
             repo_id=repo_url.repo_id,
             commit_message="initial commit",
             operations=[
-                CommitOperationAdd(path_or_fileobj=b"Regular file content", path_in_repo="file.txt"),
-                CommitOperationAdd(path_or_fileobj=b"LFS content", path_in_repo="lfs.bin"),
+                CommitOperationAdd(
+                    path_or_fileobj=b"Regular file content", path_in_repo="file.txt"
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"LFS content", path_in_repo="lfs.bin"
+                ),
             ],
         )
         with self.assertLogs("huggingface_hub", level="INFO") as logs:
@@ -1056,11 +1220,18 @@ class CommitApiTest(HfApiCommonTest):
                 repo_id=repo_url.repo_id,
                 commit_message="Empty commit",
                 operations=[
-                    CommitOperationAdd(path_or_fileobj=b"Regular file content", path_in_repo="file.txt"),
-                    CommitOperationAdd(path_or_fileobj=b"LFS content", path_in_repo="lfs.bin"),
+                    CommitOperationAdd(
+                        path_or_fileobj=b"Regular file content", path_in_repo="file.txt"
+                    ),
+                    CommitOperationAdd(
+                        path_or_fileobj=b"LFS content", path_in_repo="lfs.bin"
+                    ),
                 ],
             )
-        assert logs.records[0].message == "Removing 2 file(s) from commit that have not changed."
+        assert (
+            logs.records[0].message
+            == "Removing 2 file(s) from commit that have not changed."
+        )
         assert logs.records[0].levelname == "INFO"
 
         assert (
@@ -1076,10 +1247,19 @@ class CommitApiTest(HfApiCommonTest):
             repo_id=repo_url.repo_id,
             commit_message="initial commit",
             operations=[
-                CommitOperationAdd(path_or_fileobj=b"Regular file content", path_in_repo="file.txt"),
-                CommitOperationAdd(path_or_fileobj=b"Regular file content", path_in_repo="file_copy.txt"),
-                CommitOperationAdd(path_or_fileobj=b"LFS content", path_in_repo="lfs.bin"),
-                CommitOperationAdd(path_or_fileobj=b"LFS content", path_in_repo="lfs_copy.bin"),
+                CommitOperationAdd(
+                    path_or_fileobj=b"Regular file content", path_in_repo="file.txt"
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"Regular file content",
+                    path_in_repo="file_copy.txt",
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"LFS content", path_in_repo="lfs.bin"
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"LFS content", path_in_repo="lfs_copy.bin"
+                ),
             ],
         )
         with self.assertLogs("huggingface_hub", level="INFO") as logs:
@@ -1087,11 +1267,18 @@ class CommitApiTest(HfApiCommonTest):
                 repo_id=repo_url.repo_id,
                 commit_message="Empty commit",
                 operations=[
-                    CommitOperationCopy(src_path_in_repo="file.txt", path_in_repo="file_copy.txt"),
-                    CommitOperationCopy(src_path_in_repo="lfs.bin", path_in_repo="lfs_copy.bin"),
+                    CommitOperationCopy(
+                        src_path_in_repo="file.txt", path_in_repo="file_copy.txt"
+                    ),
+                    CommitOperationCopy(
+                        src_path_in_repo="lfs.bin", path_in_repo="lfs_copy.bin"
+                    ),
                 ],
             )
-        assert logs.records[0].message == "Removing 2 file(s) from commit that have not changed."
+        assert (
+            logs.records[0].message
+            == "Removing 2 file(s) from commit that have not changed."
+        )
         assert logs.records[0].levelname == "INFO"
 
         assert (
@@ -1117,7 +1304,9 @@ class CommitApiTest(HfApiCommonTest):
                 revision=pr.git_reference,
             )
 
-        commits = self._api.list_repo_commits(repo_id=repo_url.repo_id, revision=pr.git_reference)
+        commits = self._api.list_repo_commits(
+            repo_id=repo_url.repo_id, revision=pr.git_reference
+        )
         assert len(commits) == 1  # no 2nd commit
         assert url.oid == commits[0].commit_id
 
@@ -1127,10 +1316,18 @@ class CommitApiTest(HfApiCommonTest):
             repo_id=repo_url.repo_id,
             commit_message="initial commit",
             operations=[
-                CommitOperationAdd(path_or_fileobj=b"content 1.0", path_in_repo="file.txt"),
-                CommitOperationAdd(path_or_fileobj=b"content 2.0", path_in_repo="file2.txt"),
-                CommitOperationAdd(path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs.bin"),
-                CommitOperationAdd(path_or_fileobj=b"LFS content 2.0", path_in_repo="lfs2.bin"),
+                CommitOperationAdd(
+                    path_or_fileobj=b"content 1.0", path_in_repo="file.txt"
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"content 2.0", path_in_repo="file2.txt"
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs.bin"
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"LFS content 2.0", path_in_repo="lfs2.bin"
+                ),
             ],
         )
         with self.assertLogs("huggingface_hub", level="DEBUG") as logs:
@@ -1139,25 +1336,43 @@ class CommitApiTest(HfApiCommonTest):
                 commit_message="second commit",
                 operations=[
                     # Did not change => will be removed from commit
-                    CommitOperationAdd(path_or_fileobj=b"content 1.0", path_in_repo="file.txt"),
+                    CommitOperationAdd(
+                        path_or_fileobj=b"content 1.0", path_in_repo="file.txt"
+                    ),
                     # Change => will be kept
-                    CommitOperationAdd(path_or_fileobj=b"content 2.1", path_in_repo="file2.txt"),
+                    CommitOperationAdd(
+                        path_or_fileobj=b"content 2.1", path_in_repo="file2.txt"
+                    ),
                     # New file => will be kept
-                    CommitOperationAdd(path_or_fileobj=b"content 3.0", path_in_repo="file3.txt"),
+                    CommitOperationAdd(
+                        path_or_fileobj=b"content 3.0", path_in_repo="file3.txt"
+                    ),
                     # Did not change => will be removed from commit
-                    CommitOperationAdd(path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs.bin"),
+                    CommitOperationAdd(
+                        path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs.bin"
+                    ),
                     # Change => will be kept
-                    CommitOperationAdd(path_or_fileobj=b"LFS content 2.1", path_in_repo="lfs2.bin"),
+                    CommitOperationAdd(
+                        path_or_fileobj=b"LFS content 2.1", path_in_repo="lfs2.bin"
+                    ),
                     # New file => will be kept
-                    CommitOperationAdd(path_or_fileobj=b"LFS content 3.0", path_in_repo="lfs3.bin"),
+                    CommitOperationAdd(
+                        path_or_fileobj=b"LFS content 3.0", path_in_repo="lfs3.bin"
+                    ),
                 ],
             )
         debug_logs = [log.message for log in logs.records if log.levelname == "DEBUG"]
         info_logs = [log.message for log in logs.records if log.levelname == "INFO"]
-        warning_logs = [log.message for log in logs.records if log.levelname == "WARNING"]
+        warning_logs = [
+            log.message for log in logs.records if log.levelname == "WARNING"
+        ]
 
-        assert "Skipping upload for 'file.txt' as the file has not changed." in debug_logs
-        assert "Skipping upload for 'lfs.bin' as the file has not changed." in debug_logs
+        assert (
+            "Skipping upload for 'file.txt' as the file has not changed." in debug_logs
+        )
+        assert (
+            "Skipping upload for 'lfs.bin' as the file has not changed." in debug_logs
+        )
         assert "Removing 2 file(s) from commit that have not changed." in info_logs
         assert len(warning_logs) == 0  # no warnings since the commit is not empty
 
@@ -1165,7 +1380,14 @@ class CommitApiTest(HfApiCommonTest):
             item.path: item.last_commit
             for item in self._api.get_paths_info(
                 repo_id=repo_url.repo_id,
-                paths=["file.txt", "file2.txt", "file3.txt", "lfs.bin", "lfs2.bin", "lfs3.bin"],
+                paths=[
+                    "file.txt",
+                    "file2.txt",
+                    "file3.txt",
+                    "lfs.bin",
+                    "lfs2.bin",
+                    "lfs3.bin",
+                ],
                 expand=True,
             )
         }
@@ -1184,12 +1406,24 @@ class CommitApiTest(HfApiCommonTest):
             repo_id=repo_url.repo_id,
             commit_message="initial commit",
             operations=[
-                CommitOperationAdd(path_or_fileobj=b"content 1.0", path_in_repo="file.txt"),
-                CommitOperationAdd(path_or_fileobj=b"content 1.0", path_in_repo="file_copy.txt"),
-                CommitOperationAdd(path_or_fileobj=b"content 2.0", path_in_repo="file2.txt"),
-                CommitOperationAdd(path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs.bin"),
-                CommitOperationAdd(path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs_copy.bin"),
-                CommitOperationAdd(path_or_fileobj=b"LFS content 2.0", path_in_repo="lfs2.bin"),
+                CommitOperationAdd(
+                    path_or_fileobj=b"content 1.0", path_in_repo="file.txt"
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"content 1.0", path_in_repo="file_copy.txt"
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"content 2.0", path_in_repo="file2.txt"
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs.bin"
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"LFS content 1.0", path_in_repo="lfs_copy.bin"
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"LFS content 2.0", path_in_repo="lfs2.bin"
+                ),
             ],
         )
         with self.assertLogs("huggingface_hub", level="DEBUG") as logs:
@@ -1198,22 +1432,36 @@ class CommitApiTest(HfApiCommonTest):
                 commit_message="second commit",
                 operations=[
                     # Did not change => will be removed from commit
-                    CommitOperationCopy(src_path_in_repo="file.txt", path_in_repo="file_copy.txt"),
+                    CommitOperationCopy(
+                        src_path_in_repo="file.txt", path_in_repo="file_copy.txt"
+                    ),
                     # Change => will be kept
-                    CommitOperationCopy(src_path_in_repo="file2.txt", path_in_repo="file.txt"),
+                    CommitOperationCopy(
+                        src_path_in_repo="file2.txt", path_in_repo="file.txt"
+                    ),
                     # New file => will be kept
-                    CommitOperationCopy(src_path_in_repo="file2.txt", path_in_repo="file3.txt"),
+                    CommitOperationCopy(
+                        src_path_in_repo="file2.txt", path_in_repo="file3.txt"
+                    ),
                     # Did not change => will be removed from commit
-                    CommitOperationCopy(src_path_in_repo="lfs.bin", path_in_repo="lfs_copy.bin"),
+                    CommitOperationCopy(
+                        src_path_in_repo="lfs.bin", path_in_repo="lfs_copy.bin"
+                    ),
                     # Change => will be kept
-                    CommitOperationCopy(src_path_in_repo="lfs2.bin", path_in_repo="lfs.bin"),
+                    CommitOperationCopy(
+                        src_path_in_repo="lfs2.bin", path_in_repo="lfs.bin"
+                    ),
                     # New file => will be kept
-                    CommitOperationCopy(src_path_in_repo="lfs2.bin", path_in_repo="lfs3.bin"),
+                    CommitOperationCopy(
+                        src_path_in_repo="lfs2.bin", path_in_repo="lfs3.bin"
+                    ),
                 ],
             )
         debug_logs = [log.message for log in logs.records if log.levelname == "DEBUG"]
         info_logs = [log.message for log in logs.records if log.levelname == "INFO"]
-        warning_logs = [log.message for log in logs.records if log.levelname == "WARNING"]
+        warning_logs = [
+            log.message for log in logs.records if log.levelname == "WARNING"
+        ]
 
         assert (
             "Skipping copy for 'file.txt' -> 'file_copy.txt' as the content of the source file is the same as the destination file."
@@ -1256,9 +1504,15 @@ class CommitApiTest(HfApiCommonTest):
             repo_id=repo_url.repo_id,
             commit_message="initial commit",
             operations=[
-                CommitOperationAdd(path_or_fileobj=b"content 1.0", path_in_repo="file.txt"),
-                CommitOperationAdd(path_or_fileobj=b"content 1.0", path_in_repo="file_copy.txt"),
-                CommitOperationAdd(path_or_fileobj=b"content 2.0", path_in_repo="file2.txt"),
+                CommitOperationAdd(
+                    path_or_fileobj=b"content 1.0", path_in_repo="file.txt"
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"content 1.0", path_in_repo="file_copy.txt"
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"content 2.0", path_in_repo="file2.txt"
+                ),
             ],
         )
         with self.assertLogs("huggingface_hub", level="DEBUG") as logs:
@@ -1267,18 +1521,26 @@ class CommitApiTest(HfApiCommonTest):
                 commit_message="second commit",
                 operations=[
                     # Did not change => will be removed from commit
-                    CommitOperationAdd(path_or_fileobj=b"content 1.0", path_in_repo="file.txt"),
+                    CommitOperationAdd(
+                        path_or_fileobj=b"content 1.0", path_in_repo="file.txt"
+                    ),
                     # identical to file.txt => will be removed from commit
-                    CommitOperationCopy(src_path_in_repo="file.txt", path_in_repo="file_copy.txt"),
+                    CommitOperationCopy(
+                        src_path_in_repo="file.txt", path_in_repo="file_copy.txt"
+                    ),
                     # Delete operation => kept in any case
                     CommitOperationDelete(path_in_repo="file2.txt"),
                 ],
             )
         debug_logs = [log.message for log in logs.records if log.levelname == "DEBUG"]
         info_logs = [log.message for log in logs.records if log.levelname == "INFO"]
-        warning_logs = [log.message for log in logs.records if log.levelname == "WARNING"]
+        warning_logs = [
+            log.message for log in logs.records if log.levelname == "WARNING"
+        ]
 
-        assert "Skipping upload for 'file.txt' as the file has not changed." in debug_logs
+        assert (
+            "Skipping upload for 'file.txt' as the file has not changed." in debug_logs
+        )
         assert (
             "Skipping copy for 'file.txt' -> 'file_copy.txt' as the content of the source file is the same as the destination file."
             in debug_logs
@@ -1306,7 +1568,9 @@ class HfApiUploadEmptyFileTest(HfApiComm
 
     def test_upload_empty_lfs_file(self) -> None:
         # Should have been an LFS file, but uploaded as regular (would fail otherwise)
-        self._api.upload_file(repo_id=self.repo_id, path_in_repo="empty.pkl", path_or_fileobj=b"")
+        self._api.upload_file(
+            repo_id=self.repo_id, path_in_repo="empty.pkl", path_or_fileobj=b""
+        )
         info = self._api.repo_info(repo_id=self.repo_id, files_metadata=True)
 
         repo_file = {file.rfilename: file for file in info.siblings}["empty.pkl"]
@@ -1374,7 +1638,9 @@ class HfApiListFilesInfoTest(HfApiCommon
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="file.md"),
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="lfs.bin"),
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="1/file_1.md"),
-                CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="1/2/file_1_2.md"),
+                CommitOperationAdd(
+                    path_or_fileobj=b"data", path_in_repo="1/2/file_1_2.md"
+                ),
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="2/file_2.md"),
             ],
         )
@@ -1383,7 +1649,9 @@ class HfApiListFilesInfoTest(HfApiCommon
             repo_id=cls.repo_id,
             commit_message="Another commit",
             operations=[
-                CommitOperationAdd(path_or_fileobj=b"data2", path_in_repo="3/file_3.md"),
+                CommitOperationAdd(
+                    path_or_fileobj=b"data2", path_in_repo="3/file_3.md"
+                ),
             ],
         )
 
@@ -1405,7 +1673,9 @@ class HfApiListRepoTreeTest(HfApiCommonT
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="file.md"),
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="lfs.bin"),
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="1/file_1.md"),
-                CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="1/2/file_1_2.md"),
+                CommitOperationAdd(
+                    path_or_fileobj=b"data", path_in_repo="1/2/file_1_2.md"
+                ),
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="2/file_2.md"),
             ],
         )
@@ -1414,7 +1684,9 @@ class HfApiListRepoTreeTest(HfApiCommonT
             repo_id=cls.repo_id,
             commit_message="Another commit",
             operations=[
-                CommitOperationAdd(path_or_fileobj=b"data2", path_in_repo="3/file_3.md"),
+                CommitOperationAdd(
+                    path_or_fileobj=b"data2", path_in_repo="3/file_3.md"
+                ),
             ],
         )
 
@@ -1425,7 +1697,10 @@ class HfApiListRepoTreeTest(HfApiCommonT
     def test_list_tree(self):
         tree = list(self._api.list_repo_tree(repo_id=self.repo_id))
         self.assertEqual(len(tree), 6)
-        self.assertEqual({tree_obj.path for tree_obj in tree}, {"file.md", "lfs.bin", "1", "2", "3", ".gitattributes"})
+        self.assertEqual(
+            {tree_obj.path for tree_obj in tree},
+            {"file.md", "lfs.bin", "1", "2", "3", ".gitattributes"},
+        )
 
         tree = list(self._api.list_repo_tree(repo_id=self.repo_id, path_in_repo="1"))
         self.assertEqual(len(tree), 2)
@@ -1457,8 +1732,16 @@ class HfApiListRepoTreeTest(HfApiCommonT
 
     def test_list_with_empty_path(self):
         self.assertEqual(
-            set(tree_obj.path for tree_obj in self._api.list_repo_tree(repo_id=self.repo_id, path_in_repo="")),
-            set(tree_obj.path for tree_obj in self._api.list_repo_tree(repo_id=self.repo_id)),
+            set(
+                tree_obj.path
+                for tree_obj in self._api.list_repo_tree(
+                    repo_id=self.repo_id, path_in_repo=""
+                )
+            ),
+            set(
+                tree_obj.path
+                for tree_obj in self._api.list_repo_tree(repo_id=self.repo_id)
+            ),
         )
 
     @with_production_testing
@@ -1473,17 +1756,26 @@ class HfApiListRepoTreeTest(HfApiCommonT
         assert len(tree) == 11
 
         # check last_commit and security are present for a file
-        model_ckpt = next(tree_obj for tree_obj in tree if tree_obj.path == "openjourney-v4.ckpt")
+        model_ckpt = next(
+            tree_obj for tree_obj in tree if tree_obj.path == "openjourney-v4.ckpt"
+        )
         assert model_ckpt.last_commit is not None
-        assert model_ckpt.last_commit["oid"] == "bda967fdb79a50844e4a02cccae3217a8ecc86cd"
+        assert (
+            model_ckpt.last_commit["oid"] == "bda967fdb79a50844e4a02cccae3217a8ecc86cd"
+        )
         assert model_ckpt.security is not None
         assert model_ckpt.security["safe"]
         assert isinstance(model_ckpt.security["av_scan"], dict)  # all details in here
 
         # check last_commit is present for a folder
-        feature_extractor = next(tree_obj for tree_obj in tree if tree_obj.path == "feature_extractor")
+        feature_extractor = next(
+            tree_obj for tree_obj in tree if tree_obj.path == "feature_extractor"
+        )
         self.assertIsNotNone(feature_extractor.last_commit)
-        self.assertEqual(feature_extractor.last_commit["oid"], "47b62b20b20e06b9de610e840282b7e6c3d51190")
+        self.assertEqual(
+            feature_extractor.last_commit["oid"],
+            "47b62b20b20e06b9de610e840282b7e6c3d51190",
+        )
 
     @with_production_testing
     def test_list_files_without_expand(self):
@@ -1496,12 +1788,16 @@ class HfApiListRepoTreeTest(HfApiCommonT
         self.assertEqual(len(tree), 11)
 
         # check last_commit and security are missing for a file
-        model_ckpt = next(tree_obj for tree_obj in tree if tree_obj.path == "openjourney-v4.ckpt")
+        model_ckpt = next(
+            tree_obj for tree_obj in tree if tree_obj.path == "openjourney-v4.ckpt"
+        )
         self.assertIsNone(model_ckpt.last_commit)
         self.assertIsNone(model_ckpt.security)
 
         # check last_commit is missing for a folder
-        feature_extractor = next(tree_obj for tree_obj in tree if tree_obj.path == "feature_extractor")
+        feature_extractor = next(
+            tree_obj for tree_obj in tree if tree_obj.path == "feature_extractor"
+        )
         self.assertIsNone(feature_extractor.last_commit)
 
 
@@ -1509,7 +1805,9 @@ class HfApiTagEndpointTest(HfApiCommonTe
     @use_tmp_repo("model")
     def test_create_tag_on_main(self, repo_url: RepoUrl) -> None:
         """Check `create_tag` on default main branch works."""
-        self._api.create_tag(repo_url.repo_id, tag="v0", tag_message="This is a tag message.")
+        self._api.create_tag(
+            repo_url.repo_id, tag="v0", tag_message="This is a tag message."
+        )
 
         # Check tag  is on `main`
         tag_info = self._api.model_info(repo_url.repo_id, revision="v0")
@@ -1524,15 +1822,23 @@ class HfApiTagEndpointTest(HfApiCommonTe
             repo_id=repo_url.repo_id,
             create_pr=True,
             commit_message="upload readme",
-            operations=[CommitOperationAdd(path_or_fileobj=b"this is a file content", path_in_repo="readme.md")],
+            operations=[
+                CommitOperationAdd(
+                    path_or_fileobj=b"this is a file content", path_in_repo="readme.md"
+                )
+            ],
         )
 
         # Tag the PR
-        self._api.create_tag(repo_url.repo_id, tag="v0", revision=commit_info.pr_revision)
+        self._api.create_tag(
+            repo_url.repo_id, tag="v0", revision=commit_info.pr_revision
+        )
 
         # Check tag  is on `refs/pr/1`
         tag_info = self._api.model_info(repo_url.repo_id, revision="v0")
-        pr_info = self._api.model_info(repo_url.repo_id, revision=commit_info.pr_revision)
+        pr_info = self._api.model_info(
+            repo_url.repo_id, revision=commit_info.pr_revision
+        )
         main_info = self._api.model_info(repo_url.repo_id)
 
         self.assertEqual(tag_info.sha, pr_info.sha)
@@ -1549,13 +1855,21 @@ class HfApiTagEndpointTest(HfApiCommonTe
             repo_id=repo_url.repo_id,
             repo_type="dataset",
             commit_message="upload readme",
-            operations=[CommitOperationAdd(path_or_fileobj=b"this is a file content", path_in_repo="readme.md")],
+            operations=[
+                CommitOperationAdd(
+                    path_or_fileobj=b"this is a file content", path_in_repo="readme.md"
+                )
+            ],
         )
         commit_info_2: CommitInfo = self._api.create_commit(
             repo_id=repo_url.repo_id,
             repo_type="dataset",
             commit_message="upload config",
-            operations=[CommitOperationAdd(path_or_fileobj=b"{'hello': 'world'}", path_in_repo="config.json")],
+            operations=[
+                CommitOperationAdd(
+                    path_or_fileobj=b"{'hello': 'world'}", path_in_repo="config.json"
+                )
+            ],
         )
 
         # Tag commits
@@ -1703,15 +2017,21 @@ class HfApiBranchEndpointTest(HfApiCommo
         initial_commit = self._api.model_info(repo_url.repo_id).sha
         commit = self._api.create_commit(
             repo_url.repo_id,
-            operations=[CommitOperationAdd(path_in_repo="app.py", path_or_fileobj=b"content")],
+            operations=[
+                CommitOperationAdd(path_in_repo="app.py", path_or_fileobj=b"content")
+            ],
             commit_message="test commit",
         )
         latest_commit = commit.oid
 
         # Create branches
         self._api.create_branch(repo_url.repo_id, branch="from-head")
-        self._api.create_branch(repo_url.repo_id, branch="from-initial", revision=initial_commit)
-        self._api.create_branch(repo_url.repo_id, branch="from-branch", revision="from-initial")
+        self._api.create_branch(
+            repo_url.repo_id, branch="from-initial", revision=initial_commit
+        )
+        self._api.create_branch(
+            repo_url.repo_id, branch="from-branch", revision="from-initial"
+        )
         time.sleep(0.2)  # hack: wait for server to update cache?
 
         # Checks branches start from expected commits
@@ -1722,7 +2042,10 @@ class HfApiBranchEndpointTest(HfApiCommo
                 "from-initial": initial_commit,
                 "from-branch": initial_commit,
             },
-            {ref.name: ref.target_commit for ref in self._api.list_repo_refs(repo_id=repo_url.repo_id).branches},
+            {
+                ref.name: ref.target_commit
+                for ref in self._api.list_repo_refs(repo_id=repo_url.repo_id).branches
+            },
         )
 
 
@@ -1736,12 +2059,20 @@ class HfApiDeleteFilesTest(HfApiCommonTe
             operations=[
                 # Regular files
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="file.txt"),
-                CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="nested/file.txt"),
-                CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="nested/sub/file.txt"),
+                CommitOperationAdd(
+                    path_or_fileobj=b"data", path_in_repo="nested/file.txt"
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"data", path_in_repo="nested/sub/file.txt"
+                ),
                 # LFS files
                 CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="lfs.bin"),
-                CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="nested/lfs.bin"),
-                CommitOperationAdd(path_or_fileobj=b"data", path_in_repo="nested/sub/lfs.bin"),
+                CommitOperationAdd(
+                    path_or_fileobj=b"data", path_in_repo="nested/lfs.bin"
+                ),
+                CommitOperationAdd(
+                    path_or_fileobj=b"data", path_in_repo="nested/sub/lfs.bin"
+                ),
             ],
             commit_message="Init repo structure",
         )
@@ -1758,7 +2089,9 @@ class HfApiDeleteFilesTest(HfApiCommonTe
         assert "file.txt" not in self.remote_files()
 
     def test_delete_multiple_files(self):
-        self._api.delete_files(repo_id=self.repo_id, delete_patterns=["file.txt", "lfs.bin"])
+        self._api.delete_files(
+            repo_id=self.repo_id, delete_patterns=["file.txt", "lfs.bin"]
+        )
         files = self.remote_files()
         assert "file.txt" not in files
         assert "lfs.bin" not in files
@@ -1772,7 +2105,9 @@ class HfApiDeleteFilesTest(HfApiCommonTe
         assert self.remote_files() == {".gitattributes", "file.txt", "lfs.bin"}
 
     def test_unknown_path_do_not_raise(self):
-        self._api.delete_files(repo_id=self.repo_id, delete_patterns=["not_existing", "nested/*"])
+        self._api.delete_files(
+            repo_id=self.repo_id, delete_patterns=["not_existing", "nested/*"]
+        )
         assert self.remote_files() == {".gitattributes", "file.txt", "lfs.bin"}
 
     def test_delete_bin_files_with_patterns(self):
@@ -1836,7 +2171,11 @@ class HfApiPublicProductionTest(unittest
         # Let's list the 10 most recent models
         # with tags "bert" and "jax",
         # ordered by last modified date.
-        models = list(self._api.list_models(filter=("bert", "jax"), sort="last_modified", direction=-1, limit=10))
+        models = list(
+            self._api.list_models(
+                filter=("bert", "jax"), sort="last_modified", direction=-1, limit=10
+            )
+        )
         # we have at least 1 models
         assert len(models) > 1
         assert len(models) <= 10
@@ -1869,11 +2208,15 @@ class HfApiPublicProductionTest(unittest
         assert all(model.likes is not None for model in models)
 
     def test_list_models_with_config(self):
-        for model in self._api.list_models(filter=("adapter-transformers", "bert"), fetch_config=True, limit=20):
+        for model in self._api.list_models(
+            filter=("adapter-transformers", "bert"), fetch_config=True, limit=20
+        ):
             self.assertIsNotNone(model.config)
 
     def test_list_models_without_config(self):
-        for model in self._api.list_models(filter=("adapter-transformers", "bert"), fetch_config=False, limit=20):
+        for model in self._api.list_models(
+            filter=("adapter-transformers", "bert"), fetch_config=False, limit=20
+        ):
             self.assertIsNone(model.config)
 
     def test_list_models_expand_author(self):
@@ -1888,7 +2231,9 @@ class HfApiPublicProductionTest(unittest
 
     def test_list_models_expand_multiple(self):
         # Only the selected fields are returned
-        models = list(self._api.list_models(expand=["author", "downloadsAllTime"], limit=5))
+        models = list(
+            self._api.list_models(expand=["author", "downloadsAllTime"], limit=5)
+        )
         for model in models:
             assert model.author is not None
             assert model.downloads_all_time is not None
@@ -1919,22 +2264,31 @@ class HfApiPublicProductionTest(unittest
 
     @pytest.mark.skip("Inference parameter is being revamped")
     def test_list_models_inference_warm(self):
-        for model in self._api.list_models(inference=["warm"], expand="inference", limit=5):
+        for model in self._api.list_models(
+            inference=["warm"], expand="inference", limit=5
+        ):
             assert model.inference == "warm"
 
     @pytest.mark.skip("Inference parameter is being revamped")
     def test_list_models_inference_cold(self):
-        for model in self._api.list_models(inference=["cold"], expand="inference", limit=5):
+        for model in self._api.list_models(
+            inference=["cold"], expand="inference", limit=5
+        ):
             assert model.inference == "cold"
 
     def test_model_info(self):
         model = self._api.model_info(repo_id=DUMMY_MODEL_ID)
         self.assertIsInstance(model, ModelInfo)
         self.assertNotEqual(model.sha, DUMMY_MODEL_ID_REVISION_ONE_SPECIFIC_COMMIT)
-        self.assertEqual(model.created_at, datetime.datetime(2022, 3, 2, 23, 29, 5, tzinfo=datetime.timezone.utc))
+        self.assertEqual(
+            model.created_at,
+            datetime.datetime(2022, 3, 2, 23, 29, 5, tzinfo=datetime.timezone.utc),
+        )
 
         # One particular commit (not the top of `main`)
-        model = self._api.model_info(repo_id=DUMMY_MODEL_ID, revision=DUMMY_MODEL_ID_REVISION_ONE_SPECIFIC_COMMIT)
+        model = self._api.model_info(
+            repo_id=DUMMY_MODEL_ID, revision=DUMMY_MODEL_ID_REVISION_ONE_SPECIFIC_COMMIT
+        )
         self.assertIsInstance(model, ModelInfo)
         self.assertEqual(model.sha, DUMMY_MODEL_ID_REVISION_ONE_SPECIFIC_COMMIT)
 
@@ -1947,7 +2301,9 @@ class HfApiPublicProductionTest(unittest
             securityStatus=True,
         )
         self.assertIsNotNone(model.security_repo_status)
-        self.assertEqual(model.security_repo_status, {"scansDone": True, "filesWithIssues": []})
+        self.assertEqual(
+            model.security_repo_status, {"scansDone": True, "filesWithIssues": []}
+        )
 
     def test_model_info_with_file_metadata(self):
         model = self._api.model_info(
@@ -1980,7 +2336,10 @@ class HfApiPublicProductionTest(unittest
                     "gitalyUid": "53c57f29a007fc728c968127061b7b740dcf2b1ad401d907f703b27658559413",
                     "likes": 0,
                     "private": False,
-                    "config": {"architectures": ["Wav2Vec2ForCTC"], "model_type": "wav2vec2"},
+                    "config": {
+                        "architectures": ["Wav2Vec2ForCTC"],
+                        "model_type": "wav2vec2",
+                    },
                     "downloads": 1,
                     "tags": [
                         "transformers",
@@ -2007,7 +2366,9 @@ class HfApiPublicProductionTest(unittest
 
     def test_model_info_expand_author(self):
         # Only the selected field is returned
-        model = self._api.model_info(repo_id="HuggingFaceH4/zephyr-7b-beta", expand=["author"])
+        model = self._api.model_info(
+            repo_id="HuggingFaceH4/zephyr-7b-beta", expand=["author"]
+        )
         assert model.author == "HuggingFaceH4"
         assert model.downloads is None
         assert model.created_at is None
@@ -2015,7 +2376,10 @@ class HfApiPublicProductionTest(unittest
 
     def test_model_info_expand_multiple(self):
         # Only the selected fields are returned
-        model = self._api.model_info(repo_id="HuggingFaceH4/zephyr-7b-beta", expand=["author", "downloadsAllTime"])
+        model = self._api.model_info(
+            repo_id="HuggingFaceH4/zephyr-7b-beta",
+            expand=["author", "downloadsAllTime"],
+        )
         assert model.author == "HuggingFaceH4"
         assert model.downloads is None
         assert model.downloads_all_time is not None
@@ -2031,9 +2395,13 @@ class HfApiPublicProductionTest(unittest
     def test_model_info_expand_cannot_be_used_with_other_params(self):
         # `expand` cannot be used with other params
         with self.assertRaises(ValueError):
-            self._api.model_info("HuggingFaceH4/zephyr-7b-beta", expand=["author"], securityStatus=True)
+            self._api.model_info(
+                "HuggingFaceH4/zephyr-7b-beta", expand=["author"], securityStatus=True
+            )
         with self.assertRaises(ValueError):
-            self._api.model_info("HuggingFaceH4/zephyr-7b-beta", expand=["author"], files_metadata=True)
+            self._api.model_info(
+                "HuggingFaceH4/zephyr-7b-beta", expand=["author"], files_metadata=True
+            )
 
     def test_list_repo_files(self):
         files = self._api.list_repo_files(repo_id=DUMMY_MODEL_ID)
@@ -2055,7 +2423,11 @@ class HfApiPublicProductionTest(unittest
         self.assertIsInstance(datasets[0], DatasetInfo)
 
     def test_filter_datasets_by_author_and_name(self):
-        datasets = list(self._api.list_datasets(author="huggingface", dataset_name="DataMeasurementsFiles"))
+        datasets = list(
+            self._api.list_datasets(
+                author="huggingface", dataset_name="DataMeasurementsFiles"
+            )
+        )
         assert len(datasets) > 0
         assert "huggingface" in datasets[0].author
         assert "DataMeasurementsFiles" in datasets[0].id
@@ -2081,7 +2453,9 @@ class HfApiPublicProductionTest(unittest
         assert "language:fr" in datasets[0].tags
 
     def test_filter_datasets_by_multilinguality(self):
-        datasets = list(self._api.list_datasets(multilinguality="multilingual", limit=10))
+        datasets = list(
+            self._api.list_datasets(multilinguality="multilingual", limit=10)
+        )
         assert len(datasets) > 0
         assert "multilinguality:multilingual" in datasets[0].tags
 
@@ -2091,12 +2465,16 @@ class HfApiPublicProductionTest(unittest
         assert "size_categories:100K<n<1M" in datasets[0].tags
 
     def test_filter_datasets_by_task_categories(self):
-        datasets = list(self._api.list_datasets(task_categories="audio-classification", limit=10))
+        datasets = list(
+            self._api.list_datasets(task_categories="audio-classification", limit=10)
+        )
         assert len(datasets) > 0
         assert "task_categories:audio-classification" in datasets[0].tags
 
     def test_filter_datasets_by_task_ids(self):
-        datasets = list(self._api.list_datasets(task_ids="natural-language-inference", limit=10))
+        datasets = list(
+            self._api.list_datasets(task_ids="natural-language-inference", limit=10)
+        )
         assert len(datasets) > 0
         assert "task_ids:natural-language-inference" in datasets[0].tags
 
@@ -2129,7 +2507,9 @@ class HfApiPublicProductionTest(unittest
 
     def test_list_datasets_expand_multiple(self):
         # Only the selected fields are returned
-        datasets = list(self._api.list_datasets(expand=["author", "downloadsAllTime"], limit=5))
+        datasets = list(
+            self._api.list_datasets(expand=["author", "downloadsAllTime"], limit=5)
+        )
         for dataset in datasets:
             assert dataset.author is not None
             assert dataset.downloads_all_time is not None
@@ -2155,8 +2535,14 @@ class HfApiPublicProductionTest(unittest
             assert dataset.gated is False
 
     def test_filter_datasets_with_card_data(self):
-        assert any(dataset.card_data is not None for dataset in self._api.list_datasets(full=True, limit=50))
-        assert all(dataset.card_data is None for dataset in self._api.list_datasets(full=False, limit=50))
+        assert any(
+            dataset.card_data is not None
+            for dataset in self._api.list_datasets(full=True, limit=50)
+        )
+        assert all(
+            dataset.card_data is None
+            for dataset in self._api.list_datasets(full=False, limit=50)
+        )
 
     def test_filter_datasets_by_tag(self):
         for dataset in self._api.list_datasets(tags="fiftyone", limit=5):
@@ -2164,7 +2550,10 @@ class HfApiPublicProductionTest(unittest
 
     def test_dataset_info(self):
         dataset = self._api.dataset_info(repo_id=DUMMY_DATASET_ID)
-        assert isinstance(dataset.card_data, DatasetCardData) and len(dataset.card_data) > 0
+        assert (
+            isinstance(dataset.card_data, DatasetCardData)
+            and len(dataset.card_data) > 0
+        )
         assert isinstance(dataset.siblings, list) and len(dataset.siblings) > 0
         assert isinstance(dataset, DatasetInfo)
         assert dataset.sha != DUMMY_DATASET_ID_REVISION_ONE_SPECIFIC_COMMIT
@@ -2176,7 +2565,9 @@ class HfApiPublicProductionTest(unittest
         assert dataset.sha == DUMMY_DATASET_ID_REVISION_ONE_SPECIFIC_COMMIT
 
     def test_dataset_info_with_file_metadata(self):
-        dataset = self._api.dataset_info(repo_id=SAMPLE_DATASET_IDENTIFIER, files_metadata=True)
+        dataset = self._api.dataset_info(
+            repo_id=SAMPLE_DATASET_IDENTIFIER, files_metadata=True
+        )
         files = dataset.siblings
         assert files is not None
         self._check_siblings_metadata(files)
@@ -2195,7 +2586,9 @@ class HfApiPublicProductionTest(unittest
 
     def test_dataset_info_expand_author(self):
         # Only the selected field is returned
-        dataset = self._api.dataset_info(repo_id="HuggingFaceH4/no_robots", expand=["author"])
+        dataset = self._api.dataset_info(
+            repo_id="HuggingFaceH4/no_robots", expand=["author"]
+        )
         assert dataset.author == "HuggingFaceH4"
         assert dataset.downloads is None
         assert dataset.created_at is None
@@ -2203,7 +2596,9 @@ class HfApiPublicProductionTest(unittest
 
     def test_dataset_info_expand_multiple(self):
         # Only the selected fields are returned
-        dataset = self._api.dataset_info(repo_id="HuggingFaceH4/no_robots", expand=["author", "downloadsAllTime"])
+        dataset = self._api.dataset_info(
+            repo_id="HuggingFaceH4/no_robots", expand=["author", "downloadsAllTime"]
+        )
         assert dataset.author == "HuggingFaceH4"
         assert dataset.downloads is None
         assert dataset.downloads_all_time is not None
@@ -2219,7 +2614,9 @@ class HfApiPublicProductionTest(unittest
     def test_dataset_info_expand_cannot_be_used_with_files_metadata(self):
         # `expand` cannot be used with other `files_metadata`
         with self.assertRaises(ValueError):
-            self._api.dataset_info("HuggingFaceH4/no_robots", expand=["author"], files_metadata=True)
+            self._api.dataset_info(
+                "HuggingFaceH4/no_robots", expand=["author"], files_metadata=True
+            )
 
     def test_space_info(self) -> None:
         space = self._api.space_info(repo_id="HuggingFaceH4/zephyr-chat")
@@ -2229,14 +2626,18 @@ class HfApiPublicProductionTest(unittest
 
     def test_space_info_expand_author(self):
         # Only the selected field is returned
-        space = self._api.space_info(repo_id="HuggingFaceH4/zephyr-chat", expand=["author"])
+        space = self._api.space_info(
+            repo_id="HuggingFaceH4/zephyr-chat", expand=["author"]
+        )
         assert space.author == "HuggingFaceH4"
         assert space.created_at is None
         assert space.last_modified is None
 
     def test_space_info_expand_multiple(self):
         # Only the selected fields are returned
-        space = self._api.space_info(repo_id="HuggingFaceH4/zephyr-chat", expand=["author", "likes"])
+        space = self._api.space_info(
+            repo_id="HuggingFaceH4/zephyr-chat", expand=["author", "likes"]
+        )
         assert space.author == "HuggingFaceH4"
         assert space.created_at is None
         assert space.last_modified is None
@@ -2251,7 +2652,9 @@ class HfApiPublicProductionTest(unittest
     def test_space_info_expand_cannot_be_used_with_files_metadata(self):
         # `expand` cannot be used with other files_metadata
         with self.assertRaises(ValueError):
-            self._api.space_info("HuggingFaceH4/zephyr-chat", expand=["author"], files_metadata=True)
+            self._api.space_info(
+                "HuggingFaceH4/zephyr-chat", expand=["author"], files_metadata=True
+            )
 
     def test_filter_models_by_author(self):
         models = list(self._api.list_models(author="muellerzr"))
@@ -2269,14 +2672,24 @@ class HfApiPublicProductionTest(unittest
         assert len(models) == 0
 
     def test_filter_models_with_library(self):
-        models = list(self._api.list_models(author="microsoft", model_name="wavlm-base-sd", library="tensorflow"))
+        models = list(
+            self._api.list_models(
+                author="microsoft", model_name="wavlm-base-sd", library="tensorflow"
+            )
+        )
         assert len(models) == 0
 
-        models = list(self._api.list_models(author="microsoft", model_name="wavlm-base-sd", library="pytorch"))
+        models = list(
+            self._api.list_models(
+                author="microsoft", model_name="wavlm-base-sd", library="pytorch"
+            )
+        )
         assert len(models) > 0
 
     def test_filter_models_with_task(self):
-        models = list(self._api.list_models(task="fill-mask", model_name="albert-base-v2"))
+        models = list(
+            self._api.list_models(task="fill-mask", model_name="albert-base-v2")
+        )
         assert models[0].pipeline_tag == "fill-mask"
         assert "albert" in models[0].id
         assert "base" in models[0].id
@@ -2291,7 +2704,9 @@ class HfApiPublicProductionTest(unittest
                 assert language in model.tags
 
     def test_filter_models_with_tag(self):
-        models = list(self._api.list_models(author="HuggingFaceBR4", tags=["tensorboard"]))
+        models = list(
+            self._api.list_models(author="HuggingFaceBR4", tags=["tensorboard"])
+        )
         assert models[0].id.startswith("HuggingFaceBR4/")
         assert "tensorboard" in models[0].tags
 
@@ -2310,14 +2725,19 @@ class HfApiPublicProductionTest(unittest
         # 17g is accepted and parsed correctly as a value
         # regression test for #753
         kwargs = {field.name: None for field in fields(ModelInfo) if field.init}
-        kwargs = {**kwargs, "card_data": ModelCardData(co2_eq_emissions={"emissions": "17g"})}
+        kwargs = {
+            **kwargs,
+            "card_data": ModelCardData(co2_eq_emissions={"emissions": "17g"}),
+        }
         model = ModelInfo(**kwargs)
         assert _is_emission_within_threshold(model, -1, 100)
 
     def test_filter_emissions_with_max(self):
         assert all(
             model.card_data["co2_eq_emissions"] <= 100
-            for model in self._api.list_models(emissions_thresholds=(None, 100), cardData=True, limit=1000)
+            for model in self._api.list_models(
+                emissions_thresholds=(None, 100), cardData=True, limit=1000
+            )
             if isinstance(model.card_data["co2_eq_emissions"], (float, int))
         )
 
@@ -2325,13 +2745,19 @@ class HfApiPublicProductionTest(unittest
         assert all(
             [
                 model.card_data["co2_eq_emissions"] >= 5
-                for model in self._api.list_models(emissions_thresholds=(5, None), cardData=True, limit=1000)
+                for model in self._api.list_models(
+                    emissions_thresholds=(5, None), cardData=True, limit=1000
+                )
                 if isinstance(model.card_data["co2_eq_emissions"], (float, int))
             ]
         )
 
     def test_filter_emissions_with_min_and_max(self):
-        models = list(self._api.list_models(emissions_thresholds=(5, 100), cardData=True, limit=1000))
+        models = list(
+            self._api.list_models(
+                emissions_thresholds=(5, 100), cardData=True, limit=1000
+            )
+        )
         assert all(
             [
                 model.card_data["co2_eq_emissions"] >= 5
@@ -2367,7 +2793,9 @@ class HfApiPublicProductionTest(unittest
 
     def test_list_spaces_sort_and_direction(self):
         # Descending order => first item has more likes than second
-        spaces_descending_likes = list(self._api.list_spaces(sort="likes", direction=-1, limit=100))
+        spaces_descending_likes = list(
+            self._api.list_spaces(sort="likes", direction=-1, limit=100)
+        )
         assert spaces_descending_likes[0].likes > spaces_descending_likes[1].likes
 
     def test_list_spaces_limit(self):
@@ -2385,11 +2813,19 @@ class HfApiPublicProductionTest(unittest
     def test_list_spaces_linked(self):
         space_id = "stabilityai/stable-diffusion"
 
-        spaces = [space for space in self._api.list_spaces(search=space_id) if space.id == space_id]
+        spaces = [
+            space
+            for space in self._api.list_spaces(search=space_id)
+            if space.id == space_id
+        ]
         assert spaces[0].models is None
         assert spaces[0].datasets is None
 
-        spaces = [space for space in self._api.list_spaces(search=space_id, linked=True) if space.id == space_id]
+        spaces = [
+            space
+            for space in self._api.list_spaces(search=space_id, linked=True)
+            if space.id == space_id
+        ]
         assert spaces[0].models is not None
         assert spaces[0].datasets is not None
 
@@ -2475,10 +2911,14 @@ class HfApiPublicProductionTest(unittest
 
     def test_not_a_safetensors_repo(self) -> None:
         with self.assertRaises(NotASafetensorsRepoError):
-            self._api.get_safetensors_metadata("huggingface-hub-ci/test_safetensors_metadata")
+            self._api.get_safetensors_metadata(
+                "huggingface-hub-ci/test_safetensors_metadata"
+            )
 
     def test_get_safetensors_metadata_from_revision(self) -> None:
-        info = self._api.get_safetensors_metadata("huggingface-hub-ci/test_safetensors_metadata", revision="refs/pr/1")
+        info = self._api.get_safetensors_metadata(
+            "huggingface-hub-ci/test_safetensors_metadata", revision="refs/pr/1"
+        )
         assert isinstance(info, SafetensorsRepoMetadata)
 
     def test_parse_safetensors_metadata(self) -> None:
@@ -2527,7 +2967,9 @@ class HfApiPrivateTest(HfApiCommonTest):
             ):
                 _ = self._api.model_info(repo_id=f"{USER}/{self.REPO_NAME}")
 
-            model_info = self._api.model_info(repo_id=f"{USER}/{self.REPO_NAME}", use_auth_token=self._token)
+            model_info = self._api.model_info(
+                repo_id=f"{USER}/{self.REPO_NAME}", use_auth_token=self._token
+            )
             self.assertIsInstance(model_info, ModelInfo)
 
     @patch("huggingface_hub.utils._headers.get_token", return_value=None)
@@ -2543,7 +2985,9 @@ class HfApiPrivateTest(HfApiCommonTest):
             ):
                 _ = self._api.dataset_info(repo_id=f"{USER}/{self.REPO_NAME}")
 
-            dataset_info = self._api.dataset_info(repo_id=f"{USER}/{self.REPO_NAME}", use_auth_token=self._token)
+            dataset_info = self._api.dataset_info(
+                repo_id=f"{USER}/{self.REPO_NAME}", use_auth_token=self._token
+            )
             self.assertIsInstance(dataset_info, DatasetInfo)
 
     def test_list_private_datasets(self):
@@ -2602,11 +3046,15 @@ class UploadFolderMockedTest(unittest.Te
         self.api.list_repo_files = self.repo_files_mock
 
         self.create_commit_mock = Mock()
-        self.create_commit_mock.return_value.commit_url = f"{ENDPOINT_STAGING}/username/repo_id/commit/dummy_sha"
+        self.create_commit_mock.return_value.commit_url = (
+            f"{ENDPOINT_STAGING}/username/repo_id/commit/dummy_sha"
+        )
         self.create_commit_mock.return_value.pr_url = None
         self.api.create_commit = self.create_commit_mock
 
-    def _upload_folder_alias(self, **kwargs) -> List[Union[CommitOperationAdd, CommitOperationDelete]]:
+    def _upload_folder_alias(
+        self, **kwargs
+    ) -> List[Union[CommitOperationAdd, CommitOperationDelete]]:
         """Alias to call `upload_folder` + retrieve the CommitOperation list passed to `create_commit`."""
         if "folder_path" not in kwargs:
             kwargs["folder_path"] = self.cache_dir
@@ -2619,7 +3067,9 @@ class UploadFolderMockedTest(unittest.Te
         assert {op.path_in_repo for op in operations} == self.all_local_files
 
     def test_allow_everything_in_subdir_no_trailing_slash(self):
-        operations = self._upload_folder_alias(folder_path=self.cache_dir / "subdir", path_in_repo="subdir")
+        operations = self._upload_folder_alias(
+            folder_path=self.cache_dir / "subdir", path_in_repo="subdir"
+        )
         assert all(isinstance(op, CommitOperationAdd) for op in operations)
         assert {op.path_in_repo for op in operations} == {
             # correct `path_in_repo`
@@ -2628,7 +3078,9 @@ class UploadFolderMockedTest(unittest.Te
         }
 
     def test_allow_everything_in_subdir_with_trailing_slash(self):
-        operations = self._upload_folder_alias(folder_path=self.cache_dir / "subdir", path_in_repo="subdir/")
+        operations = self._upload_folder_alias(
+            folder_path=self.cache_dir / "subdir", path_in_repo="subdir/"
+        )
         assert all(isinstance(op, CommitOperationAdd) for op in operations)
         self.assertEqual(
             {op.path_in_repo for op in operations},
@@ -2636,12 +3088,19 @@ class UploadFolderMockedTest(unittest.Te
         )
 
     def test_allow_txt_ignore_subdir(self):
-        operations = self._upload_folder_alias(allow_patterns="*.txt", ignore_patterns="subdir/*")
+        operations = self._upload_folder_alias(
+            allow_patterns="*.txt", ignore_patterns="subdir/*"
+        )
         assert all(isinstance(op, CommitOperationAdd) for op in operations)
-        assert {op.path_in_repo for op in operations} == {"sub/file.txt", "file.txt"}  # only .txt files, not in subdir
+        assert {op.path_in_repo for op in operations} == {
+            "sub/file.txt",
+            "file.txt",
+        }  # only .txt files, not in subdir
 
     def test_allow_txt_not_root_ignore_subdir(self):
-        operations = self._upload_folder_alias(allow_patterns="**/*.txt", ignore_patterns="subdir/*")
+        operations = self._upload_folder_alias(
+            allow_patterns="**/*.txt", ignore_patterns="subdir/*"
+        )
         assert all(isinstance(op, CommitOperationAdd) for op in operations)
         assert {op.path_in_repo for op in operations} == {
             # only .txt files, not in subdir, not at root
@@ -2654,15 +3113,25 @@ class UploadFolderMockedTest(unittest.Te
         Using `path_in_repo="."` or `path_in_repo=None` should be equivalent.
         See https://github.com/huggingface/huggingface_hub/pull/1382.
         """
-        operation_with_dot = self._upload_folder_alias(path_in_repo=".", allow_patterns=["file.txt"])[0]
-        operation_with_none = self._upload_folder_alias(path_in_repo=None, allow_patterns=["file.txt"])[0]
+        operation_with_dot = self._upload_folder_alias(
+            path_in_repo=".", allow_patterns=["file.txt"]
+        )[0]
+        operation_with_none = self._upload_folder_alias(
+            path_in_repo=None, allow_patterns=["file.txt"]
+        )[0]
         assert operation_with_dot.path_in_repo == "file.txt"
         assert operation_with_none.path_in_repo == "file.txt"
 
     def test_delete_txt(self):
         operations = self._upload_folder_alias(delete_patterns="*.txt")
-        added_files = {op.path_in_repo for op in operations if isinstance(op, CommitOperationAdd)}
-        deleted_files = {op.path_in_repo for op in operations if isinstance(op, CommitOperationDelete)}
+        added_files = {
+            op.path_in_repo for op in operations if isinstance(op, CommitOperationAdd)
+        }
+        deleted_files = {
+            op.path_in_repo
+            for op in operations
+            if isinstance(op, CommitOperationDelete)
+        }
 
         assert added_files == self.all_local_files
         assert deleted_files == {"file1.txt", "sub/file1.txt"}
@@ -2673,20 +3142,40 @@ class UploadFolderMockedTest(unittest.Te
 
     def test_delete_txt_in_sub(self):
         operations = self._upload_folder_alias(
-            path_in_repo="sub/", folder_path=self.cache_dir / "sub", delete_patterns="*.txt"
+            path_in_repo="sub/",
+            folder_path=self.cache_dir / "sub",
+            delete_patterns="*.txt",
         )
-        added_files = {op.path_in_repo for op in operations if isinstance(op, CommitOperationAdd)}
-        deleted_files = {op.path_in_repo for op in operations if isinstance(op, CommitOperationDelete)}
+        added_files = {
+            op.path_in_repo for op in operations if isinstance(op, CommitOperationAdd)
+        }
+        deleted_files = {
+            op.path_in_repo
+            for op in operations
+            if isinstance(op, CommitOperationDelete)
+        }
 
-        assert added_files == {"sub/file.txt", "sub/lfs_in_sub.bin"}  # added only in sub/
+        assert added_files == {
+            "sub/file.txt",
+            "sub/lfs_in_sub.bin",
+        }  # added only in sub/
         assert deleted_files == {"sub/file1.txt"}  # delete only in sub/
 
     def test_delete_txt_in_sub_ignore_sub_file_txt(self):
         operations = self._upload_folder_alias(
-            path_in_repo="sub", folder_path=self.cache_dir / "sub", ignore_patterns="file.txt", delete_patterns="*.txt"
+            path_in_repo="sub",
+            folder_path=self.cache_dir / "sub",
+            ignore_patterns="file.txt",
+            delete_patterns="*.txt",
         )
-        added_files = {op.path_in_repo for op in operations if isinstance(op, CommitOperationAdd)}
-        deleted_files = {op.path_in_repo for op in operations if isinstance(op, CommitOperationDelete)}
+        added_files = {
+            op.path_in_repo for op in operations if isinstance(op, CommitOperationAdd)
+        }
+        deleted_files = {
+            op.path_in_repo
+            for op in operations
+            if isinstance(op, CommitOperationDelete)
+        }
 
         # since "sub/file.txt" should be deleted and is not overwritten (ignore_patterns), we delete it explicitly
         assert added_files == {"sub/lfs_in_sub.bin"}  # no "sub/file.txt"
@@ -2694,8 +3183,14 @@ class UploadFolderMockedTest(unittest.Te
 
     def test_delete_if_path_in_repo(self):
         # Regression test for https://github.com/huggingface/huggingface_hub/pull/2129
-        operations = self._upload_folder_alias(path_in_repo=".", folder_path=self.cache_dir, delete_patterns="*")
-        deleted_files = {op.path_in_repo for op in operations if isinstance(op, CommitOperationDelete)}
+        operations = self._upload_folder_alias(
+            path_in_repo=".", folder_path=self.cache_dir, delete_patterns="*"
+        )
+        deleted_files = {
+            op.path_in_repo
+            for op in operations
+            if isinstance(op, CommitOperationDelete)
+        }
         assert deleted_files == {"file1.txt", "sub/file1.txt"}  # all the 'old' files
 
 
@@ -2708,7 +3203,9 @@ class HfLargefilesTest(HfApiCommonTest):
 
     def setup_local_clone(self) -> None:
         scheme = urlparse(self.repo_url).scheme
-        repo_url_auth = self.repo_url.replace(f"{scheme}://", f"{scheme}://user:{TOKEN}@")
+        repo_url_auth = self.repo_url.replace(
+            f"{scheme}://", f"{scheme}://user:{TOKEN}@"
+        )
 
         subprocess.run(
             ["git", "clone", repo_url_auth, str(self.cache_dir)],
@@ -2717,7 +3214,9 @@ class HfLargefilesTest(HfApiCommonTest):
             stderr=subprocess.PIPE,
         )
         subprocess.run(["git", "lfs", "track", "*.pdf"], check=True, cwd=self.cache_dir)
-        subprocess.run(["git", "lfs", "track", "*.epub"], check=True, cwd=self.cache_dir)
+        subprocess.run(
+            ["git", "lfs", "track", "*.epub"], check=True, cwd=self.cache_dir
+        )
 
     @require_git_lfs
     def test_end_to_end_thresh_6M(self):
@@ -2729,10 +3228,16 @@ class HfLargefilesTest(HfApiCommonTest):
         self.setup_local_clone()
 
         subprocess.run(
-            ["wget", LARGE_FILE_18MB], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=self.cache_dir
+            ["wget", LARGE_FILE_18MB],
+            check=True,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+            cwd=self.cache_dir,
         )
         subprocess.run(["git", "add", "*"], check=True, cwd=self.cache_dir)
-        subprocess.run(["git", "commit", "-m", "commit message"], check=True, cwd=self.cache_dir)
+        subprocess.run(
+            ["git", "commit", "-m", "commit message"], check=True, cwd=self.cache_dir
+        )
 
         # This will fail as we haven't set up our custom transfer agent yet.
         failed_process = subprocess.run(
@@ -2744,7 +3249,9 @@ class HfLargefilesTest(HfApiCommonTest):
         self.assertEqual(failed_process.returncode, 1)
         self.assertIn("cli lfs-enable-largefiles", failed_process.stderr.decode())
         # ^ Instructions on how to fix this are included in the error message.
-        subprocess.run(["huggingface-cli", "lfs-enable-largefiles", self.cache_dir], check=True)
+        subprocess.run(
+            ["huggingface-cli", "lfs-enable-largefiles", self.cache_dir], check=True
+        )
 
         start_time = time.time()
         subprocess.run(["git", "push"], check=True, cwd=self.cache_dir)
@@ -2774,14 +3281,28 @@ class HfLargefilesTest(HfApiCommonTest):
         self.setup_local_clone()
 
         subprocess.run(
-            ["wget", LARGE_FILE_18MB], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=self.cache_dir
+            ["wget", LARGE_FILE_18MB],
+            check=True,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+            cwd=self.cache_dir,
         )
         subprocess.run(
-            ["wget", LARGE_FILE_14MB], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=self.cache_dir
+            ["wget", LARGE_FILE_14MB],
+            check=True,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+            cwd=self.cache_dir,
         )
         subprocess.run(["git", "add", "*"], check=True, cwd=self.cache_dir)
-        subprocess.run(["git", "commit", "-m", "both files in same commit"], check=True, cwd=self.cache_dir)
-        subprocess.run(["huggingface-cli", "lfs-enable-largefiles", self.cache_dir], check=True)
+        subprocess.run(
+            ["git", "commit", "-m", "both files in same commit"],
+            check=True,
+            cwd=self.cache_dir,
+        )
+        subprocess.run(
+            ["huggingface-cli", "lfs-enable-largefiles", self.cache_dir], check=True
+        )
 
         start_time = time.time()
         subprocess.run(["git", "push"], check=True, cwd=self.cache_dir)
@@ -2798,7 +3319,11 @@ class HfLargefilesTest(HfApiCommonTest):
             "_upload_parts_iteratively",
             wraps=huggingface_hub.lfs._upload_parts_iteratively,
         ) as mock:
-            self._api.upload_file(repo_id=self.repo_id, path_or_fileobj=b"0" * 18 * 10**6, path_in_repo="lfs.bin")
+            self._api.upload_file(
+                repo_id=self.repo_id,
+                path_or_fileobj=b"0" * 18 * 10**6,
+                path_in_repo="lfs.bin",
+            )
             mock.assert_called_once()  # It used multipart upload
 
 
@@ -2851,7 +3376,9 @@ class HfApiDiscussionsTest(HfApiCommonTe
         self._api.delete_repo(repo_id=self.repo_id)
 
     def test_create_discussion(self):
-        discussion = self._api.create_discussion(repo_id=self.repo_id, title=" Test discussion !  ")
+        discussion = self._api.create_discussion(
+            repo_id=self.repo_id, title=" Test discussion !  "
+        )
         self.assertEqual(discussion.num, 3)
         self.assertEqual(discussion.author, USER)
         self.assertEqual(discussion.is_pull_request, False)
@@ -2864,11 +3391,15 @@ class HfApiDiscussionsTest(HfApiCommonTe
         Computed URL was malformed with `dataset` and `space` repo_types.
         See https://github.com/huggingface/huggingface_hub/issues/1463.
         """
-        discussion = self._api.create_discussion(repo_id=repo_url.repo_id, repo_type="dataset", title="title")
+        discussion = self._api.create_discussion(
+            repo_id=repo_url.repo_id, repo_type="dataset", title="title"
+        )
         self.assertEqual(discussion.url, f"{repo_url}/discussions/1")
 
     def test_create_pull_request(self):
-        discussion = self._api.create_discussion(repo_id=self.repo_id, title=" Test PR !  ", pull_request=True)
+        discussion = self._api.create_discussion(
+            repo_id=self.repo_id, title=" Test PR !  ", pull_request=True
+        )
         self.assertEqual(discussion.num, 3)
         self.assertEqual(discussion.author, USER)
         self.assertEqual(discussion.is_pull_request, True)
@@ -2881,55 +3412,84 @@ class HfApiDiscussionsTest(HfApiCommonTe
         discussions_generator = self._api.get_repo_discussions(repo_id=self.repo_id)
         self.assertIsInstance(discussions_generator, types.GeneratorType)
         self.assertListEqual(
-            list([d.num for d in discussions_generator]), [self.discussion.num, self.pull_request.num]
+            list([d.num for d in discussions_generator]),
+            [self.discussion.num, self.pull_request.num],
         )
 
     def test_get_repo_discussion_by_type(self):
-        discussions_generator = self._api.get_repo_discussions(repo_id=self.repo_id, discussion_type="pull_request")
+        discussions_generator = self._api.get_repo_discussions(
+            repo_id=self.repo_id, discussion_type="pull_request"
+        )
         self.assertIsInstance(discussions_generator, types.GeneratorType)
-        self.assertListEqual(list([d.num for d in discussions_generator]), [self.pull_request.num])
+        self.assertListEqual(
+            list([d.num for d in discussions_generator]), [self.pull_request.num]
+        )
 
-        discussions_generator = self._api.get_repo_discussions(repo_id=self.repo_id, discussion_type="discussion")
+        discussions_generator = self._api.get_repo_discussions(
+            repo_id=self.repo_id, discussion_type="discussion"
+        )
         self.assertIsInstance(discussions_generator, types.GeneratorType)
-        self.assertListEqual(list([d.num for d in discussions_generator]), [self.discussion.num])
+        self.assertListEqual(
+            list([d.num for d in discussions_generator]), [self.discussion.num]
+        )
 
-        discussions_generator = self._api.get_repo_discussions(repo_id=self.repo_id, discussion_type="all")
+        discussions_generator = self._api.get_repo_discussions(
+            repo_id=self.repo_id, discussion_type="all"
+        )
         self.assertIsInstance(discussions_generator, types.GeneratorType)
         self.assertListEqual(
-            list([d.num for d in discussions_generator]), [self.discussion.num, self.pull_request.num]
+            list([d.num for d in discussions_generator]),
+            [self.discussion.num, self.pull_request.num],
         )
 
     def test_get_repo_discussion_by_author(self):
-        discussions_generator = self._api.get_repo_discussions(repo_id=self.repo_id, author="unknown")
+        discussions_generator = self._api.get_repo_discussions(
+            repo_id=self.repo_id, author="unknown"
+        )
         self.assertIsInstance(discussions_generator, types.GeneratorType)
         self.assertListEqual(list([d.num for d in discussions_generator]), [])
 
     def test_get_repo_discussion_by_status(self):
         self._api.change_discussion_status(self.repo_id, self.discussion.num, "closed")
 
-        discussions_generator = self._api.get_repo_discussions(repo_id=self.repo_id, discussion_status="open")
+        discussions_generator = self._api.get_repo_discussions(
+            repo_id=self.repo_id, discussion_status="open"
+        )
         self.assertIsInstance(discussions_generator, types.GeneratorType)
-        self.assertListEqual(list([d.num for d in discussions_generator]), [self.pull_request.num])
+        self.assertListEqual(
+            list([d.num for d in discussions_generator]), [self.pull_request.num]
+        )
 
-        discussions_generator = self._api.get_repo_discussions(repo_id=self.repo_id, discussion_status="closed")
+        discussions_generator = self._api.get_repo_discussions(
+            repo_id=self.repo_id, discussion_status="closed"
+        )
         self.assertIsInstance(discussions_generator, types.GeneratorType)
-        self.assertListEqual(list([d.num for d in discussions_generator]), [self.discussion.num])
+        self.assertListEqual(
+            list([d.num for d in discussions_generator]), [self.discussion.num]
+        )
 
-        discussions_generator = self._api.get_repo_discussions(repo_id=self.repo_id, discussion_status="all")
+        discussions_generator = self._api.get_repo_discussions(
+            repo_id=self.repo_id, discussion_status="all"
+        )
         self.assertIsInstance(discussions_generator, types.GeneratorType)
         self.assertListEqual(
-            list([d.num for d in discussions_generator]), [self.discussion.num, self.pull_request.num]
+            list([d.num for d in discussions_generator]),
+            [self.discussion.num, self.pull_request.num],
         )
 
     @with_production_testing
     def test_get_repo_discussion_pagination(self):
         discussions = list(
-            HfApi().get_repo_discussions(repo_id="open-llm-leaderboard/open_llm_leaderboard", repo_type="space")
+            HfApi().get_repo_discussions(
+                repo_id="open-llm-leaderboard/open_llm_leaderboard", repo_type="space"
+            )
         )
         assert len(discussions) > 50
 
     def test_get_discussion_details(self):
-        retrieved = self._api.get_discussion_details(repo_id=self.repo_id, discussion_num=2)
+        retrieved = self._api.get_discussion_details(
+            repo_id=self.repo_id, discussion_num=2
+        )
         self.assertEqual(retrieved, self.discussion)
 
     def test_edit_discussion_comment(self):
@@ -2942,9 +3502,13 @@ class HfApiDiscussionsTest(HfApiCommonTe
             comment_id=get_first_comment(self.pull_request).id,
             new_content="**Edited** comment 🤗",
         )
-        retrieved = self._api.get_discussion_details(repo_id=self.repo_id, discussion_num=self.pull_request.num)
+        retrieved = self._api.get_discussion_details(
+            repo_id=self.repo_id, discussion_num=self.pull_request.num
+        )
         self.assertEqual(get_first_comment(retrieved).edited, True)
-        self.assertEqual(get_first_comment(retrieved).id, get_first_comment(self.pull_request).id)
+        self.assertEqual(
+            get_first_comment(retrieved).id, get_first_comment(self.pull_request).id
+        )
         self.assertEqual(get_first_comment(retrieved).content, "**Edited** comment 🤗")
 
         self.assertEqual(get_first_comment(retrieved), edited_comment)
@@ -2960,45 +3524,63 @@ class HfApiDiscussionsTest(HfApiCommonTe
                 And even [links](http://hf.co)! 💥🤯
             """,
         )
-        retrieved = self._api.get_discussion_details(repo_id=self.repo_id, discussion_num=self.discussion.num)
+        retrieved = self._api.get_discussion_details(
+            repo_id=self.repo_id, discussion_num=self.discussion.num
+        )
         self.assertEqual(len(retrieved.events), 2)
         self.assertIn(new_comment.id, {event.id for event in retrieved.events})
 
     def test_rename_discussion(self):
         rename_event = self._api.rename_discussion(
-            repo_id=self.repo_id, discussion_num=self.discussion.num, new_title="New title2"
+            repo_id=self.repo_id,
+            discussion_num=self.discussion.num,
+            new_title="New title2",
+        )
+        retrieved = self._api.get_discussion_details(
+            repo_id=self.repo_id, discussion_num=self.discussion.num
         )
-        retrieved = self._api.get_discussion_details(repo_id=self.repo_id, discussion_num=self.discussion.num)
         self.assertIn(rename_event.id, (event.id for event in retrieved.events))
         self.assertEqual(rename_event.old_title, self.discussion.title)
         self.assertEqual(rename_event.new_title, "New title2")
 
     def test_change_discussion_status(self):
         status_change_event = self._api.change_discussion_status(
-            repo_id=self.repo_id, discussion_num=self.discussion.num, new_status="closed"
+            repo_id=self.repo_id,
+            discussion_num=self.discussion.num,
+            new_status="closed",
+        )
+        retrieved = self._api.get_discussion_details(
+            repo_id=self.repo_id, discussion_num=self.discussion.num
         )
-        retrieved = self._api.get_discussion_details(repo_id=self.repo_id, discussion_num=self.discussion.num)
         self.assertIn(status_change_event.id, (event.id for event in retrieved.events))
         self.assertEqual(status_change_event.new_status, "closed")
 
         with self.assertRaises(ValueError):
             self._api.change_discussion_status(
-                repo_id=self.repo_id, discussion_num=self.discussion.num, new_status="published"
+                repo_id=self.repo_id,
+                discussion_num=self.discussion.num,
+                new_status="published",
             )
 
     def test_merge_pull_request(self):
         self._api.create_commit(
             repo_id=self.repo_id,
             commit_message="Commit some file",
-            operations=[CommitOperationAdd(path_in_repo="file.test", path_or_fileobj=b"Content")],
+            operations=[
+                CommitOperationAdd(path_in_repo="file.test", path_or_fileobj=b"Content")
+            ],
             revision=self.pull_request.git_reference,
         )
         self._api.change_discussion_status(
-            repo_id=self.repo_id, discussion_num=self.pull_request.num, new_status="open"
+            repo_id=self.repo_id,
+            discussion_num=self.pull_request.num,
+            new_status="open",
         )
         self._api.merge_pull_request(self.repo_id, self.pull_request.num)
 
-        retrieved = self._api.get_discussion_details(repo_id=self.repo_id, discussion_num=self.pull_request.num)
+        retrieved = self._api.get_discussion_details(
+            repo_id=self.repo_id, discussion_num=self.pull_request.num
+        )
         self.assertEqual(retrieved.status, "merged")
         self.assertIsNotNone(retrieved.merge_commit_oid)
 
@@ -3026,7 +3608,9 @@ class ActivityApiTest(unittest.TestCase)
     def test_list_repo_likers(self) -> None:
         # a repo with > 5000 likes
         all_likers = list(
-            HfApi().list_repo_likers(repo_id="open-llm-leaderboard/open_llm_leaderboard", repo_type="space")
+            HfApi().list_repo_likers(
+                repo_id="open-llm-leaderboard/open_llm_leaderboard", repo_type="space"
+            )
         )
         self.assertIsInstance(all_likers[0], User)
         self.assertGreater(len(all_likers), 5000)
@@ -3035,7 +3619,9 @@ class ActivityApiTest(unittest.TestCase)
     def test_list_likes_on_production(self) -> None:
         # Test julien-c likes a lot of repos !
         likes = HfApi().list_liked_repos("julien-c")
-        self.assertEqual(len(likes.models) + len(likes.datasets) + len(likes.spaces), likes.total)
+        self.assertEqual(
+            len(likes.models) + len(likes.datasets) + len(likes.spaces), likes.total
+        )
         self.assertGreater(len(likes.models), 0)
         self.assertGreater(len(likes.datasets), 0)
         self.assertGreater(len(likes.spaces), 0)
@@ -3046,32 +3632,53 @@ class TestSquashHistory(HfApiCommonTest)
     def test_super_squash_history_on_branch(self, repo_url: RepoUrl) -> None:
         # Upload + update file on main
         repo_id = repo_url.repo_id
-        self._api.upload_file(repo_id=repo_id, path_in_repo="file.txt", path_or_fileobj=b"content")
-        self._api.upload_file(repo_id=repo_id, path_in_repo="lfs.bin", path_or_fileobj=b"content")
-        self._api.upload_file(repo_id=repo_id, path_in_repo="file.txt", path_or_fileobj=b"another_content")
+        self._api.upload_file(
+            repo_id=repo_id, path_in_repo="file.txt", path_or_fileobj=b"content"
+        )
+        self._api.upload_file(
+            repo_id=repo_id, path_in_repo="lfs.bin", path_or_fileobj=b"content"
+        )
+        self._api.upload_file(
+            repo_id=repo_id, path_in_repo="file.txt", path_or_fileobj=b"another_content"
+        )
 
         # Upload file on a new branch
         self._api.create_branch(repo_id=repo_id, branch="v0.1", exist_ok=True)
-        self._api.upload_file(repo_id=repo_id, path_in_repo="file.txt", path_or_fileobj=b"foo", revision="v0.1")
+        self._api.upload_file(
+            repo_id=repo_id,
+            path_in_repo="file.txt",
+            path_or_fileobj=b"foo",
+            revision="v0.1",
+        )
 
         # Squash history on main
         self._api.super_squash_history(repo_id=repo_id)
 
         # List history
-        squashed_main_commits = self._api.list_repo_commits(repo_id=repo_id, revision="main")
+        squashed_main_commits = self._api.list_repo_commits(
+            repo_id=repo_id, revision="main"
+        )
         branch_commits = self._api.list_repo_commits(repo_id=repo_id, revision="v0.1")
 
         # Main branch has been squashed but initial commits still exists on other branch
         assert len(squashed_main_commits) == 1
-        assert squashed_main_commits[0].title == "Super-squash branch 'main' using huggingface_hub"
+        assert (
+            squashed_main_commits[0].title
+            == "Super-squash branch 'main' using huggingface_hub"
+        )
         assert len(branch_commits) == 5
         assert branch_commits[-1].title == "initial commit"
 
         # Squash history on branch
         self._api.super_squash_history(repo_id=repo_id, branch="v0.1")
-        squashed_branch_commits = self._api.list_repo_commits(repo_id=repo_id, revision="v0.1")
+        squashed_branch_commits = self._api.list_repo_commits(
+            repo_id=repo_id, revision="v0.1"
+        )
         assert len(squashed_branch_commits) == 1
-        assert squashed_branch_commits[0].title == "Super-squash branch 'v0.1' using huggingface_hub"
+        assert (
+            squashed_branch_commits[0].title
+            == "Super-squash branch 'v0.1' using huggingface_hub"
+        )
 
     @use_tmp_repo()
     def test_super_squash_history_on_special_ref(self, repo_url: RepoUrl) -> None:
@@ -3081,23 +3688,36 @@ class TestSquashHistory(HfApiCommonTest)
         The only case where it's useful is for the dataset-viewer on refs/convert/parquet.
         """
         repo_id = repo_url.repo_id
-        pr = self._api.create_pull_request(repo_id=repo_id, title="Test super squash on PR")
+        pr = self._api.create_pull_request(
+            repo_id=repo_id, title="Test super squash on PR"
+        )
 
         # Upload + update file on PR
         self._api.upload_file(
-            repo_id=repo_id, path_in_repo="file.txt", path_or_fileobj=b"content", revision=pr.git_reference
+            repo_id=repo_id,
+            path_in_repo="file.txt",
+            path_or_fileobj=b"content",
+            revision=pr.git_reference,
         )
         self._api.upload_file(
-            repo_id=repo_id, path_in_repo="lfs.bin", path_or_fileobj=b"content", revision=pr.git_reference
+            repo_id=repo_id,
+            path_in_repo="lfs.bin",
+            path_or_fileobj=b"content",
+            revision=pr.git_reference,
         )
         self._api.upload_file(
-            repo_id=repo_id, path_in_repo="file.txt", path_or_fileobj=b"another_content", revision=pr.git_reference
+            repo_id=repo_id,
+            path_in_repo="file.txt",
+            path_or_fileobj=b"another_content",
+            revision=pr.git_reference,
         )
 
         # Squash history PR
         self._api.super_squash_history(repo_id=repo_id, branch=pr.git_reference)
 
-        squashed_branch_commits = self._api.list_repo_commits(repo_id=repo_id, revision=pr.git_reference)
+        squashed_branch_commits = self._api.list_repo_commits(
+            repo_id=repo_id, revision=pr.git_reference
+        )
         assert len(squashed_branch_commits) == 1
 
 
@@ -3126,11 +3746,15 @@ iface.launch()
         super().setUp()
 
         # If generating new VCR => use personal token and REMOVE IT from the VCR
-        self.repo_id = "user/tmp_test_space"  # no need to be unique as it's a VCRed test
+        self.repo_id = (
+            "user/tmp_test_space"  # no need to be unique as it's a VCRed test
+        )
         self.api = HfApi(token="hf_fake_token", endpoint=ENDPOINT_PRODUCTION)
 
         # Create a Space
-        self.api.create_repo(repo_id=self.repo_id, repo_type="space", space_sdk="gradio", private=True)
+        self.api.create_repo(
+            repo_id=self.repo_id, repo_type="space", space_sdk="gradio", private=True
+        )
         self.api.upload_file(
             path_or_fileobj=self._BASIC_APP_PY_TEMPLATE,
             repo_id=self.repo_id,
@@ -3149,14 +3773,20 @@ iface.launch()
         self.api.add_space_secret(self.repo_id, "gh_api_key", "******")
 
         # Add secret with optional description
-        self.api.add_space_secret(self.repo_id, "bar", "123", description="This is a secret")
+        self.api.add_space_secret(
+            self.repo_id, "bar", "123", description="This is a secret"
+        )
 
         # Update secret
         self.api.add_space_secret(self.repo_id, "foo", "456")
 
         # Update secret with optional description
-        self.api.add_space_secret(self.repo_id, "foo", "789", description="This is a secret")
-        self.api.add_space_secret(self.repo_id, "bar", "456", description="This is another secret")
+        self.api.add_space_secret(
+            self.repo_id, "foo", "789", description="This is a secret"
+        )
+        self.api.add_space_secret(
+            self.repo_id, "bar", "456", description="This is another secret"
+        )
 
         # Delete secret
         self.api.delete_space_secret(self.repo_id, "gh_api_key")
@@ -3173,13 +3803,17 @@ iface.launch()
         self.api.add_space_variable(self.repo_id, "MODEL_REPO_ID", "user/repo")
 
         # Add 1 variable with optional description
-        self.api.add_space_variable(self.repo_id, "MODEL_PAPER", "arXiv", description="found it there")
+        self.api.add_space_variable(
+            self.repo_id, "MODEL_PAPER", "arXiv", description="found it there"
+        )
 
         # Update variable
         self.api.add_space_variable(self.repo_id, "foo", "456")
 
         # Update variable with optional description
-        self.api.add_space_variable(self.repo_id, "foo", "456", description="updated description")
+        self.api.add_space_variable(
+            self.repo_id, "foo", "456", description="updated description"
+        )
 
         # Delete variable
         self.api.delete_space_variable(self.repo_id, "gh_api_key")
@@ -3216,7 +3850,12 @@ iface.launch()
     @with_production_testing
     def test_pause_and_restart_space(self) -> None:
         # Upload a fake app.py file
-        self.api.upload_file(path_or_fileobj=b"", path_in_repo="app.py", repo_id=self.repo_id, repo_type="space")
+        self.api.upload_file(
+            path_or_fileobj=b"",
+            path_in_repo="app.py",
+            repo_id=self.repo_id,
+            repo_type="space",
+        )
 
         # Wait for the Space to be "BUILDING"
         count = 0
@@ -3251,13 +3890,24 @@ class TestCommitInBackground(HfApiCommon
 
         t0 = time.time()
         upload_future_1 = self._api.upload_file(
-            path_or_fileobj=b"1", path_in_repo="1.txt", repo_id=repo_id, commit_message="Upload 1", run_as_future=True
+            path_or_fileobj=b"1",
+            path_in_repo="1.txt",
+            repo_id=repo_id,
+            commit_message="Upload 1",
+            run_as_future=True,
         )
         upload_future_2 = self._api.upload_file(
-            path_or_fileobj=b"2", path_in_repo="2.txt", repo_id=repo_id, commit_message="Upload 2", run_as_future=True
+            path_or_fileobj=b"2",
+            path_in_repo="2.txt",
+            repo_id=repo_id,
+            commit_message="Upload 2",
+            run_as_future=True,
         )
         upload_future_3 = self._api.upload_folder(
-            repo_id=repo_id, folder_path=self.cache_dir, commit_message="Upload folder", run_as_future=True
+            repo_id=repo_id,
+            folder_path=self.cache_dir,
+            commit_message="Upload folder",
+            run_as_future=True,
         )
         t1 = time.time()
 
@@ -3284,11 +3934,15 @@ class TestCommitInBackground(HfApiCommon
     def test_run_as_future(self, repo_url: RepoUrl) -> None:
         repo_id = repo_url.repo_id
         # update repo visibility to private
-        self._api.run_as_future(self._api.update_repo_settings, repo_id=repo_id, private=True)
+        self._api.run_as_future(
+            self._api.update_repo_settings, repo_id=repo_id, private=True
+        )
         future_1 = self._api.run_as_future(self._api.model_info, repo_id=repo_id)
 
         # update repo visibility to public
-        self._api.run_as_future(self._api.update_repo_settings, repo_id=repo_id, private=False)
+        self._api.run_as_future(
+            self._api.update_repo_settings, repo_id=repo_id, private=False
+        )
         future_2 = self._api.run_as_future(self._api.model_info, repo_id=repo_id)
 
         self.assertIsInstance(future_1, Future)
@@ -3485,11 +4139,19 @@ class TestSpaceAPIMocked(unittest.TestCa
             repo_type="space",
             space_sdk="gradio",
             space_secrets=[
-                {"key": "Testsecret", "value": "Testvalue", "description": "Testdescription"},
+                {
+                    "key": "Testsecret",
+                    "value": "Testvalue",
+                    "description": "Testdescription",
+                },
                 {"key": "Testsecret2", "value": "Testvalue"},
             ],
             space_variables=[
-                {"key": "Testvariable", "value": "Testvalue", "description": "Testdescription"},
+                {
+                    "key": "Testvariable",
+                    "value": "Testvalue",
+                    "description": "Testdescription",
+                },
                 {"key": "Testvariable2", "value": "Testvalue"},
             ],
         )
@@ -3502,11 +4164,19 @@ class TestSpaceAPIMocked(unittest.TestCa
                 "type": "space",
                 "sdk": "gradio",
                 "secrets": [
-                    {"key": "Testsecret", "value": "Testvalue", "description": "Testdescription"},
+                    {
+                        "key": "Testsecret",
+                        "value": "Testvalue",
+                        "description": "Testdescription",
+                    },
                     {"key": "Testsecret2", "value": "Testvalue"},
                 ],
                 "variables": [
-                    {"key": "Testvariable", "value": "Testvalue", "description": "Testdescription"},
+                    {
+                        "key": "Testvariable",
+                        "value": "Testvalue",
+                        "description": "Testdescription",
+                    },
                     {"key": "Testvariable2", "value": "Testvalue"},
                 ],
             },
@@ -3521,11 +4191,19 @@ class TestSpaceAPIMocked(unittest.TestCa
             storage=SpaceStorage.LARGE,
             sleep_time=123,
             secrets=[
-                {"key": "Testsecret", "value": "Testvalue", "description": "Testdescription"},
+                {
+                    "key": "Testsecret",
+                    "value": "Testvalue",
+                    "description": "Testdescription",
+                },
                 {"key": "Testsecret2", "value": "Testvalue"},
             ],
             variables=[
-                {"key": "Testvariable", "value": "Testvalue", "description": "Testdescription"},
+                {
+                    "key": "Testvariable",
+                    "value": "Testvalue",
+                    "description": "Testdescription",
+                },
                 {"key": "Testvariable2", "value": "Testvalue"},
             ],
         )
@@ -3539,11 +4217,19 @@ class TestSpaceAPIMocked(unittest.TestCa
                 "storageTier": "large",
                 "sleepTimeSeconds": 123,
                 "secrets": [
-                    {"key": "Testsecret", "value": "Testvalue", "description": "Testdescription"},
+                    {
+                        "key": "Testsecret",
+                        "value": "Testvalue",
+                        "description": "Testdescription",
+                    },
                     {"key": "Testsecret2", "value": "Testvalue"},
                 ],
                 "variables": [
-                    {"key": "Testvariable", "value": "Testvalue", "description": "Testdescription"},
+                    {
+                        "key": "Testvariable",
+                        "value": "Testvalue",
+                        "description": "Testdescription",
+                    },
                     {"key": "Testvariable2", "value": "Testvalue"},
                 ],
             },
@@ -3558,7 +4244,9 @@ class TestSpaceAPIMocked(unittest.TestCa
         )
 
     def test_request_space_hardware_with_sleep_time(self) -> None:
-        self.api.request_space_hardware(self.repo_id, SpaceHardware.T4_MEDIUM, sleep_time=123)
+        self.api.request_space_hardware(
+            self.repo_id, SpaceHardware.T4_MEDIUM, sleep_time=123
+        )
         self.post_mock.assert_called_once_with(
             f"{self.api.endpoint}/api/spaces/{self.repo_id}/hardware",
             headers=self.api._build_hf_headers(),
@@ -3574,7 +4262,9 @@ class TestSpaceAPIMocked(unittest.TestCa
         )
 
     def test_set_space_sleep_time_cpu_basic(self) -> None:
-        self.post_mock.return_value.json.return_value["hardware"]["requested"] = "cpu-basic"
+        self.post_mock.return_value.json.return_value["hardware"][
+            "requested"
+        ] = "cpu-basic"
         with self.assertWarns(UserWarning):
             self.api.set_space_sleep_time(self.repo_id, sleep_time=123)
 
@@ -3628,7 +4318,9 @@ class ListGitRefsTest(unittest.TestCase)
         main_branch = [branch for branch in refs.branches if branch.name == "main"][0]
         self.assertEqual(main_branch.ref, "refs/heads/main")
 
-        convert_branch = [branch for branch in refs.converts if branch.name == "parquet"][0]
+        convert_branch = [
+            branch for branch in refs.converts if branch.name == "parquet"
+        ][0]
         self.assertEqual(convert_branch.ref, "refs/convert/parquet")
 
         # Can get info by convert revision
@@ -3639,7 +4331,9 @@ class ListGitRefsTest(unittest.TestCase)
         )
 
     def test_list_refs_with_prs(self) -> None:
-        refs = self.api.list_repo_refs("openchat/openchat_3.5", include_pull_requests=True)
+        refs = self.api.list_repo_refs(
+            "openchat/openchat_3.5", include_pull_requests=True
+        )
         self.assertGreater(len(refs.pull_requests), 1)
         assert refs.pull_requests[0].ref.startswith("refs/pr/")
 
@@ -3652,13 +4346,22 @@ class ListGitCommitsTest(unittest.TestCa
         cls.repo_id = cls.api.create_repo(repo_name()).repo_id
 
         # Create a commit on `main` branch
-        cls.api.upload_file(repo_id=cls.repo_id, path_or_fileobj=b"content", path_in_repo="content.txt")
+        cls.api.upload_file(
+            repo_id=cls.repo_id, path_or_fileobj=b"content", path_in_repo="content.txt"
+        )
 
         # Create a commit in a PR
-        cls.api.upload_file(repo_id=cls.repo_id, path_or_fileobj=b"on_pr", path_in_repo="on_pr.txt", create_pr=True)
+        cls.api.upload_file(
+            repo_id=cls.repo_id,
+            path_or_fileobj=b"on_pr",
+            path_in_repo="on_pr.txt",
+            create_pr=True,
+        )
 
         # Create another commit on `main` branch
-        cls.api.upload_file(repo_id=cls.repo_id, path_or_fileobj=b"on_main", path_in_repo="on_main.txt")
+        cls.api.upload_file(
+            repo_id=cls.repo_id, path_or_fileobj=b"on_main", path_in_repo="on_main.txt"
+        )
         return super().setUpClass()
 
     @classmethod
@@ -3728,15 +4431,21 @@ class HfApiTokenAttributeTest(unittest.T
         HfApi()._build_hf_headers(token=None)
         self._assert_token_is(mock_build_hf_headers, None)
 
-    def _assert_token_is(self, mock_build_hf_headers: Mock, expected_value: str) -> None:
+    def _assert_token_is(
+        self, mock_build_hf_headers: Mock, expected_value: str
+    ) -> None:
         self.assertEqual(mock_build_hf_headers.call_args[1]["token"], expected_value)
 
-    def test_library_name_and_version_are_set(self, mock_build_hf_headers: Mock) -> None:
+    def test_library_name_and_version_are_set(
+        self, mock_build_hf_headers: Mock
+    ) -> None:
         HfApi(library_name="a", library_version="b")._build_hf_headers()
         self.assertEqual(mock_build_hf_headers.call_args[1]["library_name"], "a")
         self.assertEqual(mock_build_hf_headers.call_args[1]["library_version"], "b")
 
-    def test_library_name_and_version_are_overwritten(self, mock_build_hf_headers: Mock) -> None:
+    def test_library_name_and_version_are_overwritten(
+        self, mock_build_hf_headers: Mock
+    ) -> None:
         api = HfApi(library_name="a", library_version="b")
         api._build_hf_headers(library_name="A", library_version="B")
         self.assertEqual(mock_build_hf_headers.call_args[1]["library_name"], "A")
@@ -3825,7 +4534,11 @@ class RepoUrlTest(unittest.TestCase):
                 self.assertEqual(url.repo_type, "model")
 
     def test_repo_url_canonical_dataset(self):
-        for _id in ("datasets/squad", "hf://datasets/squad", "https://huggingface.co/datasets/squad"):
+        for _id in (
+            "datasets/squad",
+            "hf://datasets/squad",
+            "https://huggingface.co/datasets/squad",
+        ):
             with self.subTest(_id):
                 url = RepoUrl(_id)
                 self.assertEqual(url.repo_id, "squad")
@@ -3874,11 +4587,13 @@ class HfApiDuplicateSpaceTest(HfApiCommo
             "style.css",
             "temp/new_file.md",
         ]
-        assert self._api.list_repo_files(repo_id=to_repo_id, repo_type="space") == self._api.list_repo_files(
-            repo_id=from_repo_id, repo_type="space"
-        )
+        assert self._api.list_repo_files(
+            repo_id=to_repo_id, repo_type="space"
+        ) == self._api.list_repo_files(repo_id=from_repo_id, repo_type="space")
 
-        self._api.delete_repo(repo_id=from_repo_id, repo_type="space", token=OTHER_TOKEN)
+        self._api.delete_repo(
+            repo_id=from_repo_id, repo_type="space", token=OTHER_TOKEN
+        )
         self._api.delete_repo(repo_id=to_repo_id, repo_type="space")
 
     def test_duplicate_space_from_missing_repo(self) -> None:
@@ -3893,7 +4608,9 @@ class CollectionAPITest(HfApiCommonTest)
         id = uuid.uuid4()
         self.title = f"My cool stuff {id}"
         self.slug_prefix = f"{USER}/my-cool-stuff-{id}"
-        self.slug: Optional[str] = None  # Populated by the tests => use to delete in tearDown
+        self.slug: Optional[str] = (
+            None  # Populated by the tests => use to delete in tearDown
+        )
         return super().setUp()
 
     def tearDown(self) -> None:
@@ -3906,7 +4623,9 @@ class CollectionAPITest(HfApiCommonTest)
         item_id = "teknium/OpenHermes-2.5-Mistral-7B"
         item_type = "model"
         limit = 3
-        collections = HfApi().list_collections(item=f"{item_type}s/{item_id}", limit=limit)
+        collections = HfApi().list_collections(
+            item=f"{item_type}s/{item_id}", limit=limit
+        )
 
         # Check return type
         self.assertIsInstance(collections, Iterable)
@@ -3919,10 +4638,15 @@ class CollectionAPITest(HfApiCommonTest)
         for collection in collections:
             # all items are not necessarily returned when listing collections => retrieve complete one
             full_collection = HfApi().get_collection(collection.slug)
-            assert any(item.item_id == item_id and item.item_type == item_type for item in full_collection.items)
+            assert any(
+                item.item_id == item_id and item.item_type == item_type
+                for item in full_collection.items
+            )
 
     def test_create_collection_with_description(self) -> None:
-        collection = self._api.create_collection(self.title, description="Contains a lot of cool stuff")
+        collection = self._api.create_collection(
+            self.title, description="Contains a lot of cool stuff"
+        )
         self.slug = collection.slug
 
         self.assertIsInstance(collection, Collection)
@@ -3930,7 +4654,9 @@ class CollectionAPITest(HfApiCommonTest)
         self.assertEqual(collection.description, "Contains a lot of cool stuff")
         self.assertEqual(collection.items, [])
         assert collection.slug.startswith(self.slug_prefix)
-        self.assertEqual(collection.url, f"{ENDPOINT_STAGING}/collections/{collection.slug}")
+        self.assertEqual(
+            collection.url, f"{ENDPOINT_STAGING}/collections/{collection.slug}"
+        )
 
     @pytest.mark.skip("Creating duplicated collections work on staging")
     def test_create_collection_exists_ok(self) -> None:
@@ -3943,7 +4669,9 @@ class CollectionAPITest(HfApiCommonTest)
             self._api.create_collection(self.title)
 
         # Can ignore error
-        collection_2 = self._api.create_collection(self.title, description="description", exists_ok=True)
+        collection_2 = self._api.create_collection(
+            self.title, description="description", exists_ok=True
+        )
 
         self.assertEqual(collection_1.slug, collection_2.slug)
         self.assertIsNone(collection_1.description)
@@ -3956,7 +4684,9 @@ class CollectionAPITest(HfApiCommonTest)
         # Get private collection
         self._api.get_collection(collection.slug)  # no error
         with self.assertRaises(HTTPError):
-            self._api.get_collection(collection.slug, token=OTHER_TOKEN)  # not authorized
+            self._api.get_collection(
+                collection.slug, token=OTHER_TOKEN
+            )  # not authorized
 
         # Get public collection
         self._api.update_collection_metadata(collection.slug, private=False)
@@ -3985,11 +4715,17 @@ class CollectionAPITest(HfApiCommonTest)
         self.assertNotEqual(collection_1.slug, collection_2.slug)
 
         # Different slug, same id
-        self.assertEqual(collection_1.slug.split("-")[-1], collection_2.slug.split("-")[-1])
+        self.assertEqual(
+            collection_1.slug.split("-")[-1], collection_2.slug.split("-")[-1]
+        )
 
         # Works with both slugs, same collection returned
-        self.assertEqual(self._api.get_collection(collection_1.slug).slug, collection_2.slug)
-        self.assertEqual(self._api.get_collection(collection_2.slug).slug, collection_2.slug)
+        self.assertEqual(
+            self._api.get_collection(collection_1.slug).slug, collection_2.slug
+        )
+        self.assertEqual(
+            self._api.get_collection(collection_2.slug).slug, collection_2.slug
+        )
 
     def test_delete_collection(self) -> None:
         collection = self._api.create_collection(self.title)
@@ -4010,8 +4746,12 @@ class CollectionAPITest(HfApiCommonTest)
 
         # Create collection + add items to it
         collection = self._api.create_collection(self.title)
-        self._api.add_collection_item(collection.slug, model_id, "model", note="This is my model")
-        self._api.add_collection_item(collection.slug, dataset_id, "dataset")  # note is optional
+        self._api.add_collection_item(
+            collection.slug, model_id, "model", note="This is my model"
+        )
+        self._api.add_collection_item(
+            collection.slug, dataset_id, "dataset"
+        )  # note is optional
 
         # Check consistency
         collection = self._api.get_collection(collection.slug)
@@ -4027,7 +4767,9 @@ class CollectionAPITest(HfApiCommonTest)
         # Add existing item fails (except if ignore error)
         with self.assertRaises(HTTPError):
             self._api.add_collection_item(collection.slug, model_id, "model")
-        self._api.add_collection_item(collection.slug, model_id, "model", exists_ok=True)
+        self._api.add_collection_item(
+            collection.slug, model_id, "model", exists_ok=True
+        )
 
         # Add inexistent item fails
         with self.assertRaises(HTTPError):
@@ -4035,23 +4777,34 @@ class CollectionAPITest(HfApiCommonTest)
 
         # Update first item
         self._api.update_collection_item(
-            collection.slug, collection.items[0].item_object_id, note="New note", position=1
+            collection.slug,
+            collection.items[0].item_object_id,
+            note="New note",
+            position=1,
         )
 
         # Check consistency
         collection = self._api.get_collection(collection.slug)
-        self.assertEqual(collection.items[0].item_id, dataset_id)  # position got updated
+        self.assertEqual(
+            collection.items[0].item_id, dataset_id
+        )  # position got updated
         self.assertEqual(collection.items[1].item_id, model_id)
         self.assertEqual(collection.items[1].note, "New note")  # note got updated
 
         # Delete last item
-        self._api.delete_collection_item(collection.slug, collection.items[1].item_object_id)
-        self._api.delete_collection_item(collection.slug, collection.items[1].item_object_id, missing_ok=True)
+        self._api.delete_collection_item(
+            collection.slug, collection.items[1].item_object_id
+        )
+        self._api.delete_collection_item(
+            collection.slug, collection.items[1].item_object_id, missing_ok=True
+        )
 
         # Check consistency
         collection = self._api.get_collection(collection.slug)
         self.assertEqual(len(collection.items), 1)  # only 1 item remaining
-        self.assertEqual(collection.items[0].item_id, dataset_id)  # position got updated
+        self.assertEqual(
+            collection.items[0].item_id, dataset_id
+        )  # position got updated
 
         # Delete everything
         self._api.delete_repo(model_id)
@@ -4106,7 +4859,9 @@ class AccessRequestAPITest(HfApiCommonTe
         assert requests[0].username == OTHER_USER
 
         # Reject access
-        self._api.reject_access_request(self.repo_id, OTHER_USER, rejection_reason="This is a rejection reason")
+        self._api.reject_access_request(
+            self.repo_id, OTHER_USER, rejection_reason="This is a rejection reason"
+        )
         requests = self._api.list_pending_access_requests(self.repo_id)
         assert len(requests) == 0  # not pending anymore
         requests = self._api.list_rejected_access_requests(self.repo_id)
@@ -4132,9 +4887,13 @@ class AccessRequestAPITest(HfApiCommonTe
             self._api.accept_access_request(self.repo_id, OTHER_USER)
 
         # Cannot reject to already rejected
-        self._api.reject_access_request(self.repo_id, OTHER_USER, rejection_reason="This is a rejection reason")
+        self._api.reject_access_request(
+            self.repo_id, OTHER_USER, rejection_reason="This is a rejection reason"
+        )
         with self.assertRaises(HTTPError):
-            self._api.reject_access_request(self.repo_id, OTHER_USER, rejection_reason="This is a rejection reason")
+            self._api.reject_access_request(
+                self.repo_id, OTHER_USER, rejection_reason="This is a rejection reason"
+            )
 
         # Cannot cancel to already cancelled
         self._api.cancel_access_request(self.repo_id, OTHER_USER)
@@ -4199,7 +4958,9 @@ class WebhookApiTest(HfApiCommonTest):
         super().setUp()
         self.webhook_url = "https://webhook.site/test"
         self.watched_items = [
-            WebhookWatchedItem(type="user", name="julien-c"),  # can be either a dataclass
+            WebhookWatchedItem(
+                type="user", name="julien-c"
+            ),  # can be either a dataclass
             {"type": "org", "name": "HuggingFaceH4"},  # or a simple dictionary
         ]
         self.domains = ["repo", "discussion"]
@@ -4207,7 +4968,10 @@ class WebhookApiTest(HfApiCommonTest):
 
         # Create a webhook to be used in the tests
         self.webhook = self._api.create_webhook(
-            url=self.webhook_url, watched=self.watched_items, domains=self.domains, secret=self.secret
+            url=self.webhook_url,
+            watched=self.watched_items,
+            domains=self.domains,
+            secret=self.secret,
         )
 
     def tearDown(self) -> None:
@@ -4227,7 +4991,10 @@ class WebhookApiTest(HfApiCommonTest):
 
     def test_create_webhook(self) -> None:
         new_webhook = self._api.create_webhook(
-            url=self.webhook_url, watched=self.watched_items, domains=self.domains, secret=self.secret
+            url=self.webhook_url,
+            watched=self.watched_items,
+            domains=self.domains,
+            secret=self.secret,
         )
         self.assertIsInstance(new_webhook, WebhookInfo)
         self.assertEqual(new_webhook.url, self.webhook_url)
@@ -4238,7 +5005,11 @@ class WebhookApiTest(HfApiCommonTest):
     def test_update_webhook(self) -> None:
         updated_url = "https://webhook.site/new"
         updated_webhook = self._api.update_webhook(
-            self.webhook.id, url=updated_url, watched=self.watched_items, domains=self.domains, secret=self.secret
+            self.webhook.id,
+            url=updated_url,
+            watched=self.watched_items,
+            domains=self.domains,
+            secret=self.secret,
         )
         self.assertEqual(updated_webhook.url, updated_url)
 
@@ -4253,7 +5024,10 @@ class WebhookApiTest(HfApiCommonTest):
     def test_delete_webhook(self) -> None:
         # Create another webhook to test deletion
         webhook_to_delete = self._api.create_webhook(
-            url=self.webhook_url, watched=self.watched_items, domains=self.domains, secret=self.secret
+            url=self.webhook_url,
+            watched=self.watched_items,
+            domains=self.domains,
+            secret=self.secret,
         )
         self._api.delete_webhook(webhook_to_delete.id)
         with self.assertRaises(HTTPError):
@@ -4279,16 +5053,26 @@ class TestExpandPropertyType(HfApiCommon
         property_type = (
             ExpandModelProperty_T
             if repo_type == "model"
-            else (ExpandDatasetProperty_T if repo_type == "dataset" else ExpandSpaceProperty_T)
+            else (
+                ExpandDatasetProperty_T
+                if repo_type == "dataset"
+                else ExpandSpaceProperty_T
+            )
         )
         property_type_name = (
             "ExpandModelProperty_T"
             if repo_type == "model"
-            else ("ExpandDatasetProperty_T" if repo_type == "dataset" else "ExpandSpaceProperty_T")
+            else (
+                "ExpandDatasetProperty_T"
+                if repo_type == "dataset"
+                else "ExpandSpaceProperty_T"
+            )
         )
 
         try:
-            self._api.repo_info(repo_id=repo_id, repo_type=repo_type, expand=["does_not_exist"])
+            self._api.repo_info(
+                repo_id=repo_id, repo_type=repo_type, expand=["does_not_exist"]
+            )
             raise Exception("Should have raised an exception")
         except HfHubHTTPError as e:
             assert e.response.status_code == 400
@@ -4296,7 +5080,9 @@ class TestExpandPropertyType(HfApiCommon
 
         assert message.startswith('"expand" must be one of ')
         defined_args = set(get_args(property_type))
-        expected_args = set(message.replace('"expand" must be one of ', "").strip("[]").split(", "))
+        expected_args = set(
+            message.replace('"expand" must be one of ', "").strip("[]").split(", ")
+        )
         expected_args.discard("gitalyUid")  # internal one, do not document
 
         if defined_args != expected_args:
@@ -4325,16 +5111,25 @@ class TestLargeUpload(HfApiCommonTest):
                 subfolder = folder / f"subfolder_{i}"
                 subfolder.mkdir(parents=True, exist_ok=True)
                 for j in range(N_FILES_PER_FOLDER):
-                    (subfolder / f"file_lfs_{i}_{j}.bin").write_bytes(f"content_lfs_{i}_{j}".encode())
-                    (subfolder / f"file_regular_{i}_{j}.txt").write_bytes(f"content_regular_{i}_{j}".encode())
+                    (subfolder / f"file_lfs_{i}_{j}.bin").write_bytes(
+                        f"content_lfs_{i}_{j}".encode()
+                    )
+                    (subfolder / f"file_regular_{i}_{j}.txt").write_bytes(
+                        f"content_regular_{i}_{j}".encode()
+                    )
 
             # Upload the folder
             self._api.upload_large_folder(
-                repo_id=repo_url.repo_id, repo_type=repo_url.repo_type, folder_path=folder, num_workers=4
+                repo_id=repo_url.repo_id,
+                repo_type=repo_url.repo_type,
+                folder_path=folder,
+                num_workers=4,
             )
 
         # Check all files have been uploaded
-        uploaded_files = self._api.list_repo_files(repo_url.repo_id, repo_type=repo_url.repo_type)
+        uploaded_files = self._api.list_repo_files(
+            repo_url.repo_id, repo_type=repo_url.repo_type
+        )
         for i in range(N_FILES_PER_FOLDER):
             for j in range(N_FILES_PER_FOLDER):
                 assert f"subfolder_{i}/file_lfs_{i}_{j}.bin" in uploaded_files
--- python3-huggingface-hub-0.29.3.orig/tests/test_hf_file_system.py
+++ python3-huggingface-hub-0.29.3/tests/test_hf_file_system.py
@@ -13,7 +13,11 @@ import pytest
 
 from huggingface_hub import hf_file_system
 from huggingface_hub.errors import RepositoryNotFoundError, RevisionNotFoundError
-from huggingface_hub.hf_file_system import HfFileSystem, HfFileSystemFile, HfFileSystemStreamFile
+from huggingface_hub.hf_file_system import (
+    HfFileSystem,
+    HfFileSystemFile,
+    HfFileSystemStreamFile,
+)
 
 from .testing_constants import ENDPOINT_STAGING, TOKEN
 from .testing_utils import repo_name
@@ -27,7 +31,9 @@ class HfFileSystemTests(unittest.TestCas
             fsspec.register_implementation(HfFileSystem.protocol, HfFileSystem)
 
     def setUp(self):
-        self.hffs = HfFileSystem(endpoint=ENDPOINT_STAGING, token=TOKEN, skip_instance_cache=True)
+        self.hffs = HfFileSystem(
+            endpoint=ENDPOINT_STAGING, token=TOKEN, skip_instance_cache=True
+        )
         self.api = self.hffs._api
 
         # Create dummy repo
@@ -82,7 +88,9 @@ class HfFileSystemTests(unittest.TestCas
         self.assertIsNone(text_data_file["lfs"])
         self.assertIsNotNone(text_data_file["last_commit"])
         self.assertIsNotNone(text_data_file["blob_id"])
-        self.assertIn("security", text_data_file)  # the staging endpoint does not run security checks
+        self.assertIn(
+            "security", text_data_file
+        )  # the staging endpoint does not run security checks
 
         # cached info
         self.assertEqual(self.hffs.info(self.text_file), text_data_file)
@@ -103,7 +111,12 @@ class HfFileSystemTests(unittest.TestCas
         )
         self.assertEqual(
             sorted(self.hffs.glob(self.hf_path + "@main" + "/*")),
-            sorted([self.hf_path + "@main" + "/.gitattributes", self.hf_path + "@main" + "/data"]),
+            sorted(
+                [
+                    self.hf_path + "@main" + "/.gitattributes",
+                    self.hf_path + "@main" + "/data",
+                ]
+            ),
         )
         self.assertEqual(
             self.hffs.glob(self.hf_path + "@refs%2Fpr%2F1" + "/data/*"),
@@ -122,12 +135,15 @@ class HfFileSystemTests(unittest.TestCas
             self.hffs.dircache[self.hf_path + "@main"][0]["last_commit"]
         )  # no detail -> no last_commit in cache
 
-        files = self.hffs.glob(self.hf_path + "@main" + "/*", detail=True, expand_info=False)
+        files = self.hffs.glob(
+            self.hf_path + "@main" + "/*", detail=True, expand_info=False
+        )
         self.assertIsInstance(files, dict)
         self.assertEqual(len(files), 2)
         keys = sorted(files)
         self.assertTrue(
-            files[keys[0]]["name"].endswith("/.gitattributes") and files[keys[1]]["name"].endswith("/data")
+            files[keys[0]]["name"].endswith("/.gitattributes")
+            and files[keys[1]]["name"].endswith("/data")
         )
         self.assertIsNone(
             self.hffs.dircache[self.hf_path + "@main"][0]["last_commit"]
@@ -138,7 +154,8 @@ class HfFileSystemTests(unittest.TestCas
         self.assertEqual(len(files), 2)
         keys = sorted(files)
         self.assertTrue(
-            files[keys[0]]["name"].endswith("/.gitattributes") and files[keys[1]]["name"].endswith("/data")
+            files[keys[0]]["name"].endswith("/.gitattributes")
+            and files[keys[1]]["name"].endswith("/data")
         )
         self.assertIsNotNone(files[keys[0]]["last_commit"])
 
@@ -154,13 +171,20 @@ class HfFileSystemTests(unittest.TestCas
 
     def test_file_type(self):
         self.assertTrue(
-            self.hffs.isdir(self.hf_path + "/data") and not self.hffs.isdir(self.hf_path + "/.gitattributes")
+            self.hffs.isdir(self.hf_path + "/data")
+            and not self.hffs.isdir(self.hf_path + "/.gitattributes")
+        )
+        self.assertTrue(
+            self.hffs.isfile(self.text_file)
+            and not self.hffs.isfile(self.hf_path + "/data")
         )
-        self.assertTrue(self.hffs.isfile(self.text_file) and not self.hffs.isfile(self.hf_path + "/data"))
 
     def test_remove_file(self):
         self.hffs.rm_file(self.text_file)
-        self.assertEqual(self.hffs.glob(self.hf_path + "/data/*"), [self.hf_path + "/data/binary_data.bin"])
+        self.assertEqual(
+            self.hffs.glob(self.hf_path + "/data/*"),
+            [self.hf_path + "/data/binary_data.bin"],
+        )
         self.hffs.rm_file(self.hf_path + "@refs/pr/1" + "/data/binary_data_for_pr.bin")
         self.assertEqual(self.hffs.glob(self.hf_path + "@refs/pr/1" + "/data/*"), [])
 
@@ -168,7 +192,9 @@ class HfFileSystemTests(unittest.TestCas
         self.hffs.rm(self.hf_path + "/data", recursive=True)
         self.assertNotIn(self.hf_path + "/data", self.hffs.ls(self.hf_path))
         self.hffs.rm(self.hf_path + "@refs/pr/1" + "/data", recursive=True)
-        self.assertNotIn(self.hf_path + "@refs/pr/1" + "/data", self.hffs.ls(self.hf_path))
+        self.assertNotIn(
+            self.hf_path + "@refs/pr/1" + "/data", self.hffs.ls(self.hf_path)
+        )
 
     def test_read_file(self):
         with self.hffs.open(self.text_file, "r") as f:
@@ -188,17 +214,24 @@ class HfFileSystemTests(unittest.TestCas
             # Simulate that streaming fails mid-way
             f.response.raw.read = None
             self.assertEqual(f.read(6), b"binary")
-            self.assertIsNotNone(f.response.raw.read)  # a new connection has been created
+            self.assertIsNotNone(
+                f.response.raw.read
+            )  # a new connection has been created
 
     def test_read_file_with_revision(self):
-        with self.hffs.open(self.hf_path + "/data/binary_data_for_pr.bin", "rb", revision="refs/pr/1") as f:
+        with self.hffs.open(
+            self.hf_path + "/data/binary_data_for_pr.bin", "rb", revision="refs/pr/1"
+        ) as f:
             self.assertEqual(f.read(), b"dummy binary data on pr")
 
     def test_write_file(self):
         data = "new text data"
         with self.hffs.open(self.hf_path + "/data/new_text_data.txt", "w") as f:
             f.write(data)
-        self.assertIn(self.hf_path + "/data/new_text_data.txt", self.hffs.glob(self.hf_path + "/data/*"))
+        self.assertIn(
+            self.hf_path + "/data/new_text_data.txt",
+            self.hffs.glob(self.hf_path + "/data/*"),
+        )
         with self.hffs.open(self.hf_path + "/data/new_text_data.txt", "r") as f:
             self.assertEqual(f.read(), data)
 
@@ -208,7 +241,10 @@ class HfFileSystemTests(unittest.TestCas
             for _ in range(8):  # 32MB in total
                 f.write(data)
 
-        self.assertIn(self.hf_path + "/data/new_text_data_big.txt", self.hffs.glob(self.hf_path + "/data/*"))
+        self.assertIn(
+            self.hf_path + "/data/new_text_data_big.txt",
+            self.hffs.glob(self.hf_path + "/data/*"),
+        )
         with self.hffs.open(self.hf_path + "/data/new_text_data_big.txt", "r") as f:
             for _ in range(8):
                 self.assertEqual(f.read(len(data)), data)
@@ -227,17 +263,28 @@ class HfFileSystemTests(unittest.TestCas
         self.hffs.cp_file(self.text_file, self.hf_path + "/data/text_data_copy.txt")
         with self.hffs.open(self.hf_path + "/data/text_data_copy.txt", "r") as f:
             self.assertEqual(f.read(), "dummy text data")
-        self.assertIsNone(self.hffs.info(self.hf_path + "/data/text_data_copy.txt")["lfs"])
+        self.assertIsNone(
+            self.hffs.info(self.hf_path + "/data/text_data_copy.txt")["lfs"]
+        )
         # LFS file
-        self.assertIsNotNone(self.hffs.info(self.hf_path + "/data/binary_data.bin")["lfs"])
-        self.hffs.cp_file(self.hf_path + "/data/binary_data.bin", self.hf_path + "/data/binary_data_copy.bin")
+        self.assertIsNotNone(
+            self.hffs.info(self.hf_path + "/data/binary_data.bin")["lfs"]
+        )
+        self.hffs.cp_file(
+            self.hf_path + "/data/binary_data.bin",
+            self.hf_path + "/data/binary_data_copy.bin",
+        )
         with self.hffs.open(self.hf_path + "/data/binary_data_copy.bin", "rb") as f:
             self.assertEqual(f.read(), b"dummy binary data")
-        self.assertIsNotNone(self.hffs.info(self.hf_path + "/data/binary_data_copy.bin")["lfs"])
+        self.assertIsNotNone(
+            self.hffs.info(self.hf_path + "/data/binary_data_copy.bin")["lfs"]
+        )
 
     def test_modified_time(self):
         self.assertIsInstance(self.hffs.modified(self.text_file), datetime.datetime)
-        self.assertIsInstance(self.hffs.modified(self.hf_path + "/data"), datetime.datetime)
+        self.assertIsInstance(
+            self.hffs.modified(self.hf_path + "/data"), datetime.datetime
+        )
         # should fail on a non-existing file
         with self.assertRaises(FileNotFoundError):
             self.hffs.modified(self.hf_path + "/data/not_existing_file.txt")
@@ -264,7 +311,9 @@ class HfFileSystemTests(unittest.TestCas
         self.assertEqual(fs.token, TOKEN)
         self.assertEqual(paths, [self.text_file])
 
-        fs, _, paths = fsspec.get_fs_token_paths(f"hf://{self.repo_id}/data/text_data.txt")
+        fs, _, paths = fsspec.get_fs_token_paths(
+            f"hf://{self.repo_id}/data/text_data.txt"
+        )
         self.assertIsInstance(fs, HfFileSystem)
         self.assertEqual(paths, [f"{self.repo_id}/data/text_data.txt"])
 
@@ -283,7 +332,9 @@ class HfFileSystemTests(unittest.TestCas
         self.assertTrue(files[1]["name"].endswith("/.gitattributes"))
         self.assertIsNotNone(files[1]["last_commit"])
         self.assertIsNotNone(files[1]["blob_id"])
-        self.assertIn("security", files[1])  # the staging endpoint does not run security checks
+        self.assertIn(
+            "security", files[1]
+        )  # the staging endpoint does not run security checks
 
     def test_list_data_directory_no_revision(self):
         files = self.hffs.ls(self.hf_path + "/data")
@@ -298,7 +349,9 @@ class HfFileSystemTests(unittest.TestCas
         self.assertIn("pointer_size", files[0]["lfs"])
         self.assertIsNotNone(files[0]["last_commit"])
         self.assertIsNotNone(files[0]["blob_id"])
-        self.assertIn("security", files[0])  # the staging endpoint does not run security checks
+        self.assertIn(
+            "security", files[0]
+        )  # the staging endpoint does not run security checks
 
         self.assertEqual(files[1]["type"], "file")
         self.assertGreater(files[1]["size"], 0)  # not empty
@@ -306,7 +359,9 @@ class HfFileSystemTests(unittest.TestCas
         self.assertIsNone(files[1]["lfs"])
         self.assertIsNotNone(files[1]["last_commit"])
         self.assertIsNotNone(files[1]["blob_id"])
-        self.assertIn("security", files[1])  # the staging endpoint does not run security checks
+        self.assertIn(
+            "security", files[1]
+        )  # the staging endpoint does not run security checks
 
     def test_list_data_file_no_revision(self):
         files = self.hffs.ls(self.text_file)
@@ -318,13 +373,17 @@ class HfFileSystemTests(unittest.TestCas
         self.assertIsNone(files[0]["lfs"])
         self.assertIsNotNone(files[0]["last_commit"])
         self.assertIsNotNone(files[0]["blob_id"])
-        self.assertIn("security", files[0])  # the staging endpoint does not run security checks
+        self.assertIn(
+            "security", files[0]
+        )  # the staging endpoint does not run security checks
 
     def test_list_data_directory_with_revision(self):
         files = self.hffs.ls(self.hf_path + "@refs%2Fpr%2F1" + "/data")
 
         for test_name, files in {
-            "quoted_rev_in_path": self.hffs.ls(self.hf_path + "@refs%2Fpr%2F1" + "/data"),
+            "quoted_rev_in_path": self.hffs.ls(
+                self.hf_path + "@refs%2Fpr%2F1" + "/data"
+            ),
             "rev_in_path": self.hffs.ls(self.hf_path + "@refs/pr/1" + "/data"),
             "rev_as_arg": self.hffs.ls(self.hf_path + "/data", revision="refs/pr/1"),
             "quoted_rev_in_path_and_rev_as_arg": self.hffs.ls(
@@ -334,7 +393,9 @@ class HfFileSystemTests(unittest.TestCas
             with self.subTest(test_name):
                 self.assertEqual(len(files), 1)  # only one file in PR
                 self.assertEqual(files[0]["type"], "file")
-                self.assertTrue(files[0]["name"].endswith("/data/binary_data_for_pr.bin"))  # PR file
+                self.assertTrue(
+                    files[0]["name"].endswith("/data/binary_data_for_pr.bin")
+                )  # PR file
                 if "quoted_rev_in_path" in test_name:
                     self.assertIn("@refs%2Fpr%2F1", files[0]["name"])
                 elif "rev_in_path" in test_name:
@@ -343,18 +404,27 @@ class HfFileSystemTests(unittest.TestCas
     def test_list_root_directory_no_revision_no_detail_then_with_detail(self):
         files = self.hffs.ls(self.hf_path, detail=False)
         self.assertEqual(len(files), 2)
-        self.assertTrue(files[0].endswith("/data") and files[1].endswith("/.gitattributes"))
-        self.assertIsNone(self.hffs.dircache[self.hf_path][0]["last_commit"])  # no detail -> no last_commit in cache
+        self.assertTrue(
+            files[0].endswith("/data") and files[1].endswith("/.gitattributes")
+        )
+        self.assertIsNone(
+            self.hffs.dircache[self.hf_path][0]["last_commit"]
+        )  # no detail -> no last_commit in cache
 
         files = self.hffs.ls(self.hf_path, detail=True)
         self.assertEqual(len(files), 2)
-        self.assertTrue(files[0]["name"].endswith("/data") and files[1]["name"].endswith("/.gitattributes"))
+        self.assertTrue(
+            files[0]["name"].endswith("/data")
+            and files[1]["name"].endswith("/.gitattributes")
+        )
         self.assertIsNotNone(self.hffs.dircache[self.hf_path][0]["last_commit"])
 
     def test_find_root_directory_no_revision(self):
         files = self.hffs.find(self.hf_path, detail=False)
         self.assertEqual(
-            files, self.hffs.ls(self.hf_path, detail=False)[1:] + self.hffs.ls(self.hf_path + "/data", detail=False)
+            files,
+            self.hffs.ls(self.hf_path, detail=False)[1:]
+            + self.hffs.ls(self.hf_path + "/data", detail=False),
         )
 
         files = self.hffs.find(self.hf_path, detail=True)
@@ -478,9 +548,27 @@ class HfFileSystemTests(unittest.TestCas
         ("username/my_model", None, "model", "username/my_model", "main"),
         ("username/my_model", "dev", "model", "username/my_model", "dev"),
         ("username/my_model@dev", None, "model", "username/my_model", "dev"),
-        ("datasets/username/my_dataset", None, "dataset", "username/my_dataset", "main"),
-        ("datasets/username/my_dataset", "dev", "dataset", "username/my_dataset", "dev"),
-        ("datasets/username/my_dataset@dev", None, "dataset", "username/my_dataset", "dev"),
+        (
+            "datasets/username/my_dataset",
+            None,
+            "dataset",
+            "username/my_dataset",
+            "main",
+        ),
+        (
+            "datasets/username/my_dataset",
+            "dev",
+            "dataset",
+            "username/my_dataset",
+            "dev",
+        ),
+        (
+            "datasets/username/my_dataset@dev",
+            None,
+            "dataset",
+            "username/my_dataset",
+            "dev",
+        ),
         # Parse with hf:// protocol
         ("hf://gpt2", None, "model", "gpt2", "main"),
         ("hf://gpt2", "dev", "model", "gpt2", "dev"),
@@ -490,7 +578,13 @@ class HfFileSystemTests(unittest.TestCas
         ("hf://datasets/squad@dev", None, "dataset", "squad", "dev"),
         # Parse with `refs/convert/parquet` and `refs/pr/(\d)+` revisions.
         # Regression tests for https://github.com/huggingface/huggingface_hub/issues/1710.
-        ("datasets/squad@refs/convert/parquet", None, "dataset", "squad", "refs/convert/parquet"),
+        (
+            "datasets/squad@refs/convert/parquet",
+            None,
+            "dataset",
+            "squad",
+            "refs/convert/parquet",
+        ),
         (
             "hf://datasets/username/my_dataset@refs/convert/parquet",
             None,
@@ -500,9 +594,27 @@ class HfFileSystemTests(unittest.TestCas
         ),
         ("gpt2@refs/pr/2", None, "model", "gpt2", "refs/pr/2"),
         ("gpt2@refs%2Fpr%2F2", None, "model", "gpt2", "refs/pr/2"),
-        ("hf://username/my_model@refs/pr/10", None, "model", "username/my_model", "refs/pr/10"),
-        ("hf://username/my_model@refs/pr/10", "refs/pr/10", "model", "username/my_model", "refs/pr/10"),
-        ("hf://username/my_model@refs%2Fpr%2F10", "refs/pr/10", "model", "username/my_model", "refs/pr/10"),
+        (
+            "hf://username/my_model@refs/pr/10",
+            None,
+            "model",
+            "username/my_model",
+            "refs/pr/10",
+        ),
+        (
+            "hf://username/my_model@refs/pr/10",
+            "refs/pr/10",
+            "model",
+            "username/my_model",
+            "refs/pr/10",
+        ),
+        (
+            "hf://username/my_model@refs%2Fpr%2F10",
+            "refs/pr/10",
+            "model",
+            "username/my_model",
+            "refs/pr/10",
+        ),
     ],
 )
 def test_resolve_path(
@@ -533,15 +645,27 @@ def test_resolve_path(
     "path,revision,expected_path",
     [
         ("hf://datasets/squad@dev", None, "datasets/squad@dev"),
-        ("datasets/squad@refs/convert/parquet", None, "datasets/squad@refs/convert/parquet"),
+        (
+            "datasets/squad@refs/convert/parquet",
+            None,
+            "datasets/squad@refs/convert/parquet",
+        ),
         ("hf://username/my_model@refs/pr/10", None, "username/my_model@refs/pr/10"),
-        ("username/my_model", "refs/weirdo", "username/my_model@refs%2Fweirdo"),  # not a "special revision" -> encode
+        (
+            "username/my_model",
+            "refs/weirdo",
+            "username/my_model@refs%2Fweirdo",
+        ),  # not a "special revision" -> encode
     ],
 )
-def test_unresolve_path(path: str, revision: Optional[str], expected_path: str, path_in_repo: str) -> None:
+def test_unresolve_path(
+    path: str, revision: Optional[str], expected_path: str, path_in_repo: str
+) -> None:
     fs = HfFileSystem()
     path = path + "/" + path_in_repo if path_in_repo else path
-    expected_path = expected_path + "/" + path_in_repo if path_in_repo else expected_path
+    expected_path = (
+        expected_path + "/" + path_in_repo if path_in_repo else expected_path
+    )
 
     with mock_repo_info(fs):
         assert fs.resolve_path(path, revision=revision).unresolve() == expected_path
@@ -565,7 +689,11 @@ def mock_repo_info(fs: HfFileSystem):
     def _inner(repo_id: str, *, revision: str, repo_type: str, **kwargs):
         if repo_id not in ["gpt2", "squad", "username/my_dataset", "username/my_model"]:
             raise RepositoryNotFoundError(repo_id)
-        if revision is not None and revision not in ["main", "dev", "refs"] and not revision.startswith("refs/"):
+        if (
+            revision is not None
+            and revision not in ["main", "dev", "refs"]
+            and not revision.startswith("refs/")
+        ):
             raise RevisionNotFoundError(revision)
 
     return patch.object(fs._api, "repo_info", _inner)
@@ -591,7 +719,9 @@ def test_access_repositories_lists(not_s
 def test_exists_after_repo_deletion():
     """Test that exists() correctly reflects repository deletion."""
     # Initialize with staging endpoint and skip cache
-    hffs = HfFileSystem(endpoint=ENDPOINT_STAGING, token=TOKEN, skip_instance_cache=True)
+    hffs = HfFileSystem(
+        endpoint=ENDPOINT_STAGING, token=TOKEN, skip_instance_cache=True
+    )
     api = hffs._api
 
     # Create a new repo
--- python3-huggingface-hub-0.29.3.orig/tests/test_hub_mixin.py
+++ python3-huggingface-hub-0.29.3/tests/test_hub_mixin.py
@@ -91,7 +91,9 @@ class DummyModelFromPretrainedExpectsCon
         return cls(**kwargs)
 
 
-class BaseModelForInheritance(ModelHubMixin, repo_url="https://hf.co/my-repo", library_name="my-cool-library"):
+class BaseModelForInheritance(
+    ModelHubMixin, repo_url="https://hf.co/my-repo", library_name="my-cool-library"
+):
     pass
 
 
@@ -105,7 +107,9 @@ class DummyModelSavingConfig(ModelHubMix
 
         This file must not be overwritten by the default config saved by `ModelHubMixin`.
         """
-        (save_directory / "config.json").write_text(json.dumps({"custom_config": "custom_config"}))
+        (save_directory / "config.json").write_text(
+            json.dumps({"custom_config": "custom_config"})
+        )
 
 
 @dataclass
@@ -245,7 +249,9 @@ class HubMixinTest(unittest.TestCase):
         model = DummyModelWithKwargs()
         model.save_pretrained(self.cache_dir)
         with patch.object(
-            DummyModelWithKwargs, "_from_pretrained", return_value=DummyModelWithKwargs()
+            DummyModelWithKwargs,
+            "_from_pretrained",
+            return_value=DummyModelWithKwargs(),
         ) as from_pretrained_mock:
             model = DummyModelWithKwargs.from_pretrained(self.cache_dir)
             assert "config" not in from_pretrained_mock.call_args_list[0].kwargs
@@ -273,7 +279,9 @@ class HubMixinTest(unittest.TestCase):
         model.save_pretrained(self.cache_dir)
         assert model._hub_mixin_config == {"something": "else"}
 
-        with patch.object(DummyModelWithKwargs, "__init__", return_value=None) as init_call_mock:
+        with patch.object(
+            DummyModelWithKwargs, "__init__", return_value=None
+        ) as init_call_mock:
             DummyModelWithKwargs.from_pretrained(self.cache_dir)
 
         # 'something' is passed to __init__ both as kwarg and in config.
@@ -293,12 +301,18 @@ class HubMixinTest(unittest.TestCase):
         mocked_model.push_to_hub.assert_not_called()
 
         # Push to hub with repo_id (config is pushed)
-        mocked_model.save_pretrained(save_directory, push_to_hub=True, repo_id="CustomID")
-        mocked_model.push_to_hub.assert_called_with(repo_id="CustomID", config=CONFIG_AS_DICT, model_card_kwargs={})
+        mocked_model.save_pretrained(
+            save_directory, push_to_hub=True, repo_id="CustomID"
+        )
+        mocked_model.push_to_hub.assert_called_with(
+            repo_id="CustomID", config=CONFIG_AS_DICT, model_card_kwargs={}
+        )
 
         # Push to hub with default repo_id (based on dir name)
         mocked_model.save_pretrained(save_directory, push_to_hub=True)
-        mocked_model.push_to_hub.assert_called_with(repo_id=repo_id, config=CONFIG_AS_DICT, model_card_kwargs={})
+        mocked_model.push_to_hub.assert_called_with(
+            repo_id=repo_id, config=CONFIG_AS_DICT, model_card_kwargs={}
+        )
 
     @patch.object(DummyModelNoConfig, "_from_pretrained")
     def test_from_pretrained_model_id_only(self, from_pretrained_mock: Mock) -> None:
@@ -307,10 +321,14 @@ class HubMixinTest(unittest.TestCase):
         assert model is from_pretrained_mock.return_value
 
     @patch.object(DummyModelNoConfig, "_from_pretrained")
-    def test_from_pretrained_model_id_and_revision(self, from_pretrained_mock: Mock) -> None:
+    def test_from_pretrained_model_id_and_revision(
+        self, from_pretrained_mock: Mock
+    ) -> None:
         """Regression test for #1313.
         See https://github.com/huggingface/huggingface_hub/issues/1313."""
-        model = DummyModelNoConfig.from_pretrained("namespace/repo_name", revision="123456789")
+        model = DummyModelNoConfig.from_pretrained(
+            "namespace/repo_name", revision="123456789"
+        )
         from_pretrained_mock.assert_called_once_with(
             model_id="namespace/repo_name",
             revision="123456789",  # Revision is passed correctly!
@@ -326,25 +344,33 @@ class HubMixinTest(unittest.TestCase):
     def test_from_pretrained_from_relative_path(self):
         with SoftTemporaryDirectory(dir=Path(".")) as tmp_relative_dir:
             relative_save_directory = Path(tmp_relative_dir) / "model"
-            DummyModelConfigAsDataclass(config=CONFIG_AS_DATACLASS).save_pretrained(relative_save_directory)
+            DummyModelConfigAsDataclass(config=CONFIG_AS_DATACLASS).save_pretrained(
+                relative_save_directory
+            )
             model = DummyModelConfigAsDataclass.from_pretrained(relative_save_directory)
             assert model._hub_mixin_config == CONFIG_AS_DATACLASS
 
     def test_from_pretrained_from_absolute_path(self):
         save_directory = self.cache_dir / "subfolder"
-        DummyModelConfigAsDataclass(config=CONFIG_AS_DATACLASS).save_pretrained(save_directory)
+        DummyModelConfigAsDataclass(config=CONFIG_AS_DATACLASS).save_pretrained(
+            save_directory
+        )
         model = DummyModelConfigAsDataclass.from_pretrained(save_directory)
         assert model._hub_mixin_config == CONFIG_AS_DATACLASS
 
     def test_from_pretrained_from_absolute_string_path(self):
         save_directory = str(self.cache_dir / "subfolder")
-        DummyModelConfigAsDataclass(config=CONFIG_AS_DATACLASS).save_pretrained(save_directory)
+        DummyModelConfigAsDataclass(config=CONFIG_AS_DATACLASS).save_pretrained(
+            save_directory
+        )
         model = DummyModelConfigAsDataclass.from_pretrained(save_directory)
         assert model._hub_mixin_config == CONFIG_AS_DATACLASS
 
     def test_push_to_hub(self):
         repo_id = f"{USER}/{repo_name('push_to_hub')}"
-        DummyModelConfigAsDataclass(CONFIG_AS_DATACLASS).push_to_hub(repo_id=repo_id, token=TOKEN)
+        DummyModelConfigAsDataclass(CONFIG_AS_DATACLASS).push_to_hub(
+            repo_id=repo_id, token=TOKEN
+        )
 
         # Test model id exists
         self._api.model_info(repo_id)
@@ -367,10 +393,16 @@ class HubMixinTest(unittest.TestCase):
             "token": TOKEN,
         }
         for cls in (DummyModelConfigAsDataclass, DummyModelConfigAsOptionalDataclass):
-            assert cls.from_pretrained(**from_pretrained_kwargs)._hub_mixin_config == CONFIG_AS_DATACLASS
+            assert (
+                cls.from_pretrained(**from_pretrained_kwargs)._hub_mixin_config
+                == CONFIG_AS_DATACLASS
+            )
 
         for cls in (DummyModelConfigAsDict, DummyModelConfigAsOptionalDict):
-            assert cls.from_pretrained(**from_pretrained_kwargs)._hub_mixin_config == CONFIG_AS_DICT
+            assert (
+                cls.from_pretrained(**from_pretrained_kwargs)._hub_mixin_config
+                == CONFIG_AS_DICT
+            )
 
         # Delete repo
         self._api.delete_repo(repo_id=repo_id)
@@ -392,7 +424,9 @@ class HubMixinTest(unittest.TestCase):
         If a previously existing config file exists, it should be overwritten.
         """
         # Something existing in the cache dir
-        (self.cache_dir / "config.json").write_text(json.dumps({"something_legacy": 123}))
+        (self.cache_dir / "config.json").write_text(
+            json.dumps({"something_legacy": 123})
+        )
 
         # Save model
         model = DummyModelWithKwargs(a=1, b=2)
@@ -411,7 +445,9 @@ class HubMixinTest(unittest.TestCase):
 
         See https://github.com/huggingface/huggingface_hub/issues/2157.
         """
-        (self.cache_dir / "config.json").write_text('{"foo": 42, "bar": "baz", "other": "value"}')
+        (self.cache_dir / "config.json").write_text(
+            '{"foo": 42, "bar": "baz", "other": "value"}'
+        )
         model = DummyModelThatIsAlsoADataclass.from_pretrained(self.cache_dir)
         assert model.foo == 42
         assert model.bar == "baz"
@@ -444,7 +480,10 @@ class HubMixinTest(unittest.TestCase):
         assert model_reloaded.bar == "bar"
         assert model_reloaded.baz == 1.0
         assert model_reloaded.custom.value == "custom"
-        assert model_reloaded.optional_custom_1 is not None and model_reloaded.optional_custom_1.value == "optional"
+        assert (
+            model_reloaded.optional_custom_1 is not None
+            and model_reloaded.optional_custom_1.value == "optional"
+        )
         assert model_reloaded.optional_custom_2 is None
         assert model_reloaded.custom_default.value == "default"
 
@@ -473,7 +512,9 @@ a.dum""".strip()
         script = jedi.Script(source, path="example.py")
         source_lines = source.split("\n")
         completions = script.complete(len(source_lines), len(source_lines[-1]))
-        assert any(completion.name == "dummy_example_for_test" for completion in completions)
+        assert any(
+            completion.name == "dummy_example_for_test" for completion in completions
+        )
 
     def test_get_type_hints_works_as_expected(self):
         """
--- python3-huggingface-hub-0.29.3.orig/tests/test_hub_mixin_pytorch.py
+++ python3-huggingface-hub-0.29.3/tests/test_hub_mixin_pytorch.py
@@ -86,7 +86,13 @@ if is_torch_available():
             self.not_jsonable = not_jsonable
 
     class DummyModelWithConfigAndKwargs(nn.Module, PyTorchModelHubMixin):
-        def __init__(self, num_classes: int = 42, state: str = "layernorm", config: Optional[Dict] = None, **kwargs):
+        def __init__(
+            self,
+            num_classes: int = 42,
+            state: str = "layernorm",
+            config: Optional[Dict] = None,
+            **kwargs,
+        ):
             super().__init__()
 
     class DummyModelWithModelCardAndCustomKwargs(
@@ -174,12 +180,18 @@ class PytorchHubMixinTest(unittest.TestC
         mocked_model.push_to_hub.assert_not_called()
 
         # Push to hub with repo_id
-        mocked_model.save_pretrained(save_directory, push_to_hub=True, repo_id="CustomID", config=config)
-        mocked_model.push_to_hub.assert_called_with(repo_id="CustomID", config=config, model_card_kwargs={})
+        mocked_model.save_pretrained(
+            save_directory, push_to_hub=True, repo_id="CustomID", config=config
+        )
+        mocked_model.push_to_hub.assert_called_with(
+            repo_id="CustomID", config=config, model_card_kwargs={}
+        )
 
         # Push to hub with default repo_id (based on dir name)
         mocked_model.save_pretrained(save_directory, push_to_hub=True, config=config)
-        mocked_model.push_to_hub.assert_called_with(repo_id=repo_id, config=config, model_card_kwargs={})
+        mocked_model.push_to_hub.assert_called_with(
+            repo_id=repo_id, config=config, model_card_kwargs={}
+        )
 
     @patch.object(DummyModel, "_from_pretrained")
     def test_from_pretrained_model_id_only(self, from_pretrained_mock: Mock) -> None:
@@ -194,7 +206,9 @@ class PytorchHubMixinTest(unittest.TestC
         return self.cache_dir / "model.safetensors"
 
     @patch("huggingface_hub.hub_mixin.hf_hub_download")
-    def test_from_pretrained_model_from_hub_prefer_safetensor(self, hf_hub_download_mock: Mock) -> None:
+    def test_from_pretrained_model_from_hub_prefer_safetensor(
+        self, hf_hub_download_mock: Mock
+    ) -> None:
         hf_hub_download_mock.side_effect = self.pretend_file_download
         model = DummyModel.from_pretrained("namespace/repo_name")
         hf_hub_download_mock.assert_any_call(
@@ -217,13 +231,18 @@ class PytorchHubMixinTest(unittest.TestC
 
         class TestMixin(ModelHubMixin):
             def _save_pretrained(self, save_directory: Path) -> None:
-                torch.save(DummyModel().state_dict(), save_directory / constants.PYTORCH_WEIGHTS_NAME)
+                torch.save(
+                    DummyModel().state_dict(),
+                    save_directory / constants.PYTORCH_WEIGHTS_NAME,
+                )
 
         TestMixin().save_pretrained(self.cache_dir)
         return self.cache_dir / constants.PYTORCH_WEIGHTS_NAME
 
     @patch("huggingface_hub.hub_mixin.hf_hub_download")
-    def test_from_pretrained_model_from_hub_fallback_pickle(self, hf_hub_download_mock: Mock) -> None:
+    def test_from_pretrained_model_from_hub_fallback_pickle(
+        self, hf_hub_download_mock: Mock
+    ) -> None:
         hf_hub_download_mock.side_effect = self.pretend_file_download_fallback
         model = DummyModel.from_pretrained("namespace/repo_name")
         hf_hub_download_mock.assert_any_call(
@@ -251,7 +270,9 @@ class PytorchHubMixinTest(unittest.TestC
         self.assertIsNotNone(model)
 
     @patch.object(DummyModel, "_from_pretrained")
-    def test_from_pretrained_model_id_and_revision(self, from_pretrained_mock: Mock) -> None:
+    def test_from_pretrained_model_id_and_revision(
+        self, from_pretrained_mock: Mock
+    ) -> None:
         """Regression test for #1313.
         See https://github.com/huggingface/huggingface_hub/issues/1313."""
         model = DummyModel.from_pretrained("namespace/repo_name", revision="123456789")
@@ -298,7 +319,9 @@ class PytorchHubMixinTest(unittest.TestC
             "`PyTorchModelHubMixin.from_pretrained` return type annotation is not a TypeVar.",
         )
         self.assertEqual(
-            DummyModel.from_pretrained.__annotations__["return"].__bound__.__forward_arg__,
+            DummyModel.from_pretrained.__annotations__[
+                "return"
+            ].__bound__.__forward_arg__,
             "ModelHubMixin",
             "`PyTorchModelHubMixin.from_pretrained` return type annotation is not a TypeVar bound by `ModelHubMixin`.",
         )
@@ -330,7 +353,12 @@ class PytorchHubMixinTest(unittest.TestC
         assert card.data.library_name == "my-dummy-lib"
         assert card.data.license == "apache-2.0"
         assert card.data.pipeline_tag == "text-classification"
-        assert card.data.tags == ["model_hub_mixin", "pytorch_model_hub_mixin", "tag1", "tag2"]
+        assert card.data.tags == [
+            "model_hub_mixin",
+            "pytorch_model_hub_mixin",
+            "tag1",
+            "tag2",
+        ]
 
         # Model card template has been used
         assert "This is a dummy model card" in str(card)
@@ -351,7 +379,10 @@ class PytorchHubMixinTest(unittest.TestC
         # Test saving model => auto-generated config is saved
         model.save_pretrained(self.cache_dir)
         assert config_file.exists()
-        assert json.loads(config_file.read_text()) == {"num_classes": 50, "state": "layernorm"}
+        assert json.loads(config_file.read_text()) == {
+            "num_classes": 50,
+            "state": "layernorm",
+        }
 
         # Reload model => config is reloaded
         reloaded = DummyModelNoConfig.from_pretrained(self.cache_dir)
@@ -360,14 +391,22 @@ class PytorchHubMixinTest(unittest.TestC
         assert reloaded._hub_mixin_config == {"num_classes": 50, "state": "layernorm"}
 
         # Reload model with custom config => custom config is used
-        reloaded_with_default = DummyModelNoConfig.from_pretrained(self.cache_dir, state="other")
+        reloaded_with_default = DummyModelNoConfig.from_pretrained(
+            self.cache_dir, state="other"
+        )
         assert reloaded_with_default.num_classes == 50
         assert reloaded_with_default.state == "other"
-        assert reloaded_with_default._hub_mixin_config == {"num_classes": 50, "state": "other"}
+        assert reloaded_with_default._hub_mixin_config == {
+            "num_classes": 50,
+            "state": "other",
+        }
 
         config_file.unlink()  # Remove config file
         reloaded_with_default.save_pretrained(self.cache_dir)
-        assert json.loads(config_file.read_text()) == {"num_classes": 50, "state": "other"}
+        assert json.loads(config_file.read_text()) == {
+            "num_classes": 50,
+            "state": "other",
+        }
 
     def test_save_with_non_jsonable_config(self):
         # Save with a non-jsonable value
@@ -424,9 +463,17 @@ class PytorchHubMixinTest(unittest.TestC
 
     def test_save_pretrained_when_config_and_kwargs_are_passed(self):
         # Test creating model with config and kwargs => all values are saved together in config.json
-        model = DummyModelWithConfigAndKwargs(num_classes=50, state="layernorm", config={"a": 1}, b=2, c=3)
+        model = DummyModelWithConfigAndKwargs(
+            num_classes=50, state="layernorm", config={"a": 1}, b=2, c=3
+        )
         model.save_pretrained(self.cache_dir)
-        assert model._hub_mixin_config == {"num_classes": 50, "state": "layernorm", "a": 1, "b": 2, "c": 3}
+        assert model._hub_mixin_config == {
+            "num_classes": 50,
+            "state": "layernorm",
+            "a": 1,
+            "b": 2,
+            "c": 3,
+        }
 
         reloaded = DummyModelWithConfigAndKwargs.from_pretrained(self.cache_dir)
         assert reloaded._hub_mixin_config == model._hub_mixin_config
--- python3-huggingface-hub-0.29.3.orig/tests/test_inference_api.py
+++ python3-huggingface-hub-0.29.3/tests/test_inference_api.py
@@ -33,7 +33,9 @@ class InferenceApiTest(unittest.TestCase
     @classmethod
     @with_production_testing
     def setUpClass(cls) -> None:
-        cls.image_file = hf_hub_download(repo_id="Narsil/image_dummy", repo_type="dataset", filename="lena.png")
+        cls.image_file = hf_hub_download(
+            repo_id="Narsil/image_dummy", repo_type="dataset", filename="lena.png"
+        )
         return super().setUpClass()
 
     @expect_deprecation("huggingface_hub.inference_api")
@@ -129,7 +131,9 @@ class InferenceApiTest(unittest.TestCase
 
     @expect_deprecation("huggingface_hub.inference_api")
     def test_inference_overriding_invalid_task(self):
-        with self.assertRaises(ValueError, msg="Invalid task invalid-task. Make sure it's valid."):
+        with self.assertRaises(
+            ValueError, msg="Invalid task invalid-task. Make sure it's valid."
+        ):
             InferenceApi("bert-base-uncased", task="invalid-task")
 
     @expect_deprecation("huggingface_hub.inference_api")
--- python3-huggingface-hub-0.29.3.orig/tests/test_inference_async_client.py
+++ python3-huggingface-hub-0.29.3/tests/test_inference_async_client.py
@@ -42,16 +42,24 @@ from huggingface_hub import (
     InferenceTimeoutError,
     TextGenerationOutputPrefillToken,
 )
-from huggingface_hub.inference._common import ValidationError as TextGenerationValidationError
+from huggingface_hub.inference._common import (
+    ValidationError as TextGenerationValidationError,
+)
 from huggingface_hub.inference._common import _get_unsupported_text_generation_kwargs
 
-from .test_inference_client import CHAT_COMPLETE_NON_TGI_MODEL, CHAT_COMPLETION_MESSAGES, CHAT_COMPLETION_MODEL
+from .test_inference_client import (
+    CHAT_COMPLETE_NON_TGI_MODEL,
+    CHAT_COMPLETION_MESSAGES,
+    CHAT_COMPLETION_MODEL,
+)
 from .testing_utils import with_production_testing
 
 
 @pytest.fixture(autouse=True)
 def patch_non_tgi_server(monkeypatch: pytest.MonkeyPatch):
-    monkeypatch.setattr(huggingface_hub.inference._common, "_UNSUPPORTED_TEXT_GENERATION_KWARGS", {})
+    monkeypatch.setattr(
+        huggingface_hub.inference._common, "_UNSUPPORTED_TEXT_GENERATION_KWARGS", {}
+    )
 
 
 @pytest.fixture
@@ -72,14 +80,18 @@ async def test_async_generate_no_details
 @pytest.mark.asyncio
 @with_production_testing
 async def test_async_generate_with_details(tgi_client: AsyncInferenceClient) -> None:
-    response = await tgi_client.text_generation("test", details=True, max_new_tokens=1, decoder_input_details=True)
+    response = await tgi_client.text_generation(
+        "test", details=True, max_new_tokens=1, decoder_input_details=True
+    )
 
     assert response.generated_text == "."
     assert response.details.finish_reason == "length"
     assert response.details.generated_tokens == 1
     assert response.details.seed is None
     assert len(response.details.prefill) == 1
-    assert response.details.prefill[0] == TextGenerationOutputPrefillToken(id=9288, logprob=None, text="test")
+    assert response.details.prefill[0] == TextGenerationOutputPrefillToken(
+        id=9288, logprob=None, text="test"
+    )
     assert len(response.details.tokens) == 1
     assert response.details.tokens[0].id == 13
     assert response.details.tokens[0].text == "."
@@ -91,7 +103,12 @@ async def test_async_generate_with_detai
 @with_production_testing
 async def test_async_generate_best_of(tgi_client: AsyncInferenceClient) -> None:
     response = await tgi_client.text_generation(
-        "test", max_new_tokens=1, best_of=2, do_sample=True, decoder_input_details=True, details=True
+        "test",
+        max_new_tokens=1,
+        best_of=2,
+        do_sample=True,
+        decoder_input_details=True,
+        details=True,
     )
 
     assert response.details.seed is not None
@@ -103,39 +120,61 @@ async def test_async_generate_best_of(tg
 @pytest.mark.vcr
 @pytest.mark.asyncio
 @with_production_testing
-async def test_async_generate_validation_error(tgi_client: AsyncInferenceClient) -> None:
+async def test_async_generate_validation_error(
+    tgi_client: AsyncInferenceClient,
+) -> None:
     with pytest.raises(TextGenerationValidationError):
         await tgi_client.text_generation("test", max_new_tokens=10_000)
 
 
 @pytest.mark.vcr
 @pytest.mark.asyncio
-@pytest.mark.skip("skipping this test, as InferenceAPI seems to not throw an error when sending unsupported params")
-async def test_async_generate_non_tgi_endpoint(tgi_client: AsyncInferenceClient) -> None:
+@pytest.mark.skip(
+    "skipping this test, as InferenceAPI seems to not throw an error when sending unsupported params"
+)
+async def test_async_generate_non_tgi_endpoint(
+    tgi_client: AsyncInferenceClient,
+) -> None:
     text = await tgi_client.text_generation("0 1 2", model="gpt2", max_new_tokens=10)
     assert text == " 3 4 5 6 7 8 9 10 11 12"
-    assert _get_unsupported_text_generation_kwargs("gpt2") == ["details", "stop", "watermark", "decoder_input_details"]
+    assert _get_unsupported_text_generation_kwargs("gpt2") == [
+        "details",
+        "stop",
+        "watermark",
+        "decoder_input_details",
+    ]
 
     # Watermark is ignored (+ warning)
     with pytest.warns(UserWarning):
-        await tgi_client.text_generation("4 5 6", model="gpt2", max_new_tokens=10, watermark=True)
+        await tgi_client.text_generation(
+            "4 5 6", model="gpt2", max_new_tokens=10, watermark=True
+        )
 
     # Return as detail even if details=True (+ warning)
     with pytest.warns(UserWarning):
-        text = await tgi_client.text_generation("0 1 2", model="gpt2", max_new_tokens=10, details=True)
+        text = await tgi_client.text_generation(
+            "0 1 2", model="gpt2", max_new_tokens=10, details=True
+        )
     assert isinstance(text, str)
 
     # Return as stream raises error
     with pytest.raises(ValueError):
-        await tgi_client.text_generation("0 1 2", model="gpt2", max_new_tokens=10, stream=True)
+        await tgi_client.text_generation(
+            "0 1 2", model="gpt2", max_new_tokens=10, stream=True
+        )
 
 
 @pytest.mark.vcr
 @pytest.mark.asyncio
 @with_production_testing
-async def test_async_generate_stream_no_details(tgi_client: AsyncInferenceClient) -> None:
+async def test_async_generate_stream_no_details(
+    tgi_client: AsyncInferenceClient,
+) -> None:
     responses = [
-        response async for response in await tgi_client.text_generation("test", max_new_tokens=1, stream=True)
+        response
+        async for response in await tgi_client.text_generation(
+            "test", max_new_tokens=1, stream=True
+        )
     ]
 
     assert len(responses) == 1
@@ -148,10 +187,14 @@ async def test_async_generate_stream_no_
 @pytest.mark.vcr
 @pytest.mark.asyncio
 @with_production_testing
-async def test_async_generate_stream_with_details(tgi_client: AsyncInferenceClient) -> None:
+async def test_async_generate_stream_with_details(
+    tgi_client: AsyncInferenceClient,
+) -> None:
     responses = [
         response
-        async for response in await tgi_client.text_generation("test", max_new_tokens=1, stream=True, details=True)
+        async for response in await tgi_client.text_generation(
+            "test", max_new_tokens=1, stream=True, details=True
+        )
     ]
 
     assert len(responses) == 1
@@ -174,7 +217,9 @@ async def test_async_chat_completion_no_
         id="",
         model="HuggingFaceH4/zephyr-7b-beta",
         system_fingerprint="3.0.1-sha-bb9095a",
-        usage=ChatCompletionOutputUsage(completion_tokens=10, prompt_tokens=46, total_tokens=56),
+        usage=ChatCompletionOutputUsage(
+            completion_tokens=10, prompt_tokens=46, total_tokens=56
+        ),
         choices=[
             ChatCompletionOutputComplete(
                 finish_reason="length",
@@ -202,7 +247,9 @@ async def test_async_chat_completion_not
                 finish_reason="length",
                 index=0,
                 message=ChatCompletionOutputMessage(
-                    role="assistant", content="Deep learning isn't even an algorithm though.", tool_calls=None
+                    role="assistant",
+                    content="Deep learning isn't even an algorithm though.",
+                    tool_calls=None,
                 ),
                 logprobs=None,
             )
@@ -211,7 +258,9 @@ async def test_async_chat_completion_not
         id="",
         model="microsoft/DialoGPT-small",
         system_fingerprint="3.0.1-sha-bb9095a",
-        usage=ChatCompletionOutputUsage(completion_tokens=10, prompt_tokens=13, total_tokens=23),
+        usage=ChatCompletionOutputUsage(
+            completion_tokens=10, prompt_tokens=13, total_tokens=23
+        ),
     )
 
 
@@ -220,7 +269,9 @@ async def test_async_chat_completion_not
 @with_production_testing
 async def test_async_chat_completion_with_stream() -> None:
     async_client = AsyncInferenceClient(model=CHAT_COMPLETION_MODEL)
-    output = await async_client.chat_completion(CHAT_COMPLETION_MESSAGES, max_tokens=10, stream=True)
+    output = await async_client.chat_completion(
+        CHAT_COMPLETION_MESSAGES, max_tokens=10, stream=True
+    )
 
     all_items = []
     generated_text = ""
@@ -302,7 +353,9 @@ def test_sync_vs_async_signatures() -> N
 @pytest.mark.asyncio
 @pytest.mark.skip("Deprecated (get_model_status)")
 async def test_get_status_too_big_model() -> None:
-    model_status = await AsyncInferenceClient(token=False).get_model_status("facebook/nllb-moe-54b")
+    model_status = await AsyncInferenceClient(token=False).get_model_status(
+        "facebook/nllb-moe-54b"
+    )
     assert model_status.loaded is False
     assert model_status.state == "TooBig"
     assert model_status.compute_type == "cpu"
@@ -312,10 +365,14 @@ async def test_get_status_too_big_model(
 @pytest.mark.asyncio
 @pytest.mark.skip("Deprecated (get_model_status)")
 async def test_get_status_loaded_model() -> None:
-    model_status = await AsyncInferenceClient(token=False).get_model_status("bigscience/bloom")
+    model_status = await AsyncInferenceClient(token=False).get_model_status(
+        "bigscience/bloom"
+    )
     assert model_status.loaded is True
     assert model_status.state == "Loaded"
-    assert isinstance(model_status.compute_type, dict)  # e.g. {'gpu': {'gpu': 'a100', 'count': 8}}
+    assert isinstance(
+        model_status.compute_type, dict
+    )  # e.g. {'gpu': {'gpu': 'a100', 'count': 8}}
     assert model_status.framework == "text-generation-inference"
 
 
@@ -336,7 +393,9 @@ async def test_get_status_model_as_url()
 @pytest.mark.asyncio
 @pytest.mark.skip("Deprecated (list_deployed_models)")
 async def test_list_deployed_models_single_frameworks() -> None:
-    models_by_task = await AsyncInferenceClient().list_deployed_models("text-generation-inference")
+    models_by_task = await AsyncInferenceClient().list_deployed_models(
+        "text-generation-inference"
+    )
     assert isinstance(models_by_task, dict)
     for task, models in models_by_task.items():
         assert isinstance(task, str)
@@ -365,7 +424,9 @@ class CustomException(Exception):
 @patch("aiohttp.ClientSession.post", side_effect=CustomException())
 @patch("aiohttp.ClientSession.close")
 @pytest.mark.asyncio
-async def test_close_connection_on_post_error(mock_close: Mock, mock_post: Mock) -> None:
+async def test_close_connection_on_post_error(
+    mock_close: Mock, mock_post: Mock
+) -> None:
     async_client = AsyncInferenceClient()
 
     with pytest.warns(FutureWarning, match=".*'post'.*"):
@@ -428,7 +489,9 @@ async def test_openai_compatibility_with
     )
 
     chunked_text = [
-        chunk.choices[0].delta.content async for chunk in output if chunk.choices[0].delta.content is not None
+        chunk.choices[0].delta.content
+        async for chunk in output
+        if chunk.choices[0].delta.content is not None
     ]
     assert len(chunked_text) == 35
     output_text = "".join(chunked_text)
--- python3-huggingface-hub-0.29.3.orig/tests/test_inference_client.py
+++ python3-huggingface-hub-0.29.3/tests/test_inference_client.py
@@ -256,19 +256,25 @@ def client(request):
 @pytest.fixture(scope="module")
 @with_production_testing
 def audio_file():
-    return hf_hub_download(repo_id="Narsil/image_dummy", repo_type="dataset", filename="sample1.flac")
+    return hf_hub_download(
+        repo_id="Narsil/image_dummy", repo_type="dataset", filename="sample1.flac"
+    )
 
 
 @pytest.fixture(scope="module")
 @with_production_testing
 def image_file():
-    return hf_hub_download(repo_id="Narsil/image_dummy", repo_type="dataset", filename="lena.png")
+    return hf_hub_download(
+        repo_id="Narsil/image_dummy", repo_type="dataset", filename="lena.png"
+    )
 
 
 @pytest.fixture(scope="module")
 @with_production_testing
 def document_file():
-    return hf_hub_download(repo_id="impira/docquery", repo_type="space", filename="contract.jpeg")
+    return hf_hub_download(
+        repo_id="impira/docquery", repo_type="space", filename="contract.jpeg"
+    )
 
 
 class TestBase:
@@ -283,13 +289,18 @@ class TestBase:
         def mock_fetch():
             return _RECOMMENDED_MODELS_FOR_VCR["hf-inference"]
 
-        monkeypatch.setattr("huggingface_hub.inference._providers.hf_inference._fetch_recommended_models", mock_fetch)
+        monkeypatch.setattr(
+            "huggingface_hub.inference._providers.hf_inference._fetch_recommended_models",
+            mock_fetch,
+        )
 
 
 @pytest.mark.vcr
 @with_production_testing
 class TestInferenceClient(TestBase):
-    @pytest.mark.parametrize("client", list_clients("audio-classification"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("audio-classification"), indirect=True
+    )
     def test_audio_classification(self, client: InferenceClient):
         output = client.audio_classification(self.audio_file)
         assert isinstance(output, list)
@@ -308,12 +319,19 @@ class TestInferenceClient(TestBase):
             assert isinstance(item.blob, bytes)
             assert item.content_type == "audio/flac"
 
-    @pytest.mark.parametrize("client", list_clients("automatic-speech-recognition"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("automatic-speech-recognition"), indirect=True
+    )
     def test_automatic_speech_recognition(self, client: InferenceClient):
         output = client.automatic_speech_recognition(self.audio_file)
         # Remove punctuation from the output
-        normalized_output = output.text.translate(str.maketrans("", "", string.punctuation))
-        assert normalized_output.lower().strip() == "a man said to the universe sir i exist"
+        normalized_output = output.text.translate(
+            str.maketrans("", "", string.punctuation)
+        )
+        assert (
+            normalized_output.lower().strip()
+            == "a man said to the universe sir i exist"
+        )
 
     @pytest.mark.parametrize("client", list_clients("conversational"), indirect=True)
     def test_chat_completion_no_stream(self, client: InferenceClient):
@@ -381,7 +399,10 @@ class TestInferenceClient(TestBase):
         tool_call = output.message.tool_calls[0]
         assert tool_call.type == "function"
         # Since tool_choice="auto", we can't know which tool will be called
-        assert tool_call.function.name in ["get_n_day_weather_forecast", "get_current_weather"]
+        assert tool_call.function.name in [
+            "get_n_day_weather_forecast",
+            "get_current_weather",
+        ]
         args = tool_call.function.arguments
         if isinstance(args, str):
             args = json.loads(args)
@@ -440,9 +461,13 @@ class TestInferenceClient(TestBase):
                 model="meta-llama/Meta-Llama-3-70B-Instruct",
             )
 
-    @pytest.mark.parametrize("client", list_clients("document-question-answering"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("document-question-answering"), indirect=True
+    )
     def test_document_question_answering(self, client: InferenceClient):
-        output = client.document_question_answering(self.document_file, "What is the purchase amount?")
+        output = client.document_question_answering(
+            self.document_file, "What is the purchase amount?"
+        )
         assert output == [
             DocumentQuestionAnsweringOutputElement(
                 answer="$1,0000,000,00",
@@ -452,14 +477,20 @@ class TestInferenceClient(TestBase):
             )
         ]
 
-    @pytest.mark.parametrize("client", list_clients("feature-extraction"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("feature-extraction"), indirect=True
+    )
     def test_feature_extraction_with_transformers(self, client: InferenceClient):
         embedding = client.feature_extraction("Hi, who are you?")
         assert isinstance(embedding, np.ndarray)
         assert embedding.shape == (1, 8, 768)
 
-    @pytest.mark.parametrize("client", list_clients("feature-extraction"), indirect=True)
-    def test_feature_extraction_with_sentence_transformers(self, client: InferenceClient):
+    @pytest.mark.parametrize(
+        "client", list_clients("feature-extraction"), indirect=True
+    )
+    def test_feature_extraction_with_sentence_transformers(
+        self, client: InferenceClient
+    ):
         embedding = client.feature_extraction("Hi, who are you?")
         assert isinstance(embedding, np.ndarray)
         assert embedding.shape == (1, 8, 768)
@@ -508,7 +539,9 @@ class TestInferenceClient(TestBase):
     def test_hf_inference_get_recommended_model_has_recommendation(self) -> None:
         from huggingface_hub.inference._providers.hf_inference import HFInferenceTask
 
-        HFInferenceTask("feature-extraction")._prepare_mapped_model(None) == "facebook/bart-base"
+        HFInferenceTask("feature-extraction")._prepare_mapped_model(
+            None
+        ) == "facebook/bart-base"
         HFInferenceTask("translation")._prepare_mapped_model(None) == "t5-small"
 
     def test_hf_inference_get_recommended_model_no_recommendation(self) -> None:
@@ -517,18 +550,32 @@ class TestInferenceClient(TestBase):
         with pytest.raises(ValueError):
             HFInferenceTask("text-generation")._prepare_mapped_model(None)
 
-    @pytest.mark.parametrize("client", list_clients("image-classification"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("image-classification"), indirect=True
+    )
     def test_image_classification(self, client: InferenceClient):
         output = client.image_classification(self.image_file)
         assert output == [
-            ImageClassificationOutputElement(label="brassiere, bra, bandeau", score=0.11767438799142838),
-            ImageClassificationOutputElement(label="sombrero", score=0.09572819620370865),
-            ImageClassificationOutputElement(label="cowboy hat, ten-gallon hat", score=0.0900089293718338),
-            ImageClassificationOutputElement(label="bonnet, poke bonnet", score=0.06615174561738968),
-            ImageClassificationOutputElement(label="fur coat", score=0.061511047184467316),
+            ImageClassificationOutputElement(
+                label="brassiere, bra, bandeau", score=0.11767438799142838
+            ),
+            ImageClassificationOutputElement(
+                label="sombrero", score=0.09572819620370865
+            ),
+            ImageClassificationOutputElement(
+                label="cowboy hat, ten-gallon hat", score=0.0900089293718338
+            ),
+            ImageClassificationOutputElement(
+                label="bonnet, poke bonnet", score=0.06615174561738968
+            ),
+            ImageClassificationOutputElement(
+                label="fur coat", score=0.061511047184467316
+            ),
         ]
 
-    @pytest.mark.parametrize("client", list_clients("image-segmentation"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("image-segmentation"), indirect=True
+    )
     def test_image_segmentation(self, client: InferenceClient):
         output = client.image_segmentation(self.image_file)
         assert isinstance(output, list)
@@ -587,12 +634,20 @@ class TestInferenceClient(TestBase):
             ),
         ]
 
-    @pytest.mark.parametrize("client", list_clients("question-answering"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("question-answering"), indirect=True
+    )
     def test_question_answering(self, client: InferenceClient):
-        output = client.question_answering(question="What is the meaning of life?", context="42")
-        assert output == QuestionAnsweringOutputElement(answer="42", end=2, score=1.4291124728060822e-08, start=0)
+        output = client.question_answering(
+            question="What is the meaning of life?", context="42"
+        )
+        assert output == QuestionAnsweringOutputElement(
+            answer="42", end=2, score=1.4291124728060822e-08, start=0
+        )
 
-    @pytest.mark.parametrize("client", list_clients("sentence-similarity"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("sentence-similarity"), indirect=True
+    )
     def test_sentence_similarity(self, client: InferenceClient):
         scores = client.sentence_similarity(
             "Machine learning is so easy.",
@@ -610,7 +665,9 @@ class TestInferenceClient(TestBase):
         assert isinstance(summary.summary_text, str)
 
     @pytest.mark.skip(reason="This model is not available on InferenceAPI")
-    @pytest.mark.parametrize("client", list_clients("tabular-classification"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("tabular-classification"), indirect=True
+    )
     def test_tabular_classification(self, client: InferenceClient):
         table = {
             "fixed_acidity": ["7.4", "7.8", "10.3"],
@@ -629,7 +686,9 @@ class TestInferenceClient(TestBase):
         assert output == ["5", "5", "5"]
 
     @pytest.mark.skip(reason="This model is not available on InferenceAPI")
-    @pytest.mark.parametrize("client", list_clients("tabular-regression"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("tabular-regression"), indirect=True
+    )
     def test_tabular_regression(self, client: InferenceClient):
         table = {
             "Height": ["11.52", "12.48", "12.3778"],
@@ -642,7 +701,9 @@ class TestInferenceClient(TestBase):
         output = client.tabular_regression(table=table)
         assert output == [110, 120, 130]
 
-    @pytest.mark.parametrize("client", list_clients("table-question-answering"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("table-question-answering"), indirect=True
+    )
     def test_table_question_answering(self, client: InferenceClient):
         table = {
             "Repository": ["Transformers", "Datasets", "Tokenizers"],
@@ -651,15 +712,22 @@ class TestInferenceClient(TestBase):
         query = "How many stars does the transformers repository have?"
         output = client.table_question_answering(query=query, table=table)
         assert output == TableQuestionAnsweringOutputElement(
-            answer="AVERAGE > 36542", cells=["36542"], coordinates=[[0, 1]], aggregator="AVERAGE"
+            answer="AVERAGE > 36542",
+            cells=["36542"],
+            coordinates=[[0, 1]],
+            aggregator="AVERAGE",
         )
 
-    @pytest.mark.parametrize("client", list_clients("text-classification"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("text-classification"), indirect=True
+    )
     def test_text_classification(self, client: InferenceClient):
         output = client.text_classification("I like you")
         assert output == [
             TextClassificationOutputElement(label="POSITIVE", score=0.9998695850372314),
-            TextClassificationOutputElement(label="NEGATIVE", score=0.00013043530634604394),
+            TextClassificationOutputElement(
+                label="NEGATIVE", score=0.00013043530634604394
+            ),
         ]
 
     def test_text_generation(self) -> None:
@@ -670,10 +738,14 @@ class TestInferenceClient(TestBase):
         image = client.text_to_image("An astronaut riding a horse on the moon.")
         assert isinstance(image, Image.Image)
 
-    @pytest.mark.skip(reason="Need to check why fal.ai doesn't take image_size into account")
+    @pytest.mark.skip(
+        reason="Need to check why fal.ai doesn't take image_size into account"
+    )
     @pytest.mark.parametrize("client", list_clients("text-to-image"), indirect=True)
     def test_text_to_image_with_parameters(self, client: InferenceClient):
-        image = client.text_to_image("An astronaut riding a horse on the moon.", height=256, width=256)
+        image = client.text_to_image(
+            "An astronaut riding a horse on the moon.", height=256, width=256
+        )
         assert isinstance(image, Image.Image)
         assert image.height == 256
         assert image.width == 256
@@ -691,73 +763,148 @@ class TestInferenceClient(TestBase):
     @pytest.mark.parametrize("client", list_clients("translation"), indirect=True)
     def test_translation_with_source_and_target_language(self, client: InferenceClient):
         output_with_langs = client.translation(
-            "Hello world", model="facebook/mbart-large-50-many-to-many-mmt", src_lang="en_XX", tgt_lang="fr_XX"
+            "Hello world",
+            model="facebook/mbart-large-50-many-to-many-mmt",
+            src_lang="en_XX",
+            tgt_lang="fr_XX",
         )
         assert isinstance(output_with_langs, TranslationOutput)
 
         with pytest.raises(ValueError):
-            client.translation("Hello world", model="facebook/mbart-large-50-many-to-many-mmt", src_lang="en_XX")
+            client.translation(
+                "Hello world",
+                model="facebook/mbart-large-50-many-to-many-mmt",
+                src_lang="en_XX",
+            )
 
         with pytest.raises(ValueError):
-            client.translation("Hello world", model="facebook/mbart-large-50-many-to-many-mmt", tgt_lang="en_XX")
+            client.translation(
+                "Hello world",
+                model="facebook/mbart-large-50-many-to-many-mmt",
+                tgt_lang="en_XX",
+            )
 
-    @pytest.mark.parametrize("client", list_clients("token-classification"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("token-classification"), indirect=True
+    )
     def test_token_classification(self, client: InferenceClient):
-        output = client.token_classification(text="My name is Sarah Jessica Parker but you can call me Jessica")
+        output = client.token_classification(
+            text="My name is Sarah Jessica Parker but you can call me Jessica"
+        )
         assert output == [
             TokenClassificationOutputElement(
-                score=0.9991335868835449, end=31, entity_group="PER", start=11, word="Sarah Jessica Parker"
+                score=0.9991335868835449,
+                end=31,
+                entity_group="PER",
+                start=11,
+                word="Sarah Jessica Parker",
             ),
             TokenClassificationOutputElement(
-                score=0.9979913234710693, end=59, entity_group="PER", start=52, word="Jessica"
+                score=0.9979913234710693,
+                end=59,
+                entity_group="PER",
+                start=52,
+                word="Jessica",
             ),
         ]
 
-    @pytest.mark.parametrize("client", list_clients("visual-question-answering"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("visual-question-answering"), indirect=True
+    )
     def test_visual_question_answering(self, client: InferenceClient):
-        output = client.visual_question_answering(image=self.image_file, question="Who's in the picture?")
+        output = client.visual_question_answering(
+            image=self.image_file, question="Who's in the picture?"
+        )
         assert output == [
-            VisualQuestionAnsweringOutputElement(score=0.9386942982673645, answer="woman"),
-            VisualQuestionAnsweringOutputElement(score=0.3431190550327301, answer="girl"),
-            VisualQuestionAnsweringOutputElement(score=0.08407800644636154, answer="lady"),
-            VisualQuestionAnsweringOutputElement(score=0.05075192078948021, answer="female"),
-            VisualQuestionAnsweringOutputElement(score=0.017771074548363686, answer="man"),
+            VisualQuestionAnsweringOutputElement(
+                score=0.9386942982673645, answer="woman"
+            ),
+            VisualQuestionAnsweringOutputElement(
+                score=0.3431190550327301, answer="girl"
+            ),
+            VisualQuestionAnsweringOutputElement(
+                score=0.08407800644636154, answer="lady"
+            ),
+            VisualQuestionAnsweringOutputElement(
+                score=0.05075192078948021, answer="female"
+            ),
+            VisualQuestionAnsweringOutputElement(
+                score=0.017771074548363686, answer="man"
+            ),
         ]
 
-    @pytest.mark.parametrize("client", list_clients("zero-shot-classification"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("zero-shot-classification"), indirect=True
+    )
     def test_zero_shot_classification_single_label(self, client: InferenceClient):
         output = client.zero_shot_classification(
             "A new model offers an explanation for how the Galilean satellites formed around the solar system's"
             "largest world. Konstantin Batygin did not set out to solve one of the solar system's most puzzling"
             " mysteries when he went for a run up a hill in Nice, France.",
-            candidate_labels=["space & cosmos", "scientific discovery", "microbiology", "robots", "archeology"],
+            candidate_labels=[
+                "space & cosmos",
+                "scientific discovery",
+                "microbiology",
+                "robots",
+                "archeology",
+            ],
         )
         assert output == [
-            ZeroShotClassificationOutputElement(label="scientific discovery", score=0.796166181564331),
-            ZeroShotClassificationOutputElement(label="space & cosmos", score=0.18570725619792938),
-            ZeroShotClassificationOutputElement(label="microbiology", score=0.007308819331228733),
-            ZeroShotClassificationOutputElement(label="archeology", score=0.0062583745457232),
-            ZeroShotClassificationOutputElement(label="robots", score=0.004559362772852182),
+            ZeroShotClassificationOutputElement(
+                label="scientific discovery", score=0.796166181564331
+            ),
+            ZeroShotClassificationOutputElement(
+                label="space & cosmos", score=0.18570725619792938
+            ),
+            ZeroShotClassificationOutputElement(
+                label="microbiology", score=0.007308819331228733
+            ),
+            ZeroShotClassificationOutputElement(
+                label="archeology", score=0.0062583745457232
+            ),
+            ZeroShotClassificationOutputElement(
+                label="robots", score=0.004559362772852182
+            ),
         ]
 
-    @pytest.mark.parametrize("client", list_clients("zero-shot-classification"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("zero-shot-classification"), indirect=True
+    )
     def test_zero_shot_classification_multi_label(self, client: InferenceClient):
         output = client.zero_shot_classification(
             text="A new model offers an explanation for how the Galilean satellites formed around the solar system's"
             "largest world. Konstantin Batygin did not set out to solve one of the solar system's most puzzling"
             " mysteries when he went for a run up a hill in Nice, France.",
-            candidate_labels=["space & cosmos", "scientific discovery", "microbiology", "robots", "archeology"],
+            candidate_labels=[
+                "space & cosmos",
+                "scientific discovery",
+                "microbiology",
+                "robots",
+                "archeology",
+            ],
             multi_label=True,
         )
         assert output == [
-            ZeroShotClassificationOutputElement(label="scientific discovery", score=0.9829296469688416),
-            ZeroShotClassificationOutputElement(label="space & cosmos", score=0.7551906108856201),
-            ZeroShotClassificationOutputElement(label="microbiology", score=0.0005462627159431577),
-            ZeroShotClassificationOutputElement(label="archeology", score=0.0004713202069979161),
-            ZeroShotClassificationOutputElement(label="robots", score=0.000304485292872414),
+            ZeroShotClassificationOutputElement(
+                label="scientific discovery", score=0.9829296469688416
+            ),
+            ZeroShotClassificationOutputElement(
+                label="space & cosmos", score=0.7551906108856201
+            ),
+            ZeroShotClassificationOutputElement(
+                label="microbiology", score=0.0005462627159431577
+            ),
+            ZeroShotClassificationOutputElement(
+                label="archeology", score=0.0004713202069979161
+            ),
+            ZeroShotClassificationOutputElement(
+                label="robots", score=0.000304485292872414
+            ),
         ]
 
-    @pytest.mark.parametrize("client", list_clients("zero-shot-image-classification"), indirect=True)
+    @pytest.mark.parametrize(
+        "client", list_clients("zero-shot-image-classification"), indirect=True
+    )
     def test_zero_shot_image_classification(self, client: InferenceClient):
         output = client.zero_shot_image_classification(
             image=self.image_file, candidate_labels=["tree", "woman", "cat"]
@@ -789,7 +936,9 @@ class TestOpenAsBinary:
             assert isinstance(content, io.BufferedReader)
 
     def test_open_as_binary_from_url(self) -> None:
-        with _open_as_binary("https://huggingface.co/datasets/Narsil/image_dummy/resolve/main/tree.png") as content:
+        with _open_as_binary(
+            "https://huggingface.co/datasets/Narsil/image_dummy/resolve/main/tree.png"
+        ) as content:
             assert isinstance(content, bytes)
 
     def test_open_as_binary_opened_file(self) -> None:
@@ -806,7 +955,9 @@ class TestOpenAsBinary:
 
 class TestHeadersAndCookies(TestBase):
     def test_headers_and_cookies(self) -> None:
-        client = InferenceClient(headers={"X-My-Header": "foo"}, cookies={"my-cookie": "bar"})
+        client = InferenceClient(
+            headers={"X-My-Header": "foo"}, cookies={"my-cookie": "bar"}
+        )
         assert client.headers["X-My-Header"] == "foo"
         assert client.cookies["my-cookie"] == "bar"
 
@@ -816,9 +967,13 @@ class TestHeadersAndCookies(TestBase):
     def test_mocked_post(self, get_session_mock: MagicMock) -> None:
         """Test that headers and cookies are correctly passed to the request."""
         client = InferenceClient(
-            headers={"X-My-Header": "foo"}, cookies={"my-cookie": "bar"}, proxies="custom proxies"
+            headers={"X-My-Header": "foo"},
+            cookies={"my-cookie": "bar"},
+            proxies="custom proxies",
+        )
+        response = client.post(
+            data=b"content", model="username/repo_name", task="text-classification"
         )
-        response = client.post(data=b"content", model="username/repo_name", task="text-classification")
         assert response == get_session_mock().post.return_value.content
 
         expected_headers = build_hf_headers()
@@ -835,7 +990,9 @@ class TestHeadersAndCookies(TestBase):
 
     @patch("huggingface_hub.inference._client._bytes_to_image")
     @patch("huggingface_hub.inference._client.get_session")
-    def test_accept_header_image(self, get_session_mock: MagicMock, bytes_to_image_mock: MagicMock) -> None:
+    def test_accept_header_image(
+        self, get_session_mock: MagicMock, bytes_to_image_mock: MagicMock
+    ) -> None:
         """Test that Accept: image/png header is set for image tasks."""
         client = InferenceClient()
 
@@ -862,7 +1019,9 @@ class TestModelStatus(TestBase):
         model_status = client.get_model_status("bigscience/bloom")
         assert model_status.loaded
         assert model_status.state == "Loaded"
-        assert isinstance(model_status.compute_type, dict)  # e.g. {'gpu': {'gpu': 'a100', 'count': 8}}
+        assert isinstance(
+            model_status.compute_type, dict
+        )  # e.g. {'gpu': {'gpu': 'a100', 'count': 8}}
         assert model_status.framework == "text-generation-inference"
 
     @expect_deprecation("get_model_status")
@@ -881,19 +1040,29 @@ class TestModelStatus(TestBase):
 class TestListDeployedModels(TestBase):
     @expect_deprecation("list_deployed_models")
     @patch("huggingface_hub.inference._client.get_session")
-    def test_list_deployed_models_main_frameworks_mock(self, get_session_mock: MagicMock) -> None:
+    def test_list_deployed_models_main_frameworks_mock(
+        self, get_session_mock: MagicMock
+    ) -> None:
         InferenceClient().list_deployed_models()
-        assert len(get_session_mock.return_value.get.call_args_list) == len(constants.MAIN_INFERENCE_API_FRAMEWORKS)
+        assert len(get_session_mock.return_value.get.call_args_list) == len(
+            constants.MAIN_INFERENCE_API_FRAMEWORKS
+        )
 
     @expect_deprecation("list_deployed_models")
     @patch("huggingface_hub.inference._client.get_session")
-    def test_list_deployed_models_all_frameworks_mock(self, get_session_mock: MagicMock) -> None:
+    def test_list_deployed_models_all_frameworks_mock(
+        self, get_session_mock: MagicMock
+    ) -> None:
         InferenceClient().list_deployed_models("all")
-        assert len(get_session_mock.return_value.get.call_args_list) == len(constants.ALL_INFERENCE_API_FRAMEWORKS)
+        assert len(get_session_mock.return_value.get.call_args_list) == len(
+            constants.ALL_INFERENCE_API_FRAMEWORKS
+        )
 
     @expect_deprecation("list_deployed_models")
     def test_list_deployed_models_single_frameworks(self) -> None:
-        models_by_task = InferenceClient().list_deployed_models("text-generation-inference")
+        models_by_task = InferenceClient().list_deployed_models(
+            "text-generation-inference"
+        )
         assert isinstance(models_by_task, dict)
         for task, models in models_by_task.items():
             assert isinstance(task, str)
@@ -960,7 +1129,10 @@ class TestOpenAICompatibility(TestBase):
 
     def test_model_and_base_url_mutually_exclusive(self):
         with pytest.raises(ValueError):
-            InferenceClient(model="meta-llama/Meta-Llama-3-8B-Instruct", base_url="http://127.0.0.1:8000")
+            InferenceClient(
+                model="meta-llama/Meta-Llama-3-8B-Instruct",
+                base_url="http://127.0.0.1:8000",
+            )
 
 
 @pytest.mark.parametrize(
@@ -1025,7 +1197,9 @@ def test_chat_completion_error_in_stream
 
 
 INFERENCE_API_URL = "https://api-inference.huggingface.co/models"
-INFERENCE_ENDPOINT_URL = "https://rur2d6yoccusjxgn.us-east-1.aws.endpoints.huggingface.cloud"  # example
+INFERENCE_ENDPOINT_URL = (
+    "https://rur2d6yoccusjxgn.us-east-1.aws.endpoints.huggingface.cloud"  # example
+)
 LOCAL_TGI_URL = "http://0.0.0.0:8080"
 
 
@@ -1073,6 +1247,10 @@ def test_pass_url_as_base_url():
     client = InferenceClient(base_url="http://localhost:8082/v1/")
     provider = get_provider_helper("hf-inference", "text-generation")
     request = provider.prepare_request(
-        inputs="The huggingface_hub library is ", parameters={}, headers={}, model=client.model, api_key=None
+        inputs="The huggingface_hub library is ",
+        parameters={},
+        headers={},
+        model=client.model,
+        api_key=None,
     )
     assert request.url == "http://localhost:8082/v1/"
--- python3-huggingface-hub-0.29.3.orig/tests/test_inference_endpoints.py
+++ python3-huggingface-hub-0.29.3/tests/test_inference_endpoints.py
@@ -160,8 +160,12 @@ def test_from_raw_initialization():
     assert endpoint.type == "protected"
 
     # Datetime parsed correctly
-    assert endpoint.created_at == datetime(2023, 10, 26, 12, 41, 53, 263078, tzinfo=timezone.utc)
-    assert endpoint.updated_at == datetime(2023, 10, 26, 12, 41, 53, 263079, tzinfo=timezone.utc)
+    assert endpoint.created_at == datetime(
+        2023, 10, 26, 12, 41, 53, 263078, tzinfo=timezone.utc
+    )
+    assert endpoint.updated_at == datetime(
+        2023, 10, 26, 12, 41, 53, 263079, tzinfo=timezone.utc
+    )
 
     # Not initialized yet
     assert endpoint.url is None
@@ -173,7 +177,9 @@ def test_from_raw_initialization():
 def test_from_raw_with_hf_api():
     """Test that the HfApi is correctly passed to the InferenceEndpoint."""
     endpoint = InferenceEndpoint.from_raw(
-        MOCK_INITIALIZING, namespace="foo", api=HfApi(library_name="my-library", token="hf_***")
+        MOCK_INITIALIZING,
+        namespace="foo",
+        api=HfApi(library_name="my-library", token="hf_***"),
     )
     assert endpoint._api.library_name == "my-library"
     assert endpoint._api.token == "hf_***"
@@ -192,11 +198,16 @@ def test_get_client_not_ready():
 
 def test_get_client_ready():
     """Test clients are created correctly when endpoint is ready."""
-    endpoint = InferenceEndpoint.from_raw(MOCK_RUNNING, namespace="foo", token="my-token")
+    endpoint = InferenceEndpoint.from_raw(
+        MOCK_RUNNING, namespace="foo", token="my-token"
+    )
 
     # Endpoint is ready
     assert endpoint.status == "running"
-    assert endpoint.url == "https://vksrvs8pc1xnifhq.us-east-1.aws.endpoints.huggingface.cloud"
+    assert (
+        endpoint.url
+        == "https://vksrvs8pc1xnifhq.us-east-1.aws.endpoints.huggingface.cloud"
+    )
 
     # => Client available
     client = endpoint.client
@@ -217,7 +228,10 @@ def test_fetch(mock_get: Mock):
     endpoint.fetch()
 
     assert endpoint.status == "running"
-    assert endpoint.url == "https://vksrvs8pc1xnifhq.us-east-1.aws.endpoints.huggingface.cloud"
+    assert (
+        endpoint.url
+        == "https://vksrvs8pc1xnifhq.us-east-1.aws.endpoints.huggingface.cloud"
+    )
 
 
 @patch("huggingface_hub._inference_endpoints.get_session")
@@ -314,4 +328,6 @@ def test_resume(mock: Mock):
     endpoint = InferenceEndpoint.from_raw(MOCK_RUNNING, namespace="foo")
     mock.return_value = InferenceEndpoint.from_raw(MOCK_INITIALIZING, namespace="foo")
     endpoint.resume()
-    mock.assert_called_once_with(namespace="foo", name="my-endpoint-name", token=None, running_ok=True)
+    mock.assert_called_once_with(
+        namespace="foo", name="my-endpoint-name", token=None, running_ok=True
+    )
--- python3-huggingface-hub-0.29.3.orig/tests/test_inference_providers.py
+++ python3-huggingface-hub-0.29.3/tests/test_inference_providers.py
@@ -8,7 +8,9 @@ from huggingface_hub.inference._provider
     BaseTextGenerationTask,
     recursive_merge,
 )
-from huggingface_hub.inference._providers.black_forest_labs import BlackForestLabsTextToImageTask
+from huggingface_hub.inference._providers.black_forest_labs import (
+    BlackForestLabsTextToImageTask,
+)
 from huggingface_hub.inference._providers.cohere import CohereConversationalTask
 from huggingface_hub.inference._providers.fal_ai import (
     FalAIAutomaticSpeechRecognitionTask,
@@ -16,7 +18,9 @@ from huggingface_hub.inference._provider
     FalAITextToSpeechTask,
     FalAITextToVideoTask,
 )
-from huggingface_hub.inference._providers.fireworks_ai import FireworksAIConversationalTask
+from huggingface_hub.inference._providers.fireworks_ai import (
+    FireworksAIConversationalTask,
+)
 from huggingface_hub.inference._providers.hf_inference import (
     HFInferenceBinaryInputTask,
     HFInferenceConversational,
@@ -31,7 +35,10 @@ from huggingface_hub.inference._provider
     NovitaConversationalTask,
     NovitaTextGenerationTask,
 )
-from huggingface_hub.inference._providers.replicate import ReplicateTask, ReplicateTextToSpeechTask
+from huggingface_hub.inference._providers.replicate import (
+    ReplicateTask,
+    ReplicateTextToSpeechTask,
+)
 from huggingface_hub.inference._providers.sambanova import SambanovaConversationalTask
 from huggingface_hub.inference._providers.together import (
     TogetherTextToImageTask,
@@ -90,10 +97,15 @@ class TestBlackForestLabsProvider:
     def test_get_response_success(self, mocker):
         """Test successful response handling with polling."""
         helper = BlackForestLabsTextToImageTask()
-        mock_session = mocker.patch("huggingface_hub.inference._providers.black_forest_labs.get_session")
+        mock_session = mocker.patch(
+            "huggingface_hub.inference._providers.black_forest_labs.get_session"
+        )
         mock_session.return_value.get.side_effect = [
             mocker.Mock(
-                json=lambda: {"status": "Ready", "result": {"sample": "https://example.com/image.jpg"}},
+                json=lambda: {
+                    "status": "Ready",
+                    "result": {"sample": "https://example.com/image.jpg"},
+                },
                 raise_for_status=lambda: None,
             ),
             mocker.Mock(content=b"image_bytes", raise_for_status=lambda: None),
@@ -105,7 +117,10 @@ class TestBlackForestLabsProvider:
         assert mock_session.return_value.get.call_count == 2
         mock_session.return_value.get.assert_has_calls(
             [
-                mocker.call("https://example.com/poll", headers={"Content-Type": "application/json"}),
+                mocker.call(
+                    "https://example.com/poll",
+                    headers={"Content-Type": "application/json"},
+                ),
                 mocker.call("https://example.com/image.jpg"),
             ]
         )
@@ -121,7 +136,9 @@ class TestCohereConversationalTask:
     def test_prepare_payload_as_dict(self):
         helper = CohereConversationalTask()
         payload = helper._prepare_payload_as_dict(
-            [{"role": "user", "content": "Hello!"}], {}, "CohereForAI/command-r7b-12-2024"
+            [{"role": "user", "content": "Hello!"}],
+            {},
+            "CohereForAI/command-r7b-12-2024",
         )
         assert payload == {
             "messages": [{"role": "user", "content": "Hello!"}],
@@ -146,11 +163,17 @@ class TestFalAIProvider:
 
     def test_automatic_speech_recognition_payload(self):
         helper = FalAIAutomaticSpeechRecognitionTask()
-        payload = helper._prepare_payload_as_dict("https://example.com/audio.mp3", {}, "username/repo_name")
+        payload = helper._prepare_payload_as_dict(
+            "https://example.com/audio.mp3", {}, "username/repo_name"
+        )
         assert payload == {"audio_url": "https://example.com/audio.mp3"}
 
-        payload = helper._prepare_payload_as_dict(b"dummy_audio_data", {}, "username/repo_name")
-        assert payload == {"audio_url": f"data:audio/mpeg;base64,{base64.b64encode(b'dummy_audio_data').decode()}"}
+        payload = helper._prepare_payload_as_dict(
+            b"dummy_audio_data", {}, "username/repo_name"
+        )
+        assert payload == {
+            "audio_url": f"data:audio/mpeg;base64,{base64.b64encode(b'dummy_audio_data').decode()}"
+        }
 
     def test_automatic_speech_recognition_response(self):
         helper = FalAIAutomaticSpeechRecognitionTask()
@@ -179,7 +202,9 @@ class TestFalAIProvider:
 
     def test_text_to_speech_payload(self):
         helper = FalAITextToSpeechTask()
-        payload = helper._prepare_payload_as_dict("Hello world", {}, "username/repo_name")
+        payload = helper._prepare_payload_as_dict(
+            "Hello world", {}, "username/repo_name"
+        )
         assert payload == {"lyrics": "Hello world"}
 
     def test_text_to_speech_response(self, mocker):
@@ -191,7 +216,9 @@ class TestFalAIProvider:
 
     def test_text_to_video_payload(self):
         helper = FalAITextToVideoTask()
-        payload = helper._prepare_payload_as_dict("a cat walking", {"num_frames": 16}, "username/repo_name")
+        payload = helper._prepare_payload_as_dict(
+            "a cat walking", {"num_frames": 16}, "username/repo_name"
+        )
         assert payload == {"prompt": "a cat walking", "num_frames": 16}
 
     def test_text_to_video_response(self, mocker):
@@ -211,7 +238,9 @@ class TestFireworksAIConversationalTask:
     def test_prepare_payload_as_dict(self):
         helper = FireworksAIConversationalTask()
         payload = helper._prepare_payload_as_dict(
-            [{"role": "user", "content": "Hello!"}], {}, "meta-llama/Llama-3.1-8B-Instruct"
+            [{"role": "user", "content": "Hello!"}],
+            {},
+            "meta-llama/Llama-3.1-8B-Instruct",
         )
         assert payload == {
             "messages": [{"role": "user", "content": "Hello!"}],
@@ -222,8 +251,12 @@ class TestFireworksAIConversationalTask:
 class TestHFInferenceProvider:
     def test_prepare_mapped_model(self, mocker):
         helper = HFInferenceTask("text-classification")
-        assert helper._prepare_mapped_model("username/repo_name") == "username/repo_name"
-        assert helper._prepare_mapped_model("https://any-url.com") == "https://any-url.com"
+        assert (
+            helper._prepare_mapped_model("username/repo_name") == "username/repo_name"
+        )
+        assert (
+            helper._prepare_mapped_model("https://any-url.com") == "https://any-url.com"
+        )
 
         mocker.patch(
             "huggingface_hub.inference._providers.hf_inference._fetch_recommended_models",
@@ -231,7 +264,9 @@ class TestHFInferenceProvider:
         )
         assert helper._prepare_mapped_model(None) == "username/repo_name"
 
-        with pytest.raises(ValueError, match="Task unknown-task has no recommended model"):
+        with pytest.raises(
+            ValueError, match="Task unknown-task has no recommended model"
+        ):
             assert HFInferenceTask("unknown-task")._prepare_mapped_model(None)
 
     def test_prepare_url(self):
@@ -241,7 +276,10 @@ class TestHFInferenceProvider:
             == "https://router.huggingface.co/hf-inference/models/username/repo_name"
         )
 
-        assert helper._prepare_url("hf_test_token", "https://any-url.com") == "https://any-url.com"
+        assert (
+            helper._prepare_url("hf_test_token", "https://any-url.com")
+            == "https://any-url.com"
+        )
 
     def test_prepare_payload_as_dict(self):
         helper = HFInferenceTask("text-classification")
@@ -254,8 +292,12 @@ class TestHFInferenceProvider:
             "parameters": {"a": 1},
         }
 
-        with pytest.raises(ValueError, match="Unexpected binary input for task text-classification."):
-            helper._prepare_payload_as_dict(b"dummy binary data", {}, "username/repo_name")
+        with pytest.raises(
+            ValueError, match="Unexpected binary input for task text-classification."
+        ):
+            helper._prepare_payload_as_dict(
+                b"dummy binary data", {}, "username/repo_name"
+            )
 
     def test_prepare_payload_as_bytes(self):
         helper = HFInferenceBinaryInputTask("image-classification")
@@ -285,8 +327,12 @@ class TestHFInferenceProvider:
         helper._prepare_url(
             "hf_test_token", "username/repo_name"
         ) == "https://router.huggingface.co/hf-inference/models/username/repo_name/v1/chat/completions"
-        helper._prepare_url("hf_test_token", "https://any-url.com") == "https://any-url.com/v1/chat/completions"
-        helper._prepare_url("hf_test_token", "https://any-url.com/v1") == "https://any-url.com/v1/chat/completions"
+        helper._prepare_url(
+            "hf_test_token", "https://any-url.com"
+        ) == "https://any-url.com/v1/chat/completions"
+        helper._prepare_url(
+            "hf_test_token", "https://any-url.com/v1"
+        ) == "https://any-url.com/v1/chat/completions"
 
     def test_prepare_request(self):
         helper = HFInferenceTask("text-classification")
@@ -298,7 +344,10 @@ class TestHFInferenceProvider:
             api_key="hf_test_token",
         )
 
-        assert request.url == "https://router.huggingface.co/hf-inference/models/username/repo_name"
+        assert (
+            request.url
+            == "https://router.huggingface.co/hf-inference/models/username/repo_name"
+        )
         assert request.task == "text-classification"
         assert request.model == "username/repo_name"
         assert request.headers["authorization"] == "Bearer hf_test_token"
@@ -315,7 +364,8 @@ class TestHFInferenceProvider:
         )
 
         assert (
-            request.url == "https://router.huggingface.co/hf-inference/models/username/repo_name/v1/chat/completions"
+            request.url
+            == "https://router.huggingface.co/hf-inference/models/username/repo_name/v1/chat/completions"
         )
         assert request.task == "text-generation"
         assert request.model == "username/repo_name"
@@ -358,7 +408,9 @@ class TestHFInferenceProvider:
             ),
         ],
     )
-    def test_prepare_payload_as_dict_conversational(self, mapped_model, parameters, expected_model):
+    def test_prepare_payload_as_dict_conversational(
+        self, mapped_model, parameters, expected_model
+    ):
         helper = HFInferenceConversational()
         messages = [{"role": "user", "content": "Hello!"}]
 
@@ -388,7 +440,9 @@ class TestHyperbolicProvider:
         """Test payload preparation for conversational task."""
         helper = HyperbolicTextGenerationTask("conversational")
         payload = helper._prepare_payload_as_dict(
-            [{"role": "user", "content": "Hello!"}], {"temperature": 0.7}, "meta-llama/Llama-3.2-3B-Instruct"
+            [{"role": "user", "content": "Hello!"}],
+            {"temperature": 0.7},
+            "meta-llama/Llama-3.2-3B-Instruct",
         )
         assert payload == {
             "messages": [{"role": "user", "content": "Hello!"}],
@@ -424,7 +478,9 @@ class TestHyperbolicProvider:
         """Test response handling for text-to-image task."""
         helper = HyperbolicTextToImageTask()
         dummy_image = b"image_bytes"
-        response = helper.get_response({"images": [{"image": base64.b64encode(dummy_image).decode()}]})
+        response = helper.get_response(
+            {"images": [{"image": base64.b64encode(dummy_image).decode()}]}
+        )
         assert response == dummy_image
 
 
@@ -437,7 +493,12 @@ class TestNebiusProvider:
         helper = NebiusTextToImageTask()
         payload = helper._prepare_payload_as_dict(
             "a beautiful cat",
-            {"num_inference_steps": 10, "width": 512, "height": 512, "guidance_scale": 7.5},
+            {
+                "num_inference_steps": 10,
+                "width": 512,
+                "height": 512,
+                "guidance_scale": 7.5,
+            },
             "black-forest-labs/flux-schnell",
         )
         assert payload == {
@@ -451,7 +512,9 @@ class TestNebiusProvider:
 
     def test_text_to_image_get_response(self):
         helper = NebiusTextToImageTask()
-        response = helper.get_response({"data": [{"b64_json": base64.b64encode(b"image_bytes").decode()}]})
+        response = helper.get_response(
+            {"data": [{"b64_json": base64.b64encode(b"image_bytes").decode()}]}
+        )
         assert response == b"image_bytes"
 
 
@@ -490,13 +553,19 @@ class TestReplicateProvider:
 
         # No model version
         payload = helper._prepare_payload_as_dict(
-            "a beautiful cat", {"num_inference_steps": 20}, "black-forest-labs/FLUX.1-schnell"
+            "a beautiful cat",
+            {"num_inference_steps": 20},
+            "black-forest-labs/FLUX.1-schnell",
         )
-        assert payload == {"input": {"prompt": "a beautiful cat", "num_inference_steps": 20}}
+        assert payload == {
+            "input": {"prompt": "a beautiful cat", "num_inference_steps": 20}
+        }
 
         # Model with specific version
         payload = helper._prepare_payload_as_dict(
-            "a beautiful cat", {"num_inference_steps": 20}, "black-forest-labs/FLUX.1-schnell:1944af04d098ef"
+            "a beautiful cat",
+            {"num_inference_steps": 20},
+            "black-forest-labs/FLUX.1-schnell:1944af04d098ef",
         )
         assert payload == {
             "input": {"prompt": "a beautiful cat", "num_inference_steps": 20},
@@ -506,7 +575,9 @@ class TestReplicateProvider:
     def test_text_to_speech_payload(self):
         helper = ReplicateTextToSpeechTask()
         payload = helper._prepare_payload_as_dict(
-            "Hello world", {}, "hexgrad/Kokoro-82M:f559560eb822dc509045f3921a1921234918b91739db4bf3daab2169b71c7a13"
+            "Hello world",
+            {},
+            "hexgrad/Kokoro-82M:f559560eb822dc509045f3921a1921234918b91739db4bf3daab2169b71c7a13",
         )
         assert payload == {
             "input": {"text": "Hello world"},
@@ -515,12 +586,18 @@ class TestReplicateProvider:
 
     def test_get_response_timeout(self):
         helper = ReplicateTask("text-to-image")
-        with pytest.raises(TimeoutError, match="Inference request timed out after 60 seconds."):
-            helper.get_response({"model": "black-forest-labs/FLUX.1-schnell"})  # no 'output' key
+        with pytest.raises(
+            TimeoutError, match="Inference request timed out after 60 seconds."
+        ):
+            helper.get_response(
+                {"model": "black-forest-labs/FLUX.1-schnell"}
+            )  # no 'output' key
 
     def test_get_response_single_output(self, mocker):
         helper = ReplicateTask("text-to-image")
-        mock = mocker.patch("huggingface_hub.inference._providers.replicate.get_session")
+        mock = mocker.patch(
+            "huggingface_hub.inference._providers.replicate.get_session"
+        )
         response = helper.get_response({"output": "https://example.com/image.jpg"})
         mock.return_value.get.assert_called_once_with("https://example.com/image.jpg")
         assert response == mock.return_value.get.return_value.content
@@ -544,7 +621,12 @@ class TestTogetherProvider:
         helper = TogetherTextToImageTask()
         payload = helper._prepare_payload_as_dict(
             "a beautiful cat",
-            {"num_inference_steps": 10, "guidance_scale": 1, "width": 512, "height": 512},
+            {
+                "num_inference_steps": 10,
+                "guidance_scale": 1,
+                "width": 512,
+                "height": 512,
+            },
             "black-forest-labs/FLUX.1-schnell",
         )
         assert payload == {
@@ -559,18 +641,24 @@ class TestTogetherProvider:
 
     def test_text_to_image_get_response(self):
         helper = TogetherTextToImageTask()
-        response = helper.get_response({"data": [{"b64_json": base64.b64encode(b"image_bytes").decode()}]})
+        response = helper.get_response(
+            {"data": [{"b64_json": base64.b64encode(b"image_bytes").decode()}]}
+        )
         assert response == b"image_bytes"
 
 
 class TestBaseConversationalTask:
     def test_prepare_route(self):
-        helper = BaseConversationalTask(provider="test-provider", base_url="https://api.test.com")
+        helper = BaseConversationalTask(
+            provider="test-provider", base_url="https://api.test.com"
+        )
         assert helper._prepare_route("dummy-model") == "/v1/chat/completions"
         assert helper.task == "conversational"
 
     def test_prepare_payload(self):
-        helper = BaseConversationalTask(provider="test-provider", base_url="https://api.test.com")
+        helper = BaseConversationalTask(
+            provider="test-provider", base_url="https://api.test.com"
+        )
         messages = [{"role": "user", "content": "Hello!"}]
         parameters = {"temperature": 0.7, "max_tokens": 100}
 
@@ -590,12 +678,16 @@ class TestBaseConversationalTask:
 
 class TestBaseTextGenerationTask:
     def test_prepare_route(self):
-        helper = BaseTextGenerationTask(provider="test-provider", base_url="https://api.test.com")
+        helper = BaseTextGenerationTask(
+            provider="test-provider", base_url="https://api.test.com"
+        )
         assert helper._prepare_route("dummy-model") == "/v1/completions"
         assert helper.task == "text-generation"
 
     def test_prepare_payload(self):
-        helper = BaseTextGenerationTask(provider="test-provider", base_url="https://api.test.com")
+        helper = BaseTextGenerationTask(
+            provider="test-provider", base_url="https://api.test.com"
+        )
         prompt = "Once upon a time"
         parameters = {"temperature": 0.7, "max_tokens": 100}
 
--- python3-huggingface-hub-0.29.3.orig/tests/test_inference_text_generation.py
+++ python3-huggingface-hub-0.29.3/tests/test_inference_text_generation.py
@@ -18,7 +18,9 @@ from huggingface_hub.inference._common i
     OverloadedError,
     raise_text_generation_error,
 )
-from huggingface_hub.inference._common import ValidationError as TextGenerationValidationError
+from huggingface_hub.inference._common import (
+    ValidationError as TextGenerationValidationError,
+)
 
 from .testing_utils import with_production_testing
 
@@ -67,14 +69,18 @@ class TestTextGenerationClientVCR(unitte
         assert response == ""
 
     def test_generate_with_details(self):
-        response = self.client.text_generation("test", details=True, max_new_tokens=1, decoder_input_details=True)
+        response = self.client.text_generation(
+            "test", details=True, max_new_tokens=1, decoder_input_details=True
+        )
 
         assert response.generated_text == ""
         assert response.details.finish_reason == "length"
         assert response.details.generated_tokens == 1
         assert response.details.seed is None
         assert len(response.details.prefill) == 1
-        assert response.details.prefill[0] == TextGenerationOutputPrefillToken(id=0, text="<pad>", logprob=None)
+        assert response.details.prefill[0] == TextGenerationOutputPrefillToken(
+            id=0, text="<pad>", logprob=None
+        )
         assert len(response.details.tokens) == 1
         assert response.details.tokens[0].id == 3
         assert response.details.tokens[0].text == " "
@@ -82,7 +88,12 @@ class TestTextGenerationClientVCR(unitte
 
     def test_generate_best_of(self):
         response = self.client.text_generation(
-            "test", max_new_tokens=1, best_of=2, do_sample=True, decoder_input_details=True, details=True
+            "test",
+            max_new_tokens=1,
+            best_of=2,
+            do_sample=True,
+            decoder_input_details=True,
+            details=True,
         )
 
         assert response.details.seed is not None
@@ -96,7 +107,10 @@ class TestTextGenerationClientVCR(unitte
 
     def test_generate_stream_no_details(self):
         responses = [
-            response for response in self.client.text_generation("test", max_new_tokens=1, stream=True, details=True)
+            response
+            for response in self.client.text_generation(
+                "test", max_new_tokens=1, stream=True, details=True
+            )
         ]
 
         assert len(responses) == 1
@@ -109,7 +123,10 @@ class TestTextGenerationClientVCR(unitte
 
     def test_generate_stream_with_details(self):
         responses = [
-            response for response in self.client.text_generation("test", max_new_tokens=1, stream=True, details=True)
+            response
+            for response in self.client.text_generation(
+                "test", max_new_tokens=1, stream=True, details=True
+            )
         ]
 
         assert len(responses) == 1
@@ -127,22 +144,33 @@ class TestTextGenerationClientVCR(unitte
 
         # Watermark is ignored (+ warning)
         with self.assertWarns(UserWarning):
-            self.client.text_generation("4 5 6", model="gpt2", max_new_tokens=10, watermark=True)
+            self.client.text_generation(
+                "4 5 6", model="gpt2", max_new_tokens=10, watermark=True
+            )
 
         # Return as detail even if details=True (+ warning)
         with self.assertWarns(UserWarning):
-            text = self.client.text_generation("0 1 2", model="gpt2", max_new_tokens=10, details=True)
+            text = self.client.text_generation(
+                "0 1 2", model="gpt2", max_new_tokens=10, details=True
+            )
             self.assertIsInstance(text, str)
 
         # Return as stream raises error
         with self.assertRaises(ValueError):
-            self.client.text_generation("0 1 2", model="gpt2", max_new_tokens=10, stream=True)
+            self.client.text_generation(
+                "0 1 2", model="gpt2", max_new_tokens=10, stream=True
+            )
 
     def test_generate_non_tgi_endpoint_regression_test(self):
         # Regression test for https://github.com/huggingface/huggingface_hub/issues/2135
-        with self.assertWarnsRegex(UserWarning, "Ignoring following parameters: return_full_text"):
+        with self.assertWarnsRegex(
+            UserWarning, "Ignoring following parameters: return_full_text"
+        ):
             text = self.client.text_generation(
-                prompt="How are you today?", max_new_tokens=20, model="google/flan-t5-large", return_full_text=True
+                prompt="How are you today?",
+                max_new_tokens=20,
+                model="google/flan-t5-large",
+                return_full_text=True,
             )
         assert text == "I am at work"
 
--- python3-huggingface-hub-0.29.3.orig/tests/test_inference_types.py
+++ python3-huggingface-hub-0.29.3/tests/test_inference_types.py
@@ -5,8 +5,13 @@ from typing import List, Optional, Union
 import pytest
 
 import huggingface_hub.inference._generated.types as types
-from huggingface_hub.inference._generated.types import AutomaticSpeechRecognitionParameters
-from huggingface_hub.inference._generated.types.base import BaseInferenceType, dataclass_with_extra
+from huggingface_hub.inference._generated.types import (
+    AutomaticSpeechRecognitionParameters,
+)
+from huggingface_hub.inference._generated.types.base import (
+    BaseInferenceType,
+    dataclass_with_extra,
+)
 
 
 @dataclass_with_extra
@@ -109,7 +114,9 @@ def test_all_fields_are_optional():
 def test_normalize_keys():
     # all fields are normalized in the dataclasses (by convention)
     # if server response uses different keys, they will be normalized
-    instance = DummyNestedType.parse_obj({"ItEm": DUMMY_AS_DICT, "Maybe-Items": [DUMMY_AS_DICT]})
+    instance = DummyNestedType.parse_obj(
+        {"ItEm": DUMMY_AS_DICT, "Maybe-Items": [DUMMY_AS_DICT]}
+    )
     assert isinstance(instance.item, DummyType)
     assert isinstance(instance.maybe_items, list)
     assert len(instance.maybe_items) == 1
@@ -121,7 +128,9 @@ def test_optional_are_set_to_none():
         parameters = inspect.signature(_type).parameters
         for parameter in parameters.values():
             if _is_optional(parameter.annotation):
-                assert parameter.default is None, f"Parameter {parameter} of {_type} should be set to None"
+                assert (
+                    parameter.default is None
+                ), f"Parameter {parameter} of {_type} should be set to None"
 
 
 def test_none_inferred():
@@ -142,22 +151,23 @@ def test_other_fields_are_set():
     )
     assert instance.extra == "value"
     assert instance.items[0].another_extra == "value"
-    assert str(instance.items[0]) == "DummyType(foo=42, bar='baz', another_extra='value')"  # extra field always last
     assert (
-        repr(instance)  # works both with __str__ and __repr__
-        == (
-            "DummyNestedType("
-            "item=DummyType(foo=42, bar='baz'), "
-            "items=[DummyType(foo=42, bar='baz', another_extra='value')], "
-            "maybe_items=None, extra='value'"
-            ")"
-        )
+        str(instance.items[0]) == "DummyType(foo=42, bar='baz', another_extra='value')"
+    )  # extra field always last
+    assert repr(instance) == (  # works both with __str__ and __repr__
+        "DummyNestedType("
+        "item=DummyType(foo=42, bar='baz'), "
+        "items=[DummyType(foo=42, bar='baz', another_extra='value')], "
+        "maybe_items=None, extra='value'"
+        ")"
     )
 
 
 def test_other_fields_not_proper_dataclass_fields():
     instance_1 = DummyType.parse_obj({"foo": 42, "bar": "baz", "extra": "value1"})
-    instance_2 = DummyType.parse_obj({"foo": 42, "bar": "baz", "extra": "value2", "another_extra": "value2.1"})
+    instance_2 = DummyType.parse_obj(
+        {"foo": 42, "bar": "baz", "extra": "value2", "another_extra": "value2.1"}
+    )
     assert instance_1.extra == "value1"
     assert instance_2.extra == "value2"
     assert instance_2.another_extra == "value2.1"
--- python3-huggingface-hub-0.29.3.orig/tests/test_init_lazy_loading.py
+++ python3-huggingface-hub-0.29.3/tests/test_init_lazy_loading.py
@@ -30,8 +30,14 @@ class TestHuggingfaceHubInit(unittest.Te
                 # Assert docstring is find. This means autocomplete can also provide
                 # the help section.
                 signature_list = goto_list[0].get_signatures()
-                self.assertEqual(len(signature_list), 2)  # create_commit has 2 signatures (normal and `run_as_future`)
-                self.assertTrue(signature_list[0].docstring().startswith("create_commit(repo_id: str,"))
+                self.assertEqual(
+                    len(signature_list), 2
+                )  # create_commit has 2 signatures (normal and `run_as_future`)
+                self.assertTrue(
+                    signature_list[0]
+                    .docstring()
+                    .startswith("create_commit(repo_id: str,")
+                )
                 break
         else:
             self.fail(
--- python3-huggingface-hub-0.29.3.orig/tests/test_keras_integration.py
+++ python3-huggingface-hub-0.29.3/tests/test_keras_integration.py
@@ -12,7 +12,12 @@ from huggingface_hub.keras_mixin import
     push_to_hub_keras,
     save_pretrained_keras,
 )
-from huggingface_hub.utils import is_graphviz_available, is_pydot_available, is_tf_available, logging
+from huggingface_hub.utils import (
+    is_graphviz_available,
+    is_pydot_available,
+    is_tf_available,
+    logging,
+)
 
 from .testing_constants import ENDPOINT_STAGING, TOKEN, USER
 from .testing_utils import repo_name
@@ -99,7 +104,11 @@ class HubMixinTestKeras(CommonKerasTest)
         # Check a new model's weights are not the same as the reloaded model's weights
         another_model = DummyModel()
         another_model(tf.ones([2, 2]))
-        self.assertFalse(tf.reduce_all(tf.equal(new_model.weights[0], another_model.weights[0])).numpy().item())
+        self.assertFalse(
+            tf.reduce_all(tf.equal(new_model.weights[0], another_model.weights[0]))
+            .numpy()
+            .item()
+        )
 
     def test_abs_path_from_pretrained(self):
         model = DummyModel()
@@ -114,14 +123,19 @@ class HubMixinTestKeras(CommonKerasTest)
         model = DummyModel()
         model(model.dummy_inputs)
 
-        model.push_to_hub(repo_id=repo_id, token=TOKEN, config={"num": 7, "act": "gelu_fast"})
+        model.push_to_hub(
+            repo_id=repo_id, token=TOKEN, config={"num": 7, "act": "gelu_fast"}
+        )
 
         # Test model id exists
         assert self._api.model_info(repo_id).id == repo_id
 
         # Test config has been pushed to hub
         config_path = hf_hub_download(
-            repo_id=repo_id, filename="config.json", use_auth_token=TOKEN, cache_dir=self.cache_dir
+            repo_id=repo_id,
+            filename="config.json",
+            use_auth_token=TOKEN,
+            cache_dir=self.cache_dir,
         )
         with open(config_path) as f:
             assert json.load(f) == {"num": 7, "act": "gelu_fast"}
@@ -195,7 +209,9 @@ class HubKerasSequentialTest(CommonKeras
 
             # Check that there is no "Training Metrics" section in the model card.
             # This was done in an older version.
-            self.assertNotIn("Training Metrics", (self.cache_dir / "README.md").read_text())
+            self.assertNotIn(
+                "Training Metrics", (self.cache_dir / "README.md").read_text()
+            )
 
     def test_save_pretrained_optimizer_state(self):
         model = self.model_init()
@@ -214,7 +230,11 @@ class HubKerasSequentialTest(CommonKeras
         # Check a new model's weights are not the same as the reloaded model's weights
         another_model = DummyModel()
         another_model(tf.ones([2, 2]))
-        self.assertFalse(tf.reduce_all(tf.equal(new_model.weights[0], another_model.weights[0])).numpy().item())
+        self.assertFalse(
+            tf.reduce_all(tf.equal(new_model.weights[0], another_model.weights[0]))
+            .numpy()
+            .item()
+        )
 
     def test_save_pretrained_task_name_deprecation(self):
         model = self.model_init()
@@ -224,13 +244,19 @@ class HubKerasSequentialTest(CommonKeras
             FutureWarning,
             match="`task_name` input argument is deprecated. Pass `tags` instead.",
         ):
-            save_pretrained_keras(model, self.cache_dir, tags=["test"], task_name="test", save_traces=True)
+            save_pretrained_keras(
+                model, self.cache_dir, tags=["test"], task_name="test", save_traces=True
+            )
 
     def test_abs_path_from_pretrained(self):
         model = self.model_init()
         model.build((None, 2))
         save_pretrained_keras(
-            model, self.cache_dir, config={"num": 10, "act": "gelu_fast"}, plot_model=True, tags=None
+            model,
+            self.cache_dir,
+            config={"num": 10, "act": "gelu_fast"},
+            plot_model=True,
+            tags=None,
         )
         new_model = from_pretrained_keras(self.cache_dir)
         self.assertTrue(tf.reduce_all(tf.equal(new_model.weights[0], model.weights[0])))
@@ -241,7 +267,9 @@ class HubKerasSequentialTest(CommonKeras
         model = self.model_init()
         model = self.model_fit(model)
 
-        push_to_hub_keras(model, repo_id=repo_id, token=TOKEN, api_endpoint=ENDPOINT_STAGING)
+        push_to_hub_keras(
+            model, repo_id=repo_id, token=TOKEN, api_endpoint=ENDPOINT_STAGING
+        )
         assert self._api.model_info(repo_id).id == repo_id
         repo_files = self._api.list_repo_files(repo_id)
         assert "README.md" in repo_files
@@ -253,7 +281,13 @@ class HubKerasSequentialTest(CommonKeras
         model = self.model_init()
         model = self.model_fit(model)
 
-        push_to_hub_keras(model, repo_id=repo_id, token=TOKEN, api_endpoint=ENDPOINT_STAGING, plot_model=False)
+        push_to_hub_keras(
+            model,
+            repo_id=repo_id,
+            token=TOKEN,
+            api_endpoint=ENDPOINT_STAGING,
+            plot_model=False,
+        )
         repo_files = self._api.list_repo_files(repo_id)
         self.assertNotIn("model.png", repo_files)
         self._api.delete_repo(repo_id=repo_id)
@@ -268,12 +302,24 @@ class HubKerasSequentialTest(CommonKeras
 
         model = self.model_init()
         model.build((None, 2))
-        push_to_hub_keras(model, repo_id=repo_id, log_dir=log_dir, api_endpoint=ENDPOINT_STAGING, token=TOKEN)
+        push_to_hub_keras(
+            model,
+            repo_id=repo_id,
+            log_dir=log_dir,
+            api_endpoint=ENDPOINT_STAGING,
+            token=TOKEN,
+        )
 
         log_dir2 = self.cache_dir / "tb_log_dir2"
         log_dir2.mkdir(parents=True, exist_ok=True)
         (log_dir2 / "override.txt").write_text("Keras FTW")
-        push_to_hub_keras(model, repo_id=repo_id, log_dir=log_dir2, api_endpoint=ENDPOINT_STAGING, token=TOKEN)
+        push_to_hub_keras(
+            model,
+            repo_id=repo_id,
+            log_dir=log_dir2,
+            api_endpoint=ENDPOINT_STAGING,
+            token=TOKEN,
+        )
 
         files = self._api.list_repo_files(repo_id)
         self.assertIn("logs/override.txt", files)
--- python3-huggingface-hub-0.29.3.orig/tests/test_lfs.py
+++ python3-huggingface-hub-0.29.3/tests/test_lfs.py
@@ -119,7 +119,9 @@ class TestSliceFileObj(unittest.TestCase
             with open(filepath, "rb") as fileobj:
                 prev_pos = fileobj.tell()
                 # Test read
-                with SliceFileObj(fileobj, seek_from=24, read_limit=18) as fileobj_slice:
+                with SliceFileObj(
+                    fileobj, seek_from=24, read_limit=18
+                ) as fileobj_slice:
                     self.assertEqual(fileobj_slice.tell(), 0)
                     self.assertEqual(fileobj_slice.read(), self.content[24:42])
                     self.assertEqual(fileobj_slice.tell(), 18)
@@ -128,7 +130,9 @@ class TestSliceFileObj(unittest.TestCase
 
                 self.assertEqual(fileobj.tell(), prev_pos)
 
-                with SliceFileObj(fileobj, seek_from=0, read_limit=990) as fileobj_slice:
+                with SliceFileObj(
+                    fileobj, seek_from=0, read_limit=990
+                ) as fileobj_slice:
                     self.assertEqual(fileobj_slice.tell(), 0)
                     self.assertEqual(fileobj_slice.read(200), self.content[0:200])
                     self.assertEqual(fileobj_slice.read(500), self.content[200:700])
@@ -137,7 +141,9 @@ class TestSliceFileObj(unittest.TestCase
                     self.assertEqual(fileobj_slice.read(200), b"")
 
                 # Test seek with whence = os.SEEK_SET
-                with SliceFileObj(fileobj, seek_from=100, read_limit=100) as fileobj_slice:
+                with SliceFileObj(
+                    fileobj, seek_from=100, read_limit=100
+                ) as fileobj_slice:
                     self.assertEqual(fileobj_slice.tell(), 0)
                     fileobj_slice.seek(2, os.SEEK_SET)
                     self.assertEqual(fileobj_slice.tell(), 2)
@@ -150,7 +156,9 @@ class TestSliceFileObj(unittest.TestCase
                     self.assertEqual(fileobj_slice.fileobj.tell(), 200)
 
                 # Test seek with whence = os.SEEK_CUR
-                with SliceFileObj(fileobj, seek_from=100, read_limit=100) as fileobj_slice:
+                with SliceFileObj(
+                    fileobj, seek_from=100, read_limit=100
+                ) as fileobj_slice:
                     self.assertEqual(fileobj_slice.tell(), 0)
                     fileobj_slice.seek(-5, os.SEEK_CUR)
                     self.assertEqual(fileobj_slice.tell(), 0)
@@ -166,7 +174,9 @@ class TestSliceFileObj(unittest.TestCase
                     self.assertEqual(fileobj_slice.fileobj.tell(), 100)
 
                 # Test seek with whence = os.SEEK_END
-                with SliceFileObj(fileobj, seek_from=100, read_limit=100) as fileobj_slice:
+                with SliceFileObj(
+                    fileobj, seek_from=100, read_limit=100
+                ) as fileobj_slice:
                     self.assertEqual(fileobj_slice.tell(), 0)
                     fileobj_slice.seek(-5, os.SEEK_END)
                     self.assertEqual(fileobj_slice.tell(), 95)
--- python3-huggingface-hub-0.29.3.orig/tests/test_local_folder.py
+++ python3-huggingface-hub-0.29.3/tests/test_local_folder.py
@@ -53,7 +53,9 @@ def test_creates_huggingface_dir_with_gi
 def test_gitignore_lock_timeout_is_ignored(tmp_path: Path):
     local_dir = tmp_path / "path" / "to" / "local"
 
-    threads = [threading.Thread(target=_huggingface_dir, args=(local_dir,)) for _ in range(10)]
+    threads = [
+        threading.Thread(target=_huggingface_dir, args=(local_dir,)) for _ in range(10)
+    ]
     for thread in threads:
         thread.start()
     for thread in threads:
@@ -70,9 +72,25 @@ def test_local_download_paths(tmp_path:
     assert isinstance(paths, LocalDownloadFilePaths)
     assert paths.file_path == tmp_path / "path" / "in" / "repo.txt"
     assert (
-        paths.metadata_path == tmp_path / ".cache" / "huggingface" / "download" / "path" / "in" / "repo.txt.metadata"
+        paths.metadata_path
+        == tmp_path
+        / ".cache"
+        / "huggingface"
+        / "download"
+        / "path"
+        / "in"
+        / "repo.txt.metadata"
+    )
+    assert (
+        paths.lock_path
+        == tmp_path
+        / ".cache"
+        / "huggingface"
+        / "download"
+        / "path"
+        / "in"
+        / "repo.txt.lock"
     )
-    assert paths.lock_path == tmp_path / ".cache" / "huggingface" / "download" / "path" / "in" / "repo.txt.lock"
 
     # Paths are usable (parent directories have been created)
     assert paths.file_path.parent.is_dir()
@@ -81,14 +99,19 @@ def test_local_download_paths(tmp_path:
 
     # Incomplete paths are etag-based
     incomplete_path = paths.incomplete_path("etag123")
-    assert incomplete_path.parent == tmp_path / ".cache" / "huggingface" / "download" / "path" / "in"
+    assert (
+        incomplete_path.parent
+        == tmp_path / ".cache" / "huggingface" / "download" / "path" / "in"
+    )
     assert incomplete_path.name.endswith(".etag123.incomplete")
     assert paths.incomplete_path("etag123").parent.is_dir()
 
     # Incomplete paths are unique per file per etag
     other_paths = get_local_download_paths(tmp_path, "path/in/repo_other.txt")
     other_incomplete_path = other_paths.incomplete_path("etag123")
-    assert incomplete_path != other_incomplete_path  # different .incomplete files to prevent concurrency issues
+    assert (
+        incomplete_path != other_incomplete_path
+    )  # different .incomplete files to prevent concurrency issues
 
 
 def test_local_download_paths_are_recreated_each_time(tmp_path: Path):
@@ -124,8 +147,12 @@ def test_local_download_paths_long_paths
 def test_write_download_metadata(tmp_path: Path):
     """Test download metadata content is valid."""
     # Write metadata
-    write_download_metadata(tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789")
-    metadata_path = tmp_path / ".cache" / "huggingface" / "download" / "file.txt.metadata"
+    write_download_metadata(
+        tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789"
+    )
+    metadata_path = (
+        tmp_path / ".cache" / "huggingface" / "download" / "file.txt.metadata"
+    )
     assert metadata_path.exists()
 
     # Metadata is valid
@@ -134,12 +161,16 @@ def test_write_download_metadata(tmp_pat
         assert f.readline() == "123456789\n"
         timestamp = float(f.readline().strip())
     assert timestamp <= time.time()  # in the past
-    assert timestamp >= time.time() - 1  # but less than 1 seconds ago (we're not that slow)
+    assert (
+        timestamp >= time.time() - 1
+    )  # but less than 1 seconds ago (we're not that slow)
 
     time.sleep(0.2)  # for deterministic tests
 
     # Overwriting works as expected
-    write_download_metadata(tmp_path, filename="file.txt", commit_hash="commit_hash2", etag="987654321")
+    write_download_metadata(
+        tmp_path, filename="file.txt", commit_hash="commit_hash2", etag="987654321"
+    )
     with metadata_path.open() as f:
         assert f.readline() == "commit_hash2\n"
         assert f.readline() == "987654321\n"
@@ -151,7 +182,9 @@ def test_read_download_metadata_valid_me
     """Test reading download metadata when metadata is valid."""
     # Create file + write correct metadata
     (tmp_path / "file.txt").write_text("content")
-    write_download_metadata(tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789")
+    write_download_metadata(
+        tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789"
+    )
 
     # Read metadata
     metadata = read_download_metadata(tmp_path, filename="file.txt")
@@ -168,10 +201,14 @@ def test_read_download_metadata_no_metad
     assert read_download_metadata(tmp_path, filename="file.txt") is None
 
 
-def test_read_download_metadata_corrupted_metadata(tmp_path: Path, caplog: pytest.LogCaptureFixture):
+def test_read_download_metadata_corrupted_metadata(
+    tmp_path: Path, caplog: pytest.LogCaptureFixture
+):
     """Test reading download metadata when metadata is corrupted."""
     # Write corrupted metadata
-    metadata_path = tmp_path / ".cache" / "huggingface" / "download" / "file.txt.metadata"
+    metadata_path = (
+        tmp_path / ".cache" / "huggingface" / "download" / "file.txt.metadata"
+    )
     metadata_path.parent.mkdir(parents=True, exist_ok=True)
     metadata_path.write_text("invalid content")
 
@@ -185,7 +222,9 @@ def test_read_download_metadata_corrupte
 def test_read_download_metadata_correct_metadata_missing_file(tmp_path: Path):
     """Test reading download metadata when metadata is correct but file is missing."""
     # Write correct metadata
-    write_download_metadata(tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789")
+    write_download_metadata(
+        tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789"
+    )
 
     # File missing => return None
     assert read_download_metadata(tmp_path, filename="file.txt") is None
@@ -194,7 +233,9 @@ def test_read_download_metadata_correct_
 def test_read_download_metadata_correct_metadata_but_outdated(tmp_path: Path):
     """Test reading download metadata when metadata is correct but outdated."""
     # Write correct metadata
-    write_download_metadata(tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789")
+    write_download_metadata(
+        tmp_path, filename="file.txt", commit_hash="commit_hash", etag="123456789"
+    )
     time.sleep(2)  # We allow for a 1s difference in practice, so let's wait a bit
 
     # File is outdated => return None
@@ -209,8 +250,26 @@ def test_local_upload_paths(tmp_path: Pa
     # Correct paths (also sanitized on windows)
     assert isinstance(paths, LocalUploadFilePaths)
     assert paths.file_path == tmp_path / "path" / "in" / "repo.txt"
-    assert paths.metadata_path == tmp_path / ".cache" / "huggingface" / "upload" / "path" / "in" / "repo.txt.metadata"
-    assert paths.lock_path == tmp_path / ".cache" / "huggingface" / "upload" / "path" / "in" / "repo.txt.lock"
+    assert (
+        paths.metadata_path
+        == tmp_path
+        / ".cache"
+        / "huggingface"
+        / "upload"
+        / "path"
+        / "in"
+        / "repo.txt.metadata"
+    )
+    assert (
+        paths.lock_path
+        == tmp_path
+        / ".cache"
+        / "huggingface"
+        / "upload"
+        / "path"
+        / "in"
+        / "repo.txt.lock"
+    )
 
     # Paths are usable (parent directories have been created)
     assert paths.file_path.parent.is_dir()
--- python3-huggingface-hub-0.29.3.orig/tests/test_login_utils.py
+++ python3-huggingface-hub-0.29.3/tests/test_login_utils.py
@@ -12,7 +12,9 @@ class TestSetGlobalStore(unittest.TestCa
     def setUp(self) -> None:
         """Get current global config value."""
         try:
-            self.previous_config = run_subprocess("git config --global credential.helper").stdout
+            self.previous_config = run_subprocess(
+                "git config --global credential.helper"
+            ).stdout
         except subprocess.CalledProcessError:
             self.previous_config = None  # Means global credential.helper value not set
 
@@ -23,7 +25,9 @@ class TestSetGlobalStore(unittest.TestCa
         if self.previous_config is None:
             run_subprocess("git config --global --unset credential.helper")
         else:
-            run_subprocess(f"git config --global credential.helper {self.previous_config}")
+            run_subprocess(
+                f"git config --global credential.helper {self.previous_config}"
+            )
 
     def test_set_store_as_git_credential_helper_globally(self) -> None:
         """Test `_set_store_as_git_credential_helper_globally` works as expected.
--- python3-huggingface-hub-0.29.3.orig/tests/test_repocard.py
+++ python3-huggingface-hub-0.29.3/tests/test_repocard.py
@@ -297,7 +297,9 @@ class RepocardMetadataUpdateTest(unittes
             repo_id=self.repo_id,
             path_in_repo=constants.REPOCARD_NAME,
         )
-        self.existing_metadata = yaml.safe_load(DUMMY_MODELCARD_EVAL_RESULT.strip().strip("-"))
+        self.existing_metadata = yaml.safe_load(
+            DUMMY_MODELCARD_EVAL_RESULT.strip().strip("-")
+        )
 
     def tearDown(self) -> None:
         self.api.delete_repo(repo_id=self.repo_id)
@@ -317,7 +319,9 @@ class RepocardMetadataUpdateTest(unittes
 
     def test_update_existing_result_with_overwrite(self):
         new_metadata = copy.deepcopy(self.existing_metadata)
-        new_metadata["model-index"][0]["results"][0]["metrics"][0]["value"] = 0.2862102282047272
+        new_metadata["model-index"][0]["results"][0]["metrics"][0][
+            "value"
+        ] = 0.2862102282047272
         metadata_update(self.repo_id, new_metadata, token=self.token, overwrite=True)
 
         updated_metadata = metadata_load(self._get_remote_card())
@@ -329,7 +333,9 @@ class RepocardMetadataUpdateTest(unittes
         Regression test for https://github.com/huggingface/huggingface_hub/issues/1210
         """
         new_metadata = copy.deepcopy(self.existing_metadata)
-        new_metadata["model-index"][0]["results"][0]["metrics"][0]["verifyToken"] = "1234"
+        new_metadata["model-index"][0]["results"][0]["metrics"][0][
+            "verifyToken"
+        ] = "1234"
         metadata_update(self.repo_id, new_metadata, token=self.token, overwrite=True)
 
         updated_metadata = metadata_load(self._get_remote_card())
@@ -348,7 +354,9 @@ class RepocardMetadataUpdateTest(unittes
 
     def test_update_existing_result_without_overwrite(self):
         new_metadata = copy.deepcopy(self.existing_metadata)
-        new_metadata["model-index"][0]["results"][0]["metrics"][0]["value"] = 0.2862102282047272
+        new_metadata["model-index"][0]["results"][0]["metrics"][0][
+            "value"
+        ] = 0.2862102282047272
 
         with pytest.raises(
             ValueError,
@@ -357,7 +365,9 @@ class RepocardMetadataUpdateTest(unittes
                 " accuracy'. Set `overwrite=True` to overwrite existing metrics."
             ),
         ):
-            metadata_update(self.repo_id, new_metadata, token=self.token, overwrite=False)
+            metadata_update(
+                self.repo_id, new_metadata, token=self.token, overwrite=False
+            )
 
     def test_update_existing_field_without_overwrite(self):
         new_datasets_data = {"datasets": ["Open-Orca/OpenOrca"]}
@@ -371,7 +381,9 @@ class RepocardMetadataUpdateTest(unittes
             ),
         ):
             new_datasets_data = {"datasets": ["HuggingFaceH4/no_robots"]}
-            metadata_update(self.repo_id, new_datasets_data, token=self.token, overwrite=False)
+            metadata_update(
+                self.repo_id, new_datasets_data, token=self.token, overwrite=False
+            )
 
     def test_update_new_result_existing_dataset(self):
         new_result = metadata_eval_result(
@@ -418,7 +430,9 @@ class RepocardMetadataUpdateTest(unittes
         metadata_update(self.repo_id, new_result, token=self.token, overwrite=False)
 
         expected_metadata = copy.deepcopy(self.existing_metadata)
-        expected_metadata["model-index"][0]["results"].append(new_result["model-index"][0]["results"][0])
+        expected_metadata["model-index"][0]["results"].append(
+            new_result["model-index"][0]["results"][0]
+        )
 
         updated_metadata = metadata_load(self._get_remote_card())
         self.assertDictEqual(updated_metadata, expected_metadata)
@@ -444,15 +458,23 @@ class RepocardMetadataUpdateTest(unittes
     def test_update_with_existing_name(self):
         new_metadata = copy.deepcopy(self.existing_metadata)
         new_metadata["model-index"][0].pop("name")
-        new_metadata["model-index"][0]["results"][0]["metrics"][0]["value"] = 0.2862102282047272
+        new_metadata["model-index"][0]["results"][0]["metrics"][0][
+            "value"
+        ] = 0.2862102282047272
         metadata_update(self.repo_id, new_metadata, token=self.token, overwrite=True)
 
         card_data = ModelCard.load(self.repo_id)
-        self.assertEqual(card_data.data.model_name, self.existing_metadata["model-index"][0]["name"])
+        self.assertEqual(
+            card_data.data.model_name, self.existing_metadata["model-index"][0]["name"]
+        )
 
     def test_update_without_existing_name(self):
         # delete existing metadata
-        self.api.upload_file(path_or_fileobj="# Test".encode(), repo_id=self.repo_id, path_in_repo="README.md")
+        self.api.upload_file(
+            path_or_fileobj="# Test".encode(),
+            repo_id=self.repo_id,
+            path_in_repo="README.md",
+        )
 
         new_metadata = copy.deepcopy(self.existing_metadata)
         new_metadata["model-index"][0].pop("name")
@@ -474,7 +496,9 @@ class RepocardMetadataUpdateTest(unittes
         )
         card = ModelCard.load(self.repo_id)
         metadata = card.data.to_dict()
-        metadata_update(self.repo_id, metadata=metadata, overwrite=True, token=self.token)
+        metadata_update(
+            self.repo_id, metadata=metadata, overwrite=True, token=self.token
+        )
 
         new_card = ModelCard.load(self.repo_id)
         self.assertEqual(len(new_card.data.eval_results), 2)
@@ -575,7 +599,9 @@ class RepoCardTest(TestCaseWithHfApi):
             card.save(updated_card_path)
 
             updated_card = RepoCard.load(updated_card_path)
-            self.assertEqual(updated_card.data.language, ["fr"], "Card data not updated properly")
+            self.assertEqual(
+                updated_card.data.language, ["fr"], "Card data not updated properly"
+            )
 
     @require_jinja
     def test_repo_card_from_default_template(self):
@@ -646,7 +672,9 @@ class RepoCardTest(TestCaseWithHfApi):
 
     def test_repo_card_data_must_be_dict(self):
         sample_path = SAMPLE_CARDS_DIR / "sample_invalid_card_data.md"
-        with pytest.raises(ValueError, match="repo card metadata block should be a dict"):
+        with pytest.raises(
+            ValueError, match="repo card metadata block should be a dict"
+        ):
             RepoCard(sample_path.read_text())
 
     def test_repo_card_without_metadata(self):
@@ -656,7 +684,8 @@ class RepoCardTest(TestCaseWithHfApi):
             card = RepoCard(sample_path.read_text())
         self.assertTrue(
             any(
-                "Repo card metadata block was not found. Setting CardData to empty." in log
+                "Repo card metadata block was not found. Setting CardData to empty."
+                in log
                 for log in warning_logs.output
             )
         )
@@ -777,7 +806,10 @@ class ModelCardTest(TestCaseWithHfApi):
         with self.assertLogs("huggingface_hub", level="WARNING") as warning_logs:
             card = ModelCard.load(sample_path, ignore_metadata_errors=True)
         self.assertTrue(
-            any("Invalid model-index. Not loading eval results into CardData." in log for log in warning_logs.output)
+            any(
+                "Invalid model-index. Not loading eval results into CardData." in log
+                for log in warning_logs.output
+            )
         )
         self.assertIsNone(card.data.eval_results)
 
@@ -860,12 +892,17 @@ class ModelCardTest(TestCaseWithHfApi):
         self.assertIsInstance(card, ModelCard)
         self.assertTrue(card.text.endswith("asdf"))
         self.assertTrue(card.data.to_dict().get("eval_results") is None)
-        self.assertEqual(str(card)[: len(DUMMY_MODELCARD_EVAL_RESULT)], DUMMY_MODELCARD_EVAL_RESULT)
+        self.assertEqual(
+            str(card)[: len(DUMMY_MODELCARD_EVAL_RESULT)], DUMMY_MODELCARD_EVAL_RESULT
+        )
 
     def test_preserve_order_load_save(self):
         model_card = ModelCard(DUMMY_MODELCARD)
         model_card.data.license = "test"
-        self.assertEqual(model_card.content, "---\nlicense: test\ndatasets:\n- foo\n- bar\n---\n\nHello\n")
+        self.assertEqual(
+            model_card.content,
+            "---\nlicense: test\ndatasets:\n- foo\n- bar\n---\n\nHello\n",
+        )
 
 
 class DatasetCardTest(TestCaseWithHfApi):
@@ -909,7 +946,9 @@ class DatasetCardTest(TestCaseWithHfApi)
 
         # Here we pass the card data as kwargs as well so template picks up pretty_name.
         card = DatasetCard.from_template(card_data, **card_data.to_dict())
-        self.assertTrue(card.text.strip().startswith("# Dataset Card for My Cool Dataset"))
+        self.assertTrue(
+            card.text.strip().startswith("# Dataset Card for My Cool Dataset")
+        )
 
         self.assertIsInstance(card, DatasetCard)
 
@@ -931,11 +970,18 @@ class DatasetCardTest(TestCaseWithHfApi)
                 "in the dataset card template are working."
             ),
         )
-        self.assertTrue(card.text.strip().startswith("# Dataset Card for My Cool Dataset"))
+        self.assertTrue(
+            card.text.strip().startswith("# Dataset Card for My Cool Dataset")
+        )
         self.assertIsInstance(card, DatasetCard)
 
-        matches = re.findall(r"Repository:\*\* https://github\.com/huggingface/huggingface_hub", str(card))
-        self.assertEqual(matches[0], "Repository:** https://github.com/huggingface/huggingface_hub")
+        matches = re.findall(
+            r"Repository:\*\* https://github\.com/huggingface/huggingface_hub",
+            str(card),
+        )
+        self.assertEqual(
+            matches[0], "Repository:** https://github.com/huggingface/huggingface_hub"
+        )
 
     @require_jinja
     def test_dataset_card_from_custom_template(self):
--- python3-huggingface-hub-0.29.3.orig/tests/test_repocard_data.py
+++ python3-huggingface-hub-0.29.3/tests/test_repocard_data.py
@@ -14,7 +14,9 @@ from huggingface_hub.repocard_data impor
 )
 
 
-OPEN_LLM_LEADERBOARD_URL = "https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard"
+OPEN_LLM_LEADERBOARD_URL = (
+    "https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard"
+)
 DUMMY_METADATA_WITH_MODEL_INDEX = """
 language: en
 license: mit
@@ -166,7 +168,9 @@ class ModelCardDataTest(unittest.TestCas
         self.assertEqual(eval_results[2].source_url, OPEN_LLM_LEADERBOARD_URL)
 
     def test_card_data_requires_model_name_for_eval_results(self):
-        with pytest.raises(ValueError, match="`eval_results` requires `model_name` to be set."):
+        with pytest.raises(
+            ValueError, match="`eval_results` requires `model_name` to be set."
+        ):
             ModelCardData(
                 eval_results=[
                     EvalResult(
@@ -195,7 +199,9 @@ class ModelCardDataTest(unittest.TestCas
         model_index = eval_results_to_model_index(data.model_name, data.eval_results)
 
         self.assertEqual(model_index[0]["name"], "my-cool-model")
-        self.assertEqual(model_index[0]["results"][0]["task"]["type"], "image-classification")
+        self.assertEqual(
+            model_index[0]["results"][0]["task"]["type"], "image-classification"
+        )
 
     def test_arbitrary_incoming_card_data(self):
         data = ModelCardData(
@@ -244,14 +250,22 @@ class ModelCardDataTest(unittest.TestCas
         assert data.tags == ["tag2", "tag1", "tag3"]
 
     def test_remove_top_level_none_values(self):
-        as_obj = ModelCardData(tags=["tag1", None], foo={"bar": 3, "baz": None}, pipeline_tag=None)
+        as_obj = ModelCardData(
+            tags=["tag1", None], foo={"bar": 3, "baz": None}, pipeline_tag=None
+        )
         as_dict = as_obj.to_dict()
 
         assert as_obj.tags == ["tag1", None]
-        assert as_dict["tags"] == ["tag1", None]  # none value inside list should be kept
+        assert as_dict["tags"] == [
+            "tag1",
+            None,
+        ]  # none value inside list should be kept
 
         assert as_obj.foo == {"bar": 3, "baz": None}
-        assert as_dict["foo"] == {"bar": 3, "baz": None}  # none value inside dict should be kept
+        assert as_dict["foo"] == {
+            "bar": 3,
+            "baz": None,
+        }  # none value inside dict should be kept
 
         assert as_obj.pipeline_tag is None
         assert "pipeline_tag" not in as_dict  # top level none value should be removed
--- python3-huggingface-hub-0.29.3.orig/tests/test_repository.py
+++ python3-huggingface-hub-0.29.3/tests/test_repository.py
@@ -107,12 +107,17 @@ class TestRepositoryShared(RepositoryTes
     @expect_deprecation("Repository")
     def test_clone_from_repo_name_no_namespace_fails(self):
         with self.assertRaises(EnvironmentError):
-            Repository(self.repo_path, clone_from=self.repo_id.split("/")[1], token=TOKEN)
+            Repository(
+                self.repo_path, clone_from=self.repo_id.split("/")[1], token=TOKEN
+            )
 
     @expect_deprecation("Repository")
     def test_clone_from_not_hf_url(self):
         # Should not error out
-        Repository(self.repo_path, clone_from="https://hf.co/hf-internal-testing/huggingface-hub-dummy-repository")
+        Repository(
+            self.repo_path,
+            clone_from="https://hf.co/hf-internal-testing/huggingface-hub-dummy-repository",
+        )
 
     @expect_deprecation("Repository")
     def test_clone_from_missing_repo(self):
@@ -128,7 +133,11 @@ class TestRepositoryShared(RepositoryTes
     @expect_deprecation("Repository")
     @with_production_testing
     def test_clone_from_prod_canonical_repo_url(self):
-        Repository(self.repo_path, clone_from="https://huggingface.co/bert-base-cased", skip_lfs_files=True)
+        Repository(
+            self.repo_path,
+            clone_from="https://huggingface.co/bert-base-cased",
+            skip_lfs_files=True,
+        )
 
     @expect_deprecation("Repository")
     def test_init_from_existing_local_clone(self):
@@ -188,7 +197,9 @@ class TestRepositoryShared(RepositoryTes
     def test_push_errors_on_wrong_checkout(self):
         repo = Repository(self.repo_path, clone_from=self.repo_id)
 
-        head_commit_ref = run_subprocess("git show --oneline -s", folder=self.repo_path).stdout.split()[0]
+        head_commit_ref = run_subprocess(
+            "git show --oneline -s", folder=self.repo_path
+        ).stdout.split()[0]
 
         repo.git_checkout(head_commit_ref)
 
@@ -209,7 +220,9 @@ class TestRepositoryUniqueRepos(Reposito
         self.repo_url = self._api.create_repo(repo_id=repo_name())
         self.repo_id = self.repo_url.repo_id
         self._api.upload_file(
-            path_or_fileobj=self.binary_content.encode(), path_in_repo="random_file.txt", repo_id=self.repo_id
+            path_or_fileobj=self.binary_content.encode(),
+            path_in_repo="random_file.txt",
+            repo_id=self.repo_id,
         )
 
     def tearDown(self):
@@ -245,7 +258,9 @@ class TestRepositoryUniqueRepos(Reposito
 
         # Add to the remote repository without doing anything to the local repository.
         self._api.upload_file(
-            path_or_fileobj=self.binary_content.encode(), path_in_repo="random_file_3.txt", repo_id=self.repo_id
+            path_or_fileobj=self.binary_content.encode(),
+            path_in_repo="random_file_3.txt",
+            repo_id=self.repo_id,
         )
 
         # Cloning the repository in the same directory should not result in a git pull.
@@ -357,7 +372,9 @@ class TestRepositoryUniqueRepos(Reposito
 
     def test_clone_skip_lfs_files(self):
         # Upload LFS file
-        self._api.upload_file(path_or_fileobj=b"Bin file", path_in_repo="file.bin", repo_id=self.repo_id)
+        self._api.upload_file(
+            path_or_fileobj=b"Bin file", path_in_repo="file.bin", repo_id=self.repo_id
+        )
 
         repo = self.clone_repo(skip_lfs_files=True)
         file_bin = self.repo_path / "file.bin"
@@ -503,9 +520,13 @@ class TestRepositoryUniqueRepos(Reposito
                 f.write("Random string 2")
 
         root_directory = self.repo_path / ".git" / "lfs"
-        git_lfs_files_size = sum(f.stat().st_size for f in root_directory.glob("**/*") if f.is_file())
+        git_lfs_files_size = sum(
+            f.stat().st_size for f in root_directory.glob("**/*") if f.is_file()
+        )
         repo.lfs_prune()
-        post_prune_git_lfs_files_size = sum(f.stat().st_size for f in root_directory.glob("**/*") if f.is_file())
+        post_prune_git_lfs_files_size = sum(
+            f.stat().st_size for f in root_directory.glob("**/*") if f.is_file()
+        )
 
         # Size of the directory holding LFS files was reduced
         self.assertLess(post_prune_git_lfs_files_size, git_lfs_files_size)
@@ -517,7 +538,9 @@ class TestRepositoryUniqueRepos(Reposito
                 f.write("Random string 1")
 
         root_directory = self.repo_path / ".git" / "lfs"
-        git_lfs_files_size = sum(f.stat().st_size for f in root_directory.glob("**/*") if f.is_file())
+        git_lfs_files_size = sum(
+            f.stat().st_size for f in root_directory.glob("**/*") if f.is_file()
+        )
 
         with open(os.path.join(repo.local_dir, "file.bin"), "w+") as f:
             f.write("Random string 2")
@@ -526,7 +549,9 @@ class TestRepositoryUniqueRepos(Reposito
         repo.git_commit("New commit")
         repo.git_push(auto_lfs_prune=True)
 
-        post_prune_git_lfs_files_size = sum(f.stat().st_size for f in root_directory.glob("**/*") if f.is_file())
+        post_prune_git_lfs_files_size = sum(
+            f.stat().st_size for f in root_directory.glob("**/*") if f.is_file()
+        )
 
         # Size of the directory holding LFS files is the exact same
         self.assertEqual(post_prune_git_lfs_files_size, git_lfs_files_size)
@@ -758,7 +783,9 @@ class TestRepositoryOffline(RepositoryTe
         self.assertFalse(is_tracked_upstream(self.repo.local_dir))
 
     def test_no_branch_checked_out_raises(self):
-        head_commit_ref = run_subprocess("git show --oneline -s", folder=self.repo_path).stdout.split()[0]
+        head_commit_ref = run_subprocess(
+            "git show --oneline -s", folder=self.repo_path
+        ).stdout.split()[0]
 
         self.repo.git_checkout(head_commit_ref)
         self.assertRaises(OSError, is_tracked_upstream, self.repo.local_dir)
@@ -806,7 +833,12 @@ class TestRepositoryOffline(RepositoryTe
 
     @expect_deprecation("Repository")
     def test_repo_passed_user(self):
-        _ = Repository(self.repo_path, token=TOKEN, git_user="RANDOM_USER", git_email="EMAIL@EMAIL.EMAIL")
+        _ = Repository(
+            self.repo_path,
+            token=TOKEN,
+            git_user="RANDOM_USER",
+            git_email="EMAIL@EMAIL.EMAIL",
+        )
         username = run_subprocess("git config user.name", folder=self.repo_path).stdout
         email = run_subprocess("git config user.email", folder=self.repo_path).stdout
 
@@ -860,20 +892,33 @@ class TestRepositoryDataset(RepositoryTe
     @expect_deprecation("Repository")
     def test_clone_dataset_with_endpoint_explicit_repo_type(self):
         Repository(
-            self.repo_path, clone_from=self.repo_url, repo_type="dataset", git_user="ci", git_email="ci@dummy.com"
+            self.repo_path,
+            clone_from=self.repo_url,
+            repo_type="dataset",
+            git_user="ci",
+            git_email="ci@dummy.com",
         )
         self.assertTrue((self.repo_path / "file.txt").exists())
 
     @expect_deprecation("Repository")
     def test_clone_dataset_with_endpoint_implicit_repo_type(self):
         self.assertIn("dataset", self.repo_url)  # Implicit
-        Repository(self.repo_path, clone_from=self.repo_url, git_user="ci", git_email="ci@dummy.com")
+        Repository(
+            self.repo_path,
+            clone_from=self.repo_url,
+            git_user="ci",
+            git_email="ci@dummy.com",
+        )
         self.assertTrue((self.repo_path / "file.txt").exists())
 
     @expect_deprecation("Repository")
     def test_clone_dataset_with_repo_id_and_repo_type(self):
         Repository(
-            self.repo_path, clone_from=self.repo_id, repo_type="dataset", git_user="ci", git_email="ci@dummy.com"
+            self.repo_path,
+            clone_from=self.repo_id,
+            repo_type="dataset",
+            git_user="ci",
+            git_email="ci@dummy.com",
         )
         self.assertTrue((self.repo_path / "file.txt").exists())
 
--- python3-huggingface-hub-0.29.3.orig/tests/test_serialization.py
+++ python3-huggingface-hub-0.29.3/tests/test_serialization.py
@@ -83,11 +83,21 @@ def dummy_model():
 
             def __init__(self):
                 super().__init__()
-                self.register_parameter("layer_1", torch.nn.Parameter(torch.tensor([4.0])))
-                self.register_parameter("layer_2", torch.nn.Parameter(torch.tensor([10.0])))
-                self.register_parameter("layer_3", torch.nn.Parameter(torch.tensor([30.0])))
-                self.register_parameter("layer_4", torch.nn.Parameter(torch.tensor([2.0])))
-                self.register_parameter("layer_5", torch.nn.Parameter(torch.tensor([2.0])))
+                self.register_parameter(
+                    "layer_1", torch.nn.Parameter(torch.tensor([4.0]))
+                )
+                self.register_parameter(
+                    "layer_2", torch.nn.Parameter(torch.tensor([10.0]))
+                )
+                self.register_parameter(
+                    "layer_3", torch.nn.Parameter(torch.tensor([30.0]))
+                )
+                self.register_parameter(
+                    "layer_4", torch.nn.Parameter(torch.tensor([2.0]))
+                )
+                self.register_parameter(
+                    "layer_5", torch.nn.Parameter(torch.tensor([2.0]))
+                )
 
         return DummyModel()
     except ImportError:
@@ -214,7 +224,9 @@ def test_tensor_same_storage():
             "layer_4": [2],
             "layer_5": [1],
         },
-        get_storage_id=lambda x: (x[0]),  # dummy for test: storage id based on first element
+        get_storage_id=lambda x: (
+            x[0]
+        ),  # dummy for test: storage id based on first element
         get_storage_size=_dummy_get_storage_size,
         max_shard_size=1,
         filename_pattern="model{suffix}.safetensors",
@@ -246,12 +258,20 @@ def test_get_tf_storage_size():
 def test_get_torch_storage_size():
     import torch  # type: ignore[import]
 
-    assert get_torch_storage_size(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float64)) == 5 * 8
-    assert get_torch_storage_size(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float16)) == 5 * 2
+    assert (
+        get_torch_storage_size(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float64))
+        == 5 * 8
+    )
+    assert (
+        get_torch_storage_size(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float16))
+        == 5 * 2
+    )
 
 
 @requires("torch")
-@pytest.mark.skipif(not is_wrapper_tensor_subclass_available(), reason="requires torch 2.1 or higher")
+@pytest.mark.skipif(
+    not is_wrapper_tensor_subclass_available(), reason="requires torch 2.1 or higher"
+)
 def test_get_torch_storage_size_wrapper_tensor_subclass():
     import torch  # type: ignore[import]
     from torch.testing._internal.two_tensor import TwoTensor  # type: ignore[import]
@@ -279,7 +299,9 @@ def test_parse_size_to_int():
 def test_save_torch_model(mocker: MockerFixture, tmp_path: Path) -> None:
     """Test `save_torch_model` is only a wrapper around `save_torch_state_dict`."""
     model_mock = Mock()
-    safe_state_dict_mock = mocker.patch("huggingface_hub.serialization._torch.save_torch_state_dict")
+    safe_state_dict_mock = mocker.patch(
+        "huggingface_hub.serialization._torch.save_torch_state_dict"
+    )
     save_torch_model(
         model_mock,
         save_directory=tmp_path,
@@ -304,14 +326,18 @@ def test_save_torch_model(mocker: Mocker
     )
 
 
-def test_save_torch_state_dict_not_sharded(tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]) -> None:
+def test_save_torch_state_dict_not_sharded(
+    tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]
+) -> None:
     """Save as safetensors without sharding."""
     save_torch_state_dict(torch_state_dict, tmp_path, max_shard_size="1GB")
     assert (tmp_path / "model.safetensors").is_file()
     assert not (tmp_path / "model.safetensors.index.json").is_file()
 
 
-def test_save_torch_state_dict_sharded(tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]) -> None:
+def test_save_torch_state_dict_sharded(
+    tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]
+) -> None:
     """Save as safetensors with sharding."""
     save_torch_state_dict(torch_state_dict, tmp_path, max_shard_size=30)
     assert not (tmp_path / "model.safetensors").is_file()
@@ -319,7 +345,9 @@ def test_save_torch_state_dict_sharded(t
     assert (tmp_path / "model-00001-of-00002.safetensors").is_file()
     assert (tmp_path / "model-00001-of-00002.safetensors").is_file()
 
-    assert json.loads((tmp_path / "model.safetensors.index.json").read_text("utf-8")) == {
+    assert json.loads(
+        (tmp_path / "model.safetensors.index.json").read_text("utf-8")
+    ) == {
         "metadata": {"total_size": 40},
         "weight_map": {
             "layer_1": "model-00001-of-00002.safetensors",
@@ -332,25 +360,36 @@ def test_save_torch_state_dict_sharded(t
 
 
 def test_save_torch_state_dict_unsafe_not_sharded(
-    tmp_path: Path, caplog: pytest.LogCaptureFixture, torch_state_dict: Dict[str, "torch.Tensor"]
+    tmp_path: Path,
+    caplog: pytest.LogCaptureFixture,
+    torch_state_dict: Dict[str, "torch.Tensor"],
 ) -> None:
     """Save as pickle without sharding."""
     with caplog.at_level("WARNING"):
-        save_torch_state_dict(torch_state_dict, tmp_path, max_shard_size="1GB", safe_serialization=False)
+        save_torch_state_dict(
+            torch_state_dict, tmp_path, max_shard_size="1GB", safe_serialization=False
+        )
     assert "we strongly recommend using safe serialization" in caplog.text
 
     assert (tmp_path / "pytorch_model.bin").is_file()
     assert not (tmp_path / "pytorch_model.bin.index.json").is_file()
 
 
-@pytest.mark.skipif(not is_wrapper_tensor_subclass_available(), reason="requires torch 2.1 or higher")
+@pytest.mark.skipif(
+    not is_wrapper_tensor_subclass_available(), reason="requires torch 2.1 or higher"
+)
 def test_save_torch_state_dict_tensor_subclass_unsafe_not_sharded(
-    tmp_path: Path, caplog: pytest.LogCaptureFixture, torch_state_dict_tensor_subclass: Dict[str, "torch.Tensor"]
+    tmp_path: Path,
+    caplog: pytest.LogCaptureFixture,
+    torch_state_dict_tensor_subclass: Dict[str, "torch.Tensor"],
 ) -> None:
     """Save as pickle without sharding."""
     with caplog.at_level("WARNING"):
         save_torch_state_dict(
-            torch_state_dict_tensor_subclass, tmp_path, max_shard_size="1GB", safe_serialization=False
+            torch_state_dict_tensor_subclass,
+            tmp_path,
+            max_shard_size="1GB",
+            safe_serialization=False,
         )
     assert "we strongly recommend using safe serialization" in caplog.text
 
@@ -358,7 +397,9 @@ def test_save_torch_state_dict_tensor_su
     assert not (tmp_path / "pytorch_model.bin.index.json").is_file()
 
 
-@pytest.mark.skipif(not is_wrapper_tensor_subclass_available(), reason="requires torch 2.1 or higher")
+@pytest.mark.skipif(
+    not is_wrapper_tensor_subclass_available(), reason="requires torch 2.1 or higher"
+)
 def test_save_torch_state_dict_shared_layers_tensor_subclass_unsafe_not_sharded(
     tmp_path: Path,
     caplog: pytest.LogCaptureFixture,
@@ -367,7 +408,10 @@ def test_save_torch_state_dict_shared_la
     """Save as pickle without sharding."""
     with caplog.at_level("WARNING"):
         save_torch_state_dict(
-            torch_state_dict_shared_layers_tensor_subclass, tmp_path, max_shard_size="1GB", safe_serialization=False
+            torch_state_dict_shared_layers_tensor_subclass,
+            tmp_path,
+            max_shard_size="1GB",
+            safe_serialization=False,
         )
     assert "we strongly recommend using safe serialization" in caplog.text
 
@@ -376,12 +420,16 @@ def test_save_torch_state_dict_shared_la
 
 
 def test_save_torch_state_dict_unsafe_sharded(
-    tmp_path: Path, caplog: pytest.LogCaptureFixture, torch_state_dict: Dict[str, "torch.Tensor"]
+    tmp_path: Path,
+    caplog: pytest.LogCaptureFixture,
+    torch_state_dict: Dict[str, "torch.Tensor"],
 ) -> None:
     """Save as pickle with sharding."""
     # Check logs
     with caplog.at_level("WARNING"):
-        save_torch_state_dict(torch_state_dict, tmp_path, max_shard_size=30, safe_serialization=False)
+        save_torch_state_dict(
+            torch_state_dict, tmp_path, max_shard_size=30, safe_serialization=False
+        )
     assert "we strongly recommend using safe serialization" in caplog.text
 
     assert not (tmp_path / "pytorch_model.bin").is_file()
@@ -389,7 +437,9 @@ def test_save_torch_state_dict_unsafe_sh
     assert (tmp_path / "pytorch_model-00001-of-00002.bin").is_file()
     assert (tmp_path / "pytorch_model-00001-of-00002.bin").is_file()
 
-    assert json.loads((tmp_path / "pytorch_model.bin.index.json").read_text("utf-8")) == {
+    assert json.loads(
+        (tmp_path / "pytorch_model.bin.index.json").read_text("utf-8")
+    ) == {
         "metadata": {"total_size": 40},
         "weight_map": {
             "layer_1": "pytorch_model-00001-of-00002.bin",
@@ -406,7 +456,9 @@ def test_save_torch_state_dict_shared_la
 ) -> None:
     from safetensors.torch import load_file
 
-    save_torch_state_dict(torch_state_dict_shared_layers, tmp_path, safe_serialization=True)
+    save_torch_state_dict(
+        torch_state_dict_shared_layers, tmp_path, safe_serialization=True
+    )
     safetensors_file = tmp_path / "model.safetensors"
     assert safetensors_file.is_file()
 
@@ -428,7 +480,12 @@ def test_save_torch_state_dict_shared_la
 ) -> None:
     from safetensors.torch import load_file
 
-    save_torch_state_dict(torch_state_dict_shared_layers, tmp_path, max_shard_size=2, safe_serialization=True)
+    save_torch_state_dict(
+        torch_state_dict_shared_layers,
+        tmp_path,
+        max_shard_size=2,
+        safe_serialization=True,
+    )
     index_file = tmp_path / "model.safetensors.index.json"
     assert index_file.is_file()
 
@@ -492,7 +549,8 @@ def test_save_torch_state_dict_discard_s
 
 
 def test_split_torch_state_dict_into_shards(
-    tmp_path: Path, torch_state_dict_shared_layers_tensor_subclass: Dict[str, "torch.Tensor"]
+    tmp_path: Path,
+    torch_state_dict_shared_layers_tensor_subclass: Dict[str, "torch.Tensor"],
 ):
     # the model size is 72, setting max_shard_size to 32 means we'll shard the file
     state_dict_split = split_torch_state_dict_into_shards(
@@ -503,15 +561,22 @@ def test_split_torch_state_dict_into_sha
     assert state_dict_split.is_sharded
 
 
-def test_save_torch_state_dict_custom_filename(tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]) -> None:
+def test_save_torch_state_dict_custom_filename(
+    tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]
+) -> None:
     """Custom filename pattern is respected."""
     # Not sharded
-    save_torch_state_dict(torch_state_dict, tmp_path, filename_pattern="model.variant{suffix}.safetensors")
+    save_torch_state_dict(
+        torch_state_dict, tmp_path, filename_pattern="model.variant{suffix}.safetensors"
+    )
     assert (tmp_path / "model.variant.safetensors").is_file()
 
     # Sharded
     save_torch_state_dict(
-        torch_state_dict, tmp_path, filename_pattern="model.variant{suffix}.safetensors", max_shard_size=30
+        torch_state_dict,
+        tmp_path,
+        filename_pattern="model.variant{suffix}.safetensors",
+        max_shard_size=30,
     )
     assert (tmp_path / "model.variant.safetensors.index.json").is_file()
     assert (tmp_path / "model.variant-00001-of-00002.safetensors").is_file()
@@ -576,7 +641,9 @@ def test_save_torch_state_dict_not_main_
 
 
 @requires("torch")
-def test_load_state_dict_from_file(tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]):
+def test_load_state_dict_from_file(
+    tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"]
+):
     """Test saving and loading a state dict with both safetensors and pickle formats."""
     import torch  # type: ignore[import]
 
@@ -629,7 +696,9 @@ def test_load_sharded_state_dict(
 
 @requires("torch")
 def test_load_from_directory_not_sharded(
-    tmp_path: Path, torch_state_dict: Dict[str, "torch.Tensor"], dummy_model: "torch.nn.Module"
+    tmp_path: Path,
+    torch_state_dict: Dict[str, "torch.Tensor"],
+    dummy_model: "torch.nn.Module",
 ):
     import torch
 
@@ -661,13 +730,17 @@ def test_load_state_dict_missing_file(sa
 
 def test_load_torch_model_directory_does_not_exist():
     """Test proper error handling when directory does not contain a valid checkpoint."""
-    with pytest.raises(ValueError, match="Checkpoint path does_not_exist does not exist"):
+    with pytest.raises(
+        ValueError, match="Checkpoint path does_not_exist does not exist"
+    ):
         load_torch_model(Mock(), "does_not_exist")
 
 
 def test_load_torch_model_directory_does_not_contain_checkpoint(tmp_path):
     """Test proper error handling when directory does not contain a valid checkpoint."""
-    with pytest.raises(ValueError, match=r"Directory .* does not contain a valid checkpoint."):
+    with pytest.raises(
+        ValueError, match=r"Directory .* does not contain a valid checkpoint."
+    ):
         load_torch_model(Mock(), tmp_path)
 
 
@@ -678,7 +751,9 @@ def test_load_torch_model_directory_does
         False,
     ],
 )
-def test_load_sharded_model_strict_mode(tmp_path, torch_state_dict, dummy_model, strict):
+def test_load_sharded_model_strict_mode(
+    tmp_path, torch_state_dict, dummy_model, strict
+):
     """Test loading model with strict mode behavior for both sharded and non-sharded checkpoints."""
 
     import torch
@@ -709,7 +784,9 @@ def test_load_sharded_model_strict_mode(
         assert "extra_key" in result.unexpected_keys
 
 
-def test_load_torch_model_with_filename_pattern(tmp_path, torch_state_dict, dummy_model):
+def test_load_torch_model_with_filename_pattern(
+    tmp_path, torch_state_dict, dummy_model
+):
     """Test loading a model with a custom filename pattern."""
 
     import torch
@@ -796,7 +873,9 @@ def test_load_torch_model_index_selectio
         (tmp_path / filename).touch()
 
     # Mock _load_sharded_checkpoint to capture the safe parameter
-    mock_load = mocker.patch("huggingface_hub.serialization._torch._load_sharded_checkpoint")
+    mock_load = mocker.patch(
+        "huggingface_hub.serialization._torch._load_sharded_checkpoint"
+    )
     load_torch_model(model, tmp_path, safe=safe, filename_pattern=filename_pattern)
     mock_load.assert_called_once()
     assert mock_load.call_args.kwargs["filename_pattern"] == expected_filename_pattern
--- python3-huggingface-hub-0.29.3.orig/tests/test_snapshot_download.py
+++ python3-huggingface-hub-0.29.3/tests/test_snapshot_download.py
@@ -24,8 +24,13 @@ class SnapshotDownloadTests(unittest.Tes
         cls.first_commit_hash = cls.api.create_commit(
             repo_id=cls.repo_id,
             operations=[
-                CommitOperationAdd(path_in_repo="dummy_file.txt", path_or_fileobj=b"v1"),
-                CommitOperationAdd(path_in_repo="subpath/file.txt", path_or_fileobj=b"content in subpath"),
+                CommitOperationAdd(
+                    path_in_repo="dummy_file.txt", path_or_fileobj=b"v1"
+                ),
+                CommitOperationAdd(
+                    path_in_repo="subpath/file.txt",
+                    path_or_fileobj=b"content in subpath",
+                ),
             ],
             commit_message="Add file to main branch",
         ).oid
@@ -34,8 +39,12 @@ class SnapshotDownloadTests(unittest.Tes
         cls.second_commit_hash = cls.api.create_commit(
             repo_id=cls.repo_id,
             operations=[
-                CommitOperationAdd(path_in_repo="dummy_file.txt", path_or_fileobj=b"v2"),
-                CommitOperationAdd(path_in_repo="dummy_file_2.txt", path_or_fileobj=b"v3"),
+                CommitOperationAdd(
+                    path_in_repo="dummy_file.txt", path_or_fileobj=b"v2"
+                ),
+                CommitOperationAdd(
+                    path_in_repo="dummy_file_2.txt", path_or_fileobj=b"v3"
+                ),
             ],
             commit_message="Add file to main branch",
         ).oid
@@ -45,7 +54,9 @@ class SnapshotDownloadTests(unittest.Tes
         cls.third_commit_hash = cls.api.create_commit(
             repo_id=cls.repo_id,
             operations=[
-                CommitOperationAdd(path_in_repo="dummy_file_2.txt", path_or_fileobj=b"v4"),
+                CommitOperationAdd(
+                    path_in_repo="dummy_file_2.txt", path_or_fileobj=b"v4"
+                ),
             ],
             commit_message="Add file to other branch",
             revision="other",
@@ -58,7 +69,9 @@ class SnapshotDownloadTests(unittest.Tes
     def test_download_model(self):
         # Test `main` branch
         with SoftTemporaryDirectory() as tmpdir:
-            storage_folder = snapshot_download(self.repo_id, revision="main", cache_dir=tmpdir)
+            storage_folder = snapshot_download(
+                self.repo_id, revision="main", cache_dir=tmpdir
+            )
 
             # folder contains the two files contributed and the .gitattributes
             folder_contents = os.listdir(storage_folder)
@@ -106,12 +119,16 @@ class SnapshotDownloadTests(unittest.Tes
         # Test we can download with token from cache
         with patch("huggingface_hub.utils._headers.get_token", return_value=TOKEN):
             with SoftTemporaryDirectory() as tmpdir:
-                storage_folder = snapshot_download(self.repo_id, revision="main", cache_dir=tmpdir)
+                storage_folder = snapshot_download(
+                    self.repo_id, revision="main", cache_dir=tmpdir
+                )
                 self.assertTrue(self.second_commit_hash in storage_folder)
 
         # Test we can download with explicit token
         with SoftTemporaryDirectory() as tmpdir:
-            storage_folder = snapshot_download(self.repo_id, revision="main", cache_dir=tmpdir, token=TOKEN)
+            storage_folder = snapshot_download(
+                self.repo_id, revision="main", cache_dir=tmpdir, token=TOKEN
+            )
             self.assertTrue(self.second_commit_hash in storage_folder)
 
         self.api.update_repo_settings(repo_id=self.repo_id, private=False)
@@ -122,33 +139,50 @@ class SnapshotDownloadTests(unittest.Tes
             # first download folder to cache it
             snapshot_download(self.repo_id, cache_dir=tmpdir)
             # now load from cache
-            storage_folder = snapshot_download(self.repo_id, cache_dir=tmpdir, local_files_only=True)
-            self.assertTrue(self.second_commit_hash in storage_folder)  # has expected revision
+            storage_folder = snapshot_download(
+                self.repo_id, cache_dir=tmpdir, local_files_only=True
+            )
+            self.assertTrue(
+                self.second_commit_hash in storage_folder
+            )  # has expected revision
 
         # Test with specific revision branch
         with SoftTemporaryDirectory() as tmpdir:
             # first download folder to cache it
             snapshot_download(self.repo_id, revision="other", cache_dir=tmpdir)
             # now load from cache
-            storage_folder = snapshot_download(self.repo_id, revision="other", cache_dir=tmpdir, local_files_only=True)
-            self.assertTrue(self.third_commit_hash in storage_folder)  # has expected revision
+            storage_folder = snapshot_download(
+                self.repo_id, revision="other", cache_dir=tmpdir, local_files_only=True
+            )
+            self.assertTrue(
+                self.third_commit_hash in storage_folder
+            )  # has expected revision
 
         # Test with specific revision hash
         with SoftTemporaryDirectory() as tmpdir:
             # first download folder to cache it
-            snapshot_download(self.repo_id, revision=self.first_commit_hash, cache_dir=tmpdir)
+            snapshot_download(
+                self.repo_id, revision=self.first_commit_hash, cache_dir=tmpdir
+            )
             # now load from cache
             storage_folder = snapshot_download(
-                self.repo_id, revision=self.first_commit_hash, cache_dir=tmpdir, local_files_only=True
+                self.repo_id,
+                revision=self.first_commit_hash,
+                cache_dir=tmpdir,
+                local_files_only=True,
             )
-            self.assertTrue(self.first_commit_hash in storage_folder)  # has expected revision
+            self.assertTrue(
+                self.first_commit_hash in storage_folder
+            )  # has expected revision
 
         # Test with local_dir
         with SoftTemporaryDirectory() as tmpdir:
             # first download folder to local_dir
             snapshot_download(self.repo_id, local_dir=tmpdir)
             # now load from local_dir
-            storage_folder = snapshot_download(self.repo_id, local_dir=tmpdir, local_files_only=True)
+            storage_folder = snapshot_download(
+                self.repo_id, local_dir=tmpdir, local_files_only=True
+            )
             self.assertEqual(str(tmpdir), storage_folder)
 
     def test_download_model_to_local_dir_with_offline_mode(self):
@@ -195,7 +229,9 @@ class SnapshotDownloadTests(unittest.Tes
 
             # now make sure that loading "main" branch gives correct branch
             # folder name contains the 2nd commit sha and not the 3rd
-            storage_folder = snapshot_download(self.repo_id, cache_dir=tmpdir, local_files_only=True)
+            storage_folder = snapshot_download(
+                self.repo_id, cache_dir=tmpdir, local_files_only=True
+            )
             self.assertTrue(self.second_commit_hash in storage_folder)
 
     def check_download_model_with_pattern(self, pattern, allow=True):
@@ -223,7 +259,9 @@ class SnapshotDownloadTests(unittest.Tes
         self.check_download_model_with_pattern("*.txt")
 
     def test_download_model_with_allow_pattern_list(self):
-        self.check_download_model_with_pattern(["dummy_file.txt", "dummy_file_2.txt", "subpath/*"])
+        self.check_download_model_with_pattern(
+            ["dummy_file.txt", "dummy_file_2.txt", "subpath/*"]
+        )
 
     def test_download_model_with_ignore_pattern(self):
         self.check_download_model_with_pattern(".gitattributes", allow=False)
@@ -242,7 +280,9 @@ class SnapshotDownloadTests(unittest.Tes
         """
         with SoftTemporaryDirectory() as cache_dir:
             with SoftTemporaryDirectory() as local_dir:
-                returned_path = snapshot_download(self.repo_id, cache_dir=cache_dir, local_dir=local_dir)
+                returned_path = snapshot_download(
+                    self.repo_id, cache_dir=cache_dir, local_dir=local_dir
+                )
 
                 # Files have been downloaded in correct structure
                 assert (Path(local_dir) / "dummy_file.txt").is_file()
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_assets.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_assets.py
@@ -26,12 +26,16 @@ class CacheAssetsTest(unittest.TestCase)
         self.assertTrue(path.is_dir())  # And dir is created
 
     def test_cached_assets_path_without_subfolder(self) -> None:
-        path = cached_assets_path(library_name="datasets", namespace="SQuAD", assets_dir=self.cache_dir)
+        path = cached_assets_path(
+            library_name="datasets", namespace="SQuAD", assets_dir=self.cache_dir
+        )
         self.assertEqual(path, self.cache_dir / "datasets" / "SQuAD" / "default")
         self.assertTrue(path.is_dir())
 
     def test_cached_assets_path_without_namespace(self) -> None:
-        path = cached_assets_path(library_name="datasets", subfolder="download", assets_dir=self.cache_dir)
+        path = cached_assets_path(
+            library_name="datasets", subfolder="download", assets_dir=self.cache_dir
+        )
         self.assertEqual(path, self.cache_dir / "datasets" / "default" / "download")
         self.assertTrue(path.is_dir())
 
@@ -49,7 +53,10 @@ class CacheAssetsTest(unittest.TestCase)
         )
         self.assertEqual(
             path,
-            self.cache_dir / "ReAlLy--dumb" / "user--repo_name" / "this--is--not--clever",
+            self.cache_dir
+            / "ReAlLy--dumb"
+            / "user--repo_name"
+            / "this--is--not--clever",
         )
         self.assertTrue(path.is_dir())
 
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_cache.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_cache.py
@@ -9,7 +9,12 @@ import pytest
 
 from huggingface_hub._snapshot_download import snapshot_download
 from huggingface_hub.commands.scan_cache import ScanCacheCommand
-from huggingface_hub.utils import DeleteCacheStrategy, HFCacheInfo, capture_output, scan_cache_dir
+from huggingface_hub.utils import (
+    DeleteCacheStrategy,
+    HFCacheInfo,
+    capture_output,
+    scan_cache_dir,
+)
 from huggingface_hub.utils._cache_manager import (
     CacheNotFound,
     _format_size,
@@ -46,7 +51,9 @@ class TestMissingCacheUtils(unittest.Tes
 
     def test_cache_dir_is_missing(self) -> None:
         """Directory to scan does not exist raises CacheNotFound."""
-        self.assertRaises(CacheNotFound, scan_cache_dir, self.cache_dir / "does_not_exist")
+        self.assertRaises(
+            CacheNotFound, scan_cache_dir, self.cache_dir / "does_not_exist"
+        )
 
     def test_cache_dir_is_a_file(self) -> None:
         """Directory to scan is a file raises ValueError."""
@@ -66,16 +73,36 @@ class TestValidCacheUtils(unittest.TestC
         snapshot_download(repo_id=MODEL_ID, repo_type="model", cache_dir=self.cache_dir)
 
         # Download latest commit which is same as `main`
-        snapshot_download(repo_id=MODEL_ID, revision=REPO_A_MAIN_HASH, repo_type="model", cache_dir=self.cache_dir)
+        snapshot_download(
+            repo_id=MODEL_ID,
+            revision=REPO_A_MAIN_HASH,
+            repo_type="model",
+            cache_dir=self.cache_dir,
+        )
 
         # Download the first commit
-        snapshot_download(repo_id=MODEL_ID, revision=REPO_A_OTHER_HASH, repo_type="model", cache_dir=self.cache_dir)
+        snapshot_download(
+            repo_id=MODEL_ID,
+            revision=REPO_A_OTHER_HASH,
+            repo_type="model",
+            cache_dir=self.cache_dir,
+        )
 
         # Download from a PR
-        snapshot_download(repo_id=MODEL_ID, revision="refs/pr/1", repo_type="model", cache_dir=self.cache_dir)
+        snapshot_download(
+            repo_id=MODEL_ID,
+            revision="refs/pr/1",
+            repo_type="model",
+            cache_dir=self.cache_dir,
+        )
 
         # Download a Dataset repo from "main"
-        snapshot_download(repo_id=DATASET_ID, revision="main", repo_type="dataset", cache_dir=self.cache_dir)
+        snapshot_download(
+            repo_id=DATASET_ID,
+            revision="main",
+            repo_type="dataset",
+            cache_dir=self.cache_dir,
+        )
 
     @unittest.skipIf(os.name == "nt", "Windows cache is tested separately")
     def test_scan_cache_on_valid_cache_unix(self) -> None:
@@ -136,7 +163,9 @@ class TestValidCacheUtils(unittest.TestC
         )
 
         # Check readme file from "main" revision
-        main_readme_file = [file for file in main_revision.files if file.file_name == "README.md"][0]
+        main_readme_file = [
+            file for file in main_revision.files if file.file_name == "README.md"
+        ][0]
         main_readme_file_path = main_revision_path / "README.md"
         main_readme_blob_path = repo_a_path / "blobs" / REPO_A_MAIN_README_BLOB_HASH
 
@@ -147,7 +176,9 @@ class TestValidCacheUtils(unittest.TestC
         # Check readme file from "refs/pr/1" revision
         pr_1_revision = repo_a.refs[REF_1_NAME]
         pr_1_revision_path = repo_a_path / "snapshots" / REPO_A_PR_1_HASH
-        pr_1_readme_file = [file for file in pr_1_revision.files if file.file_name == "README.md"][0]
+        pr_1_readme_file = [
+            file for file in pr_1_revision.files if file.file_name == "README.md"
+        ][0]
         pr_1_readme_file_path = pr_1_revision_path / "README.md"
 
         # file_path in "refs/pr/1" revision is different than "main" but same blob path
@@ -216,19 +247,25 @@ class TestValidCacheUtils(unittest.TestC
         )
 
         # Check readme file from "main" revision
-        main_readme_file = [file for file in main_revision.files if file.file_name == "README.md"][0]
+        main_readme_file = [
+            file for file in main_revision.files if file.file_name == "README.md"
+        ][0]
         main_readme_file_path = main_revision_path / "README.md"
         main_readme_blob_path = repo_a_path / "blobs" / REPO_A_MAIN_README_BLOB_HASH
 
         self.assertEqual(main_readme_file.file_name, "README.md")
         self.assertEqual(main_readme_file.file_path, main_readme_file_path)
-        self.assertEqual(main_readme_file.blob_path, main_readme_file_path)  # Windows-specific: no blob file
+        self.assertEqual(
+            main_readme_file.blob_path, main_readme_file_path
+        )  # Windows-specific: no blob file
         self.assertFalse(main_readme_blob_path.exists())  # Windows-specific
 
         # Check readme file from "refs/pr/1" revision
         pr_1_revision = repo_a.refs[REF_1_NAME]
         pr_1_revision_path = repo_a_path / "snapshots" / REPO_A_PR_1_HASH
-        pr_1_readme_file = [file for file in pr_1_revision.files if file.file_name == "README.md"][0]
+        pr_1_readme_file = [
+            file for file in pr_1_revision.files if file.file_name == "README.md"
+        ][0]
         pr_1_readme_file_path = pr_1_revision_path / "README.md"
 
         # file_path in "refs/pr/1" revision is different than "main"
@@ -343,7 +380,9 @@ class TestCorruptedCacheUtils(unittest.T
         self.assertEqual(len(report.repos), 1)  # Scan still worked !
 
         self.assertEqual(len(report.warnings), 1)
-        self.assertEqual(str(report.warnings[0]), f"Repo path is not a directory: {repo_path}")
+        self.assertEqual(
+            str(report.warnings[0]), f"Repo path is not a directory: {repo_path}"
+        )
 
         # Case 2: a folder with wrong naming
         os.remove(repo_path)
@@ -471,8 +510,12 @@ class TestCorruptedCacheUtils(unittest.T
         # Values from first report
         repo_1 = list(report_1.repos)[0]
         revision_1 = list(repo_1.revisions)[0]
-        readme_file_1 = [file for file in revision_1.files if file.file_name == "README.md"][0]
-        another_file_1 = [file for file in revision_1.files if file.file_name == ".gitattributes"][0]
+        readme_file_1 = [
+            file for file in revision_1.files if file.file_name == "README.md"
+        ][0]
+        another_file_1 = [
+            file for file in revision_1.files if file.file_name == ".gitattributes"
+        ][0]
 
         # Comparison of last_accessed/last_modified between file and repo
         self.assertLessEqual(readme_file_1.blob_last_accessed, repo_1.last_accessed)
@@ -495,8 +538,12 @@ class TestCorruptedCacheUtils(unittest.T
         # Values from second report
         repo_2 = list(report_2.repos)[0]
         revision_2 = list(repo_2.revisions)[0]
-        readme_file_2 = [file for file in revision_2.files if file.file_name == "README.md"][0]
-        another_file_2 = [file for file in revision_1.files if file.file_name == ".gitattributes"][0]
+        readme_file_2 = [
+            file for file in revision_2.files if file.file_name == "README.md"
+        ][0]
+        another_file_2 = [
+            file for file in revision_1.files if file.file_name == ".gitattributes"
+        ][0]
 
         # Report 1 is not updated when cache changes
         self.assertLess(repo_1.last_accessed, repo_2.last_accessed)
@@ -548,7 +595,9 @@ class TestDeleteRevisionsDryRun(unittest
         pr_1_only_file.size_on_disk = 100
 
         detached_and_pr_1_only_file = Mock()
-        detached_and_pr_1_only_file.blob_path = blobs_path / "detached_and_pr_1_only_hash"
+        detached_and_pr_1_only_file.blob_path = (
+            blobs_path / "detached_and_pr_1_only_hash"
+        )
         detached_and_pr_1_only_file.size_on_disk = 1000
 
         shared_file = Mock()
@@ -622,7 +671,9 @@ class TestDeleteRevisionsDryRun(unittest
         self.assertEqual(strategy, expected)
 
     def test_delete_pr_1_and_detached(self) -> None:
-        strategy = HFCacheInfo.delete_revisions(self.cache_info, "repo_A_rev_detached", "repo_A_rev_pr_1")
+        strategy = HFCacheInfo.delete_revisions(
+            self.cache_info, "repo_A_rev_detached", "repo_A_rev_pr_1"
+        )
         expected = DeleteCacheStrategy(
             expected_freed_size=1110,
             blobs={
@@ -655,7 +706,9 @@ class TestDeleteRevisionsDryRun(unittest
 
     def test_delete_unknown_revision(self) -> None:
         with self.assertLogs() as captured:
-            strategy = HFCacheInfo.delete_revisions(self.cache_info, "repo_A_rev_detached", "abcdef123456789")
+            strategy = HFCacheInfo.delete_revisions(
+                self.cache_info, "repo_A_rev_detached", "abcdef123456789"
+            )
 
         # Expected is same strategy as without "abcdef123456789"
         expected = HFCacheInfo.delete_revisions(self.cache_info, "repo_A_rev_detached")
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_datetime.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_datetime.py
@@ -26,7 +26,9 @@ class TestDatetimeUtils(unittest.TestCas
             datetime(2024, 11, 16, 0, 27, 2, 0, tzinfo=timezone.utc),
         )
 
-        with pytest.raises(ValueError, match=r".*Cannot parse '2022-08-19T07:19:38' as a datetime.*"):
+        with pytest.raises(
+            ValueError, match=r".*Cannot parse '2022-08-19T07:19:38' as a datetime.*"
+        ):
             parse_datetime("2022-08-19T07:19:38")
 
         with pytest.raises(
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_errors.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_errors.py
@@ -11,7 +11,13 @@ from huggingface_hub.errors import (
     RepositoryNotFoundError,
     RevisionNotFoundError,
 )
-from huggingface_hub.utils._http import REPO_API_REGEX, X_AMZN_TRACE_ID, X_REQUEST_ID, _format, hf_raise_for_status
+from huggingface_hub.utils._http import (
+    REPO_API_REGEX,
+    X_AMZN_TRACE_ID,
+    X_REQUEST_ID,
+    _format,
+    hf_raise_for_status,
+)
 
 
 class TestErrorUtils(unittest.TestCase):
@@ -19,7 +25,9 @@ class TestErrorUtils(unittest.TestCase):
         response = Response()
         response.headers = {"X-Error-Code": "RepoNotFound", X_REQUEST_ID: 123}
         response.status_code = 404
-        with self.assertRaisesRegex(RepositoryNotFoundError, "Repository Not Found") as context:
+        with self.assertRaisesRegex(
+            RepositoryNotFoundError, "Repository Not Found"
+        ) as context:
             hf_raise_for_status(response)
 
         assert context.exception.response.status_code == 404
@@ -27,7 +35,10 @@ class TestErrorUtils(unittest.TestCase):
 
     def test_hf_raise_for_status_disabled_repo(self) -> None:
         response = Response()
-        response.headers = {"X-Error-Message": "Access to this resource is disabled.", X_REQUEST_ID: 123}
+        response.headers = {
+            "X-Error-Message": "Access to this resource is disabled.",
+            X_REQUEST_ID: 123,
+        }
 
         response.status_code = 403
         with self.assertRaises(DisabledRepoError) as context:
@@ -42,7 +53,9 @@ class TestErrorUtils(unittest.TestCase):
         response.status_code = 401
         response.request = PreparedRequest()
         response.request.url = "https://huggingface.co/api/models/username/reponame"
-        with self.assertRaisesRegex(RepositoryNotFoundError, "Repository Not Found") as context:
+        with self.assertRaisesRegex(
+            RepositoryNotFoundError, "Repository Not Found"
+        ) as context:
             hf_raise_for_status(response)
 
         assert context.exception.response.status_code == 401
@@ -50,11 +63,16 @@ class TestErrorUtils(unittest.TestCase):
 
     def test_hf_raise_for_status_401_repo_url_invalid_token(self) -> None:
         response = Response()
-        response.headers = {X_REQUEST_ID: 123, "X-Error-Message": "Invalid credentials in Authorization header"}
+        response.headers = {
+            X_REQUEST_ID: 123,
+            "X-Error-Message": "Invalid credentials in Authorization header",
+        }
         response.status_code = 401
         response.request = PreparedRequest()
         response.request.url = "https://huggingface.co/api/models/username/reponame"
-        with self.assertRaisesRegex(HfHubHTTPError, "Invalid credentials in Authorization header") as context:
+        with self.assertRaisesRegex(
+            HfHubHTTPError, "Invalid credentials in Authorization header"
+        ) as context:
             hf_raise_for_status(response)
 
         assert context.exception.response.status_code == 401
@@ -62,7 +80,10 @@ class TestErrorUtils(unittest.TestCase):
 
     def test_hf_raise_for_status_403_wrong_token_scope(self) -> None:
         response = Response()
-        response.headers = {X_REQUEST_ID: 123, "X-Error-Message": "specific error message"}
+        response.headers = {
+            X_REQUEST_ID: 123,
+            "X-Error-Message": "specific error message",
+        }
         response.status_code = 403
         response.request = PreparedRequest()
         response.request.url = "https://huggingface.co/api/repos/create"
@@ -89,7 +110,9 @@ class TestErrorUtils(unittest.TestCase):
         response = Response()
         response.headers = {"X-Error-Code": "RevisionNotFound", X_REQUEST_ID: 123}
         response.status_code = 404
-        with self.assertRaisesRegex(RevisionNotFoundError, "Revision Not Found") as context:
+        with self.assertRaisesRegex(
+            RevisionNotFoundError, "Revision Not Found"
+        ) as context:
             hf_raise_for_status(response)
 
         assert context.exception.response.status_code == 404
@@ -117,7 +140,9 @@ class TestErrorUtils(unittest.TestCase):
         """Test endpoint name is added to BadRequestError message."""
         response = Response()
         response.status_code = 400
-        with self.assertRaisesRegex(BadRequestError, "Bad request for preupload endpoint:") as context:
+        with self.assertRaisesRegex(
+            BadRequestError, "Bad request for preupload endpoint:"
+        ) as context:
             hf_raise_for_status(response, endpoint_name="preupload")
         assert context.exception.response.status_code == 400
 
@@ -163,33 +188,63 @@ class TestHfHubHTTPError(unittest.TestCa
     def test_hf_hub_http_error_init_with_request_id_and_multiline_message(self) -> None:
         """Test request id is added to the end of the first line."""
         self.response.headers = {X_REQUEST_ID: "test-id"}
-        error = _format(HfHubHTTPError, "this is a message\nthis is more details", response=self.response)
-        assert str(error) == "this is a message (Request ID: test-id)\nthis is more details"
+        error = _format(
+            HfHubHTTPError,
+            "this is a message\nthis is more details",
+            response=self.response,
+        )
+        assert (
+            str(error)
+            == "this is a message (Request ID: test-id)\nthis is more details"
+        )
 
-        error = _format(HfHubHTTPError, "this is a message\n\nthis is more details", response=self.response)
-        assert str(error) == "this is a message (Request ID: test-id)\n\nthis is more details"
+        error = _format(
+            HfHubHTTPError,
+            "this is a message\n\nthis is more details",
+            response=self.response,
+        )
+        assert (
+            str(error)
+            == "this is a message (Request ID: test-id)\n\nthis is more details"
+        )
 
     def test_hf_hub_http_error_init_with_request_id_already_in_message(self) -> None:
         """Test request id is not duplicated in error message (case insensitive)"""
         self.response.headers = {X_REQUEST_ID: "test-id"}
-        error = _format(HfHubHTTPError, "this is a message on request TEST-ID", response=self.response)
+        error = _format(
+            HfHubHTTPError,
+            "this is a message on request TEST-ID",
+            response=self.response,
+        )
         assert str(error) == "this is a message on request TEST-ID"
         assert error.request_id == "test-id"
 
     def test_hf_hub_http_error_init_with_server_error(self) -> None:
         """Test server error is added to the error message."""
-        self.response._content = b'{"error": "This is a message returned by the server"}'
+        self.response._content = (
+            b'{"error": "This is a message returned by the server"}'
+        )
         error = _format(HfHubHTTPError, "this is a message", response=self.response)
-        assert str(error) == "this is a message\n\nThis is a message returned by the server"
+        assert (
+            str(error)
+            == "this is a message\n\nThis is a message returned by the server"
+        )
         assert error.server_message == "This is a message returned by the server"
 
     def test_hf_hub_http_error_init_with_server_error_and_multiline_message(
         self,
     ) -> None:
         """Test server error is added to the error message after the details."""
-        self.response._content = b'{"error": "This is a message returned by the server"}'
-        error = _format(HfHubHTTPError, "this is a message\n\nSome details.", response=self.response)
-        assert str(error) == "this is a message\n\nSome details.\nThis is a message returned by the server"
+        self.response._content = (
+            b'{"error": "This is a message returned by the server"}'
+        )
+        error = _format(
+            HfHubHTTPError, "this is a message\n\nSome details.", response=self.response
+        )
+        assert (
+            str(error)
+            == "this is a message\n\nSome details.\nThis is a message returned by the server"
+        )
 
     def test_hf_hub_http_error_init_with_multiple_server_errors(
         self,
@@ -202,8 +257,13 @@ class TestHfHubHTTPError(unittest.TestCa
             b'{"httpStatusCode": 400, "errors": [{"message": "this is error 1", "type":'
             b' "error"}, {"message": "this is error 2", "type": "error"}]}'
         )
-        error = _format(HfHubHTTPError, "this is a message\n\nSome details.", response=self.response)
-        assert str(error) == "this is a message\n\nSome details.\nthis is error 1\nthis is error 2"
+        error = _format(
+            HfHubHTTPError, "this is a message\n\nSome details.", response=self.response
+        )
+        assert (
+            str(error)
+            == "this is a message\n\nSome details.\nthis is error 1\nthis is error 2"
+        )
 
     def test_hf_hub_http_error_init_with_server_error_already_in_message(
         self,
@@ -235,7 +295,12 @@ class TestHfHubHTTPError(unittest.TestCa
         error.args = error.args + (1, 2, 3)  # faking some extra args
 
         error.append_to_message("\nthis is an additional message")
-        assert error.args == ("this is a message\nthis is an additional message", 1, 2, 3)
+        assert error.args == (
+            "this is a message\nthis is an additional message",
+            1,
+            2,
+            3,
+        )
 
         assert error.server_message is None  # added message is not from server
 
@@ -253,8 +318,14 @@ class TestHfHubHTTPError(unittest.TestCa
         self.response._content = b'{"error": "Error message from body."}'
         self.response.headers = {"X-Error-Message": "Error message from headers."}
         error = _format(HfHubHTTPError, "this is a message", response=self.response)
-        assert str(error) == "this is a message\n\nError message from headers.\nError message from body."
-        assert error.server_message == "Error message from headers.\nError message from body."
+        assert (
+            str(error)
+            == "this is a message\n\nError message from headers.\nError message from body."
+        )
+        assert (
+            error.server_message
+            == "Error message from headers.\nError message from body."
+        )
 
     def test_hf_hub_http_error_init_with_error_message_duplicated_in_header_and_body(
         self,
@@ -263,10 +334,17 @@ class TestHfHubHTTPError(unittest.TestCa
 
         Should not duplicate it in the raised `HfHubHTTPError`.
         """
-        self.response._content = b'{"error": "Error message duplicated in headers and body."}'
-        self.response.headers = {"X-Error-Message": "Error message duplicated in headers and body."}
+        self.response._content = (
+            b'{"error": "Error message duplicated in headers and body."}'
+        )
+        self.response.headers = {
+            "X-Error-Message": "Error message duplicated in headers and body."
+        }
         error = _format(HfHubHTTPError, "this is a message", response=self.response)
-        assert str(error) == "this is a message\n\nError message duplicated in headers and body."
+        assert (
+            str(error)
+            == "this is a message\n\nError message duplicated in headers and body."
+        )
         assert error.server_message == "Error message duplicated in headers and body."
 
     def test_hf_hub_http_error_without_request_id_with_amzn_trace_id(self) -> None:
@@ -278,7 +356,10 @@ class TestHfHubHTTPError(unittest.TestCa
 
     def test_hf_hub_http_error_with_request_id_and_amzn_trace_id(self) -> None:
         """Test request id is not duplicated in error message (case insensitive)"""
-        self.response.headers = {X_AMZN_TRACE_ID: "test-trace-id", X_REQUEST_ID: "test-id"}
+        self.response.headers = {
+            X_AMZN_TRACE_ID: "test-trace-id",
+            X_REQUEST_ID: "test-id",
+        }
         error = _format(HfHubHTTPError, "this is a message", response=self.response)
         assert str(error) == "this is a message (Request ID: test-id)"
         assert error.request_id == "test-id"
@@ -311,10 +392,16 @@ class TestHfHubHTTPError(unittest.TestCa
         ("https://hub-ci.huggingface.co/api/spaces/repo_id", True),
         # /resolve Endpoint => True
         ("https://huggingface.co/gpt2/resolve/main/README.md", True),
-        ("https://huggingface.co/datasets/google/fleurs/resolve/revision/README.md", True),
+        (
+            "https://huggingface.co/datasets/google/fleurs/resolve/revision/README.md",
+            True,
+        ),
         # Regression tests
         ("https://huggingface.co/bert-base/resolve/main/pytorch_model.bin", True),
-        ("https://hub-ci.huggingface.co/__DUMMY_USER__/repo-1470b5/resolve/main/file.txt", True),
+        (
+            "https://hub-ci.huggingface.co/__DUMMY_USER__/repo-1470b5/resolve/main/file.txt",
+            True,
+        ),
     ],
 )
 def test_repo_api_regex(url: str, should_match: bool) -> None:
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_experimental.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_experimental.py
@@ -12,14 +12,18 @@ def dummy_function():
 
 class TestExperimentalFlag(unittest.TestCase):
     def test_experimental_warning(self):
-        with patch("huggingface_hub.constants.HF_HUB_DISABLE_EXPERIMENTAL_WARNING", False):
+        with patch(
+            "huggingface_hub.constants.HF_HUB_DISABLE_EXPERIMENTAL_WARNING", False
+        ):
             with warnings.catch_warnings(record=True) as w:
                 warnings.simplefilter("always")
                 self.assertEqual(dummy_function(), "success")
             self.assertEqual(len(w), 1)
 
     def test_experimental_no_warning(self):
-        with patch("huggingface_hub.constants.HF_HUB_DISABLE_EXPERIMENTAL_WARNING", True):
+        with patch(
+            "huggingface_hub.constants.HF_HUB_DISABLE_EXPERIMENTAL_WARNING", True
+        ):
             with warnings.catch_warnings(record=True) as w:
                 warnings.simplefilter("always")
                 self.assertEqual(dummy_function(), "success")
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_fixes.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_fixes.py
@@ -13,10 +13,14 @@ class TestYamlDump(unittest.TestCase):
         self.assertEqual(yaml_dump({"emoji": "👀"}), "emoji: 👀\n")
 
     def test_yaml_dump_japanese_characters(self) -> None:
-        self.assertEqual(yaml_dump({"some unicode": "日本か"}), "some unicode: 日本か\n")
+        self.assertEqual(
+            yaml_dump({"some unicode": "日本か"}), "some unicode: 日本か\n"
+        )
 
     def test_yaml_dump_explicit_no_unicode(self) -> None:
-        self.assertEqual(yaml_dump({"emoji": "👀"}, allow_unicode=False), 'emoji: "\\U0001F440"\n')
+        self.assertEqual(
+            yaml_dump({"emoji": "👀"}, allow_unicode=False), 'emoji: "\\U0001F440"\n'
+        )
 
 
 class TestTemporaryDirectory(unittest.TestCase):
@@ -32,7 +36,10 @@ class TestTemporaryDirectory(unittest.Te
 
 class TestWeakFileLock:
     def test_lock_log_every(
-        self, tmp_path: Path, monkeypatch: pytest.MonkeyPatch, caplog: pytest.LogCaptureFixture
+        self,
+        tmp_path: Path,
+        monkeypatch: pytest.MonkeyPatch,
+        caplog: pytest.LogCaptureFixture,
     ) -> None:
         monkeypatch.setattr("huggingface_hub.constants.FILELOCK_LOG_EVERY_SECONDS", 0.1)
         lock_file = tmp_path / ".lock"
@@ -45,4 +52,6 @@ class TestWeakFileLock:
                 assert exc_info.value.lock_file == str(lock_file)
 
         assert len(caplog.records) >= 3
-        assert caplog.records[0].message.startswith(f"Still waiting to acquire lock on {lock_file}")
+        assert caplog.records[0].message.startswith(
+            f"Still waiting to acquire lock on {lock_file}"
+        )
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_git_credentials.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_git_credentials.py
@@ -43,10 +43,14 @@ class TestGitCredentials(unittest.TestCa
         username = "hf_test_user_" + str(round(time.time()))  # make username unique
 
         # Set credentials
-        set_git_credential(token="hf_test_token", username=username, folder=self.cache_dir)
+        set_git_credential(
+            token="hf_test_token", username=username, folder=self.cache_dir
+        )
 
         # Check credentials are stored
-        with run_interactive_subprocess("git credential fill", folder=self.cache_dir) as (stdin, stdout):
+        with run_interactive_subprocess(
+            "git credential fill", folder=self.cache_dir
+        ) as (stdin, stdout):
             stdin.write(f"url={ENDPOINT}\nusername={username}\n\n")
             stdin.flush()
             output = stdout.read()
@@ -58,7 +62,9 @@ class TestGitCredentials(unittest.TestCa
         # Check credentials are NOT stored
         # Cannot check with `git credential fill` as it would hang forever: only
         # checking `store` helper instead.
-        with run_interactive_subprocess("git credential-store get", folder=self.cache_dir) as (stdin, stdout):
+        with run_interactive_subprocess(
+            "git credential-store get", folder=self.cache_dir
+        ) as (stdin, stdout):
             stdin.write(f"url={ENDPOINT}\nusername={username}\n\n")
             stdin.flush()
             output = stdout.read()
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_headers.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_headers.py
@@ -2,7 +2,11 @@ import unittest
 from unittest.mock import Mock, patch
 
 from huggingface_hub.utils import get_hf_hub_version, get_python_version
-from huggingface_hub.utils._headers import _deduplicate_user_agent, _http_user_agent, build_hf_headers
+from huggingface_hub.utils._headers import (
+    _deduplicate_user_agent,
+    _http_user_agent,
+    build_hf_headers,
+)
 
 from .testing_utils import handle_injection_in_test
 
@@ -122,13 +126,21 @@ class TestUserAgentHeadersUtil(unittest.
         )
 
     def test_user_agent_with_library_name_no_version(self) -> None:
-        self.assertTrue(self._get_user_agent(library_name="foo").startswith("foo/None;"))
+        self.assertTrue(
+            self._get_user_agent(library_name="foo").startswith("foo/None;")
+        )
 
     def test_user_agent_with_custom_agent_string(self) -> None:
-        self.assertTrue(self._get_user_agent(user_agent="this is a custom agent").endswith("this is a custom agent"))
+        self.assertTrue(
+            self._get_user_agent(user_agent="this is a custom agent").endswith(
+                "this is a custom agent"
+            )
+        )
 
     def test_user_agent_with_custom_agent_dict(self) -> None:
-        self.assertTrue(self._get_user_agent(user_agent={"a": "b", "c": "d"}).endswith("a/b; c/d"))
+        self.assertTrue(
+            self._get_user_agent(user_agent={"a": "b", "c": "d"}).endswith("a/b; c/d")
+        )
 
     def test_user_agent_deduplicate(self) -> None:
         self.assertEqual(
@@ -142,16 +154,31 @@ class TestUserAgentHeadersUtil(unittest.
             "python/3.7; python/3.8; hf_hub/0.12; transformers/None; diffusers/0.12.1",
         )
 
-    @patch("huggingface_hub.utils._telemetry.constants.HF_HUB_USER_AGENT_ORIGIN", "custom-origin")
+    @patch(
+        "huggingface_hub.utils._telemetry.constants.HF_HUB_USER_AGENT_ORIGIN",
+        "custom-origin",
+    )
     def test_user_agent_with_origin(self) -> None:
         self.assertTrue(self._get_user_agent().endswith("origin/custom-origin"))
 
-    @patch("huggingface_hub.utils._telemetry.constants.HF_HUB_USER_AGENT_ORIGIN", "custom-origin")
+    @patch(
+        "huggingface_hub.utils._telemetry.constants.HF_HUB_USER_AGENT_ORIGIN",
+        "custom-origin",
+    )
     def test_user_agent_with_origin_and_user_agent(self) -> None:
         self.assertTrue(
-            self._get_user_agent(user_agent={"a": "b", "c": "d"}).endswith("a/b; c/d; origin/custom-origin")
+            self._get_user_agent(user_agent={"a": "b", "c": "d"}).endswith(
+                "a/b; c/d; origin/custom-origin"
+            )
         )
 
-    @patch("huggingface_hub.utils._telemetry.constants.HF_HUB_USER_AGENT_ORIGIN", "custom-origin")
+    @patch(
+        "huggingface_hub.utils._telemetry.constants.HF_HUB_USER_AGENT_ORIGIN",
+        "custom-origin",
+    )
     def test_user_agent_with_origin_and_user_agent_str(self) -> None:
-        self.assertTrue(self._get_user_agent(user_agent="a/b;c/d").endswith("a/b; c/d; origin/custom-origin"))
+        self.assertTrue(
+            self._get_user_agent(user_agent="a/b;c/d").endswith(
+                "a/b; c/d; origin/custom-origin"
+            )
+        )
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_http.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_http.py
@@ -31,7 +31,9 @@ class TestHttpBackoff(unittest.TestCase)
         get_session_mock = Mock()
         self.mock_request = get_session_mock().request
 
-        self.patcher = patch("huggingface_hub.utils._http.get_session", get_session_mock)
+        self.patcher = patch(
+            "huggingface_hub.utils._http.get_session", get_session_mock
+        )
         self.patcher.start()
 
     def tearDown(self) -> None:
@@ -112,7 +114,9 @@ class TestHttpBackoff(unittest.TestCase)
         mock_200.status_code = 200
         self.mock_request.side_effect = (mock_200, mock_200, mock_200, mock_200)
 
-        response = http_backoff("GET", URL, base_wait_time=0.0, max_retries=3, retry_on_status_codes=200)
+        response = http_backoff(
+            "GET", URL, base_wait_time=0.0, max_retries=3, retry_on_status_codes=200
+        )
 
         self.assertEqual(self.mock_request.call_count, 4)
         self.assertIs(response, mock_200)
@@ -139,7 +143,9 @@ class TestHttpBackoff(unittest.TestCase)
         self.mock_request.side_effect = _side_effect_timer()
 
         with self.assertRaises(ConnectTimeout):
-            http_backoff("GET", URL, base_wait_time=0.1, max_wait_time=0.5, max_retries=5)
+            http_backoff(
+                "GET", URL, base_wait_time=0.1, max_wait_time=0.5, max_retries=5
+            )
 
         self.assertEqual(self.mock_request.call_count, 6)
 
@@ -166,7 +172,9 @@ class TestConfigureSession(unittest.Test
 
     def test_default_configuration(self) -> None:
         session = get_session()
-        self.assertEqual(session.headers["connection"], "keep-alive")  # keep connection alive by default
+        self.assertEqual(
+            session.headers["connection"], "keep-alive"
+        )  # keep connection alive by default
         self.assertIsNone(session.auth)
         self.assertEqual(session.proxies, {})
         self.assertEqual(session.verify, True)
@@ -210,7 +218,10 @@ class TestConfigureSession(unittest.Test
         main_session = get_session()
 
         # Start 3 threads and get sessions in each of them
-        threads = [threading.Thread(target=_get_session_in_thread, args=(index,)) for index in range(N)]
+        threads = [
+            threading.Thread(target=_get_session_in_thread, args=(index,))
+            for index in range(N)
+        ]
         for th in threads:
             th.start()
             print(th)
@@ -279,7 +290,9 @@ class TestUniqueRequestId(unittest.TestC
         self.assertTrue(_is_uuid(request_id_2))
 
     def test_request_id_not_overwritten(self):
-        response = get_session().get(self.api_endpoint, headers={"x-request-id": "custom-id"})
+        response = get_session().get(
+            self.api_endpoint, headers={"x-request-id": "custom-id"}
+        )
 
         request_id = response.request.headers["x-request-id"]
         self.assertEqual(request_id, "custom-id")
@@ -301,16 +314,34 @@ def _is_uuid(string: str) -> bool:
     ("base_url", "endpoint", "expected_url"),
     [
         # Staging url => unchanged
-        ("https://hub-ci.huggingface.co/resolve/...", None, "https://hub-ci.huggingface.co/resolve/..."),
+        (
+            "https://hub-ci.huggingface.co/resolve/...",
+            None,
+            "https://hub-ci.huggingface.co/resolve/...",
+        ),
         # Prod url => unchanged
-        ("https://huggingface.co/resolve/...", None, "https://huggingface.co/resolve/..."),
+        (
+            "https://huggingface.co/resolve/...",
+            None,
+            "https://huggingface.co/resolve/...",
+        ),
         # Custom endpoint + staging url => fixed
-        ("https://hub-ci.huggingface.co/api/models", "https://mirror.co", "https://mirror.co/api/models"),
+        (
+            "https://hub-ci.huggingface.co/api/models",
+            "https://mirror.co",
+            "https://mirror.co/api/models",
+        ),
         # Custom endpoint + prod url => fixed
-        ("https://huggingface.co/api/models", "https://mirror.co", "https://mirror.co/api/models"),
+        (
+            "https://huggingface.co/api/models",
+            "https://mirror.co",
+            "https://mirror.co/api/models",
+        ),
     ],
 )
-def test_fix_hf_endpoint_in_url(base_url: str, endpoint: Optional[str], expected_url: str) -> None:
+def test_fix_hf_endpoint_in_url(
+    base_url: str, endpoint: Optional[str], expected_url: str
+) -> None:
     assert fix_hf_endpoint_in_url(base_url, endpoint) == expected_url
 
 
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_pagination.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_pagination.py
@@ -10,7 +10,9 @@ class TestPagination(unittest.TestCase):
     @patch("huggingface_hub.utils._pagination.get_session")
     @patch("huggingface_hub.utils._pagination.hf_raise_for_status")
     @handle_injection_in_test
-    def test_mocked_paginate(self, mock_get_session: Mock, mock_hf_raise_for_status: Mock) -> None:
+    def test_mocked_paginate(
+        self, mock_get_session: Mock, mock_hf_raise_for_status: Mock
+    ) -> None:
         mock_get = mock_get_session().get
         mock_params = Mock()
         mock_headers = Mock()
@@ -63,7 +65,11 @@ class TestPagination(unittest.TestCase):
         # Real test: paginate over huggingface repos on Github
         # Use enumerate and stop after first page to avoid loading all repos
         for num, _ in enumerate(
-            paginate("https://api.github.com/orgs/huggingface/repos?limit=4", params={}, headers={})
+            paginate(
+                "https://api.github.com/orgs/huggingface/repos?limit=4",
+                params={},
+                headers={},
+            )
         ):
             if num == 6:
                 break
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_paths.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_paths.py
@@ -145,6 +145,7 @@ class TestDefaultIgnorePatterns(unittest
 
     def test_exclude_git_folder(self):
         filtered_paths = filter_repo_objects(
-            items=self.PATHS_TO_IGNORE + self.VALID_PATHS, ignore_patterns=DEFAULT_IGNORE_PATTERNS
+            items=self.PATHS_TO_IGNORE + self.VALID_PATHS,
+            ignore_patterns=DEFAULT_IGNORE_PATTERNS,
         )
         self.assertListEqual(list(filtered_paths), self.VALID_PATHS)
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_sha.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_sha.py
@@ -36,5 +36,7 @@ def test_git_hash(tmpdir):
     with open(path, "wb") as file:
         file.write(b"Hello, World!")
 
-    output = subprocess.run(f"git hash-object -t blob {path}", shell=True, capture_output=True, text=True)
+    output = subprocess.run(
+        f"git hash-object -t blob {path}", shell=True, capture_output=True, text=True
+    )
     assert output.stdout.strip() == git_hash(b"Hello, World!")
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_telemetry.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_telemetry.py
@@ -14,7 +14,9 @@ class TestSendTelemetry(unittest.TestCas
         get_session_mock = Mock()
         self.mock_head = get_session_mock().head
 
-        self.patcher = patch("huggingface_hub.utils._telemetry.get_session", get_session_mock)
+        self.patcher = patch(
+            "huggingface_hub.utils._telemetry.get_session", get_session_mock
+        )
         self.patcher.start()
 
     def tearDown(self) -> None:
@@ -24,7 +26,9 @@ class TestSendTelemetry(unittest.TestCas
         send_telemetry(topic="examples")
         queue.join()  # Wait for the telemetry tasks to be completed
         self.mock_head.assert_called_once()
-        self.assertEqual(self.mock_head.call_args[0][0], f"{ENDPOINT_STAGING}/api/telemetry/examples")
+        self.assertEqual(
+            self.mock_head.call_args[0][0], f"{ENDPOINT_STAGING}/api/telemetry/examples"
+        )
 
     def test_topic_multiple(self, queue: Queue) -> None:
         send_telemetry(topic="example1")
@@ -33,21 +37,36 @@ class TestSendTelemetry(unittest.TestCas
         queue.join()  # Wait for the telemetry tasks to be completed
 
         self.assertEqual(self.mock_head.call_count, 3)  # 3 calls and order is preserved
-        self.assertEqual(self.mock_head.call_args_list[0][0][0], f"{ENDPOINT_STAGING}/api/telemetry/example1")
-        self.assertEqual(self.mock_head.call_args_list[1][0][0], f"{ENDPOINT_STAGING}/api/telemetry/example2")
-        self.assertEqual(self.mock_head.call_args_list[2][0][0], f"{ENDPOINT_STAGING}/api/telemetry/example3")
+        self.assertEqual(
+            self.mock_head.call_args_list[0][0][0],
+            f"{ENDPOINT_STAGING}/api/telemetry/example1",
+        )
+        self.assertEqual(
+            self.mock_head.call_args_list[1][0][0],
+            f"{ENDPOINT_STAGING}/api/telemetry/example2",
+        )
+        self.assertEqual(
+            self.mock_head.call_args_list[2][0][0],
+            f"{ENDPOINT_STAGING}/api/telemetry/example3",
+        )
 
     def test_topic_with_subtopic(self, queue: Queue) -> None:
         send_telemetry(topic="gradio/image/this_one")
         queue.join()  # Wait for the telemetry tasks to be completed
         self.mock_head.assert_called_once()
-        self.assertEqual(self.mock_head.call_args[0][0], f"{ENDPOINT_STAGING}/api/telemetry/gradio/image/this_one")
+        self.assertEqual(
+            self.mock_head.call_args[0][0],
+            f"{ENDPOINT_STAGING}/api/telemetry/gradio/image/this_one",
+        )
 
     def test_topic_quoted(self, queue: Queue) -> None:
         send_telemetry(topic="foo bar")
         queue.join()  # Wait for the telemetry tasks to be completed
         self.mock_head.assert_called_once()
-        self.assertEqual(self.mock_head.call_args[0][0], f"{ENDPOINT_STAGING}/api/telemetry/foo%20bar")
+        self.assertEqual(
+            self.mock_head.call_args[0][0],
+            f"{ENDPOINT_STAGING}/api/telemetry/foo%20bar",
+        )
 
     @patch("huggingface_hub.utils._telemetry.constants.HF_HUB_OFFLINE", True)
     def test_hub_offline(self, queue: Queue) -> None:
@@ -62,12 +81,16 @@ class TestSendTelemetry(unittest.TestCas
         self.mock_head.assert_not_called()
 
     @patch("huggingface_hub.utils._telemetry.build_hf_headers")
-    def test_telemetry_use_build_hf_headers(self, mock_headers: Mock, queue: Queue) -> None:
+    def test_telemetry_use_build_hf_headers(
+        self, mock_headers: Mock, queue: Queue
+    ) -> None:
         send_telemetry(topic="topic")
         queue.join()  # Wait for the telemetry tasks to be completed
         self.mock_head.assert_called_once()
         mock_headers.assert_called_once()
-        self.assertEqual(self.mock_head.call_args[1]["headers"], mock_headers.return_value)
+        self.assertEqual(
+            self.mock_head.call_args[1]["headers"], mock_headers.return_value
+        )
 
 
 @patch("huggingface_hub.utils._telemetry._TELEMETRY_QUEUE", new_callable=Queue)
@@ -77,14 +100,18 @@ class TestSendTelemetryConnectionError(u
         get_session_mock = Mock()
         get_session_mock().head.side_effect = Exception("whatever")
 
-        self.patcher = patch("huggingface_hub.utils._telemetry.get_session", get_session_mock)
+        self.patcher = patch(
+            "huggingface_hub.utils._telemetry.get_session", get_session_mock
+        )
         self.patcher.start()
 
     def tearDown(self) -> None:
         self.patcher.stop()
 
     def test_telemetry_exception_silenced(self, queue: Queue) -> None:
-        with self.assertLogs(logger="huggingface_hub.utils._telemetry", level="DEBUG") as captured:
+        with self.assertLogs(
+            logger="huggingface_hub.utils._telemetry", level="DEBUG"
+        ) as captured:
             send_telemetry(topic="topic")
             queue.join()
 
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_typing.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_typing.py
@@ -4,7 +4,11 @@ from typing import Optional, Type, Union
 
 import pytest
 
-from huggingface_hub.utils._typing import is_jsonable, is_simple_optional_type, unwrap_simple_optional_type
+from huggingface_hub.utils._typing import (
+    is_jsonable,
+    is_simple_optional_type,
+    unwrap_simple_optional_type,
+)
 
 
 class NotSerializableClass:
--- python3-huggingface-hub-0.29.3.orig/tests/test_utils_validators.py
+++ python3-huggingface-hub-0.29.3/tests/test_utils_validators.py
@@ -56,7 +56,9 @@ class TestRepoIdValidator(unittest.TestC
     def test_not_valid_repo_ids(self) -> None:
         """Test `repo_id` validation on not valid values."""
         for repo_id in self.NOT_VALID_VALUES:
-            with self.assertRaises(HFValidationError, msg=f"'{repo_id}' must not be valid"):
+            with self.assertRaises(
+                HFValidationError, msg=f"'{repo_id}' must not be valid"
+            ):
                 validate_repo_id(repo_id)
 
 
@@ -87,15 +89,21 @@ class TestSmoothlyDeprecateUseAuthToken(
 
     def test_input_kwargs_not_mutated_by_smooth_deprecation(self) -> None:
         initial_kwargs = {"a": "b", "use_auth_token": "token"}
-        kwargs = smoothly_deprecate_use_auth_token(fn_name="name", has_token=False, kwargs=initial_kwargs)
+        kwargs = smoothly_deprecate_use_auth_token(
+            fn_name="name", has_token=False, kwargs=initial_kwargs
+        )
         self.assertEqual(kwargs, {"a": "b", "token": "token"})
-        self.assertEqual(initial_kwargs, {"a": "b", "use_auth_token": "token"})  # not mutated!
+        self.assertEqual(
+            initial_kwargs, {"a": "b", "use_auth_token": "token"}
+        )  # not mutated!
 
     def test_with_both_token_and_use_auth_token(self) -> None:
         with self.assertWarns(UserWarning):
             # `use_auth_token` is ignored !
             self.assertEqual(
-                self.dummy_token_function(token="this_is_a_token", use_auth_token="this_is_a_use_auth_token"),
+                self.dummy_token_function(
+                    token="this_is_a_token", use_auth_token="this_is_a_use_auth_token"
+                ),
                 ("this_is_a_token", {}),
             )
 
@@ -103,7 +111,9 @@ class TestSmoothlyDeprecateUseAuthToken(
         # `use_auth_token` is accepted by `dummy_use_auth_token_function`
         # => `smoothly_deprecate_use_auth_token` is not called
         self.assertEqual(
-            self.dummy_use_auth_token_function(use_auth_token="this_is_a_use_auth_token"),
+            self.dummy_use_auth_token_function(
+                use_auth_token="this_is_a_use_auth_token"
+            ),
             ("this_is_a_use_auth_token", {}),
         )
 
--- python3-huggingface-hub-0.29.3.orig/tests/test_webhooks_server.py
+++ python3-huggingface-hub-0.29.3/tests/test_webhooks_server.py
@@ -24,7 +24,10 @@ WEBHOOK_PAYLOAD_CREATE_DISCUSSION = {
         "name": "gpt2",
         "id": "621ffdc036468d709f17434d",
         "private": False,
-        "url": {"web": "https://huggingface.co/gpt2", "api": "https://huggingface.co/api/models/gpt2"},
+        "url": {
+            "web": "https://huggingface.co/gpt2",
+            "api": "https://huggingface.co/api/models/gpt2",
+        },
         "owner": {"id": "628b753283ef59b5be89e937"},
     },
     "discussion": {
@@ -45,7 +48,9 @@ WEBHOOK_PAYLOAD_CREATE_DISCUSSION = {
         "author": {"id": "61d2f90c3c2083e1c08af22d"},
         "content": "Add co2 emissions information to the model card",
         "hidden": False,
-        "url": {"web": "https://huggingface.co/gpt2/discussions/19#6399f58518721fdd27fc9caa"},
+        "url": {
+            "web": "https://huggingface.co/gpt2/discussions/19#6399f58518721fdd27fc9caa"
+        },
     },
     "webhook": {"id": "6390e855e30d9209411de93b", "version": 3},
 }
@@ -154,7 +159,9 @@ class TestWebhooksServerDontRun(unittest
         async def handler():
             pass
 
-        self.assertIn("/webhooks/test_webhook", app.registered_webhooks)  # still registered under /webhooks
+        self.assertIn(
+            "/webhooks/test_webhook", app.registered_webhooks
+        )  # still registered under /webhooks
 
     def test_add_webhook_twice_should_fail(self):
         # Test adding a webhook
@@ -233,40 +240,65 @@ class TestWebhooksServerRun(unittest.Tes
 
         instructions = output.getvalue()
         assert "Webhooks are correctly setup and ready to use:" in instructions
-        assert "- POST http://127.0.0.1:" in instructions  # port is usually 7860 but can be dynamic
+        assert (
+            "- POST http://127.0.0.1:" in instructions
+        )  # port is usually 7860 but can be dynamic
         assert "/webhooks/test_webhook" in instructions
 
     def test_run_parse_payload(self):
         """Test that the payload is correctly parsed when running the app."""
         response = self.client.post(
-            "/webhooks/test_webhook", headers=self.HEADERS_VALID_SECRET, json=WEBHOOK_PAYLOAD_CREATE_DISCUSSION
+            "/webhooks/test_webhook",
+            headers=self.HEADERS_VALID_SECRET,
+            json=WEBHOOK_PAYLOAD_CREATE_DISCUSSION,
         )
         self.assertEqual(response.status_code, 200)
         self.assertEqual(response.json(), {"scope": "discussion"})
 
     def test_with_webhook_secret_should_succeed(self):
         """Test success if valid secret is sent."""
-        for path in ["async_with_request", "sync_with_request", "async_no_request", "sync_no_request"]:
+        for path in [
+            "async_with_request",
+            "sync_with_request",
+            "async_no_request",
+            "sync_no_request",
+        ]:
             with self.subTest(path):
-                response = self.client.post(f"/webhooks/{path}", headers=self.HEADERS_VALID_SECRET)
+                response = self.client.post(
+                    f"/webhooks/{path}", headers=self.HEADERS_VALID_SECRET
+                )
                 self.assertEqual(response.status_code, 200)
                 self.assertEqual(response.json(), {"success": True})
 
     def test_no_webhook_secret_should_be_unauthorized(self):
         """Test failure if valid secret is sent."""
-        for path in ["async_with_request", "sync_with_request", "async_no_request", "sync_no_request"]:
+        for path in [
+            "async_with_request",
+            "sync_with_request",
+            "async_no_request",
+            "sync_no_request",
+        ]:
             with self.subTest(path):
                 response = self.client.post(f"/webhooks/{path}")
                 self.assertEqual(response.status_code, 401)
 
     def test_wrong_webhook_secret_should_be_forbidden(self):
         """Test failure if valid secret is sent."""
-        for path in ["async_with_request", "sync_with_request", "async_no_request", "sync_no_request"]:
+        for path in [
+            "async_with_request",
+            "sync_with_request",
+            "async_no_request",
+            "sync_no_request",
+        ]:
             with self.subTest(path):
-                response = self.client.post(f"/webhooks/{path}", headers=self.HEADERS_WRONG_SECRET)
+                response = self.client.post(
+                    f"/webhooks/{path}", headers=self.HEADERS_WRONG_SECRET
+                )
                 self.assertEqual(response.status_code, 403)
 
     def test_route_with_explicit_path(self):
         """Test that the route with an explicit path is correctly registered."""
-        response = self.client.post("/webhooks/explicit_path", headers=self.HEADERS_VALID_SECRET)
+        response = self.client.post(
+            "/webhooks/explicit_path", headers=self.HEADERS_VALID_SECRET
+        )
         self.assertEqual(response.status_code, 200)
--- python3-huggingface-hub-0.29.3.orig/tests/testing_constants.py
+++ python3-huggingface-hub-0.29.3/tests/testing_constants.py
@@ -18,4 +18,6 @@ ENTERPRISE_TOKEN = "hf_enterprise_admin_
 ENDPOINT_PRODUCTION = "https://huggingface.co"
 ENDPOINT_STAGING = "https://hub-ci.huggingface.co"
 
-ENDPOINT_PRODUCTION_URL_SCHEME = ENDPOINT_PRODUCTION + "/{repo_id}/resolve/{revision}/{filename}"
+ENDPOINT_PRODUCTION_URL_SCHEME = (
+    ENDPOINT_PRODUCTION + "/{repo_id}/resolve/{revision}/{filename}"
+)
--- python3-huggingface-hub-0.29.3.orig/tests/testing_utils.py
+++ python3-huggingface-hub-0.29.3/tests/testing_utils.py
@@ -34,7 +34,9 @@ DUMMY_MODEL_ID_REVISION_INVALID = "aaaaa
 # This commit does not exist, so we should 404.
 DUMMY_MODEL_ID_PINNED_SHA1 = "d9e9f15bc825e4b2c9249e9578f884bbcb5e3684"
 # Sha-1 of config.json on the top of `main`, for checking purposes
-DUMMY_MODEL_ID_PINNED_SHA256 = "4b243c475af8d0a7754e87d7d096c92e5199ec2fe168a2ee7998e3b8e9bcb1d3"
+DUMMY_MODEL_ID_PINNED_SHA256 = (
+    "4b243c475af8d0a7754e87d7d096c92e5199ec2fe168a2ee7998e3b8e9bcb1d3"
+)
 # Sha-256 of pytorch_model.bin on the top of `main`, for checking purposes
 
 # "hf-internal-testing/dummy-will-be-renamed" has been renamed to "hf-internal-testing/dummy-renamed"
@@ -44,7 +46,9 @@ DUMMY_RENAMED_NEW_MODEL_ID = "hf-interna
 SAMPLE_DATASET_IDENTIFIER = "lhoestq/custom_squad"
 # Example dataset ids
 DUMMY_DATASET_ID = "gaia-benchmark/GAIA"
-DUMMY_DATASET_ID_REVISION_ONE_SPECIFIC_COMMIT = "c603981e170e9e333934a39781d2ae3a2677e81f"  # on branch "test-branch"
+DUMMY_DATASET_ID_REVISION_ONE_SPECIFIC_COMMIT = (
+    "c603981e170e9e333934a39781d2ae3a2677e81f"  # on branch "test-branch"
+)
 
 YES = ("y", "yes", "t", "true", "on", "1")
 NO = ("n", "no", "f", "false", "off", "0")
@@ -172,7 +176,9 @@ def offline(mode=OfflineSimulationMode.C
             # The following changes in the error are just here to make the offline timeout error prettier
             e.request.url = url
             max_retry_error = e.args[0]
-            max_retry_error.args = (max_retry_error.args[0].replace("10.255.255.1", f"OfflineMock[{url}]"),)
+            max_retry_error.args = (
+                max_retry_error.args[0].replace("10.255.255.1", f"OfflineMock[{url}]"),
+            )
             e.args = (max_retry_error,)
             raise
 
@@ -183,14 +189,20 @@ def offline(mode=OfflineSimulationMode.C
         # inspired from https://stackoverflow.com/a/18601897
         with patch("socket.socket", offline_socket):
             with patch("huggingface_hub.utils._http.get_session") as get_session_mock:
-                with patch("huggingface_hub.file_download.get_session") as get_session_mock:
-                    get_session_mock.return_value = requests.Session()  # not an existing one
+                with patch(
+                    "huggingface_hub.file_download.get_session"
+                ) as get_session_mock:
+                    get_session_mock.return_value = (
+                        requests.Session()
+                    )  # not an existing one
                     yield
     elif mode is OfflineSimulationMode.CONNECTION_TIMES_OUT:
         # inspired from https://stackoverflow.com/a/904609
         with patch("requests.request", timeout_request):
             with patch("huggingface_hub.utils._http.get_session") as get_session_mock:
-                with patch("huggingface_hub.file_download.get_session") as get_session_mock:
+                with patch(
+                    "huggingface_hub.file_download.get_session"
+                ) as get_session_mock:
                     get_session_mock().request = timeout_request
                     yield
     elif mode is OfflineSimulationMode.HF_HUB_OFFLINE_SET_TO_1:
@@ -212,7 +224,10 @@ def rmtree_with_retry(path: Union[str, P
 
 
 def with_production_testing(func):
-    file_download = patch("huggingface_hub.file_download.HUGGINGFACE_CO_URL_TEMPLATE", ENDPOINT_PRODUCTION_URL_SCHEME)
+    file_download = patch(
+        "huggingface_hub.file_download.HUGGINGFACE_CO_URL_TEMPLATE",
+        ENDPOINT_PRODUCTION_URL_SCHEME,
+    )
     hf_api = patch("huggingface_hub.constants.ENDPOINT", ENDPOINT_PRODUCTION)
     return hf_api(file_download(func))
 
@@ -286,7 +301,9 @@ def xfail_on_windows(reason: str, raises
     """
 
     def _inner_decorator(test_function: Callable) -> Callable:
-        return pytest.mark.xfail(os.name == "nt", reason=reason, raises=raises, strict=True, run=True)(test_function)
+        return pytest.mark.xfail(
+            os.name == "nt", reason=reason, raises=raises, strict=True, run=True
+        )(test_function)
 
     return _inner_decorator
 
@@ -392,9 +409,9 @@ def handle_injection_in_test(fn: Callabl
             if name == "self":
                 continue
             assert parameter.annotation is Mock
-            assert name in mocks, (
-                f"Mock `{name}` not found for test `{fn.__name__}`. Available: {', '.join(sorted(mocks.keys()))}"
-            )
+            assert (
+                name in mocks
+            ), f"Mock `{name}` not found for test `{fn.__name__}`. Available: {', '.join(sorted(mocks.keys()))}"
             new_kwargs[name] = mocks[name]
 
         # Run test only with a subset of mocks
@@ -437,7 +454,9 @@ def use_tmp_repo(repo_type: str = "model
                 create_repo_kwargs["space_sdk"] = "gradio"
 
             repo_url = self._api.create_repo(
-                repo_id=repo_name(prefix=repo_type), repo_type=repo_type, **create_repo_kwargs
+                repo_id=repo_name(prefix=repo_type),
+                repo_type=repo_type,
+                **create_repo_kwargs,
             )
             try:
                 return test_fn(*args, **kwargs, repo_url=repo_url)
--- python3-huggingface-hub-0.29.3.orig/utils/_legacy_check_future_compatible_signatures.py
+++ python3-huggingface-hub-0.29.3/utils/_legacy_check_future_compatible_signatures.py
@@ -69,7 +69,9 @@ STUBS_SECTION_TEMPLATE = """
     ### Stubs section end ###
 """
 
-STUBS_SECTION_TEMPLATE_REGEX = re.compile(r"### Stubs section start ###.*### Stubs section end ###", re.DOTALL)
+STUBS_SECTION_TEMPLATE_REGEX = re.compile(
+    r"### Stubs section start ###.*### Stubs section end ###", re.DOTALL
+)
 
 AS_FUTURE_SIGNATURE_TEMPLATE = "as_future: bool = False"
 
@@ -104,7 +106,9 @@ def generate_future_compatible_method(me
         if match is None:
             raise ValueError(f"Could not find `Args` section in docstring of {method}.")
         args_docs = match.group(1).strip()
-        method_source = method_source.replace(args_docs, args_docs + AS_FUTURE_DOCSTRING_TEMPLATE)
+        method_source = method_source.replace(
+            args_docs, args_docs + AS_FUTURE_DOCSTRING_TEMPLATE
+        )
 
     # 2. Update signature
     # 2.a. Add `as_future` parameter
@@ -113,7 +117,9 @@ def generate_future_compatible_method(me
         if match is None:
             raise ValueError(f"Could not find signature of {method} in source.")
         method_source = method_source.replace(
-            match.group(), match.group().replace(") ->", f" {AS_FUTURE_SIGNATURE_TEMPLATE}) ->"), 1
+            match.group(),
+            match.group().replace(") ->", f" {AS_FUTURE_SIGNATURE_TEMPLATE}) ->"),
+            1,
         )
 
     # 2.b. Update return value
@@ -134,20 +140,32 @@ def generate_future_compatible_method(me
 
     match = SIGNATURE_REGEX_RETURN_TYPE_WITH_FUTURE.search(method_sig)
     if match is None:
-        raise ValueError(f"Could not find return type (with Future) of {method} in source.")
+        raise ValueError(
+            f"Could not find return type (with Future) of {method} in source."
+        )
     no_future_return_type = match.group(1).strip()
     with_future_return_type = match.group(2).strip()
 
     # 3.a. Stub when `as_future=False`
     no_future_stub = "    @overload\n" + method_sig
-    no_future_stub = no_future_stub.replace(AS_FUTURE_SIGNATURE_TEMPLATE, "as_future: Literal[False] = ...")
-    no_future_stub = SIGNATURE_REGEX_RETURN_TYPE.sub(rf"-> {no_future_return_type}:", no_future_stub)
-    no_future_stub += "  # type: ignore\n        ..."  # only the first stub requires "type: ignore"
+    no_future_stub = no_future_stub.replace(
+        AS_FUTURE_SIGNATURE_TEMPLATE, "as_future: Literal[False] = ..."
+    )
+    no_future_stub = SIGNATURE_REGEX_RETURN_TYPE.sub(
+        rf"-> {no_future_return_type}:", no_future_stub
+    )
+    no_future_stub += (
+        "  # type: ignore\n        ..."  # only the first stub requires "type: ignore"
+    )
 
     # 3.b. Stub when `as_future=True`
     with_future_stub = "    @overload\n" + method_sig
-    with_future_stub = with_future_stub.replace(AS_FUTURE_SIGNATURE_TEMPLATE, "as_future: Literal[True] = ...")
-    with_future_stub = SIGNATURE_REGEX_RETURN_TYPE.sub(rf"-> {with_future_return_type}:", with_future_stub)
+    with_future_stub = with_future_stub.replace(
+        AS_FUTURE_SIGNATURE_TEMPLATE, "as_future: Literal[True] = ..."
+    )
+    with_future_stub = SIGNATURE_REGEX_RETURN_TYPE.sub(
+        rf"-> {with_future_return_type}:", with_future_stub
+    )
     with_future_stub += "\n        ..."
 
     stubs_source = no_future_stub + "\n\n" + with_future_stub + "\n\n"
@@ -171,7 +189,9 @@ def generate_hf_api_module() -> str:
         all_stubs_source += "\n\n" + stubs_source
 
     # Generate code with stubs
-    generated_code = STUBS_SECTION_TEMPLATE_REGEX.sub(STUBS_SECTION_TEMPLATE.format(stubs=all_stubs_source), raw_code)
+    generated_code = STUBS_SECTION_TEMPLATE_REGEX.sub(
+        STUBS_SECTION_TEMPLATE.format(stubs=all_stubs_source), raw_code
+    )
 
     # Format (ruff)
     return format_generated_code(generated_code)
--- python3-huggingface-hub-0.29.3.orig/utils/check_all_variable.py
+++ python3-huggingface-hub-0.29.3/utils/check_all_variable.py
@@ -77,14 +77,20 @@ def check_static_all(update: bool) -> No
         if all_pattern.search(content):
             new_content = all_pattern.sub(new_all, content)
         else:
-            submod_attrs_pattern = re.compile(r"_SUBMOD_ATTRS\s*=\s*{[^}]*}", re.MULTILINE | re.DOTALL)
+            submod_attrs_pattern = re.compile(
+                r"_SUBMOD_ATTRS\s*=\s*{[^}]*}", re.MULTILINE | re.DOTALL
+            )
             match = submod_attrs_pattern.search(content)
             if not match:
-                print("Error: _SUBMOD_ATTRS dictionary not found in `./src/huggingface_hub/__init__.py`.")
+                print(
+                    "Error: _SUBMOD_ATTRS dictionary not found in `./src/huggingface_hub/__init__.py`."
+                )
                 exit(1)
 
             dict_end = match.end()
-            new_content = content[:dict_end] + "\n\n\n" + new_all + "\n\n" + content[dict_end:]
+            new_content = (
+                content[:dict_end] + "\n\n\n" + new_all + "\n\n" + content[dict_end:]
+            )
 
         INIT_FILE_PATH.write_text(new_content)
         print(
--- python3-huggingface-hub-0.29.3.orig/utils/check_contrib_list.py
+++ python3-huggingface-hub-0.29.3/utils/check_contrib_list.py
@@ -46,17 +46,23 @@ def check_contrib_list(update: bool) ->
     the list."""
     # List contrib test suites
     contrib_list = sorted(
-        path.name for path in CONTRIB_PATH.glob("*") if path.is_dir() and not path.name.startswith("_")
+        path.name
+        for path in CONTRIB_PATH.glob("*")
+        if path.is_dir() and not path.name.startswith("_")
     )
 
     # Check Makefile is consistent with list
     makefile_content = MAKEFILE_PATH.read_text()
-    makefile_expected_content = MAKEFILE_REGEX.sub(f"CONTRIB_LIBS := {' '.join(contrib_list)}", makefile_content)
+    makefile_expected_content = MAKEFILE_REGEX.sub(
+        f"CONTRIB_LIBS := {' '.join(contrib_list)}", makefile_content
+    )
 
     # Check workflow is consistent with list
     workflow_content = WORKFLOW_PATH.read_text()
     _substitute = "\n".join(f'{" " * 10}"{lib}",' for lib in contrib_list)
-    workflow_content_expected = WORKFLOW_REGEX.sub(rf"\g<before>{_substitute}\n\g<after>", workflow_content)
+    workflow_content_expected = WORKFLOW_REGEX.sub(
+        rf"\g<before>{_substitute}\n\g<after>", workflow_content
+    )
 
     #
     failed = False
--- python3-huggingface-hub-0.29.3.orig/utils/check_inference_input_params.py
+++ python3-huggingface-hub-0.29.3/utils/check_inference_input_params.py
@@ -46,7 +46,9 @@ UNDOCUMENTED_PARAMETERS = {
 
 
 def check_method(method_name: str, method: Any):
-    input_type_name = "".join(part.capitalize() for part in method_name.split("_")) + "Input"
+    input_type_name = (
+        "".join(part.capitalize() for part in method_name.split("_")) + "Input"
+    )
     if not hasattr(types, input_type_name):
         return [f"Missing input type for method {method_name}"]
 
@@ -63,7 +65,9 @@ def check_method(method_name: str, metho
 
         parameters_type = get_args(parameters_field.type)[0]
         if not is_dataclass(parameters_type):
-            return [f"'parameters' field is not a dataclass for type {input_type} ({parameters_type})"]
+            return [
+                f"'parameters' field is not a dataclass for type {input_type} ({parameters_type})"
+            ]
 
     # For each expected parameter, check it is defined
     logs = []
@@ -89,7 +93,9 @@ def check_method(method_name: str, metho
 # Inspect InferenceClient methods individually
 exit_code = 0
 all_logs = []  # print details only if errors are found
-for method_name, method in inspect.getmembers(InferenceClient, predicate=inspect.isfunction):
+for method_name, method in inspect.getmembers(
+    InferenceClient, predicate=inspect.isfunction
+):
     if method_name.startswith("_") or method_name in METHODS_TO_SKIP:
         continue
     if method_name not in PARAMETERS_TO_SKIP:
--- python3-huggingface-hub-0.29.3.orig/utils/check_static_imports.py
+++ python3-huggingface-hub-0.29.3/utils/check_static_imports.py
@@ -49,14 +49,18 @@ def check_static_imports(update: bool) -
     # Search and replace `_SUBMOD_ATTRS` dictionary definition. This ensures modules
     # and functions that can be lazy-loaded are alphabetically ordered for readability.
     if SUBMOD_ATTRS_PATTERN.search(init_content_before_static_checks) is None:
-        print("Error: _SUBMOD_ATTRS dictionary definition not found in `./src/huggingface_hub/__init__.py`.")
+        print(
+            "Error: _SUBMOD_ATTRS dictionary definition not found in `./src/huggingface_hub/__init__.py`."
+        )
         exit(1)
 
     _submod_attrs_definition = (
         "_SUBMOD_ATTRS = {\n"
         + "\n".join(
             f'    "{module}": [\n'
-            + "\n".join(f'        "{attr}",' for attr in sorted(set(_SUBMOD_ATTRS[module])))
+            + "\n".join(
+                f'        "{attr}",' for attr in sorted(set(_SUBMOD_ATTRS[module]))
+            )
             + "\n    ],"
             for module in sorted(set(_SUBMOD_ATTRS.keys()))
         )
@@ -77,10 +81,15 @@ def check_static_imports(update: bool) -
     with tempfile.TemporaryDirectory() as tmpdir:
         filepath = Path(tmpdir) / "__init__.py"
         filepath.write_text(
-            reordered_content_before_static_checks + IF_TYPE_CHECKING_LINE + "\n".join(static_imports) + "\n"
+            reordered_content_before_static_checks
+            + IF_TYPE_CHECKING_LINE
+            + "\n".join(static_imports)
+            + "\n"
         )
         ruff_bin = find_ruff_bin()
-        os.spawnv(os.P_WAIT, ruff_bin, ["ruff", "check", str(filepath), "--fix", "--quiet"])
+        os.spawnv(
+            os.P_WAIT, ruff_bin, ["ruff", "check", str(filepath), "--fix", "--quiet"]
+        )
         os.spawnv(os.P_WAIT, ruff_bin, ["ruff", "format", str(filepath), "--quiet"])
         expected_init_content = filepath.read_text()
 
--- python3-huggingface-hub-0.29.3.orig/utils/check_task_parameters.py
+++ python3-huggingface-hub-0.29.3/utils/check_task_parameters.py
@@ -113,13 +113,21 @@ class DataclassFieldCollector(cst.CSTVis
                 if isinstance(field, cst.SimpleStatementLine):
                     for stmt in field.body:
                         # Check if it's an annotated assignment (typical for dataclass fields)
-                        if isinstance(stmt, cst.AnnAssign) and isinstance(stmt.target, cst.Name):
+                        if isinstance(stmt, cst.AnnAssign) and isinstance(
+                            stmt.target, cst.Name
+                        ):
                             param_name = stmt.target.value
-                            param_type = cst.Module([]).code_for_node(stmt.annotation.annotation)
+                            param_type = cst.Module([]).code_for_node(
+                                stmt.annotation.annotation
+                            )
                             docstring = self._extract_docstring(body_statements, index)
                             # Check if there's a default value
                             has_default = stmt.value is not None
-                            default_value = cst.Module([]).code_for_node(stmt.value) if has_default else None
+                            default_value = (
+                                cst.Module([]).code_for_node(stmt.value)
+                                if has_default
+                                else None
+                            )
 
                             self.parameters[param_name] = {
                                 "type": param_type,
@@ -140,7 +148,9 @@ class DataclassFieldCollector(cst.CSTVis
             if isinstance(next_stmt, cst.SimpleStatementLine):
                 for stmt in next_stmt.body:
                     # Check if the statement is a string expression (potential docstring)
-                    if isinstance(stmt, cst.Expr) and isinstance(stmt.value, cst.SimpleString):
+                    if isinstance(stmt, cst.Expr) and isinstance(
+                        stmt.value, cst.SimpleString
+                    ):
                         return stmt.value.evaluated_value.strip()
         # No docstring found or there's no statement after the field
         return ""
@@ -181,8 +191,15 @@ class MethodArgumentsCollector(cst.CSTVi
         for param in node.params.params + node.params.kwonly_params:
             if param.name.value == "self" or param.name.value in CORE_PARAMETERS:
                 continue
-            param_type = cst.Module([]).code_for_node(param.annotation.annotation) if param.annotation else "Any"
-            self.parameters[param.name.value] = {"type": param_type, "docstring": param_docs.get(param.name.value, "")}
+            param_type = (
+                cst.Module([]).code_for_node(param.annotation.annotation)
+                if param.annotation
+                else "Any"
+            )
+            self.parameters[param.name.value] = {
+                "type": param_type,
+                "docstring": param_docs.get(param.name.value, ""),
+            }
 
     def _extract_docstring(self, node: cst.FunctionDef) -> str:
         """Extract docstring from function node."""
@@ -200,7 +217,9 @@ class MethodArgumentsCollector(cst.CSTVi
         lines = docstring.split("\n")
 
         # Find Args section
-        args_idx = next((i for i, line in enumerate(lines) if line.strip().lower() == "args:"), None)
+        args_idx = next(
+            (i for i, line in enumerate(lines) if line.strip().lower() == "args:"), None
+        )
         if args_idx is None:
             return param_docs
         # Parse parameter descriptions
@@ -208,7 +227,12 @@ class MethodArgumentsCollector(cst.CSTVi
         current_desc = []
         for line in lines[args_idx + 1 :]:
             stripped_line = line.strip()
-            if not stripped_line or stripped_line.lower() in ("returns:", "raises:", "example:", "examples:"):
+            if not stripped_line or stripped_line.lower() in (
+                "returns:",
+                "raises:",
+                "example:",
+                "examples:",
+            ):
                 break
 
             if stripped_line.endswith(":"):  # Parameter line
@@ -297,7 +321,9 @@ class UpdateParameters(cst.CSTTransforme
             if param_name in self.param_updates:
                 # Update the type annotation for the parameter
                 new_annotation = cst.Annotation(
-                    annotation=cst.parse_expression(self.param_updates[param_name]["type"])
+                    annotation=cst.parse_expression(
+                        self.param_updates[param_name]["type"]
+                    )
                 )
                 new_kwonly_params.append(param.with_changes(annotation=new_annotation))
             else:
@@ -307,7 +333,9 @@ class UpdateParameters(cst.CSTTransforme
         for param_name, param_info in self.param_updates.items():
             if param_name not in existing_params:
                 # Create a new parameter with the provided type and a default value of None
-                annotation = cst.Annotation(annotation=cst.parse_expression(param_info["type"]))
+                annotation = cst.Annotation(
+                    annotation=cst.parse_expression(param_info["type"])
+                )
                 new_param = cst.Param(
                     name=cst.Name(param_name),
                     annotation=annotation,
@@ -334,36 +362,54 @@ class UpdateParameters(cst.CSTTransforme
         updated_docstring = self._update_docstring_content(docstring)
         new_docstring = cst.SimpleString(f'"""{updated_docstring}"""')
         # Replace the old docstring with the updated one
-        new_body = [body.body[0].with_changes(body=[docstring_expr.with_changes(value=new_docstring)])] + list(
-            body.body[1:]
-        )
+        new_body = [
+            body.body[0].with_changes(
+                body=[docstring_expr.with_changes(value=new_docstring)]
+            )
+        ] + list(body.body[1:])
         # Return the updated function body
         return body.with_changes(body=new_body)
 
     def _update_docstring_content(self, docstring: str) -> str:
         """Update parameter descriptions in the docstring content."""
         # Split parameters into new and updated ones based on their status
-        new_params = {name: info for name, info in self.param_updates.items() if info["status"] == "new"}
+        new_params = {
+            name: info
+            for name, info in self.param_updates.items()
+            if info["status"] == "new"
+        }
         update_params = {
-            name: info for name, info in self.param_updates.items() if info["status"] in ("update_type", "update_doc")
+            name: info
+            for name, info in self.param_updates.items()
+            if info["status"] in ("update_type", "update_doc")
         }
         # Split the docstring into lines for processing
         docstring_lines = docstring.split("\n")
         # Find or create the "Args:" section and compute indentation levels
-        args_index = next((i for i, line in enumerate(docstring_lines) if line.strip().lower() == "args:"), None)
+        args_index = next(
+            (
+                i
+                for i, line in enumerate(docstring_lines)
+                if line.strip().lower() == "args:"
+            ),
+            None,
+        )
         if args_index is None:
             # If 'Args:' section is not found, insert it before 'Returns:' or at the end
             insertion_index = next(
                 (
                     i
                     for i, line in enumerate(docstring_lines)
-                    if line.strip().lower() in ("returns:", "raises:", "examples:", "example:")
+                    if line.strip().lower()
+                    in ("returns:", "raises:", "examples:", "example:")
                 ),
                 len(docstring_lines),
             )
             docstring_lines.insert(insertion_index, "Args:")
             args_index = insertion_index  # Update the args_index with the new section
-        base_indent = docstring_lines[args_index][: -len(docstring_lines[args_index].lstrip())]
+        base_indent = docstring_lines[args_index][
+            : -len(docstring_lines[args_index].lstrip())
+        ]
         param_indent = base_indent + "    "  # Indentation for parameter lines
         desc_indent = param_indent + "    "  # Indentation for description lines
         # Update existing parameters in the docstring
@@ -373,7 +419,9 @@ class UpdateParameters(cst.CSTTransforme
             )
         # Add new parameters to the docstring
         if new_params:
-            docstring_lines = self._add_new_params(docstring_lines, new_params, args_index, param_indent, desc_indent)
+            docstring_lines = self._add_new_params(
+                docstring_lines, new_params, args_index, param_indent, desc_indent
+            )
         # Join the docstring lines back into a single string
         return "\n".join(docstring_lines)
 
@@ -388,7 +436,9 @@ class UpdateParameters(cst.CSTTransforme
         # Extract and format the parameter type
         param_type = param_info["type"]
         if param_type.startswith("Optional["):
-            param_type = param_type[len("Optional[") : -1]  # Remove Optional[ and closing ]
+            param_type = param_type[
+                len("Optional[") : -1
+            ]  # Remove Optional[ and closing ]
             optional_str = ", *optional*"
         else:
             optional_str = ""
@@ -429,7 +479,12 @@ class UpdateParameters(cst.CSTTransforme
                 # Skip empty lines
                 i += 1
                 continue
-            if stripped_line.lower() in ("returns:", "raises:", "example:", "examples:"):
+            if stripped_line.lower() in (
+                "returns:",
+                "raises:",
+                "example:",
+                "examples:",
+            ):
                 # Stop processing if another section starts
                 break
             if stripped_line.endswith(":"):
@@ -440,7 +495,9 @@ class UpdateParameters(cst.CSTTransforme
                     # Get the updated parameter info
                     param_info = params_to_update[param_name]
                     # Format the new parameter docstring
-                    param_doc_lines = self._format_param_docstring(param_name, param_info, param_indent, desc_indent)
+                    param_doc_lines = self._format_param_docstring(
+                        param_name, param_info, param_indent, desc_indent
+                    )
                     # Find the end of the current parameter's description
                     start_idx = i
                     end_idx = i + 1
@@ -448,8 +505,12 @@ class UpdateParameters(cst.CSTTransforme
                         next_line = docstring_lines[end_idx]
                         # Next parameter or section starts or another section starts or empty line
                         if (
-                            (next_line.strip().endswith(":") and not next_line.startswith(desc_indent))
-                            or next_line.lower() in ("returns:", "raises:", "example:", "examples:")
+                            (
+                                next_line.strip().endswith(":")
+                                and not next_line.startswith(desc_indent)
+                            )
+                            or next_line.lower()
+                            in ("returns:", "raises:", "example:", "examples:")
                             or not next_line
                         ):
                             break
@@ -460,7 +521,9 @@ class UpdateParameters(cst.CSTTransforme
                         + param_doc_lines  # Insert new parameter docs
                         + docstring_lines[end_idx:]  # Keep everything after
                     )
-                    i = start_idx + len(param_doc_lines)  # Update index to after inserted lines
+                    i = start_idx + len(
+                        param_doc_lines
+                    )  # Update index to after inserted lines
                 i += 1
             else:
                 i += 1  # Move to the next line if not a parameter line
@@ -487,7 +550,12 @@ class UpdateParameters(cst.CSTTransforme
                     empty_line_index = insertion_index
                 insertion_index += 1
                 continue
-            if stripped_line.lower() in ("returns:", "raises:", "example:", "examples:"):
+            if stripped_line.lower() in (
+                "returns:",
+                "raises:",
+                "example:",
+                "examples:",
+            ):
                 break
             empty_line_index = None  # Reset if we find more content
             if stripped_line.endswith(":") and not line.startswith(desc_indent.strip()):
@@ -501,7 +569,9 @@ class UpdateParameters(cst.CSTTransforme
         # Prepare the new parameter documentation lines
         param_docs = []
         for param_name, param_info in new_params.items():
-            param_doc_lines = self._format_param_docstring(param_name, param_info, param_indent, desc_indent)
+            param_doc_lines = self._format_param_docstring(
+                param_name, param_info, param_indent, desc_indent
+            )
             param_docs.extend(param_doc_lines)
         # Insert the new parameters into the docstring
         docstring_lines[insertion_index:insertion_index] = param_docs
@@ -618,7 +688,10 @@ def _get_imports_to_add(
         types_to_modules = module_collector.type_to_module
         module = types_to_modules.get(type_name, DEFAULT_MODULE)
         # Maybe no need to check that since the code formatter will handle duplicate imports?
-        if module not in gather_visitor.object_mapping or type_name not in gather_visitor.object_mapping[module]:
+        if (
+            module not in gather_visitor.object_mapping
+            or type_name not in gather_visitor.object_mapping[module]
+        ):
             needed_imports.setdefault(module, []).append(type_name)
     return needed_imports
 
@@ -664,7 +737,9 @@ def _collect_type_hints_from_annotation(
     type_string = annotation_str.replace(" ", "")
     builtin_types = {d for d in dir(builtins) if isinstance(getattr(builtins, d), type)}
     types = re.findall(r"\w+|'[^']+'|\"[^\"]+\"", type_string)
-    extracted_types = {t.strip("\"'") for t in types if t.strip("\"'") not in builtin_types}
+    extracted_types = {
+        t.strip("\"'") for t in types if t.strip("\"'") not in builtin_types
+    }
     return extracted_types
 
 
@@ -723,7 +798,9 @@ def _check_and_update_parameters(
 
         if update:
             ## Get missing imports to add
-            needed_imports = _get_imports_to_add(updates, parameters_module, modified_module)
+            needed_imports = _get_imports_to_add(
+                updates, parameters_module, modified_module
+            )
             for module, imports_to_add in needed_imports.items():
                 merged_imports[module].update(imports_to_add)
             modified_module = _update_parameters(modified_module, method_name, updates)
@@ -786,7 +863,9 @@ def update_inference_client(update: bool
 
     # Construct a mapping between method names and their parameters dataclass names
     method_params = {}
-    for method_name, _ in inspect.getmembers(InferenceClient, predicate=inspect.isfunction):
+    for method_name, _ in inspect.getmembers(
+        InferenceClient, predicate=inspect.isfunction
+    ):
         if method_name.startswith("_") or method_name not in tasks:
             continue
         parameter_type_name = _get_parameter_type_name(method_name)
@@ -800,7 +879,9 @@ if __name__ == "__main__":
     parser.add_argument(
         "--update",
         action="store_true",
-        help=("Whether to update `./src/huggingface_hub/inference/_client.py` if parameters are missing."),
+        help=(
+            "Whether to update `./src/huggingface_hub/inference/_client.py` if parameters are missing."
+        ),
     )
     args = parser.parse_args()
     update_inference_client(update=args.update)
--- python3-huggingface-hub-0.29.3.orig/utils/generate_async_inference_client.py
+++ python3-huggingface-hub-0.29.3/utils/generate_async_inference_client.py
@@ -23,9 +23,16 @@ from helpers import format_source_code
 
 
 ASYNC_CLIENT_FILE_PATH = (
-    Path(__file__).parents[1] / "src" / "huggingface_hub" / "inference" / "_generated" / "_async_client.py"
+    Path(__file__).parents[1]
+    / "src"
+    / "huggingface_hub"
+    / "inference"
+    / "_generated"
+    / "_async_client.py"
+)
+SYNC_CLIENT_FILE_PATH = (
+    Path(__file__).parents[1] / "src" / "huggingface_hub" / "inference" / "_client.py"
 )
-SYNC_CLIENT_FILE_PATH = Path(__file__).parents[1] / "src" / "huggingface_hub" / "inference" / "_client.py"
 
 
 def generate_async_client_code(code: str) -> str:
@@ -385,7 +392,9 @@ def _update_examples_in_public_methods(c
         code_block = match.group(1)
 
         # Update code block in example
-        updated_match = full_match.replace(code_block, _update_example_code_block(code_block))
+        updated_match = full_match.replace(
+            code_block, _update_example_code_block(code_block)
+        )
 
         # Update example in full script
         code = code.replace(full_match, updated_match)
@@ -398,7 +407,9 @@ def _use_async_streaming_util(code: str)
         "_stream_text_generation_response",
         "_async_stream_text_generation_response",
     )
-    code = code.replace("_stream_chat_completion_response", "_async_stream_chat_completion_response")
+    code = code.replace(
+        "_stream_chat_completion_response", "_async_stream_chat_completion_response"
+    )
     return code
 
 
@@ -466,8 +477,14 @@ def _adapt_info_and_health_endpoints(cod
 
 def _add_get_client_session(code: str) -> str:
     # Add trust_env as parameter
-    code = _add_before(code, "proxies: Optional[Any] = None,", "trust_env: bool = False,")
-    code = _add_before(code, "\n        self.proxies = proxies\n", "\n        self.trust_env = trust_env")
+    code = _add_before(
+        code, "proxies: Optional[Any] = None,", "trust_env: bool = False,"
+    )
+    code = _add_before(
+        code,
+        "\n        self.proxies = proxies\n",
+        "\n        self.trust_env = trust_env",
+    )
 
     # Document `trust_env` parameter
     code = _add_before(
--- python3-huggingface-hub-0.29.3.orig/utils/generate_inference_types.py
+++ python3-huggingface-hub-0.29.3/utils/generate_inference_types.py
@@ -24,13 +24,25 @@ from helpers import check_and_update_fil
 
 
 huggingface_hub_folder_path = Path(__file__).parents[1] / "src" / "huggingface_hub"
-INFERENCE_TYPES_FOLDER_PATH = huggingface_hub_folder_path / "inference" / "_generated" / "types"
+INFERENCE_TYPES_FOLDER_PATH = (
+    huggingface_hub_folder_path / "inference" / "_generated" / "types"
+)
 MAIN_INIT_PY_FILE = huggingface_hub_folder_path / "__init__.py"
 REFERENCE_PACKAGE_EN_PATH = (
-    Path(__file__).parents[1] / "docs" / "source" / "en" / "package_reference" / "inference_types.md"
+    Path(__file__).parents[1]
+    / "docs"
+    / "source"
+    / "en"
+    / "package_reference"
+    / "inference_types.md"
 )
 REFERENCE_PACKAGE_KO_PATH = (
-    Path(__file__).parents[1] / "docs" / "source" / "ko" / "package_reference" / "inference_types.md"
+    Path(__file__).parents[1]
+    / "docs"
+    / "source"
+    / "ko"
+    / "package_reference"
+    / "inference_types.md"
 )
 
 IGNORE_FILES = [
@@ -166,7 +178,9 @@ def _inherit_from_base(content: str) ->
         "\nfrom dataclasses import",
         "\nfrom .base import BaseInferenceType, dataclass_with_extra\nfrom dataclasses import",
     )
-    content = BASE_DATACLASS_REGEX.sub(r"@dataclass_with_extra\nclass \1(BaseInferenceType):\n", content)
+    content = BASE_DATACLASS_REGEX.sub(
+        r"@dataclass_with_extra\nclass \1(BaseInferenceType):\n", content
+    )
     return content
 
 
@@ -245,7 +259,9 @@ class DeprecatedRemover(cst.CSTTransform
                 return expr.value.evaluated_value
         return None
 
-    def leave_ClassDef(self, original_node: cst.ClassDef, updated_node: cst.ClassDef) -> Optional[cst.ClassDef]:
+    def leave_ClassDef(
+        self, original_node: cst.ClassDef, updated_node: cst.ClassDef
+    ) -> Optional[cst.ClassDef]:
         """Handle class definitions - remove if deprecated."""
         docstring = self.get_docstring(original_node.body.body)
         if self.is_deprecated(docstring):
@@ -258,7 +274,9 @@ class DeprecatedRemover(cst.CSTTransform
             stmt = statements[i]
 
             # Check if this is a field (AnnAssign)
-            if isinstance(stmt, cst.SimpleStatementLine) and isinstance(stmt.body[0], cst.AnnAssign):
+            if isinstance(stmt, cst.SimpleStatementLine) and isinstance(
+                stmt.body[0], cst.AnnAssign
+            ):
                 # Look ahead for docstring
                 next_docstring = None
                 if i + 1 < len(statements):
@@ -274,7 +292,9 @@ class DeprecatedRemover(cst.CSTTransform
         if not new_body:
             return cst.RemoveFromParent()
 
-        return updated_node.with_changes(body=updated_node.body.with_changes(body=new_body))
+        return updated_node.with_changes(
+            body=updated_node.body.with_changes(body=new_body)
+        )
 
 
 def _clean_deprecated_fields(content: str) -> str:
@@ -299,35 +319,52 @@ def create_init_py(dataclasses: Dict[str
     content = INIT_PY_HEADER
     content += "\n"
     content += "\n".join(
-        [f"from .{module} import {', '.join(dataclasses_list)}" for module, dataclasses_list in dataclasses.items()]
+        [
+            f"from .{module} import {', '.join(dataclasses_list)}"
+            for module, dataclasses_list in dataclasses.items()
+        ]
     )
     return content
 
 
 def add_dataclasses_to_main_init(content: str, dataclasses: Dict[str, List[str]]):
-    dataclasses_list = sorted({cls for classes in dataclasses.values() for cls in classes})
+    dataclasses_list = sorted(
+        {cls for classes in dataclasses.values() for cls in classes}
+    )
     dataclasses_str = ", ".join(f"'{cls}'" for cls in dataclasses_list)
 
-    return MAIN_INIT_PY_REGEX.sub(f'"inference._generated.types": [{dataclasses_str}]', content)
+    return MAIN_INIT_PY_REGEX.sub(
+        f'"inference._generated.types": [{dataclasses_str}]', content
+    )
 
 
-def generate_reference_package(dataclasses: Dict[str, List[str]], language: Literal["en", "ko"]) -> str:
+def generate_reference_package(
+    dataclasses: Dict[str, List[str]], language: Literal["en", "ko"]
+) -> str:
     """Generate the reference package content."""
 
     per_task_docs = []
     for task in sorted(dataclasses.keys()):
-        lines = [f"[[autodoc]] huggingface_hub.{cls}" for cls in sorted(dataclasses[task])]
+        lines = [
+            f"[[autodoc]] huggingface_hub.{cls}" for cls in sorted(dataclasses[task])
+        ]
         lines_str = "\n\n".join(lines)
         if language == "en":
             # e.g. '## audio_classification'
             per_task_docs.append(f"\n## {task}\n\n{lines_str}\n\n")
         elif language == "ko":
             # e.g. '## audio_classification[[huggingface_hub.AudioClassificationInput]]'
-            per_task_docs.append(f"\n## {task}[[huggingface_hub.{sorted(dataclasses[task])[0]}]]\n\n{lines_str}\n\n")
+            per_task_docs.append(
+                f"\n## {task}[[huggingface_hub.{sorted(dataclasses[task])[0]}]]\n\n{lines_str}\n\n"
+            )
         else:
             raise ValueError(f"Language {language} is not supported.")
 
-    template = REFERENCE_PACKAGE_EN_CONTENT if language == "en" else REFERENCE_PACKAGE_KO_CONTENT
+    template = (
+        REFERENCE_PACKAGE_EN_CONTENT
+        if language == "en"
+        else REFERENCE_PACKAGE_KO_CONTENT
+    )
     return template.format(types="\n".join(per_task_docs))
 
 
@@ -349,21 +386,31 @@ def check_inference_types(update: bool)
         aliases[file.stem] = _list_type_aliases(formatted_content)
         check_and_update_file_content(file, formatted_content, update)
 
-    all_classes = {module: dataclasses[module] + aliases[module] for module in dataclasses.keys()}
+    all_classes = {
+        module: dataclasses[module] + aliases[module] for module in dataclasses.keys()
+    }
     init_py_content = create_init_py(all_classes)
     init_py_content = format_source_code(init_py_content)
     init_py_file = INFERENCE_TYPES_FOLDER_PATH / "__init__.py"
     check_and_update_file_content(init_py_file, init_py_content, update)
 
     main_init_py_content = MAIN_INIT_PY_FILE.read_text()
-    updated_main_init_py_content = add_dataclasses_to_main_init(main_init_py_content, all_classes)
+    updated_main_init_py_content = add_dataclasses_to_main_init(
+        main_init_py_content, all_classes
+    )
     updated_main_init_py_content = format_source_code(updated_main_init_py_content)
-    check_and_update_file_content(MAIN_INIT_PY_FILE, updated_main_init_py_content, update)
+    check_and_update_file_content(
+        MAIN_INIT_PY_FILE, updated_main_init_py_content, update
+    )
     reference_package_content_en = generate_reference_package(dataclasses, "en")
-    check_and_update_file_content(REFERENCE_PACKAGE_EN_PATH, reference_package_content_en, update)
+    check_and_update_file_content(
+        REFERENCE_PACKAGE_EN_PATH, reference_package_content_en, update
+    )
 
     reference_package_content_ko = generate_reference_package(dataclasses, "ko")
-    check_and_update_file_content(REFERENCE_PACKAGE_KO_PATH, reference_package_content_ko, update)
+    check_and_update_file_content(
+        REFERENCE_PACKAGE_KO_PATH, reference_package_content_ko, update
+    )
 
     print("✅ All good! (inference types)")
     exit(0)
--- python3-huggingface-hub-0.29.3.orig/utils/helpers.py
+++ python3-huggingface-hub-0.29.3/utils/helpers.py
@@ -28,7 +28,9 @@ def check_and_update_file_content(file:
     if content != expected_content:
         if update:
             file.write_text(expected_content)
-            print(f"  {file} has been updated. Please make sure the changes are accurate and commit them.")
+            print(
+                f"  {file} has been updated. Please make sure the changes are accurate and commit them."
+            )
         else:
             print(f"❌ Expected content mismatch in {file}.")
             exit(1)
@@ -43,7 +45,9 @@ def format_source_code(code: str) -> str
         if not ruff_bin:
             raise FileNotFoundError("Ruff executable not found.")
         try:
-            subprocess.run([ruff_bin, "check", str(filepath), "--fix", "--quiet"], check=True)
+            subprocess.run(
+                [ruff_bin, "check", str(filepath), "--fix", "--quiet"], check=True
+            )
             subprocess.run([ruff_bin, "format", str(filepath), "--quiet"], check=True)
         except subprocess.CalledProcessError as e:
             raise RuntimeError(f"Error running Ruff: {e}")
--- python3-huggingface-hub-0.29.3.orig/utils/push_repocard_examples.py
+++ python3-huggingface-hub-0.29.3/utils/push_repocard_examples.py
@@ -36,7 +36,9 @@ def check_can_push():
         print("You must be logged in to push repo card examples.")
 
     if all(org["name"] != ORG_NAME for org in me.get("orgs", [])):
-        print(f"❌ You must have access to organization '{ORG_NAME}' to push repo card examples.")
+        print(
+            f"❌ You must have access to organization '{ORG_NAME}' to push repo card examples."
+        )
         exit(1)
 
 
@@ -57,7 +59,9 @@ def push_model_card_example(overwrite: b
         ),
     )
     if not overwrite:
-        existing_content = Path(hf_hub_download(MODEL_CARD_REPO_ID, REPOCARD_NAME, repo_type="model")).read_text()
+        existing_content = Path(
+            hf_hub_download(MODEL_CARD_REPO_ID, REPOCARD_NAME, repo_type="model")
+        ).read_text()
         if content == existing_content:
             print("Model Card not pushed: did not change.")
             return
@@ -87,7 +91,9 @@ def push_dataset_card_example(overwrite:
         ),
     )
     if not overwrite:
-        existing_content = Path(hf_hub_download(DATASET_CARD_REPO_ID, REPOCARD_NAME, repo_type="dataset")).read_text()
+        existing_content = Path(
+            hf_hub_download(DATASET_CARD_REPO_ID, REPOCARD_NAME, repo_type="dataset")
+        ).read_text()
         if content == existing_content:
             print("Dataset Card not pushed: did not change.")
             return
